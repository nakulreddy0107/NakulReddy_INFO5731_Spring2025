paperId,url,title,abstract,year,authors
f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32,Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms,"We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL",2017.0,"Han Xiao, Kashif Rasul, Roland Vollgraf"
4954fa180728932959997a4768411ff9136aac81,https://www.semanticscholar.org/paper/4954fa180728932959997a4768411ff9136aac81,TensorFlow: A system for large-scale machine learning,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous ""parameter server"" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.",2016.0,"Martín Abadi, P. Barham, Jianmin Chen, Z. Chen, Andy Davis, J. Dean, M. Devin, Sanjay Ghemawat, G. Irving, M. Isard, M. Kudlur, J. Levenberg, R. Monga, Sherry Moore, D. Murray, Benoit Steiner, P. Tucker, Vijay Vasudevan, Pete Warden, M. Wicke, Yuan Yu, Xiaoqiang Zhang"
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",2016.0,"Martín Abadi, Ashish Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. Corrado, Andy Davis, J. Dean, M. Devin, Sanjay Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Yangqing Jia, R. Józefowicz, Lukasz Kaiser, M. Kudlur, J. Levenberg, Dandelion Mané, R. Monga, Sherry Moore, D. Murray, C. Olah, M. Schuster, Jonathon Shlens, Benoit Steiner, I. Sutskever, Kunal Talwar, P. Tucker, Vincent Vanhoucke, Vijay Vasudevan, F. Viégas, O. Vinyals, Pete Warden, M. Wattenberg, M. Wicke, Yuan Yu, Xiaoqiang Zheng"
f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7,Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting,"The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.",2015.0,"Xingjian Shi, Zhourong Chen, Hao Wang, D. Yeung, W. Wong, W. Woo"
597bd2e45427563cdf025e53a3239006aa364cfc,https://www.semanticscholar.org/paper/597bd2e45427563cdf025e53a3239006aa364cfc,Open Graph Benchmark: Datasets for Machine Learning on Graphs,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .",2020.0,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec"
0090023afc66cd2741568599057f4e82b566137c,https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c,A Survey on Bias and Fairness in Machine Learning,"With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.",2019.0,"Ninareh Mehrabi, Fred Morstatter, N. Saxena, Kristina Lerman, A. Galstyan"
f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d,https://www.semanticscholar.org/paper/f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d,Membership Inference Attacks Against Machine Learning Models,"We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial ""machine learning as a service"" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.",2016.0,"R. Shokri, M. Stronati, Congzheng Song, Vitaly Shmatikov"
168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74,https://www.semanticscholar.org/paper/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74,Scikit-learn: Machine Learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",2011.0,"Fabian Pedregosa, G. Varoquaux, Alexandre Gramfort, V. Michel, B. Thirion, O. Grisel, Mathieu Blondel, Gilles Louppe, P. Prettenhofer, Ron Weiss, Ron J. Weiss, J. Vanderplas, Alexandre Passos, D. Cournapeau, M. Brucher, M. Perrot, E. Duchesnay"
d422df8bff4e677a3077635db116679d25142bfc,https://www.semanticscholar.org/paper/d422df8bff4e677a3077635db116679d25142bfc,"Machine learning: Trends, perspectives, and prospects","Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",2015.0,"Michael I. Jordan, T. Mitchell"
5c39e37022661f81f79e481240ed9b175dec6513,https://www.semanticscholar.org/paper/5c39e37022661f81f79e481240ed9b175dec6513,Towards A Rigorous Science of Interpretable Machine Learning,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.",2017.0,"F. Doshi-Velez, Been Kim"
807c1f19047f96083e13614f7ce20f2ac98c239a,https://www.semanticscholar.org/paper/807c1f19047f96083e13614f7ce20f2ac98c239a,C4.5: Programs for Machine Learning,"From the Publisher: 
Classifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. 
 
C4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. 
 
This book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.",1992.0,J. R. Quinlan
360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5,https://www.semanticscholar.org/paper/360ca02e6f5a5e1af3dce4866a257aafc2d6d6f5,Machine learning - a probabilistic perspective,"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher. Machine learning : a probabilistic perspective / Kevin P. Murphy. p. cm. — (Adaptive computation and machine learning series) Includes bibliographical references and index. Contents Preface xxvii 1 Introduction 1 1.1 Machine learning: what and why? 1 1.1.1 Types of machine learning 2 1.2 Supervised learning 3 1.2.1 Classification 3 1.2.2 Regression 8 1.3 Unsupervised learning 9 1.3.1 Discovering clusters 10 1.3.2 Discovering latent factors 11 1.3.3 Discovering graph structure 13 1.3.4 Matrix completion 14 1.4 Some basic concepts in machine learning 16 1.4.1 Parametric vs non-parametric models 16 1.4.2 A simple non-parametric classifier: K-nearest neighbors 16 1.4.3 The curse of dimensionality 18 1.4.4 Parametric models for classification and regression 19 1.4.5",2012.0,Kevin P. Murphy
2e62d1345b340d5fda3b092c460264b9543bc4b5,https://www.semanticscholar.org/paper/2e62d1345b340d5fda3b092c460264b9543bc4b5,Genetic Algorithms in Search Optimization and Machine Learning,"From the Publisher: 
This book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. 
 
Major concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.",1988.0,D. Goldberg
668b1277fbece28c4841eeab1c97e4ebd0079700,https://www.semanticscholar.org/paper/668b1277fbece28c4841eeab1c97e4ebd0079700,Pattern Recognition and Machine Learning,Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.,2006.0,Radford M. Neal
b0c34618ffd1154f35863e2ce7250ac6b6f2c424,https://www.semanticscholar.org/paper/b0c34618ffd1154f35863e2ce7250ac6b6f2c424,Interpretable Machine Learning,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.",2019.0,"Bradley C. Boehmke, Brandon M. Greenwell"
53b047e503f4c24602f376a774d653f7ed56c024,https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024,Practical Black-Box Attacks against Machine Learning,"Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.",2016.0,"Nicolas Papernot, P. Mcdaniel, I. Goodfellow, S. Jha, Z. B. Celik, A. Swami"
2e2089ae76fe914706e6fa90081a79c8fe01611e,https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e,Practical Bayesian Optimization of Machine Learning Algorithms,"The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ""black art"" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",2012.0,"Jasper Snoek, H. Larochelle, Ryan P. Adams"
56e8863838b4dcc4790108cd1e7e680a104a7c30,https://www.semanticscholar.org/paper/56e8863838b4dcc4790108cd1e7e680a104a7c30,Machine Learning Algorithms: A Review,.,2022.0,Ayon Dey
12d0353ce8b41b7e5409e5a4a611110aee33c7bc,https://www.semanticscholar.org/paper/12d0353ce8b41b7e5409e5a4a611110aee33c7bc,Thumbs up? Sentiment Classification using Machine Learning Techniques,"We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.",2002.0,"B. Pang, Lillian Lee, Shivakumar Vaithyanathan"
46f74231b9afeb0c290d6d550043c55045284e5f,https://www.semanticscholar.org/paper/46f74231b9afeb0c290d6d550043c55045284e5f,The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web],"In this issue, “Best of the Web” presents the modified National Institute of Standards and Technology (MNIST) resources, consisting of a collection of handwritten digit images used extensively in optical character recognition and machine learning research.",2012.0,L. Deng
7feb0fc888cd55360949554db032d7d1cba9e947,https://www.semanticscholar.org/paper/7feb0fc888cd55360949554db032d7d1cba9e947,Programs for Machine Learning,"Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.",1994.0,"S. Salzberg, Alberto Maria Segre"
2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c,https://www.semanticscholar.org/paper/2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c,"Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems","Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details",2017.0,Aurélien Géron
6b20af22b0734757d9ead382b201a65f9dd637cc,https://www.semanticscholar.org/paper/6b20af22b0734757d9ead382b201a65f9dd637cc,Machine learning in automated text categorization,"The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.",2001.0,F. Sebastiani
e2a85a6766b982ff7c8980e57ca6342d22493827,https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827,Adversarial Machine Learning at Scale,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.",2016.0,"Alexey Kurakin, I. Goodfellow, Samy Bengio"
36eb6fea39ce06e2807f074fa3d5e79ed0f2bcef,https://www.semanticscholar.org/paper/36eb6fea39ce06e2807f074fa3d5e79ed0f2bcef,Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning,"Most tasks require a person or an automated system to reasonto reach conclusions based on available information. The framework of probabilistic graphical models, presented in this book, provides a general approach for this task. The approach is model-based, allowing interpretable models to be constructed and then manipulated by reasoning algorithms. These models can also be learned automatically from data, allowing the approach to be used in cases where manually constructing a model is difficult or even impossible. Because uncertainty is an inescapable aspect of most real-world applications, the book focuses on probabilistic models, which make the uncertainty explicit and provide models that are more faithful to reality. Probabilistic Graphical Models discusses a variety of models, spanning Bayesian networks, undirected Markov networks, discrete and continuous models, and extensions to deal with dynamical systems and relational data. For each class of models, the text describes the three fundamental cornerstones: representation, inference, and learning, presenting both basic concepts and advanced techniques. Finally, the book considers the use of the proposed framework for causal reasoning and decision making under uncertainty. The main text in each chapter provides the detailed technical development of the key ideas. Most chapters also include boxes with additional material: skill boxes, which describe techniques; case study boxes, which discuss empirical cases related to the approach described in the text, including applications in computer vision, robotics, natural language understanding, and computational biology; and concept boxes, which present significant concepts drawn from the material in the chapter. Instructors (and readers) can group chapters in various combinations, from core topics to more technically advanced material, to suit their particular needs. Adaptive Computation and Machine Learning series",2009.0,"D. Koller, N. Friedman"
d21703674ae562bae4a849a75847cdd9ead417df,https://www.semanticscholar.org/paper/d21703674ae562bae4a849a75847cdd9ead417df,Optimization Methods for Large-Scale Machine Learning,"This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.",2016.0,"L. Bottou, Frank E. Curtis, J. Nocedal"
0b544dfe355a5070b60986319a3f51fb45d1348e,https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e,Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation,"In this paper, we propose a novel neural network model called RNN Encoder‐ Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder‐Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.",2014.0,"Kyunghyun Cho, B. V. Merrienboer, Çaglar Gülçehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, Yoshua Bengio"
fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5,Neural Machine Translation by Jointly Learning to Align and Translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.",2014.0,"Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio"
f156ecbbb9243522275490d698c6825f4d2e01af,https://www.semanticscholar.org/paper/f156ecbbb9243522275490d698c6825f4d2e01af,Explainable AI: A Review of Machine Learning Interpretability Methods,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",2020.0,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis"
f75b70c9d7078724b592ec3e21de705e7b6ff73f,https://www.semanticscholar.org/paper/f75b70c9d7078724b592ec3e21de705e7b6ff73f,Double/Debiased Machine Learning for Treatment and Structural Parameters,"We revisit the classic semiparametric problem of inference on a low dimensional parameter θ_0 in the presence of high-dimensional nuisance parameters η_0. We depart from the classical setting by allowing for η_0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate η_0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η_0 cause a heavy bias in estimators of θ_0 that are obtained by naively plugging ML estimators of η_0 into estimating equations for θ_0. This bias results in the naive estimator failing to be N^(-1/2) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ_0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ_0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N^(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.",2017.0,"V. Chernozhukov, D. Chetverikov, Mert Demirer, E. Duflo, Christian Hansen, Whitney Newey, J. Robins"
9e27190f2d9b2167d4a66b88696def4585072fd5,https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5,SoilGrids250m: Global gridded soil information based on machine learning,"This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods—random forest and gradient boosting and/or multinomial logistic regression—as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10–fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.",2017.0,"T. Hengl, Jorge Mendes de Jesus, G. Heuvelink, Maria Ruiperez González, M. Kilibarda, Aleksandar Blagotić, Shangguan Wei, Marvin N. Wright, X. Geng, B. Bauer-Marschallinger, M. Guevara, R. Vargas, R. MacMillan, N. Batjes, J. Leenaars, E. Ribeiro, Ichsani Wheeler, S. Mantel, B. Kempen"
6a6ad9eb495739f4c80e7c09598720c3d5c5dff7,https://www.semanticscholar.org/paper/6a6ad9eb495739f4c80e7c09598720c3d5c5dff7,"Federated Learning: Collaborative Machine Learning without
Centralized Training Data","Federated learning (also known as collaborative learning) is a machine learning technique that trains
an algorithm without transferring data samples across numerous decentralized edge devices or
servers. This strategy differs from standard centralized machine learning techniques in which all local
datasets are uploaded to a single server, as well as more traditional decentralized alternatives, which
frequently presume that local data samples are uniformly distributed.
Federated learning allows several actors to collaborate on the development of a single, robust
machine learning model without sharing data, allowing crucial issues such as data privacy, data
security, data access rights, and access to heterogeneous data to be addressed. Defence,
telecommunications, internet of things, and pharmaceutical industries are just a few of the sectors
where it has applications.",2022.0,"Abhishek V A, Binny S, Johan T R, Nithin Raj, Vishal Thomas"
db0cc2f21b20cbc0ab8946090967399c25709614,https://www.semanticscholar.org/paper/db0cc2f21b20cbc0ab8946090967399c25709614,Practical Secure Aggregation for Privacy-Preserving Machine Learning,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.",2017.0,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. B. McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, Karn Seth"
6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91,Multimodal Machine Learning: A Survey and Taxonomy,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.",2017.0,"T. Baltrušaitis, Chaitanya Ahuja, Louis-Philippe Morency"
6bc43977fb11cceed0b9aa55b23c6dd29dd9a132,https://www.semanticscholar.org/paper/6bc43977fb11cceed0b9aa55b23c6dd29dd9a132,Correlation-based Feature Selection for Machine Learning,"A central problem in machine learning is identifying a representative set of features from which to construct a classification model for a particular task. This thesis addresses the problem of feature selection for machine learning through a correlation based approach. The central hypothesis is that good feature sets contain features that are highly correlated with the class, yet uncorrelated with each other. A feature evaluation formula, based on ideas from test theory, provides an operational definition of this hypothesis. CFS (Correlation based Feature Selection) is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuristic search strategy. CFS was evaluated by experiments on artificial and natural datasets. Three machine learning algorithms were used: C4.5 (a decision tree learner), IB1 (an instance based learner), and naive Bayes. Experiments on artificial datasets showed that CFS quickly identifies and screens irrelevant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. On natural domains, CFS typically eliminated well over half the features. In most cases, classification accuracy using the reduced feature set equaled or bettered accuracy using the complete feature set. Feature selection degraded machine learning performance in cases where some features were eliminated which were highly predictive of very small areas of the instance space. Further experiments compared CFS with a wrapper—a well known approach to feature selection that employs the target learning algorithm to evaluate feature sets. In many cases CFS gave comparable results to the wrapper, and in general, outperformed the wrapper on small datasets. CFS executes many times faster than the wrapper, which allows it to scale to larger datasets. Two methods of extending CFS to handle feature interaction are presented and experimentally evaluated. The first considers pairs of features and the second incorporates iii feature weights calculated by the RELIEF algorithm. Experiments on artificial domains showed that both methods were able to identify interacting features. On natural domains, the pairwise method gave more reliable results than using weights provided by RELIEF.",2003.0,M. Hall
7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,https://www.semanticscholar.org/paper/7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,CrypTen: Secure Multi-Party Computation Meets Machine Learning,"Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party's private model using another party's private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of flexible software frameworks that""speak the language""of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present CrypTen: a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of CrypTen and measure its performance on state-of-the-art models for text classification, speech recognition, and image classification. Our benchmarks show that CrypTen's GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efficient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using CrypTen can securely predict phonemes in speech recordings using Wav2Letter faster than real-time. We hope that CrypTen will spur adoption of secure MPC in the machine-learning community.",2021.0,"Brian Knott, Shobha Venkataraman, Awni Y. Hannun, Shubho Sengupta, Mark Ibrahim, L. Maaten"
2afa490dde7a8c582d889530c7f8b042fef6a8b7,https://www.semanticscholar.org/paper/2afa490dde7a8c582d889530c7f8b042fef6a8b7,Machine learning–accelerated computational fluid dynamics,"Significance Accurate simulation of fluids is important for many science and engineering problems but is very computationally demanding. In contrast, machine-learning models can approximate physics very quickly but at the cost of accuracy. Here we show that using machine learning inside traditional fluid simulations can improve both accuracy and speed, even on examples very different from the training data. Our approach opens the door to applying machine learning to large-scale physical modeling tasks like airplane design and climate prediction. Numerical simulation of fluids plays an essential role in modeling many physical phenomena, such as weather, climate, aerodynamics, and plasma physics. Fluids are well described by the Navier–Stokes equations, but solving these equations at scale remains daunting, limited by the computational cost of resolving the smallest spatiotemporal features. This leads to unfavorable trade-offs between accuracy and tractability. Here we use end-to-end deep learning to improve approximations inside computational fluid dynamics for modeling two-dimensional turbulent flows. For both direct numerical simulation of turbulence and large-eddy simulation, our results are as accurate as baseline solvers with 8 to 10× finer resolution in each spatial dimension, resulting in 40- to 80-fold computational speedups. Our method remains stable during long simulations and generalizes to forcing functions and Reynolds numbers outside of the flows where it is trained, in contrast to black-box machine-learning approaches. Our approach exemplifies how scientific computing can leverage machine learning and hardware accelerators to improve simulations without sacrificing accuracy or generalization.",2021.0,"Dmitrii Kochkov, Jamie A. Smith, Ayya Alieva, Qing Wang, M. Brenner, Stephan Hoyer"
f1b962fb4070fedd46758e334db3ba4f00ddc3ec,https://www.semanticscholar.org/paper/f1b962fb4070fedd46758e334db3ba4f00ddc3ec,Supervised Machine Learning: A Review of Classification Techniques,"The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.",2007.0,S. Kotsiantis
9583ac53a19cdf0db81fef6eb0b63e66adbe2324,https://www.semanticscholar.org/paper/9583ac53a19cdf0db81fef6eb0b63e66adbe2324,Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.",2017.0,"Peva Blanchard, El Mahdi El Mhamdi, R. Guerraoui, J. Stainer"
62ccd99a65bfc7c735ae1f33b75b107665de95df,https://www.semanticscholar.org/paper/62ccd99a65bfc7c735ae1f33b75b107665de95df,Federated Machine Learning,"Today’s artificial intelligence still faces two major challenges. One is that, in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated-learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated-learning framework, which includes horizontal federated learning, vertical federated learning, and federated transfer learning. We provide definitions, architectures, and applications for the federated-learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allowing knowledge to be shared without compromising user privacy.",2019.0,"Qiang Yang, Yang Liu, Tianjian Chen, Yongxin Tong"
f64670a5f54fcce339a916497a001cbf02a9a04f,https://www.semanticscholar.org/paper/f64670a5f54fcce339a916497a001cbf02a9a04f,A Review on Fairness in Machine Learning,"An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.",2022.0,"Dana Pessach, E. Shmueli"
0d6ef817813d04a3b3ec6c3ce008e104fb3e587a,https://www.semanticscholar.org/paper/0d6ef817813d04a3b3ec6c3ce008e104fb3e587a,Classification Based on Decision Tree Algorithm for Machine Learning,"Decision tree classifiers are regarded to be a standout of the most well-known methods to data classification representation of classifiers. Different researchers from various fields and backgrounds have considered the problem of extending a decision tree from available data, such as machine study, pattern recognition, and statistics. In various fields such as medical disease analysis, text classification, user smartphone classification, images, and many more the employment of Decision tree classifiers has been proposed in many ways. This paper provides a detailed approach to the decision trees. Furthermore, paper specifics, such as algorithms/approaches used, datasets, and outcomes achieved, are evaluated and outlined comprehensively. In addition, all of the approaches analyzed were discussed to illustrate the themes of the authors and identify the most accurate classifiers. As a result, the uses of different types of datasets are discussed and their findings are analyzed.",2021.0,"Bahzad Charbuty, A. Abdulazeez"
2b7f9117eb6608a58be4c078ca3d69c0e5ccb875,https://www.semanticscholar.org/paper/2b7f9117eb6608a58be4c078ca3d69c0e5ccb875,SecureML: A System for Scalable Privacy-Preserving Machine Learning,"Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.",2017.0,"Payman Mohassel, Yupeng Zhang"
256db9dba1978f004a67c86ffc321563b1aee79a,https://www.semanticscholar.org/paper/256db9dba1978f004a67c86ffc321563b1aee79a,Interpretable Machine Learning: Fundamental Principles and 10 Grand Challenges,"Interpretability in machine learning (ML) is crucial for high stakes decisions and troubleshooting. In this work, we provide fundamental principles for interpretable ML, and dispel common misunderstandings that dilute the importance of this crucial topic. We also identify 10 technical challenge areas in interpretable machine learning and provide history and background on each problem. Some of these problems are classically important, and some are recent problems that have arisen in the last few years. These problems are: (1) Optimizing sparse logical models such as decision trees; (2) Optimization of scoring systems; (3) Placing constraints into generalized additive models to encourage sparsity and better interpretability; (4) Modern case-based reasoning, including neural networks and matching for causal inference; (5) Complete supervised disentanglement of neural networks; (6) Complete or even partial unsupervised disentanglement of neural networks; (7) Dimensionality reduction for data visualization; (8) Machine learning models that can incorporate physics and other generative or causal constraints; (9) Characterization of the""Rashomon set""of good models; and (10) Interpretable reinforcement learning. This survey is suitable as a starting point for statisticians and computer scientists interested in working in interpretable machine learning.",2021.0,"C. Rudin, Chaofan Chen, Zhi Chen, Haiyang Huang, Lesia Semenova, Chudi Zhong"
4d8f0ae904779a50b2e18fec49e51a5661a98d8a,https://www.semanticscholar.org/paper/4d8f0ae904779a50b2e18fec49e51a5661a98d8a,MRI-Based Brain Tumor Classification Using Ensemble of Deep Features and Machine Learning Classifiers,"Brain tumor classification plays an important role in clinical diagnosis and effective treatment. In this work, we propose a method for brain tumor classification using an ensemble of deep features and machine learning classifiers. In our proposed framework, we adopt the concept of transfer learning and uses several pre-trained deep convolutional neural networks to extract deep features from brain magnetic resonance (MR) images. The extracted deep features are then evaluated by several machine learning classifiers. The top three deep features which perform well on several machine learning classifiers are selected and concatenated as an ensemble of deep features which is then fed into several machine learning classifiers to predict the final output. To evaluate the different kinds of pre-trained models as a deep feature extractor, machine learning classifiers, and the effectiveness of an ensemble of deep feature for brain tumor classification, we use three different brain magnetic resonance imaging (MRI) datasets that are openly accessible from the web. Experimental results demonstrate that an ensemble of deep features can help improving performance significantly, and in most cases, support vector machine (SVM) with radial basis function (RBF) kernel outperforms other machine learning classifiers, especially for large datasets.",2021.0,"Jaeyong Kang, Z. Ullah, Jeonghwan Gwak"
d0ab11de3077490c80a08abd0fb8827bac84c454,https://www.semanticscholar.org/paper/d0ab11de3077490c80a08abd0fb8827bac84c454,MoleculeNet: a benchmark for molecular machine learning,"A large scale benchmark for molecular machine learning consisting of multiple public datasets, metrics, featurizations and learning algorithms.",2017.0,"Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, C. Geniesse, Aneesh S. Pappu, K. Leswing, V. Pande"
20f63033e8775cbab0692aed92d38da7e725d64e,https://www.semanticscholar.org/paper/20f63033e8775cbab0692aed92d38da7e725d64e,Understanding Machine Learning - From Theory to Algorithms,"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability ; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning ; and emerging theoretical concepts such as the PACBayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and nonexpert readers in statistics, computer science, mathematics, and engineering.",2014.0,"Shai Shalev-Shwartz, Shai Ben-David"
c5c4142a01981787a71bf6ebcb791520c458ab5d,https://www.semanticscholar.org/paper/c5c4142a01981787a71bf6ebcb791520c458ab5d,FedML: A Research Library and Benchmark for Federated Machine Learning,"Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at this https URL.",2020.0,"Chaoyang He, Songze Li, Jinhyun So, Mi Zhang, Hongyi Wang, Xiaoyang Wang, Praneeth Vepakomma, Abhishek Singh, Hang Qiu, Li Shen, P. Zhao, Yan Kang, Yang Liu, R. Raskar, Qiang Yang, M. Annavaram, S. Avestimehr"
e24b8a9531573d284647239affc6c855505b0de4,https://www.semanticscholar.org/paper/e24b8a9531573d284647239affc6c855505b0de4,Adversarial machine learning,"In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.",2019.0,"Ling Huang, A. Joseph, B. Nelson, Benjamin I. P. Rubinstein, J. D. Tygar"
f86f1748d1b6d22870f4347fd5d65314ba800583,https://www.semanticscholar.org/paper/f86f1748d1b6d22870f4347fd5d65314ba800583,Reconciling modern machine-learning practice and the classical bias–variance trade-off,"Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias–variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias–variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This “double-descent” curve subsumes the textbook U-shaped bias–variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.",2018.0,"M. Belkin, Daniel J. Hsu, Siyuan Ma, Soumik Mandal"
696b388ee6221c6dbcfd647a06883b2bfee773d9,https://www.semanticscholar.org/paper/696b388ee6221c6dbcfd647a06883b2bfee773d9,Universal Differential Equations for Scientific Machine Learning,"
 In the context of science, the well-known adage “a picture is worth a thousand words” might well be “a model is worth a thousand datasets.” Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring ""big data"". In this work demonstrate how a mathematical object, which we denote universal differential equations (UDEs), can be utilized as a theoretical underpinning to a diverse array of problems in scientific machine learning to yield efficient algorithms and generalized approaches. The UDE model augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating the training of physics-informed neural networks and large-eddy simulations, can all be transformed into UDE training problems that are efficiently solved by a single software methodology.",2020.0,"Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin Warner, K. Zubov, R. Supekar, Dominic J. Skinner, A. Ramadhan"
fee8f63972906214b77f16cfeca0b93ee8f36ba2,https://www.semanticscholar.org/paper/fee8f63972906214b77f16cfeca0b93ee8f36ba2,Fairness in Machine Learning: A Survey,"When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.",2020.0,"Simon Caton, C. Haas"
6068d39e92aef1bb0e1291e9931894c35692a85e,https://www.semanticscholar.org/paper/6068d39e92aef1bb0e1291e9931894c35692a85e,Counterfactual Explanations for Machine Learning: A Review,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.",2020.0,"Sahil Verma, John P. Dickerson, Keegan E. Hines"
4087e84fc695bb6433d0104ee94f9d7e9f4b7da5,https://www.semanticscholar.org/paper/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5,Machine Learning for Fluid Mechanics,"The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.",2019.0,"S. Brunton, B. R. Noack, P. Koumoutsakos"
b9518627db25f05930e931f56497602363a75491,https://www.semanticscholar.org/paper/b9518627db25f05930e931f56497602363a75491,"Definitions, methods, and applications in interpretable machine learning","Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.",2019.0,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu"
62df84d6a4d26f95e4714796c2337c9848cc13b5,https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5,MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems,"MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. 
This paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.",2015.0,"Tianqi Chen, Mu Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao, Bing Xu, Chiyuan Zhang, Zheng Zhang"
4d1fdd81f033cd58f3723bfc61e7d12079647a7a,https://www.semanticscholar.org/paper/4d1fdd81f033cd58f3723bfc61e7d12079647a7a,"Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.","The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.",2016.0,"Z. Obermeyer, E. Emanuel"
46c266b3d1274dacd7fce27ee8cb4d587f087a58,https://www.semanticscholar.org/paper/46c266b3d1274dacd7fce27ee8cb4d587f087a58,Machine Learning Interpretability: A Survey on Methods and Metrics,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.",2019.0,"D. V. Carvalho, E. M. Pereira, Jaime S. Cardoso"
b415e836d447ea9efb7629a1de67cd2a6f9e7ba8,https://www.semanticscholar.org/paper/b415e836d447ea9efb7629a1de67cd2a6f9e7ba8,Machine Learning in Agriculture: A Comprehensive Updated Review,"The digital transformation of agriculture has evolved various aspects of management into artificial intelligent systems for the sake of making value from the ever-increasing data originated from numerous sources. A subset of artificial intelligence, namely machine learning, has a considerable potential to handle numerous challenges in the establishment of knowledge-based farming systems. The present study aims at shedding light on machine learning in agriculture by thoroughly reviewing the recent scholarly literature based on keywords’ combinations of “machine learning” along with “crop management”, “water management”, “soil management”, and “livestock management”, and in accordance with PRISMA guidelines. Only journal papers were considered eligible that were published within 2018–2020. The results indicated that this topic pertains to different disciplines that favour convergence research at the international level. Furthermore, crop management was observed to be at the centre of attention. A plethora of machine learning algorithms were used, with those belonging to Artificial Neural Networks being more efficient. In addition, maize and wheat as well as cattle and sheep were the most investigated crops and animals, respectively. Finally, a variety of sensors, attached on satellites and unmanned ground and aerial vehicles, have been utilized as a means of getting reliable input data for the data analyses. It is anticipated that this study will constitute a beneficial guide to all stakeholders towards enhancing awareness of the potential advantages of using machine learning in agriculture and contributing to a more systematic research on this topic.",2021.0,"L. Benos, A. Tagarakis, Georgios Dolias, R. Berruto, D. Kateris, D. Bochtis"
a9cbbef8f4426329d0687025b34287c35bdd8b38,https://www.semanticscholar.org/paper/a9cbbef8f4426329d0687025b34287c35bdd8b38,Machine learning and the physical sciences,"Machine learning (ML) encompasses a broad range of algorithms and modeling tools used for a vast array of data processing tasks, which has entered most scientific disciplines in recent years. This article reviews in a selective way the recent research on the interface between machine learning and the physical sciences. This includes conceptual developments in ML motivated by physical insights, applications of machine learning techniques to several domains in physics, and cross fertilization between the two fields. After giving a basic notion of machine learning methods and principles, examples are described of how statistical physics is used to understand methods in ML. This review then describes applications of ML methods in particle physics and cosmology, quantum many-body physics, quantum computing, and chemical and material physics. Research and development into novel computing architectures aimed at accelerating ML are also highlighted. Each of the sections describe recent successes as well as domain-specific methodology and challenges.",2019.0,"Giuseppe Carleo, I. Cirac, Kyle Cranmer, L. Daudet, M. Schuld, Naftali Tishby, Leslie Vogt-Maranto, Lenka Zdeborov'a"
a453ce8a3de86a170c79a1082ef358c3adf4e612,https://www.semanticscholar.org/paper/a453ce8a3de86a170c79a1082ef358c3adf4e612,Combining Machine Learning and Computational Chemistry for Predictive Insights Into Chemical Systems,"Machine learning models are poised to make a transformative impact on chemical sciences by dramatically accelerating computational algorithms and amplifying insights available from computational chemistry methods. However, achieving this requires a confluence and coaction of expertise in computer science and physical sciences. This Review is written for new and experienced researchers working at the intersection of both fields. We first provide concise tutorials of computational chemistry and machine learning methods, showing how insights involving both can be achieved. We follow with a critical review of noteworthy applications that demonstrate how computational chemistry and machine learning can be used together to provide insightful (and useful) predictions in molecular and materials modeling, retrosyntheses, catalysis, and drug design.",2021.0,"J. Keith, Valentin Vassilev-Galindo, Bingqing Cheng, Stefan Chmiela, M. Gastegger, Klaus-Robert Müller, A. Tkatchenko"
287ba5bf00d96af1596aaf80c178392a9c4fcc28,https://www.semanticscholar.org/paper/287ba5bf00d96af1596aaf80c178392a9c4fcc28,Machine Learning Basics,"coined in 1959 by Arthur Samuel [Samuel 1959], Tom Mitchell [Mitchell 1997] provided a more formal definition: “A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.” ML has be applied to many real-world problems or tasks, like medical diagno­ sis, robotics, recommendation systems, facial recognition, stock prices prediction, and sentiment analysis, with great success. We can divide ML algorithms into three main categories (see Figure 4.1): Machine Learning Basics",2021.0,"Konstantinos Chatzilygeroudis, I. Hatzilygeroudis, I. Perikos"
6e23398447a022fb9495c44fa80e9de593a574bc,https://www.semanticscholar.org/paper/6e23398447a022fb9495c44fa80e9de593a574bc,Machine Learning in Agriculture: A Review,"Machine learning has emerged with big data technologies and high-performance computing to create new opportunities for data intensive science in the multi-disciplinary agri-technologies domain. In this paper, we present a comprehensive review of research dedicated to applications of machine learning in agricultural production systems. The works analyzed were categorized in (a) crop management, including applications on yield prediction, disease detection, weed detection crop quality, and species recognition; (b) livestock management, including applications on animal welfare and livestock production; (c) water management; and (d) soil management. The filtering and classification of the presented articles demonstrate how agriculture will benefit from machine learning technologies. By applying machine learning to sensor data, farm management systems are evolving into real time artificial intelligence enabled programs that provide rich recommendations and insights for farmer decision support and action.",2018.0,"Konstantinos G. Liakos, P. Busato, D. Moshou, S. Pearson, D. Bochtis"
74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,https://www.semanticscholar.org/paper/74b4f16c5ac91e3e7c88ae81cc8c91416b71d151,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",2020.0,"Peter Henderson, Jie Hu, Joshua Romoff, E. Brunskill, Dan Jurafsky, Joelle Pineau"
5c7e5248d9eb7f373f10277410bf8506160907ea,https://www.semanticscholar.org/paper/5c7e5248d9eb7f373f10277410bf8506160907ea,All-optical machine learning using diffractive deep neural networks,"All-optical deep learning Deep learning uses multilayered artificial neural networks to learn digitally from large datasets. It then performs advanced identification and classification tasks. To date, these multilayered neural networks have been implemented on a computer. Lin et al. demonstrate all-optical machine learning that uses passive optical components that can be patterned and fabricated with 3D-printing. Their hardware approach comprises stacked layers of diffractive optical elements analogous to an artificial neural network that can be trained to execute complex functions at the speed of light. Science, this issue p. 1004 All-optical deep learning can be implemented with 3D-printed passive optical components. Deep learning has been transforming our ability to execute advanced inference tasks using computers. Here we introduce a physical mechanism to perform machine learning by demonstrating an all-optical diffractive deep neural network (D2NN) architecture that can implement various functions following the deep learning–based design of passive diffractive layers that work collectively. We created 3D-printed D2NNs that implement classification of images of handwritten digits and fashion products, as well as the function of an imaging lens at a terahertz spectrum. Our all-optical deep learning framework can perform, at the speed of light, various complex functions that computer-based neural networks can execute; will find applications in all-optical image analysis, feature detection, and object classification; and will also enable new camera designs and optical components that perform distinctive tasks using D2NNs.",2018.0,"Xing Lin, Y. Rivenson, N. Yardimci, Muhammed Veli, Yilin Luo, M. Jarrahi, A. Ozcan"
caf9e0fa2c340fb07cef8d547ea8849508e5c358,https://www.semanticscholar.org/paper/caf9e0fa2c340fb07cef8d547ea8849508e5c358,Empirical Asset Pricing Via Machine Learning,"
 We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.
 Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.",2018.0,"Shihao Gu, B. Kelly, D. Xiu"
4f97e87512eb8bf48ce695443e958725c54908b6,https://www.semanticscholar.org/paper/4f97e87512eb8bf48ce695443e958725c54908b6,Mathematics for Machine Learning,"Machine learning is a way to study the algorithm and statistical model that is used by computer to perform a specific task through pattern and deduction [1]. It builds a mathematical model from a sample data which may come under either supervised or unsupervised learning. It is closely
 related to computational statistics which is an interface between statistics and computer science. Also, linear algebra and probability theory are two tools of mathematics which form the basis of machine learning. In general, statistics is a science concerned with collecting, analysing, interpreting
 the data. Data are the facts and figure that can be classified as either quantitative or qualitative. From the given set of data, we can predict the expected observation, difference between the outcome of two observations and how data look like which can help in better decision making process
 [2]. Descriptive and inferential statistics are the two methods of data analysis. Descriptive statistics summarize the raw data into information through which common expectation and variation of data can be taken. It also provides graphical methods that can be used to visualize the sample
 of data and qualitative understanding of observation whereas inferential statistics refers to drawing conclusions from data. Inferences are made under the framework of probability theory. So, understanding of data and interpretation of result are two important aspects of machine learning.
 In this paper, we have reviewed the different methods of ML, mathematics behind ML, its application in day to day life and future aspects.",2020.0,"Gaurav Kumar, Rishav Banerjee, Deepak Kr Singh, Nitesh Choubey, Arnaw"
4afa7d8e2de43b0b67366b1bce8768f5a246d153,https://www.semanticscholar.org/paper/4afa7d8e2de43b0b67366b1bce8768f5a246d153,Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020,"This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS 2020 which ran from July-October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open-source black-box optimization packages as well as random search.",2021.0,"Ryan Turner, David Eriksson, M. McCourt, J. Kiili, Eero Laaksonen, Zhen Xu, Isabelle M Guyon"
739769f4862753fc80057194456d758d2a148ee3,https://www.semanticscholar.org/paper/739769f4862753fc80057194456d758d2a148ee3,Extreme Learning Machine for Regression and Multiclass Classification,"Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the “generalized” single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.",2012.0,"G. Huang, Hongming Zhou, Xiaojian Ding, Rui Zhang"
05c5b732fb92546c7d6eeabfadb5c14610d07373,https://www.semanticscholar.org/paper/05c5b732fb92546c7d6eeabfadb5c14610d07373,Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning,"Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: this https URL.",2016.0,"G. Lemaître, Fernando Nogueira, Christos K. Aridas"
71a85e735a3686bef8cce3725ae5ba82e2cabb1b,https://www.semanticscholar.org/paper/71a85e735a3686bef8cce3725ae5ba82e2cabb1b,Underspecification Presents Challenges for Credibility in Modern Machine Learning,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020.0,"A. D'Amour, K. Heller, D. Moldovan, Ben Adlam, Babak Alipanahi, Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, M. Hoffman, F. Hormozdiari, N. Houlsby, Shaobo Hou, Ghassen Jerfel, A. Karthikesalingam, Mario Lucic, Yi-An Ma, Cory Y. McLean, Diana Mincu, A. Mitani, A. Montanari, Zachary Nado, Vivek Natarajan, Christopher Nielson, T. Osborne, R. Raman, K. Ramasamy, R. Sayres, Jessica Schrouff, Martin G. Seneviratne, Shannon Sequeira, Harini Suresh, Victor Veitch, Max Vladymyrov, Xuezhi Wang, Kellie Webster, Steve Yadlowsky, T. Yun, Xiaohua Zhai, D. Sculley"
4a7eea3ec3080ecb277bfe466afce4822a1071d7,https://www.semanticscholar.org/paper/4a7eea3ec3080ecb277bfe466afce4822a1071d7,Quantum embeddings for machine learning,"Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.",2020.0,"S. Lloyd, M. Schuld, Aroosa Ijaz, J. Izaac, N. Killoran"
5fca8bbec714e403fa0f95a56b355c8ca835bcc0,https://www.semanticscholar.org/paper/5fca8bbec714e403fa0f95a56b355c8ca835bcc0,A Survey on the Explainability of Supervised Machine Learning,"Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.",2020.0,"Nadia Burkart, Marco F. Huber"
99afa67e28780754907b19b688bf2b35eb22e578,https://www.semanticscholar.org/paper/99afa67e28780754907b19b688bf2b35eb22e578,A Review on Linear Regression Comprehensive in Machine Learning,"Perhaps one of the most common and comprehensive statistical and machine learning algorithms are linear regression. Linear regression is used to find a linear relationship between one or more predictors. The linear regression has two types: simple regression and multiple regression (MLR). This paper discusses various works by different researchers on linear regression and polynomial regression and compares their performance using the best approach to optimize prediction and precision. Almost all of the articles analyzed in this review is focused on datasets; in order to determine a model's efficiency, it must be correlated with the actual values obtained for the explanatory variables.",2020.0,"Dastan Maulud, A. Abdulazeez"
a1f57b760b7d4c44490f0bb86ee5462c3e7c7272,https://www.semanticscholar.org/paper/a1f57b760b7d4c44490f0bb86ee5462c3e7c7272,Machine Learning Force Fields,"In recent years, the use of machine learning (ML) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. One of the most promising applications is the construction of ML-based force fields (FFs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical FFs. The key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. Such universal ML approximations are in principle only limited by the quality and quantity of the reference data used to train them. This review gives an overview of applications of ML-FFs and the chemical insights that can be obtained from them. The core concepts underlying ML-FFs are described in detail, and a step-by-step guide for constructing and testing them from scratch is given. The text concludes with a discussion of the challenges that remain to be overcome by the next generation of ML-FFs.",2020.0,"Oliver T. Unke, Stefan Chmiela, H. Sauceda, M. Gastegger, I. Poltavsky, Kristof T. Schütt, A. Tkatchenko, K. Müller"
6b67e38e488cdb6a5af8ae44fdbb2c069c9cea0e,https://www.semanticscholar.org/paper/6b67e38e488cdb6a5af8ae44fdbb2c069c9cea0e,"Introduction to Machine Learning, Neural Networks, and Deep Learning","Purpose To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning. Methods A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology. Results A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background. Conclusions Artificial intelligence has a promising future in medicine; however, many challenges remain. Translational Relevance The aim of this review article is to provide the nontechnical readers a layman's explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.",2020.0,"Rene Y. Choi, Aaron S. Coyner, Jayashree Kalpathy-Cramer, M. Chiang, J. Campbell"
72d3ddf1f7210d7e70144bbc09f770ec411fe909,https://www.semanticscholar.org/paper/72d3ddf1f7210d7e70144bbc09f770ec411fe909,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",2020.0,"S. Raschka, Joshua Patterson, Corey J. Nolet"
b5b98051b65da6b1b3b579862b0407d48c5bef48,https://www.semanticscholar.org/paper/b5b98051b65da6b1b3b579862b0407d48c5bef48,Principles and Practice of Explainable Machine Learning,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.",2020.0,"Vaishak Belle, I. Papantonis"
80c3c1cb86eeb9f2ab0934d6f914918889d34db7,https://www.semanticscholar.org/paper/80c3c1cb86eeb9f2ab0934d6f914918889d34db7,"Tslearn, A Machine Learning Toolkit for Time Series Data","tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.",2020.0,"R. Tavenard, Johann Faouzi, Gilles Vandewiele, Felix Divo, Guillaume Androz, Chester Holtz, Marie Payne, R. Yurchak, Marc Rußwurm, Kushal Kolar, E. Woods"
1b225474e7a5794f98cdfbde8b12ccbc56799409,https://www.semanticscholar.org/paper/1b225474e7a5794f98cdfbde8b12ccbc56799409,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .",2017.0,"Wieland Brendel, Jonas Rauber, M. Bethge"
6aae0dc122102693e8136856ffc8b72df7f78386,https://www.semanticscholar.org/paper/6aae0dc122102693e8136856ffc8b72df7f78386,A study of the behavior of several methods for balancing machine learning training data,"There are several aspects that might influence the performance achieved by existing learning systems. It has been reported that one of these aspects is related to class imbalance in which examples in training data belonging to one class heavily outnumber the examples in the other class. In this situation, which is found in real world data describing an infrequent but important event, the learning system may have difficulties to learn the concept related to the minority class. In this work we perform a broad experimental evaluation involving ten methods, three of them proposed by the authors, to deal with the class imbalance problem in thirteen UCI data sets. Our experiments provide evidence that class imbalance does not systematically hinder the performance of learning systems. In fact, the problem seems to be related to learning with too few minority class examples in the presence of other complicating factors, such as class overlapping. Two of our proposed methods deal with these conditions directly, allying a known over-sampling method with data cleaning methods in order to produce better-defined class clusters. Our comparative experiments show that, in general, over-sampling methods provide more accurate results than under-sampling methods considering the area under the ROC curve (AUC). This result seems to contradict results previously published in the literature. Two of our proposed methods, Smote + Tomek and Smote + ENN, presented very good results for data sets with a small number of positive examples. Moreover, Random over-sampling, a very simple over-sampling method, is very competitive to more complex over-sampling methods. Since the over-sampling methods provided very good performance results, we also measured the syntactic complexity of the decision trees induced from over-sampled data. Our results show that these trees are usually more complex then the ones induced from original data. Random over-sampling usually produced the smallest increase in the mean number of induced rules and Smote + ENN the smallest increase in the mean number of conditions per rule, when compared among the investigated over-sampling methods.",2004.0,"Gustavo E. A. P. A. Batista, R. Prati, M. C. Monard"
d7701e78e0bfc92b03a89582e80cfb751ac03f26,https://www.semanticscholar.org/paper/d7701e78e0bfc92b03a89582e80cfb751ac03f26,Explaining Explanations: An Overview of Interpretability of Machine Learning,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",2018.0,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, Lalana Kagal"
b1f574c47d0b6e3032246770b9cbebb9c7bd0c7f,https://www.semanticscholar.org/paper/b1f574c47d0b6e3032246770b9cbebb9c7bd0c7f,Challenges in Deploying Machine Learning: A Survey of Case Studies,"In recent years, machine learning has transitioned from a field of academic research interest to a field capable of solving real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries, and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. By mapping found challenges to the steps of the machine learning deployment workflow, we show that practitioners face issues at each stage of the deployment process. The goal of this article is to lay out a research agenda to explore approaches addressing these challenges.",2020.0,"Andrei Paleyes, Raoul-Gabriel Urma, Neil D. Lawrence"
5e331bf7887e2e634bf5b12788849d2d2b74bc7f,https://www.semanticscholar.org/paper/5e331bf7887e2e634bf5b12788849d2d2b74bc7f,Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program),"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.",2020.0,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle"
e8d330f11df9c69f38b78a7cc4b1333ebecf7c55,https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55,Ethical Machine Learning in Health Care,"The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.",2020.0,"I. Chen, E. Pierson, Sherri Rose, Shalmali Joshi, Kadija Ferryman, M. Ghassemi"
643da4c4de1954daeac571a82367241db012a8bf,https://www.semanticscholar.org/paper/643da4c4de1954daeac571a82367241db012a8bf,Automatic differentiation in machine learning: a survey,"Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply “auto-diff”, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until 
very recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other’s results. Despite its 
relevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names “dynamic computational 
graphs” and “differentiable programming”. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main imple- 
mentation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms “autodiff”, “automatic differentiation”, and “symbolic differentiation” as these are encountered more and more in machine learning settings.",2015.0,"A. G. Baydin, Barak A. Pearlmutter, Alexey Radul, J. Siskind"
8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8,https://www.semanticscholar.org/paper/8b27e2fafbe24cf9ce24f308a7e746489ff0dfb8,Trainable Weka Segmentation: a machine learning tool for microscopy pixel classification,"Summary: State‐of‐the‐art light and electron microscopes are capable of acquiring large image datasets, but quantitatively evaluating the data often involves manually annotating structures of interest. This process is time‐consuming and often a major bottleneck in the evaluation pipeline. To overcome this problem, we have introduced the Trainable Weka Segmentation (TWS), a machine learning tool that leverages a limited number of manual annotations in order to train a classifier and segment the remaining data automatically. In addition, TWS can provide unsupervised segmentation learning schemes (clustering) and can be customized to employ user‐designed image features or classifiers. Availability and Implementation: TWS is distributed as open‐source software as part of the Fiji image processing distribution of ImageJ at http://imagej.net/Trainable_Weka_Segmentation. Contact: ignacio.arganda@ehu.eus Supplementary information: Supplementary data are available at Bioinformatics online.",2017.0,"Ignacio Arganda-Carreras, V. Kaynig, C. Rueden, K. Eliceiri, J. Schindelin, Albert Cardona, H. Seung"
6f803c1729ed818ef6a5ee28cf23b3c6ca9e4291,https://www.semanticscholar.org/paper/6f803c1729ed818ef6a5ee28cf23b3c6ca9e4291,Advancing Biosensors with Machine Learning.,"Chemometrics play a critical role in biosensors-based detection, analysis, and diagnosis. Nowadays, as a branch of artificial intelligence (AI), machine learning (ML) have achieved impressive advances. However, novel advanced ML methods, especially deep learning, which is famous for image analysis, facial recognition, and speech recognition, has remained relatively elusive to the biosensor community. Herein, how ML can be beneficial to biosensors is systematically discussed. The advantages and drawbacks of most popular ML algorithms are summarized on the basis of sensing data analysis. Specially, deep learning methods such as convolutional neural network (CNN) and recurrent neural network (RNN) are emphasized. Diverse ML-assisted electrochemical biosensors, wearable electronics, SERS and other spectra-based biosensors, fluorescence biosensors and colorimetric biosensors are comprehensively discussed. Furthermore, biosensor networks and multibiosensor data fusion are introduced. This review will nicely bridge ML with biosensors, and greatly expand chemometrics for detection, analysis, and diagnosis.",2020.0,"Feiyun Cui, Yun Yue, Yi Zhang, Ziming Zhang, H. S. Zhou"
6965bc6d26fc910a6387cd6d423b35fd9e1d358b,https://www.semanticscholar.org/paper/6965bc6d26fc910a6387cd6d423b35fd9e1d358b,Integrating Physics-Based Modeling with Machine Learning: A Survey,"In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.",2020.0,"J. Willard, X. Jia, Shaoming Xu, M. Steinbach, Vipin Kumar"
561269a24f2f2a06409109723a8ab93a01696efc,https://www.semanticscholar.org/paper/561269a24f2f2a06409109723a8ab93a01696efc,Federated Optimization: Distributed Machine Learning for On-Device Intelligence,"We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. 
A motivating example arises when we keep the training data locally on users' mobile devices instead of logging it to a data center for training. In federated optimziation, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network --- as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. 
We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of \federated optimization.",2016.0,"Jakub Konecný, H. B. McMahan, Daniel Ramage, Peter Richtárik"
b3de1062d8a462dfdc2938558258f8884abe9f4e,https://www.semanticscholar.org/paper/b3de1062d8a462dfdc2938558258f8884abe9f4e,Implementation of machine-learning classification in remote sensing: an applied review,"ABSTRACT Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.",2018.0,"Aaron E. Maxwell, T. Warner, Fang Fang"
22733aac53e89446aed76dd1983bf2d74567ba88,https://www.semanticscholar.org/paper/22733aac53e89446aed76dd1983bf2d74567ba88,Darts: User-Friendly Modern Machine Learning for Time Series,"We present Darts, a Python machine learning library for time series, with a focus on forecasting. Darts offers a variety of models, from classics such as ARIMA to state-of-the-art deep neural networks. The emphasis of the library is on offering modern machine learning functionalities, such as supporting multidimensional series, meta-learning on multiple series, training on large datasets, incorporating external data, ensembling models, and providing a rich support for probabilistic forecasting. At the same time, great care goes into the API design to make it user-friendly and easy to use. For instance, all models can be used using fit()/predict(), similar to scikit-learn.",2021.0,"J. Herzen, Francesco Lässig, Samuele Giuliano Piazzetta, T. Neuer, L'eo Tafti, Guillaume Raille, Tomas Van Pottelbergh, Marek Pasieka, Andrzej Skrodzki, Nicolas Huguenin, Maxime Dumonal, Jan Ko'scisz, Dennis Bader, Frédérick Gusset, Mounir Benheddi, Camila Williamson, Michal Kosinski, M. Petrik, Gaël Grosch"
b7a717233ec3ff37385ab1b06816d0ca375f5bb3,https://www.semanticscholar.org/paper/b7a717233ec3ff37385ab1b06816d0ca375f5bb3,Data Shapley: Equitable Valuation of Data for Machine Learning,"As data becomes the fuel driving technological and economic growth, a fundamental challenge is how to quantify the value of data in algorithmic predictions and decisions. For example, in healthcare and consumer markets, it has been suggested that individuals should be compensated for the data that they generate, but it is not clear what is an equitable valuation for individual data. In this work, we develop a principled framework to address data valuation in the context of supervised machine learning. Given a learning algorithm trained on $n$ data points to produce a predictor, we propose data Shapley as a metric to quantify the value of each training datum to the predictor performance. Data Shapley value uniquely satisfies several natural properties of equitable data valuation. We develop Monte Carlo and gradient-based methods to efficiently estimate data Shapley values in practical settings where complex learning algorithms, including neural networks, are trained on large datasets. In addition to being equitable, extensive experiments across biomedical, image and synthetic data demonstrate that data Shapley has several other benefits: 1) it is more powerful than the popular leave-one-out or leverage score in providing insight on what data is more valuable for a given learning task; 2) low Shapley value data effectively capture outliers and corruptions; 3) high Shapley value data inform what type of new data to acquire to improve the predictor.",2019.0,"Amirata Ghorbani, James Y. Zou"
2abdca069a95add94f5c0c540c09efb7adeee230,https://www.semanticscholar.org/paper/2abdca069a95add94f5c0c540c09efb7adeee230,Reproducibility in machine learning for health research: Still a ways to go,"Machine learning applied to health falls short on several reproducibility metrics compared to other machine learning subfields. Machine learning for health must be reproducible to ensure reliable clinical use. We evaluated 511 scientific papers across several machine learning subfields and found that machine learning for health compared poorly to other areas regarding reproducibility metrics, such as dataset and code accessibility. We propose recommendations to address this problem.",2021.0,"Matthew B. A. McDermott, Shirly Wang, N. Marinsek, R. Ranganath, L. Foschini, M. Ghassemi"
2a41589895b84f6225bba43d928355eb2fd52c1d,https://www.semanticscholar.org/paper/2a41589895b84f6225bba43d928355eb2fd52c1d,Machine Learning for Chemical Reactions.,"Machine learning (ML) techniques applied to chemical reactions have a long history. The present contribution discusses applications ranging from small molecule reaction dynamics to computational platforms for reaction planning. ML-based techniques can be particularly relevant for problems involving both computation and experiments. For one, Bayesian inference is a powerful approach to develop models consistent with knowledge from experiments. Second, ML-based methods can also be used to handle problems that are formally intractable using conventional approaches, such as exhaustive characterization of state-to-state information in reactive collisions. Finally, the explicit simulation of reactive networks as they occur in combustion has become possible using machine-learned neural network potentials. This review provides an overview of the questions that can and have been addressed using machine learning techniques, and an outlook discusses challenges in this diverse and stimulating field. It is concluded that ML applied to chemistry problems as practiced and conceived today has the potential to transform the way with which the field approaches problems involving chemical reactions, in both research and academic teaching.",2021.0,M. Meuwly
9ceae85a0bd4231cd2efe14884c40b7bc04d3dac,https://www.semanticscholar.org/paper/9ceae85a0bd4231cd2efe14884c40b7bc04d3dac,Accounting for Variance in Machine Learning Benchmarks,"Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, data augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51 times reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on five different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons.",2021.0,"Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, B. Nichyporuk, Justin Szeto, Naz Sepah, Edward Raff, Kanika Madan, Vikram S. Voleti, Samira Ebrahimi Kahou, Vincent Michalski, Dmitriy Serdyuk, T. Arbel, C. Pal, G. Varoquaux, Pascal Vincent"
c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7,https://www.semanticscholar.org/paper/c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7,Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting,"Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.",2017.0,"Samuel Yeom, Irene Giacomelli, Matt Fredrikson, S. Jha"
2bc3644ce4de7fce5812c1455e056649a47c1bbf,https://www.semanticscholar.org/paper/2bc3644ce4de7fce5812c1455e056649a47c1bbf,Effective Heart Disease Prediction Using Hybrid Machine Learning Techniques,"Heart disease is one of the most significant causes of mortality in the world today. Prediction of cardiovascular disease is a critical challenge in the area of clinical data analysis. Machine learning (ML) has been shown to be effective in assisting in making decisions and predictions from the large quantity of data produced by the healthcare industry. We have also seen ML techniques being used in recent developments in different areas of the Internet of Things (IoT). Various studies give only a glimpse into predicting heart disease with ML techniques. In this paper, we propose a novel method that aims at finding significant features by applying machine learning techniques resulting in improving the accuracy in the prediction of cardiovascular disease. The prediction model is introduced with different combinations of features and several known classification techniques. We produce an enhanced performance level with an accuracy level of 88.7% through the prediction model for heart disease with the hybrid random forest with a linear model (HRFLM).",2019.0,"Senthilkumar Mohan, Chandrasegar Thirumalai, Gautam Srivastava"
2346d121f38fc19c77e0b062415519843f478163,https://www.semanticscholar.org/paper/2346d121f38fc19c77e0b062415519843f478163,Machine Learning in Medicine,"Spurred by advances in processing power, memory, storage, and an unprecedented wealth of data, computers are being asked to tackle increasingly complex learning tasks, often with astonishing success. Computers have now mastered a popular variant of poker, learned the laws of physics from experimental data, and become experts in video games - tasks that would have been deemed impossible not too long ago. In parallel, the number of companies centered on applying complex data analysis to varying industries has exploded, and it is thus unsurprising that some analytic companies are turning attention to problems in health care. The purpose of this review is to explore what problems in medicine might benefit from such learning approaches and use examples from the literature to introduce basic concepts in machine learning. It is important to note that seemingly large enough medical data sets and adequate learning algorithms have been available for many decades, and yet, although there are thousands of papers applying machine learning algorithms to medical data, very few have contributed meaningfully to clinical care. This lack of impact stands in stark contrast to the enormous relevance of machine learning to many other industries. Thus, part of my effort will be to identify what obstacles there may be to changing the practice of medicine through statistical learning approaches, and discuss how these might be overcome.",2015.0,"Rahul C. Deo, Karsten M. Borgwardt"
2ea6a93199c9227fa0c1c7de13725f918c9be3a4,https://www.semanticscholar.org/paper/2ea6a93199c9227fa0c1c7de13725f918c9be3a4,Dlib-ml: A Machine Learning Toolkit,"There are many excellent toolkits which provide support for developing machine learning software in Python, R, Matlab, and similar environments. Dlib-ml is an open source library, targeted at both engineers and research scientists, which aims to provide a similarly rich environment for developing machine learning software in the C++ language. Towards this end, dlib-ml contains an extensible linear algebra toolkit with built in BLAS support. It also houses implementations of algorithms for performing inference in Bayesian networks and kernel-based methods for classification, regression, clustering, anomaly detection, and feature ranking. To enable easy use of these tools, the entire library has been developed with contract programming, which provides complete and precise documentation as well as powerful debugging tools.",2009.0,Davis E. King
8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e,https://www.semanticscholar.org/paper/8a95423d0059f7c5b1422f0ef1aa60b9e26aab7e,Stealing Machine Learning Models via Prediction APIs,"Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (""predictive analytics"") systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis. 
The tension between model confidentiality and public access motivates our investigation of model extraction attacks. In such attacks, an adversary with black-box access, but no prior knowledge of an ML model's parameters or training data, aims to duplicate the functionality of (i.e., ""steal"") the model. Unlike in classical learning theory settings, ML-as-a-service offerings may accept partial feature vectors as inputs and include confidence values with predictions. Given these practices, we show simple, efficient attacks that extract target ML models with near-perfect fidelity for popular model classes including logistic regression, neural networks, and decision trees. We demonstrate these attacks against the online services of BigML and Amazon Machine Learning. We further show that the natural countermeasure of omitting confidence values from model outputs still admits potentially harmful model extraction attacks. Our results highlight the need for careful ML model deployment and new model extraction countermeasures.",2016.0,"Florian Tramèr, Fan Zhang, A. Juels, M. Reiter, Thomas Ristenpart"
3c8a456509e6c0805354bd40a35e3f2dbf8069b1,https://www.semanticscholar.org/paper/3c8a456509e6c0805354bd40a35e3f2dbf8069b1,"PyTorch: An Imperative Style, High-Performance Deep Learning Library","Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it was designed from first principles to support an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several commonly used benchmarks.",2019.0,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, N. Gimelshein, L. Antiga, Alban Desmaison, Andreas Köpf, E. Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, Soumith Chintala"
528ecb0f88a9ea6110ba309b98cc2f0678f257c9,https://www.semanticscholar.org/paper/528ecb0f88a9ea6110ba309b98cc2f0678f257c9,Data Mining Practical Machine Learning Tools And Techniques With Java Implementations,"Thank you for reading data mining practical machine learning tools and techniques with java implementations. As you may know, people have look hundreds times for their favorite novels like this data mining practical machine learning tools and techniques with java implementations, but end up in infectious downloads. Rather than reading a good book with a cup of tea in the afternoon, instead they juggled with some malicious bugs inside their laptop.",2016.0,Marcel Abendroth
f70b2f20be241f445a61f33c4b8e76e554760340,https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340,Software Engineering for Machine Learning: A Case Study,"Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be ""entangled"" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.",2019.0,"Saleema Amershi, Andrew Begel, C. Bird, R. Deline, H. Gall, Ece Kamar, Nachiappan Nagappan, Besmira Nushi, Thomas Zimmermann"
821fde6dc36d1264c765d249d4247ea66daff55f,https://www.semanticscholar.org/paper/821fde6dc36d1264c765d249d4247ea66daff55f,Edge Machine Learning for AI-Enabled IoT Devices: A Review,"In a few years, the world will be populated by billions of connected devices that will be placed in our homes, cities, vehicles, and industries. Devices with limited resources will interact with the surrounding environment and users. Many of these devices will be based on machine learning models to decode meaning and behavior behind sensors’ data, to implement accurate predictions and make decisions. The bottleneck will be the high level of connected things that could congest the network. Hence, the need to incorporate intelligence on end devices using machine learning algorithms. Deploying machine learning on such edge devices improves the network congestion by allowing computations to be performed close to the data sources. The aim of this work is to provide a review of the main techniques that guarantee the execution of machine learning models on hardware with low performances in the Internet of Things paradigm, paving the way to the Internet of Conscious Things. In this work, a detailed review on models, architecture, and requirements on solutions that implement edge machine learning on Internet of Things devices is presented, with the main goal to define the state of the art and envisioning development requirements. Furthermore, an example of edge machine learning implementation on a microcontroller will be provided, commonly regarded as the machine learning “Hello World”.",2020.0,"M. Merenda, Carlo Porcaro, D. Iero"
775a4e375cc79b53b94e37fa3eedff481823e4a6,https://www.semanticscholar.org/paper/775a4e375cc79b53b94e37fa3eedff481823e4a6,Efficient and Robust Automated Machine Learning,"The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.",2015.0,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter"
78aa018ee7d52360e15d103390ea1cdb3a0beb41,https://www.semanticscholar.org/paper/78aa018ee7d52360e15d103390ea1cdb3a0beb41,Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples,"Many machine learning models are vulnerable to adversarial examples: inputs that are specially crafted to cause a machine learning model to produce an incorrect output. Adversarial examples that affect one model often affect another model, even if the two models have different architectures or were trained on different training sets, so long as both models were trained to perform the same task. An attacker may therefore train their own substitute model, craft adversarial examples against the substitute, and transfer them to a victim model, with very little information about the victim. Recent work has further developed a technique that uses the victim model as an oracle to label a synthetic training set for the substitute, so the attacker need not even collect a training set to mount the attack. We extend these recent techniques using reservoir sampling to greatly enhance the efficiency of the training procedure for the substitute model. We introduce new transferability attacks between previously unexplored (substitute, victim) pairs of machine learning model classes, most notably SVMs and decision trees. We demonstrate our attacks on two commercial machine learning classification systems from Amazon (96.19% misclassification rate) and Google (88.94%) using only 800 queries of the victim model, thereby showing that existing machine learning approaches are in general vulnerable to systematic black-box attacks regardless of their structure.",2016.0,"Nicolas Papernot, P. Mcdaniel, I. Goodfellow"
d7d9107de19eba8228bc599f53f013245760caee,https://www.semanticscholar.org/paper/d7d9107de19eba8228bc599f53f013245760caee,A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection,"Cyber security is that the body of technologies, processes and practices designed to safeguard networks, computers, programs and knowledge from attack, harm or unauthorized access. During a computing context, the term security implies cyber security. This survey paper describes a targeted literature survey of machine learning (ML) and data processing (DM) strategies for cyber analytics in support of intrusion detection. This paper focuses totally on cyber intrusion detection as it applies to wired networks. With a wired network, associate oppose must experience many layers of defense at firewalls and operative systems, or gain physical access to the network. The quality of ML/DM algorithms is addressed, discussion of challenges for victimization ML/DM for cyber security is conferred, and some recommendations on once to use a given methodology area unit provided.",2017.0,"Lalu Banoth, M. S. Teja, M. Saicharan, N. Chandra"
efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea,https://www.semanticscholar.org/paper/efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea,Big Data and Machine Learning in Health Care.,"Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT",2018.0,"Andrew Beam, I. Kohane"
61306b52c2d292928f7cbb2f2ef5711d15a2566c,https://www.semanticscholar.org/paper/61306b52c2d292928f7cbb2f2ef5711d15a2566c,ABY3: A Mixed Protocol Framework for Machine Learning,"Machine learning is widely used to produce models for a range of applications and is increasingly offered as a service by major technology companies. However, the required massive data collection raises privacy concerns during both training and prediction stages. In this paper, we design and implement a general framework for privacy-preserving machine learning and use it to obtain new solutions for training linear regression, logistic regression and neural network models. Our protocols are in a three-server model wherein data owners secret share their data among three servers who train and evaluate models on the joint data using three-party computation (3PC). Our main contribution is a new and complete framework ($\textABY ^3$) for efficiently switching back and forth between arithmetic, binary, and Yao 3PC which is of independent interest. Many of the conversions are based on new techniques that are designed and optimized for the first time in this paper. We also propose new techniques for fixed-point multiplication of shared decimal values that extends beyond the three-party case, and customized protocols for evaluating piecewise polynomial functions. We design variants of each building block that is secure against \em malicious adversaries who deviate arbitrarily. We implement our system in C++. Our protocols are up to \em four orders of magnitude faster than the best prior work, hence significantly reducing the gap between privacy-preserving and plaintext training.",2018.0,"Payman Mohassel, Peter Rindal"
5416463537f8c6be1199951b4fd6f8d5dae14920,https://www.semanticscholar.org/paper/5416463537f8c6be1199951b4fd6f8d5dae14920,"Plans and Situated Actions: The Problem of Human-Machine Communication (Learning in Doing: Social,",Preface Acknowledgements 1. Introduction 2. Interactive artefacts 3. Plans 4. Situated actions 5. Communicative resources 6. Case and methods 7. Human-machine communication 8. Conclusion References Indices.,1987.0,L. Suchman
218062f45c15f39bc8f4fb2c930ddf20b5809b11,https://www.semanticscholar.org/paper/218062f45c15f39bc8f4fb2c930ddf20b5809b11,"Machine Learning Testing: Survey, Landscapes and Horizons","This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.",2019.0,"J Zhang, M. Harman, Lei Ma, Yang Liu"
b3ea2d9c8e5ea3b87ace121f0bece71565abc187,https://www.semanticscholar.org/paper/b3ea2d9c8e5ea3b87ace121f0bece71565abc187,Quantifying the Carbon Emissions of Machine Learning,"From an environmental standpoint, there are a few crucial aspects of training a neural network that have a major impact on the quantity of carbon that it emits. These factors include: the location of the server used for training and the energy grid that it uses, the length of the training procedure, and even the make and model of hardware on which the training takes place. In order to approximate these emissions, we present our Machine Learning Emissions Calculator, a tool for our community to better understand the environmental impact of training ML models. We accompany this tool with an explanation of the factors cited above, as well as concrete actions that individual practitioners and organizations can take to mitigate their carbon emissions.",2019.0,"Alexandre Lacoste, A. Luccioni, Victor Schmidt, Thomas Dandres"
6bf623e772d5634e33a035a3586dbab41e29c78b,https://www.semanticscholar.org/paper/6bf623e772d5634e33a035a3586dbab41e29c78b,AutoML-Zero: Evolving Machine Learning Algorithms From Scratch,"Machine learning research has advanced in multiple aspects, including model structures and learning methods. The effort to automate such research, known as AutoML, has also made significant progress. However, this progress has largely focused on the architecture of neural networks, where it has relied on sophisticated expert-designed layers as building blocks---or similarly restrictive search spaces. Our goal is to show that AutoML can go further: it is possible today to automatically discover complete machine learning algorithms just using basic mathematical operations as building blocks. We demonstrate this by introducing a novel framework that significantly reduces human bias through a generic search space. Despite the vastness of this space, evolutionary search can still discover two-layer neural networks trained by backpropagation. These simple neural networks can then be surpassed by evolving directly on tasks of interest, e.g. CIFAR-10 variants, where modern techniques emerge in the top algorithms, such as bilinear interactions, normalized gradients, and weight averaging. Moreover, evolution adapts algorithms to different task types: e.g., dropout-like techniques appear when little data is available. We believe these preliminary successes in discovering machine learning algorithms from scratch indicate a promising new direction for the field.",2020.0,"Esteban Real, Chen Liang, David R. So, Quoc V. Le"
7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7,https://www.semanticscholar.org/paper/7c63a6e6d3b31b14ae4236bfbd574ea37cab18a7,Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning,"Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.",2017.0,"T. Yarkoni, Jacob Westfall"
9d75cc322a4e06d0a3a868cb91b04219a289c12c,https://www.semanticscholar.org/paper/9d75cc322a4e06d0a3a868cb91b04219a289c12c,Machine Learning: An Applied Econometric Approach,"Machines are increasingly doing “intelligent” things. Face recognition algorithms use a large dataset of photos labeled as having a face or not to estimate a function that predicts the presence y of a face from pixels x. This similarity to econometrics raises questions: How do these new empirical tools fit with what we know? As empirical economists, how can we use them? We present a way of thinking about machine learning that gives it its own place in the econometric toolbox. Machine learning not only provides new tools, it solves a different problem. Specifically, machine learning revolves around the problem of prediction, while many economic applications revolve around parameter estimation. So applying machine learning to economics requires finding relevant tasks. Machine learning algorithms are now technically easy to use: you can download convenient packages in R or Python. This also raises the risk that the algorithms are applied naively or their output is misinterpreted. We hope to make them conceptually easier to use by providing a crisper understanding of how these algorithms work, where they excel, and where they can stumble—and thus where they can be most usefully applied.",2017.0,"S. Mullainathan, Jann Spiess"
ec6200bdcc23b79a71555962cde50306c4029f1a,https://www.semanticscholar.org/paper/ec6200bdcc23b79a71555962cde50306c4029f1a,Hyperparameter Optimization for Machine Learning Models Based on Bayesian Optimization,"Hyperparameters are important for machine learning algorithms since they directly control the behaviors of training algorithms and have a significant effect on the performance of machine learning models. Several techniques have been developed and successfully applied for certain application domains. However, this work demands professional knowledge and expert experience. And sometimes it has to resort to the brute-force search. Therefore, if an efficient hyperparameter optimization algorithm can be developed to optimize any given machine learning method, it will greatly improve the efficiency of machine learning. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters by Gaussian processes. In this way, the hyperparameter tuning problem can be abstracted as an optimization problem and Bayesian optimization is used to solve the problem. Bayesian optimization is based on the Bayesian theorem. It sets a prior over the optimization function and gathers the information from the previous sample to update the posterior of the optimization function. A utility function selects the next sample point to maximize the optimization function. Several experiments were conducted on standard test datasets. Experiment results show that the proposed method can find the best hyperparameters for the widely used machine learning models, such as the random forest algorithm and the neural networks, even multi-grained cascade forest under the consideration of time cost.",2019.0,"Jia Wu, Xiuyun Chen, H. Zhang, Li-Dong Xiong, Hang Lei, S. Deng"
e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772,https://www.semanticscholar.org/paper/e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772,Some Studies in Machine Learning Using the Game of Checkers,"Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called “alpha-beta” pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the",1967.0,A. Samuel
206261db1196e4e391ca42077f6fca6b3ece34d0,https://www.semanticscholar.org/paper/206261db1196e4e391ca42077f6fca6b3ece34d0,The Non-IID Data Quagmire of Decentralized Machine Learning,"Many large-scale machine learning (ML) applications need to perform decentralized learning over datasets generated at different devices and locations. Such datasets pose a significant challenge to decentralized learning because their different contexts result in significant data distribution skew across devices/locations. In this paper, we take a step toward better understanding this challenge by presenting a detailed experimental study of decentralized DNN training on a common type of data skew: skewed distribution of data labels across devices/locations. Our study shows that: (i) skewed data labels are a fundamental and pervasive problem for decentralized learning, causing significant accuracy loss across many ML applications, DNN models, training datasets, and decentralized learning algorithms; (ii) the problem is particularly challenging for DNN models with batch normalization; and (iii) the degree of data skew is a key determinant of the difficulty of the problem. Based on these findings, we present SkewScout, a system-level approach that adapts the communication frequency of decentralized learning algorithms to the (skew-induced) accuracy loss between data partitions. We also show that group normalization can recover much of the accuracy loss of batch normalization.",2019.0,"Kevin Hsieh, Amar Phanishayee, O. Mutlu, Phillip B. Gibbons"
8f8542a6aa8c76e8a4441d1ca722e230aa5d6c9e,https://www.semanticscholar.org/paper/8f8542a6aa8c76e8a4441d1ca722e230aa5d6c9e,Evaluating Differentially Private Machine Learning in Practice,"Differential privacy is a strong notion for privacy that can be used to prove formal guarantees, in terms of a privacy budget, $\epsilon$, about how much information is leaked by a mechanism. However, implementations of privacy-preserving machine learning often select large values of $\epsilon$ in order to get acceptable utility of the model, with little understanding of the impact of such choices on meaningful privacy. Moreover, in scenarios where iterative learning procedures are used, differential privacy variants that offer tighter analyses are used which appear to reduce the needed privacy budget but present poorly understood trade-offs between privacy and utility. In this paper, we quantify the impact of these choices on privacy in experiments with logistic regression and neural network models. Our main finding is that there is a huge gap between the upper bounds on privacy loss that can be guaranteed, even with advanced mechanisms, and the effective privacy loss that can be measured using current inference attacks. Current mechanisms for differentially private machine learning rarely offer acceptable utility-privacy trade-offs with guarantees for complex learning tasks: settings that provide limited accuracy loss provide meaningless privacy guarantees, and settings that provide strong privacy guarantees result in useless models. Code for the experiments can be found here: this https URL",2019.0,"Bargav Jayaraman, David E. Evans"
ede72940ae0246a292d644bd3c7e0ebf1e12a01a,https://www.semanticscholar.org/paper/ede72940ae0246a292d644bd3c7e0ebf1e12a01a,Opportunities and Challenges for Machine Learning in Materials Science,"Advances in machine learning have impacted myriad areas of materials science, such as the discovery of novel materials and the improvement of molecular simulations, with likely many more important developments to come. Given the rapid changes in this field, it is challenging to understand both the breadth of opportunities and the best practices for their use. In this review, we address aspects of both problems by providing an overview of the areas in which machine learning has recently had significant impact in materials science, and then we provide a more detailed discussion on determining the accuracy and domain of applicability of some common types of machine learning models. Finally, we discuss some opportunities and challenges for the materials community to fully utilize the capabilities of machine learning.",2020.0,"D. Morgan, R. Jacobs"
1696cbf7da0ee845c50591843993e6605adec177,https://www.semanticscholar.org/paper/1696cbf7da0ee845c50591843993e6605adec177,A few useful things to know about machine learning,"Tapping into the ""folk knowledge"" needed to advance machine learning applications.",2012.0,Pedro M. Domingos
2bac6b71d252f93c4841e325ca111f2752109931,https://www.semanticscholar.org/paper/2bac6b71d252f93c4841e325ca111f2752109931,Certified Data Removal from Machine Learning Models,"Good data stewardship requires removal of data at the request of the data's owner. This raises the question if and how a trained machine-learning model, which implicitly stores information about its training data, should be affected by such a removal request. Is it possible to ""remove"" data from a machine-learning model? We study this problem by defining certified removal: a very strong theoretical guarantee that a model from which data is removed cannot be distinguished from a model that never observed the data to begin with. We develop a certified-removal mechanism for linear classifiers and empirically study learning settings in which this mechanism is practical.",2019.0,"Chuan Guo, T. Goldstein, Awni Y. Hannun, L. Maaten"
bc386debfedf3b16101b6c3274485cea78ad6bb7,https://www.semanticscholar.org/paper/bc386debfedf3b16101b6c3274485cea78ad6bb7,Machine Learning for Precision Medicine.,"Precision medicine is an emerging approach to clinical research and patient care that focuses on understanding and treating disease by integrating multimodal or 'multi-omics' data from an individual to make patient-tailored decisions. With the large and complex datasets generated using precision medicine diagnostic approaches, novel techniques to process and understand these complex data were needed. At the same time, computer science has progressed rapidly to develop techniques that enable the storage, processing, and analysis of these complex datasets, a feat that traditional statistics and early computing technologies could not accomplish. Machine learning, a branch of artificial intelligence, is a computer science methodology that aims to identify complex patterns in data that can be used to make predictions or classifications on new unseen data or for advanced exploratory data analysis. Machine learning analysis of precision medicine's multimodal data allows for broad analysis of large datasets and ultimately a greater understanding of human health and disease. This review focuses on machine learning utilization for precision medicine's ""big data"", in the context of genetics, genomics, and beyond.",2020.0,"S. MacEachern, N. Forkert"
21dfbc88b21b27fe8a245ab1df98edd45f655ae7,https://www.semanticscholar.org/paper/21dfbc88b21b27fe8a245ab1df98edd45f655ae7,Machine Learning in Medicine,"Machine Learning in Medicine In this view of the future of medicine, patient–provider interactions are informed and supported by massive amounts of data from interactions with similar patients. The...",2019.0,"A. Rajkomar, Jeffrey Dean, I. Kohane"
fbf9812f29156024ec693b4633a21303eead309d,https://www.semanticscholar.org/paper/fbf9812f29156024ec693b4633a21303eead309d,Machine learning algorithm validation with a limited sample size,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.",2019.0,"A. Vabalas, E. Gowen, E. Poliakoff, A. Casson"
e7924a71ff89f37f66298a6b42bcd26fa7c0f33b,https://www.semanticscholar.org/paper/e7924a71ff89f37f66298a6b42bcd26fa7c0f33b,River: machine learning for streaming data in Python,"River is a machine learning library for dynamic data streams and continual learning. It provides multiple state-of-the-art learning methods, data generators/transformers, performance metrics and evaluators for different stream learning problems. It is the result from the merger of the two most popular packages for stream learning in Python: Creme and scikit-multiflow. River introduces a revamped architecture based on the lessons learnt from the seminal packages. River's ambition is to be the go-to library for doing machine learning on streaming data. Additionally, this open source package brings under the same umbrella a large community of practitioners and researchers. The source code is available at https://github.com/online-ml/river.",2020.0,"Jacob Montiel, Max Halford, S. Mastelini, Geoffrey Bolmier, Raphael Sourty, Robin Vaysse, Adil Zouitine, Heitor Murilo Gomes, Jesse Read, T. Abdessalem, A. Bifet"
bd9ecc05a12563445a2ef4fe758a39d7f2bcda0d,https://www.semanticscholar.org/paper/bd9ecc05a12563445a2ef4fe758a39d7f2bcda0d,DeltaGrad: Rapid retraining of machine learning models,"Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.",2020.0,"Yinjun Wu, Edgar Dobriban, S. Davidson"
4ec953de1331fe5f720320c9f2f82884b2512701,https://www.semanticscholar.org/paper/4ec953de1331fe5f720320c9f2f82884b2512701,Machine Learning Methods in Drug Discovery,"The advancements of information technology and related processing techniques have created a fertile base for progress in many scientific fields and industries. In the fields of drug discovery and development, machine learning techniques have been used for the development of novel drug candidates. The methods for designing drug targets and novel drug discovery now routinely combine machine learning and deep learning algorithms to enhance the efficiency, efficacy, and quality of developed outputs. The generation and incorporation of big data, through technologies such as high-throughput screening and high through-put computational analysis of databases used for both lead and target discovery, has increased the reliability of the machine learning and deep learning incorporated techniques. The use of these virtual screening and encompassing online information has also been highlighted in developing lead synthesis pathways. In this review, machine learning and deep learning algorithms utilized in drug discovery and associated techniques will be discussed. The applications that produce promising results and methods will be reviewed.",2020.0,"Lauv Patel, T. Shukla, Xiuzhen Huang, D. Ussery, Shanzhi Wang"
6324eb1efdaf74b5314fe88c068f6df68254597a,https://www.semanticscholar.org/paper/6324eb1efdaf74b5314fe88c068f6df68254597a,Machine Learning,"El sector salud tiene involucrado una gran cantidad de procesos y procedimientos generadores de todo tipo de información que en muchos casos no están disponibles de forma libre para los profesionales de diferentes áreas y en especial de las ciencias computacionales.¿Qué sucedería si toda esta información pudiera estar disponible? La medicina preventiva y predictiva podría desarrollarse con mayor rapidez, desarrollando modelos predictivos a través de algoritmos de Machine Learning, como apoyo a los profesionales de la salud en la toma de decisiones. Este artículo permite conocer la convergencia que existe entre la medicina predictiva y el Machine Learning, sus ventajas y los diferentes algoritmos de Machine Learning que se pueden aplicar dependiendo de los tipos de datos.",2022.0,"Luis Alfredo Blanquicett Benavides, Luis Fernando Murillo Fernandez"
7e7eb0f93c9550d7336f4bbfad5fe89604295705,https://www.semanticscholar.org/paper/7e7eb0f93c9550d7336f4bbfad5fe89604295705,Quantum Machine Learning in Feature Hilbert Spaces.,"A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.",2018.0,"M. Schuld, N. Killoran"
98a0ea52ccc31bacbb59c2e26ece9f7389abb00f,https://www.semanticscholar.org/paper/98a0ea52ccc31bacbb59c2e26ece9f7389abb00f,How the machine ‘thinks’: Understanding opacity in machine learning algorithms,"This article considers the issue of opacity as a problem for socially consequential mechanisms of classification and ranking, such as spam filters, credit card fraud detection, search engines, news trends, market segmentation and advertising, insurance or loan qualification, and credit scoring. These mechanisms of classification all frequently rely on computational algorithms, and in many cases on machine learning algorithms to do this work. In this article, I draw a distinction between three forms of opacity: (1) opacity as intentional corporate or state secrecy, (2) opacity as technical illiteracy, and (3) an opacity that arises from the characteristics of machine learning algorithms and the scale required to apply them usefully. The analysis in this article gets inside the algorithms themselves. I cite existing literatures in computer science, known industry practices (as they are publicly presented), and do some testing and manipulation of code as a form of lightweight code audit. I argue that recognizing the distinct forms of opacity that may be coming into play in a given application is a key to determining which of a variety of technical and non-technical solutions could help to prevent harm.",2016.0,J. Burrell
b293e4659e20815bcf0b6d31ce46b8bd9437c1fa,https://www.semanticscholar.org/paper/b293e4659e20815bcf0b6d31ce46b8bd9437c1fa,When Machine Learning Meets Privacy,"The newly emerged machine learning (e.g., deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning are still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This article surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning-aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field.",2020.0,"B. Liu, Ming Ding, Sina Shaham, W. Rahayu, F. Farokhi, Zihuai Lin"
638e41912f314c74436205aa8d332dca963ab1dc,https://www.semanticscholar.org/paper/638e41912f314c74436205aa8d332dca963ab1dc,Parameterized quantum circuits as machine learning models,"Hybrid quantum–classical systems make it possible to utilize existing quantum computers to their fullest extent. Within this framework, parameterized quantum circuits can be regarded as machine learning models with remarkable expressive power. This Review presents the components of these models and discusses their application to a variety of data-driven tasks, such as supervised learning and generative modeling. With an increasing number of experimental demonstrations carried out on actual quantum hardware and with software being actively developed, this rapidly growing field is poised to have a broad spectrum of real-world applications.",2019.0,"Marcello Benedetti, Erika Lloyd, Stefan H. Sack, Mattia Fiorentini"
d75356e2bf674902a06a14bb55d18ee88af5b4bb,https://www.semanticscholar.org/paper/d75356e2bf674902a06a14bb55d18ee88af5b4bb,Machine Learning Methods That Economists Should Know About,"We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.",2019.0,"S. Athey, G. Imbens"
236dfdeb4511754cf71ba220ac569b11973502cd,https://www.semanticscholar.org/paper/236dfdeb4511754cf71ba220ac569b11973502cd,Machine Learning and Deep Learning Methods for Intrusion Detection Systems: A Survey,"Networks play important roles in modern life, and cyber security has become a vital research area. An intrusion detection system (IDS) which is an important cyber security technique, monitors the state of software and hardware running in the network. Despite decades of development, existing IDSs still face challenges in improving the detection accuracy, reducing the false alarm rate and detecting unknown attacks. To solve the above problems, many researchers have focused on developing IDSs that capitalize on machine learning methods. Machine learning methods can automatically discover the essential differences between normal data and abnormal data with high accuracy. In addition, machine learning methods have strong generalizability, so they are also able to detect unknown attacks. Deep learning is a branch of machine learning, whose performance is remarkable and has become a research hotspot. This survey proposes a taxonomy of IDS that takes data objects as the main dimension to classify and summarize machine learning-based and deep learning-based IDS literature. We believe that this type of taxonomy framework is fit for cyber security researchers. The survey first clarifies the concept and taxonomy of IDSs. Then, the machine learning algorithms frequently used in IDSs, metrics, and benchmark datasets are introduced. Next, combined with the representative literature, we take the proposed taxonomic system as a baseline and explain how to solve key IDS issues with machine learning and deep learning techniques. Finally, challenges and future developments are discussed by reviewing recent representative studies.",2019.0,"Hongyu Liu, Bo Lang"
f9a855ae59579d16dca6a5133cd8daddd3305582,https://www.semanticscholar.org/paper/f9a855ae59579d16dca6a5133cd8daddd3305582,A Survey on Distributed Machine Learning,"The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.",2019.0,"Joost Verbraeken, Matthijs Wolting, J. Katzy, Jeroen Kloppenburg, Tim Verbelen, Jan S. Rellermeyer"
2b49156cf855dbb39768ae0ba7d7cb9263d17e5c,https://www.semanticscholar.org/paper/2b49156cf855dbb39768ae0ba7d7cb9263d17e5c,Informed Machine Learning – A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems,"Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.",2019.0,"Laura von Rueden, S. Mayer, Katharina Beckh, B. Georgiev, Sven Giesselbach, R. Heese, Birgit Kirsch, Julius Pfrommer, Annika Pick, Rajkumar Ramamurthy, Michal Walczak, J. Garcke, C. Bauckhage, Jannis Schuecker"
8db8166249dfb94dd8d52f88d27917b5755ae049,https://www.semanticscholar.org/paper/8db8166249dfb94dd8d52f88d27917b5755ae049,A Quick Review of Machine Learning Algorithms,Machine learning is predominantly an area of Artificial Intelligence which has been a key component of digitalization solutions that has caught major attention in the digital arena. In this paper author intends to do a brief review of various machine learning algorithms which are most frequently used and therefore are the most popular ones. The author intends to highlight the merits and demerits of the machine learning algorithms from their application perspective to aid in an informed decision making towards selecting the appropriate learning algorithm to meet the specific requirement of the application.,2019.0,Susmita Ray
4490ffac416692ab827c5a30e5f3a4b4fd6be949,https://www.semanticscholar.org/paper/4490ffac416692ab827c5a30e5f3a4b4fd6be949,Explainable machine learning in deployment,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.",2019.0,"Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, R. Puri, J. Moura, P. Eckersley"
a42e380e1b8aafecb3b1e338a8a9a579c6a5a40f,https://www.semanticscholar.org/paper/a42e380e1b8aafecb3b1e338a8a9a579c6a5a40f,A Survey of Machine Learning Techniques Applied to Software Defined Networking (SDN): Research Issues and Challenges,"In recent years, with the rapid development of current Internet and mobile communication technologies, the infrastructure, devices and resources in networking systems are becoming more complex and heterogeneous. In order to efficiently organize, manage, maintain and optimize networking systems, more intelligence needs to be deployed. However, due to the inherently distributed feature of traditional networks, machine learning techniques are hard to be applied and deployed to control and operate networks. Software defined networking (SDN) brings us new chances to provide intelligence inside the networks. The capabilities of SDN (e.g., logically centralized control, global view of the network, software-based traffic analysis, and dynamic updating of forwarding rules) make it easier to apply machine learning techniques. In this paper, we provide a comprehensive survey on the literature involving machine learning algorithms applied to SDN. First, the related works and background knowledge are introduced. Then, we present an overview of machine learning algorithms. In addition, we review how machine learning algorithms are applied in the realm of SDN, from the perspective of traffic classification, routing optimization, quality of service/quality of experience prediction, resource management and security. Finally, challenges and broader perspectives are discussed.",2019.0,"Jun-feng Xie, F. Yu, Tao Huang, Renchao Xie, Jiang Liu, Chen-meng Wang, Yun-jie Liu"
b5461f9c5d65e87561e00848921ee797902dae14,https://www.semanticscholar.org/paper/b5461f9c5d65e87561e00848921ee797902dae14,Causality for Machine Learning,"Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. 
This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.",2019.0,B. Scholkopf
4eca52f892f288c0b33b74aa4cfed56ed968fb4e,https://www.semanticscholar.org/paper/4eca52f892f288c0b33b74aa4cfed56ed968fb4e,Explainable Machine Learning for Scientific Insights and Discoveries,"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.",2019.0,"R. Roscher, B. Bohn, Marco F. Duarte, J. Garcke"
e718828e8f776d9f80daa3f8e0af6895f5d34c44,https://www.semanticscholar.org/paper/e718828e8f776d9f80daa3f8e0af6895f5d34c44,Adversarial attacks on medical machine learning,"Emerging vulnerabilities demand new conversations With public and academic attention increasingly focused on the new role of machine learning in the health information economy, an unusual and no-longer-esoteric category of vulnerabilities in machine-learning systems could prove important. These vulnerabilities allow a small, carefully designed change in how inputs are presented to a system to completely alter its output, causing it to confidently arrive at manifestly wrong conclusions. These advanced techniques to subvert otherwise-reliable machine-learning systems—so-called adversarial attacks—have, to date, been of interest primarily to computer science researchers (1). However, the landscape of often-competing interests within health care, and billions of dollars at stake in systems' outputs, implies considerable problems. We outline motivations that various players in the health care system may have to use adversarial attacks and begin a discussion of what to do about them. Far from discouraging continued innovation with medical machine learning, we call for active engagement of medical, technical, legal, and ethical experts in pursuit of efficient, broadly available, and effective health care that machine learning will enable.",2019.0,"S. G. Finlayson, John Bowers, Joichi Ito, Jonathan Zittrain, Andrew Beam, I. Kohane"
998039a4876edc440e0cabb0bc42239b0eb29644,https://www.semanticscholar.org/paper/998039a4876edc440e0cabb0bc42239b0eb29644,Tackling Climate Change with Machine Learning,"Climate change is one of the greatest challenges facing humanity, and we, as machine learning (ML) experts, may wonder how we can help. Here we describe how ML can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by ML, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the ML community to join the global effort against climate change.",2019.0,"D. Rolnick, P. Donti, L. Kaack, K. Kochanski, Alexandre Lacoste, K. Sankaran, A. Ross, Nikola Milojevic-Dupont, Natasha Jaques, Anna Waldman-Brown, A. Luccioni, Tegan Maharaj, Evan D. Sherwin, S. Mukkavilli, Konrad Paul Kording, Carla P. Gomes, Andrew Y. Ng, D. Hassabis, John C. Platt, F. Creutzig, J. Chayes, Yoshua Bengio"
4f975da00a5b2a2f7236e34edcb7274e5fdab937,https://www.semanticscholar.org/paper/4f975da00a5b2a2f7236e34edcb7274e5fdab937,Combining satellite imagery and machine learning to predict poverty,"Measuring consumption and wealth remotely Nighttime lighting is a rough proxy for economic wealth, and nighttime maps of the world show that many developing countries are sparsely illuminated. Jean et al. combined nighttime maps with high-resolution daytime satellite images (see the Perspective by Blumenstock). With a bit of machine-learning wizardry, the combined images can be converted into accurate estimates of household consumption and assets, both of which are hard to measure in poorer countries. Furthermore, the night- and day-time data are publicly available and nonproprietary. Science, this issue p. 790; see also p. 753 Satellites collect data that can be used to measure income and wealth. Reliable data on economic livelihoods remain scarce in the developing world, hampering efforts to study these outcomes and to design policies that improve them. Here we demonstrate an accurate, inexpensive, and scalable method for estimating consumption expenditure and asset wealth from high-resolution satellite imagery. Using survey and satellite data from five African countries—Nigeria, Tanzania, Uganda, Malawi, and Rwanda—we show how a convolutional neural network can be trained to identify image features that can explain up to 75% of the variation in local-level economic outcomes. Our method, which requires only publicly available data, could transform efforts to track and target poverty in developing countries. It also demonstrates how powerful machine learning techniques can be applied in a setting with limited training data, suggesting broad potential application across many scientific domains.",2016.0,"Neal Jean, M. Burke, Sang Michael Xie, W. Davis, D. Lobell, Stefano Ermon"
7d291d5fca0e9cd9e0ed72fb6f82289a197f7f02,https://www.semanticscholar.org/paper/7d291d5fca0e9cd9e0ed72fb6f82289a197f7f02,Machine Learning and Deep Learning,"Now-a-days artificial intelligence has become an asset for engineering and experimental studies, just like statistics and calculus. Data science is a growing field for researchers and artificial intelligence, machine learning and deep learning are roots of it. This paper describes the relation between these roots of data science. There is a need of machine learning if any kind of analysis is to be performed. This study describes machine learning from the scratch. It also focuses on Deep Learning. Deep learning can also be known as new trend of machine learning. This paper gives a light on basic architecture of Deep learning. A comparative study of machine learning and deep learning is also given in the paper and allows researcher to have a broad view on these techniques so that they can understand which one will be preferable solution for a particular problem.",2019.0,Dietmar P. F. Möller
3119ea9c7ad7a5e044dc7c267329a4bbf00d0158,https://www.semanticscholar.org/paper/3119ea9c7ad7a5e044dc7c267329a4bbf00d0158,A Survey of Optimization Methods From a Machine Learning Perspective,"Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.",2019.0,"Shiliang Sun, Zehui Cao, Han Zhu, Jing Zhao"
83e89d56d0d0e1dfd8b52213e6cc2e191aaaf34b,https://www.semanticscholar.org/paper/83e89d56d0d0e1dfd8b52213e6cc2e191aaaf34b,Machine Learning on Graphs: A Model and Comprehensive Taxonomy,"There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.",2020.0,"Ines Chami, Sami Abu-El-Haija, Bryan Perozzi, Christopher Ré, K. Murphy"
b4b1cbd74029f46ef9b462290a46111217552761,https://www.semanticscholar.org/paper/b4b1cbd74029f46ef9b462290a46111217552761,Understanding the Effect of Accuracy on Trust in Machine Learning Models,"We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.",2019.0,"Ming Yin, Jennifer Wortman Vaughan, Hanna M. Wallach"
d294d5246e0dd8ed8bd9ec9d24a01fd4ece4fb3c,https://www.semanticscholar.org/paper/d294d5246e0dd8ed8bd9ec9d24a01fd4ece4fb3c,A Detailed Investigation and Analysis of Using Machine Learning Techniques for Intrusion Detection,"Intrusion detection is one of the important security problems in todays cyber world. A significant number of techniques have been developed which are based on machine learning approaches. However, they are not very successful in identifying all types of intrusions. In this paper, a detailed investigation and analysis of various machine learning techniques have been carried out for finding the cause of problems associated with various machine learning techniques in detecting intrusive activities. Attack classification and mapping of the attack features is provided corresponding to each attack. Issues which are related to detecting low-frequency attacks using network attack dataset are also discussed and viable methods are suggested for improvement. Machine learning techniques have been analyzed and compared in terms of their detection capability for detecting the various category of attacks. Limitations associated with each category of them are also discussed. Various data mining tools for machine learning have also been included in the paper. At the end, future directions are provided for attack detection using machine learning techniques.",2019.0,"Preeti Mishra, V. Varadharajan, U. Tupakula, E. Pilli"
89f88f324bb3775f63f87cec90a4283a3522ab44,https://www.semanticscholar.org/paper/89f88f324bb3775f63f87cec90a4283a3522ab44,Machine learning-assisted directed protein evolution with combinatorial libraries,"Significance Proteins often function poorly when used outside their natural contexts; directed evolution can be used to engineer them to be more efficient in new roles. We propose that the expense of experimentally testing a large number of protein variants can be decreased and the outcome can be improved by incorporating machine learning with directed evolution. Simulations on an empirical fitness landscape demonstrate that the expected performance improvement is greater with this approach. Machine learning-assisted directed evolution from a single parent produced enzyme variants that selectively synthesize the enantiomeric products of a new-to-nature chemical transformation. By exploring multiple mutations simultaneously, machine learning efficiently navigates large regions of sequence space to identify improved proteins and also produces diverse solutions to engineering problems. To reduce experimental effort associated with directed protein evolution and to explore the sequence space encoded by mutating multiple positions simultaneously, we incorporate machine learning into the directed evolution workflow. Combinatorial sequence space can be quite expensive to sample experimentally, but machine-learning models trained on tested variants provide a fast method for testing sequence space computationally. We validated this approach on a large published empirical fitness landscape for human GB1 binding protein, demonstrating that machine learning-guided directed evolution finds variants with higher fitness than those found by other directed evolution approaches. We then provide an example application in evolving an enzyme to produce each of the two possible product enantiomers (i.e., stereodivergence) of a new-to-nature carbene Si–H insertion reaction. The approach predicted libraries enriched in functional enzymes and fixed seven mutations in two rounds of evolution to identify variants for selective catalysis with 93% and 79% ee (enantiomeric excess). By greatly increasing throughput with in silico modeling, machine learning enhances the quality and diversity of sequence solutions for a protein engineering problem.",2019.0,"Zachary Wu, S. Kan, Russell D. Lewis, Bruce J. Wittmann, F. Arnold"
c7b08c2e69a338e8d0c8444ce081b51caa50b273,https://www.semanticscholar.org/paper/c7b08c2e69a338e8d0c8444ce081b51caa50b273,Monte Carlo Gradient Estimation in Machine Learning,"This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.",2019.0,"S. Mohamed, Mihaela Rosca, Michael Figurnov, A. Mnih"
37f239603ce77e8f10be255be0a2cff7070122ad,https://www.semanticscholar.org/paper/37f239603ce77e8f10be255be0a2cff7070122ad,Enhancing gravitational-wave science with machine learning,"Machine learning has emerged as a popular and powerful approach for solving problems in astrophysics. We review applications of machine learning techniques for the analysis of ground-based gravitational-wave (GW) detector data. Examples include techniques for improving the sensitivity of Advanced Laser Interferometer GW Observatory and Advanced Virgo GW searches, methods for fast measurements of the astrophysical parameters of GW sources, and algorithms for reduction and characterization of non-astrophysical detector noise. These applications demonstrate how machine learning techniques may be harnessed to enhance the science that is possible with current and future GW detectors.",2020.0,"E. Cuoco, J. Powell, M. Cavaglià, K. Ackley, M. Bejger, C. Chatterjee, M. Coughlin, S. Coughlin, P. Easter, R. Essick, H. Gabbard, Timothy D. Gebhard, Shaon Ghosh, L. Haegel, A. Iess, D. Keitel, Z. Márka, S. Márka, F. Morawski, Tri Nguyen, R. Ormiston, M. Pürrer, M. Razzano, K. Staats, G. Vajente, Daniel Williams"
3df952d4a724655f7520ff95d4b2cef90fff0cae,https://www.semanticscholar.org/paper/3df952d4a724655f7520ff95d4b2cef90fff0cae,Techniques for interpretable machine learning,Uncovering the mysterious ways machine learning models make decisions.,2018.0,"Mengnan Du, Ninghao Liu, Xia Hu"
97f4a6f87f258053f2677504647696f1803c6794,https://www.semanticscholar.org/paper/97f4a6f87f258053f2677504647696f1803c6794,How to Read Articles That Use Machine Learning: Users' Guides to the Medical Literature.,"In recent years, many new clinical diagnostic tools have been developed using complicated machine learning methods. Irrespective of how a diagnostic tool is derived, it must be evaluated using a 3-step process of deriving, validating, and establishing the clinical effectiveness of the tool. Machine learning-based tools should also be assessed for the type of machine learning model used and its appropriateness for the input data type and data set size. Machine learning models also generally have additional prespecified settings called hyperparameters, which must be tuned on a data set independent of the validation set. On the validation set, the outcome against which the model is evaluated is termed the reference standard. The rigor of the reference standard must be assessed, such as against a universally accepted gold standard or expert grading.",2019.0,"Yun Liu, Po-Hsuan Cameron Chen, Jonathan Krause, L. Peng"
3784b73a1f392160523400ec0309191c0a96d86f,https://www.semanticscholar.org/paper/3784b73a1f392160523400ec0309191c0a96d86f,MLlib: Machine Learning in Apache Spark,"Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.",2015.0,"Xiangrui Meng, Joseph K. Bradley, Burak Yavuz, Evan R. Sparks, S. Venkataraman, Davies Liu, Jeremy Freeman, D. Tsai, Manish Amde, Sean Owen, Doris Xin, Reynold Xin, M. Franklin, R. Zadeh, M. Zaharia, Ameet Talwalkar"
efd7b7aafeb83b8a8d6fd90a35d6fb6a62f5f695,https://www.semanticscholar.org/paper/efd7b7aafeb83b8a8d6fd90a35d6fb6a62f5f695,Manipulating Machine Learning: Poisoning Attacks and Countermeasures for Regression Learning,"As machine learning becomes widely used for automated decisions, attackers have strong incentives to manipulate the results and models generated by machine learning algorithms. In this paper, we perform the first systematic study of poisoning attacks and their countermeasures for linear regression models. In poisoning attacks, attackers deliberately influence the training data to manipulate the results of a predictive model. We propose a theoretically-grounded optimization framework specifically designed for linear regression and demonstrate its effectiveness on a range of datasets and models. We also introduce a fast statistical attack that requires limited knowledge of the training process. Finally, we design a new principled defense method that is highly resilient against all poisoning attacks. We provide formal guarantees about its convergence and an upper bound on the effect of poisoning attacks when the defense is deployed. We evaluate extensively our attacks and defenses on three realistic datasets from health care, loan assessment, and real estate domains.",2018.0,"Matthew Jagielski, Alina Oprea, B. Biggio, Chang Liu, C. Nita-Rotaru, Bo Li"
eed9fa4483cab37eacd59db0fac4b1441431ee85,https://www.semanticscholar.org/paper/eed9fa4483cab37eacd59db0fac4b1441431ee85,Tensor Decomposition for Signal Processing and Machine Learning,"Tensors or <italic>multiway arrays</italic> are functions of three or more indices <inline-formula> <tex-math notation=""LaTeX"">$(i,j,k,\ldots)$</tex-math></inline-formula>—similar to matrices (two-way arrays), which are functions of two indices <inline-formula><tex-math notation=""LaTeX"">$(r,c)$</tex-math></inline-formula> for (row, column). Tensors have a rich history, stretching over almost a century, and touching upon numerous disciplines; but they have only recently become ubiquitous in signal and data analytics at the confluence of signal processing, statistics, data mining, and machine learning. This overview article aims to provide a good starting point for researchers and practitioners interested in learning about and working with tensors. As such, it focuses on fundamentals and motivation (using various application examples), aiming to strike an appropriate balance of breadth <italic>and depth</italic> that will enable someone having taken first graduate courses in matrix algebra and probability to get started doing research and/or developing tensor algorithms and software. Some background in applied optimization is useful but not strictly required. The material covered includes tensor rank and rank decomposition; basic tensor factorization models and their relationships and properties (including fairly good coverage of identifiability); broad coverage of algorithms ranging from alternating optimization to stochastic gradient; statistical performance analysis; and applications ranging from source separation to collaborative filtering, mixture and topic modeling, classification, and multilinear subspace learning.",2016.0,"N. Sidiropoulos, L. D. Lathauwer, Xiao Fu, Kejun Huang, E. Papalexakis, C. Faloutsos"
033f25ad905ef2ed32a8331cf38b83953ff15922,https://www.semanticscholar.org/paper/033f25ad905ef2ed32a8331cf38b83953ff15922,A Review of Relational Machine Learning for Knowledge Graphs,"Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.",2015.0,"Maximilian Nickel, K. Murphy, Volker Tresp, E. Gabrilovich"
161ce338538f94b0b9be51ae2336db0aa4b012e5,https://www.semanticscholar.org/paper/161ce338538f94b0b9be51ae2336db0aa4b012e5,Potential Biases in Machine Learning Algorithms Using Electronic Health Record Data,"A promise of machine learning in health care is the avoidance of biases in diagnosis and treatment; a computer algorithm could objectively synthesize and interpret the data in the medical record. Integration of machine learning with clinical decision support tools, such as computerized alerts or diagnostic support, may offer physicians and others who provide health care targeted and timely information that can improve clinical decisions. Machine learning algorithms, however, may also be subject to biases. The biases include those related to missing data and patients not identified by algorithms, sample size and underestimation, and misclassification and measurement error. There is concern that biases and deficiencies in the data used by machine learning algorithms may contribute to socioeconomic disparities in health care. This Special Communication outlines the potential biases that may be introduced into machine learning–based clinical decision support tools that use electronic health record data and proposes potential solutions to the problems of overreliance on automation, algorithms based on biased data, and algorithms that do not provide information that is clinically meaningful. Existing health care disparities should not be amplified by thoughtless or excessive reliance on machines.",2018.0,"M. Gianfrancesco, S. Tamang, J. Yazdany, G. Schmajuk"
b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0,https://www.semanticscholar.org/paper/b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0,Machine learning for molecular simulation,"Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation. Expected final online publication date for the Annual Review of Physical Chemistry, Volume 71 is April 20, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",2019.0,"F. Noé, A. Tkatchenko, K. Müller, C. Clementi"
adfc508b9b3d4fc3903aa383a290dc68fb8bbe5a,https://www.semanticscholar.org/paper/adfc508b9b3d4fc3903aa383a290dc68fb8bbe5a,Implementing Machine Learning in Health Care - Addressing Ethical Challenges.,Implementing Machine Learning in Health Care We need to consider the ethical challenges inherent in implementing machine learning in health care if its benefits are to be realized. Some of these ch...,2018.0,"Danton Char, N. Shah, D. Magnus"
5327bb691a1c63791a06de2d3f0478e47785add5,https://www.semanticscholar.org/paper/5327bb691a1c63791a06de2d3f0478e47785add5,Predicting Diabetes Mellitus With Machine Learning Techniques,"Diabetes mellitus is a chronic disease characterized by hyperglycemia. It may cause many complications. According to the growing morbidity in recent years, in 2040, the world’s diabetic patients will reach 642 million, which means that one of the ten adults in the future is suffering from diabetes. There is no doubt that this alarming figure needs great attention. With the rapid development of machine learning, machine learning has been applied to many aspects of medical health. In this study, we used decision tree, random forest and neural network to predict diabetes mellitus. The dataset is the hospital physical examination data in Luzhou, China. It contains 14 attributes. In this study, five-fold cross validation was used to examine the models. In order to verity the universal applicability of the methods, we chose some methods that have the better performance to conduct independent test experiments. We randomly selected 68994 healthy people and diabetic patients’ data, respectively as training set. Due to the data unbalance, we randomly extracted 5 times data. And the result is the average of these five experiments. In this study, we used principal component analysis (PCA) and minimum redundancy maximum relevance (mRMR) to reduce the dimensionality. The results showed that prediction with random forest could reach the highest accuracy (ACC = 0.8084) when all the attributes were used.",2018.0,"Q. Zou, Kaiyang Qu, Ya-ling Luo, Dehui Yin, Y. Ju, Hua Tang"
fad1bd501aa769f7701c1016f8a4d1473ca77601,https://www.semanticscholar.org/paper/fad1bd501aa769f7701c1016f8a4d1473ca77601,"Machine Learning, Neural and Statistical Classification",Survey of previous comparisons and theoretical work descriptions of methods dataset descriptions criteria for comparison and methodology (including validation) empirical results machine learning on machine learning.,2009.0,"D. Michie, D. Spiegelhalter, Charles C. Taylor"
32c709cf5d6ba1b5a729b4871c3129bb1bf578bf,https://www.semanticscholar.org/paper/32c709cf5d6ba1b5a729b4871c3129bb1bf578bf,Quantum machine learning in high energy physics,"Machine learning has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. An interesting question is whether there are ways to apply quantum machine learning to HEP. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.",2020.0,"W. Guan, G. Perdue, Arthur Pesah, M. Schuld, K. Terashi, S. Vallecorsa, J. Vlimant"
adade3149b2a6177296de352f003471eefa958b8,https://www.semanticscholar.org/paper/adade3149b2a6177296de352f003471eefa958b8,The Impact of Machine Learning on Economics,"This paper provides an assessment of the early contributions of machine learning to economics, as well as predictions about its future contributions. It begins by brieﬂy overviewing some themes from the literature on machine learning, and then draws some contrasts with traditional approaches to estimating the impact of counterfactual policies in economics. Next, we review some of the initial “oﬀ-the-shelf” applications of machine learning to economics, including applications in analyzing text and images. We then describe new types of questions that have been posed surrounding the application of machine learning to policy problems, including “prediction policy problems,” as well as considerations of fairness and manipulability. Next, we brieﬂy review of some of the emerging econometric literature combining machine learning and causal inference. Finally, we overview a set of predictions about the future impact of machine learning on economics.",2018.0,S. Athey
4ce2f55585f3156e332721b8ab4f449389dc2a3c,https://www.semanticscholar.org/paper/4ce2f55585f3156e332721b8ab4f449389dc2a3c,Hidden stratification causes clinically meaningful failures in machine learning for medical imaging,"Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.",2019.0,"Luke Oakden-Rayner, Jared A. Dunnmon, G. Carneiro, Christopher Ré"
792d90c2ed5ebbe050bd80b9c865dc416b574c09,https://www.semanticscholar.org/paper/792d90c2ed5ebbe050bd80b9c865dc416b574c09,Machine Learning–Based Model for Prediction of Outcomes in Acute Stroke,"Background and Purpose— The prediction of long-term outcomes in ischemic stroke patients may be useful in treatment decisions. Machine learning techniques are being increasingly adapted for use in the medical field because of their high accuracy. This study investigated the applicability of machine learning techniques to predict long-term outcomes in ischemic stroke patients. Methods— This was a retrospective study using a prospective cohort that enrolled patients with acute ischemic stroke. Favorable outcome was defined as modified Rankin Scale score 0, 1, or 2 at 3 months. We developed 3 machine learning models (deep neural network, random forest, and logistic regression) and compared their predictability. To evaluate the accuracy of the machine learning models, we also compared them to the Acute Stroke Registry and Analysis of Lausanne (ASTRAL) score. Results— A total of 2604 patients were included in this study, and 2043 (78%) of them had favorable outcomes. The area under the curve for the deep neural network model was significantly higher than that of the ASTRAL score (0.888 versus 0.839; P<0.001), while the areas under the curves of the random forest (0.857; P=0.136) and logistic regression (0.849; P=0.413) models were not significantly higher than that of the ASTRAL score. Using only the 6 variables that are used for the ASTRAL score, the performance of the machine learning models did not significantly differ from that of the ASTRAL score. Conclusions— Machine learning algorithms, particularly the deep neural network, can improve the prediction of long-term outcomes in ischemic stroke patients.",2019.0,"Joonnyung Heo, Jihoon G. Yoon, Hyungjong Park, Y. Kim, H. Nam, J. Heo"
f15367ed93c3505b1d62d802f3f4b769ae0f4ba5,https://www.semanticscholar.org/paper/f15367ed93c3505b1d62d802f3f4b769ae0f4ba5,Machine learning for neuroimaging with scikit-learn,"Statistical machine learning methods are increasingly used for neuroimaging data analysis. Their main virtue is their ability to model high-dimensional datasets, e.g., multivariate analysis of activation images or resting-state time series. Supervised learning is typically used in decoding or encoding settings to relate brain images to behavioral or clinical observations, while unsupervised learning can uncover hidden structures in sets of images (e.g., resting state functional MRI) or find sub-populations in large cohorts. By considering different functional neuroimaging applications, we illustrate how scikit-learn, a Python machine learning library, can be used to perform some key analysis steps. Scikit-learn contains a very large set of statistical learning algorithms, both supervised and unsupervised, and its application to neuroimaging data provides a versatile tool to study the brain.",2014.0,"Alexandre Abraham, Fabian Pedregosa, Michael Eickenberg, Philippe Gervais, A. Mueller, Jean Kossaifi, Alexandre Gramfort, B. Thirion, G. Varoquaux"
88a97c8ef539589c55a6fe869c243792e470d6a3,https://www.semanticscholar.org/paper/88a97c8ef539589c55a6fe869c243792e470d6a3,Applied Machine Learning at Facebook: A Datacenter Infrastructure Perspective,"Machine learning sits at the core of many essential products and services at Facebook. This paper describes the hardware and software infrastructure that supports machine learning at global scale. Facebook's machine learning workloads are extremely diverse: services require many different types of models in practice. This diversity has implications at all layers in the system stack. In addition, a sizable fraction of all data stored at Facebook flows through machine learning pipelines, presenting significant challenges in delivering data to high-performance distributed training flows. Computational requirements are also intense, leveraging both GPU and CPU platforms for training and abundant CPU capacity for real-time inference. Addressing these and other emerging challenges continues to require diverse efforts that span machine learning algorithms, software, and hardware design.",2018.0,"K. Hazelwood, Sarah Bird, D. Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, P. Noordhuis, M. Smelyanskiy, Liang Xiong, Xiaodong Wang"
eef183687fab4d762a381f2e80e357e08e923f0a,https://www.semanticscholar.org/paper/eef183687fab4d762a381f2e80e357e08e923f0a,"Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning","The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.",2018.0,S. Raschka
4f2baff3195b6fc43a38e3e869496dab9fe9dbc3,https://www.semanticscholar.org/paper/4f2baff3195b6fc43a38e3e869496dab9fe9dbc3,Delayed Impact of Fair Machine Learning,"Static classification has been the predominant focus of the study of fairness in machine learning. While most models do not consider how decisions change populations over time, it is conventional wisdom that fairness criteria promote the long-term well-being of groups they aim to protect. This work studies the interaction of static fairness criteria with temporal indicators of well-being. We show a simple one-step feedback model in which common criteria do not generally promote improvement over time, and may in fact cause harm. Our results highlight the importance of temporal modeling in the evaluation of fairness criteria, suggesting a range of new challenges and trade-offs.",2018.0,"Lydia T. Liu, Sarah Dean, Esther Rolf, Max Simchowitz, Moritz Hardt"
29524f145db94cab2336da99f157e869d805dead,https://www.semanticscholar.org/paper/29524f145db94cab2336da99f157e869d805dead,SoK: Security and Privacy in Machine Learning,"Advances in machine learning (ML) in recent years have enabled a dizzying array of applications such as data analytics, autonomous systems, and security diagnostics. ML is now pervasive—new systems and models are being deployed in every domain imaginable, leading to widespread deployment of software based inference and decision making. There is growing recognition that ML exposes new vulnerabilities in software systems, yet the technical community's understanding of the nature and extent of these vulnerabilities remains limited. We systematize findings on ML security and privacy, focusing on attacks identified on these systems and defenses crafted to date.We articulate a comprehensive threat model for ML, and categorize attacks and defenses within an adversarial framework. Key insights resulting from works both in the ML and security communities are identified and the effectiveness of approaches are related to structural elements of ML algorithms and the data used to train them. In particular, it is apparent that constructing a theoretical understanding of the sensitivity of modern ML algorithms to the data they analyze, à la PAC theory, will foster a science of security and privacy in ML.",2018.0,"Nicolas Papernot, P. Mcdaniel, Arunesh Sinha, Michael P. Wellman"
39361b3507c9f8b0a97780568b645f80a208d78a,https://www.semanticscholar.org/paper/39361b3507c9f8b0a97780568b645f80a208d78a,Machine Learning and Deep Learning Methods for Cybersecurity,"With the development of the Internet, cyber-attacks are changing rapidly and the cyber security situation is not optimistic. This survey report describes key literature surveys on machine learning (ML) and deep learning (DL) methods for network analysis of intrusion detection and provides a brief tutorial description of each ML/DL method. Papers representing each method were indexed, read, and summarized based on their temporal or thermal correlations. Because data are so important in ML/DL methods, we describe some of the commonly used network datasets used in ML/DL, discuss the challenges of using ML/DL for cybersecurity and provide suggestions for research directions.",2018.0,"Yang Xin, Lingshuang Kong, Zhi Liu, Yuling Chen, Yanmiao Li, Hongliang Zhu, Mingcheng Gao, Haixia Hou, Chunhua Wang"
2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0,https://www.semanticscholar.org/paper/2b9c0e4d1d473aadbe1c2a76f75bc02bfa6416b0,Extreme learning machine: a new learning scheme of feedforward neural networks,"It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradient-based learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on real-world benchmarking function approximation and classification problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.",2004.0,"G. Huang, Q. Zhu, C. Siew"
d5125164c7fec457d1442cce807a3436841715d0,https://www.semanticscholar.org/paper/d5125164c7fec457d1442cce807a3436841715d0,Machine Learning Approaches for Clinical Psychology and Psychiatry.,"Machine learning approaches for clinical psychology and psychiatry explicitly focus on learning statistical functions from multidimensional data sets to make generalizable predictions about individuals. The goal of this review is to provide an accessible understanding of why this approach is important for future practice given its potential to augment decisions associated with the diagnosis, prognosis, and treatment of people suffering from mental illness using clinical and biological data. To this end, the limitations of current statistical paradigms in mental health research are critiqued, and an introduction is provided to critical machine learning methods used in clinical studies. A selective literature review is then presented aiming to reinforce the usefulness of machine learning methods and provide evidence of their potential. In the context of promising initial results, the current limitations of machine learning approaches are addressed, and considerations for future clinical translation are outlined.",2018.0,"D. Dwyer, P. Falkai, N. Koutsouleris"
82a5a84528a7ca0409f75e2211a3b33a217e9bac,https://www.semanticscholar.org/paper/82a5a84528a7ca0409f75e2211a3b33a217e9bac,Ensuring Fairness in Machine Learning to Advance Health Equity,"Machine learning can identify the statistical patterns of data generated by tens of thousands of physicians and billions of patients to train computers to perform specific tasks with sometimes superhuman ability, such as detecting diabetic eye disease better than retinal specialists (1). However, historical data also capture patterns of health care disparities, and machine-learning models trained on these data may perpetuate these inequities. This concern is not just academic. In a model used to predict future crime on the basis of historical arrest records, African American defendants who did not reoffend were classified as high risk at a substantially higher rate than white defendants who did not reoffend (2, 3). Similar biases have been observed in predictive policing (4) and identifying which calls to a child protective services agency required an in-person investigation (5, 6). The implications for health care led the American Medical Association to pass policy recommendations to promote development of thoughtfully designed, high-quality, clinically validated health care AI [artificial or augmented intelligence, such as machine learning] that . . . identifies and takes steps to address bias and avoids introducing or exacerbating health care disparities including when testing or deploying new AI tools on vulnerable populations (7). We argue that health care organizations and policymakers should go beyond the American Medical Association's position of doing no harm and instead proactively design and use machine-learning systems to advance health equity. Whereas much health disparities work has focused on discriminatory decision making and implicit biases by clinicians, policymakers, organizational leaders, and researchers are increasingly focusing on the ill health effects of structural racism and classismhow systems are shaped in ways that harm the health of disempowered, marginalized populations (8). For example, the United States has a shameful history of purposive decisions by government and private businesses to segregate housing. Zoning laws, discrimination in mortgage lending, prejudicial practices by real estate agents, and the ghettoization of public housing all contributed to the concentration of urban African Americans in inferior housing that has led to poor health (9, 10). Even when the goal of decision makers is not outright discrimination against disadvantaged groups, actions may lead to inequities. For example, if the goal of a machine-learning system is to maximize efficiency, that might come at the expense of disadvantaged populations. As a society, we value health equity. For example, the Healthy People 2020 vision statement aims for a society in which all people live long, healthy lives, and one of the mission's goals is to achieve health equity, eliminate disparities, and improve the health of all groups (11). The 4 classic principles of Western clinical medical ethics are justice, autonomy, beneficence, and nonmaleficence. However, health equity will not be attained unless we purposely design our health and social systems, which increasingly will be infused with machine learning (12), to achieve this goal. To ensure fairness in machine learning, we recommend a participatory process that involves key stakeholders, including frequently marginalized populations, and considers distributive justice within specific clinical and organizational contexts. Different technical approaches can configure the mathematical properties of machine-learning models to render predictions that are equitable in various ways. The existence of mathematical levers must be supplemented with criteria for when and why they should be usedeach tool comes with tradeoffs that require ethical reasoning to decide what is best for a given application. We propose incorporating fairness into the design, deployment, and evaluation of machine-learning models. We discuss 2 clinical applications in which machine learning might harm protected groups by being inaccurate, diverting resources, or worsening outcomes, especially if the models are built without consideration for these patients. We then describe the mechanisms by which a model's design, data, and deployment may lead to disparities; explain how different approaches to distributive justice in machine learning can advance health equity; and explore what contexts are more appropriate for different equity approaches in machine learning. Case Study 1: Intensive Care Unit Monitoring A common area of predictive modeling research focuses on creating a monitoring systemfor example, to warn a rapid response team about inpatients at high risk for deterioration (1315), requiring their transfer to an intensive care unit within 6 hours. How might such a system inadvertently result in harm to a protected group? In this thought experiment, we consider African Americans as a protected group. To build the model, our hypothetical researchers collected historical records of patients who had clinical deterioration and those who did not. The model acts like a diagnostic test of risk for intensive care unit transfer. However, if too few African American patients were included in the training datathe data used to construct the modelthe model might be inaccurate for them. For example, it might have a lower sensitivity and miss more patients at risk for deterioration. African American patients might be harmed if clinical teams started relying on alerts to identify at-risk patients without realizing that the prediction system underdetects patients in that group (automation bias) (16). If the model had a lower positive predictive value for African Americans, it might also disproportionately harm them through dismissal biasa generalization of alert fatigue in which clinicians may learn to discount or dismiss alerts for African Americans because they are more likely to be false-positive (17). Case Study 2: Reducing Length of Stay Imagine that a hospital created a model with clinical and social variables to predict which inpatients might be discharged earliest so that it could direct limited case management resources to them to prevent delays. If residence in ZIP codes of socioeconomically depressed or predominantly African American neighborhoods predicted greater lengths of stay (18), this model might disproportionately allocate case management resources to patients from richer, predominantly white neighborhoods and away from African Americans in poorer ones. What Is Machine Learning? Traditionally, computer systems map inputs to outputs according to manually specified ifthen rules. With increasingly complex tasks, such as language translation, manually specifying rules becomes infeasible, and instead the mapping (or model) is learned by the system given only input examples represented through a set of features together with their desired output, referred to as labels. The quality of a model is assessed by computing evaluation metrics on data not used to build the model, such as sensitivity, specificity, or the c-statistic, which measures the ability of a model to distinguish patients with a condition from those without it (19, 20). Once the model's quality is deemed satisfactory, it can be deployed to make predictions on new examples for which the label is unknown when the prediction is made. The quality of the models on retrospective data must be followed with tests of clinical effectiveness, safety, and comparison with current practice, which may require clinical trials (21). Traditionally, statistical models for prediction, such as the pooled-cohort equation (22), have used few variables to predict clinical outcomes, such as cardiovascular risk (23). Modern machine-learning techniques, however, can consider many more features. For example, a recent model to predict hospital readmissions examined hundreds of thousands of pieces of information, including the free text of clinical notes (24). Complex data and models can drive more personalized and accurate predictions but may also make algorithms hard to understand and trust (25). What Can Cause a Machine-Learning System to Be Unfair? The Glossary lists key biases in the design, data, and deployment of a machine-learning model that may perpetuate or exacerbate health care disparities if left unchecked. The Figure reveals how the various biases relate to one another and how the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Biases may arise during the design of a model. For example, if the label is marred by health care disparities, such as predicting the onset of clinical depression in environments where protected groups have been systematically misdiagnosed, then the model will learn to perpetuate this disparity. This represents a generalization of test-referral bias (26) that we refer to as label bias. Moreover, the data on which the model is developed may be biased. Data on patients in the protected group might be distributed differently from those in the nonprotected group because of biological or nonbiological variation (9, 27). For example, the data may not contain enough examples from a group to properly tailor the predictions to them (minority bias) (28), or the data set of the protected group may be less informative because features are missing not at random as a result of more fragmented care (29, 30). Glossary Figure. Conceptual framework of how various biases relate to one another. During model development, differences in the distribution of features used to predict a label between the protected and nonprotected groups may bias a model to be less accurate for protected groups. Moreover, the data used to develop a model may not generalize to the data used during model deployment (trainingserving skew). Biases in model design and data affect patient outcomes through the model's interaction with clinicians and patients. The immediate effect of these differences is that the model may ",2018.0,"A. Rajkomar, Michaela Hardt, M. Howell, Greg S. Corrado, M. Chin"
3a83d8595e6727269c876fcebd23ee9ddd524b76,https://www.semanticscholar.org/paper/3a83d8595e6727269c876fcebd23ee9ddd524b76,A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective,"Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.",2018.0,"Yuji Roh, Geon Heo, Steven Euijong Whang"
64f6dab6b4bcf5cd792e352ea15aeca05572e21e,https://www.semanticscholar.org/paper/64f6dab6b4bcf5cd792e352ea15aeca05572e21e,Tunability: Importance of Hyperparameters of Machine Learning Algorithms,"Modern supervised machine learning algorithms involve hyperparameters that have to be set before running them. Options for setting hyperparameters are default values from the software package, manual configuration by the user or configuring them for optimal predictive performance by a tuning procedure. The goal of this paper is two-fold. Firstly, we formalize the problem of tuning from a statistical point of view, define data-based defaults and suggest general measures quantifying the tunability of hyperparameters of algorithms. Secondly, we conduct a large-scale benchmarking study based on 38 datasets from the OpenML platform and six common machine learning algorithms. We apply our measures to assess the tunability of their parameters. Our results yield default values for hyperparameters and enable users to decide whether it is worth conducting a possibly time consuming tuning strategy, to focus on the most important hyperparameters and to chose adequate hyperparameter spaces for tuning.",2018.0,"Philipp Probst, A. Boulesteix, B. Bischl"
7d065e649e3bfc7d6d36166f50eab37b8404eae0,https://www.semanticscholar.org/paper/7d065e649e3bfc7d6d36166f50eab37b8404eae0,Interpretable Machine Learning in Healthcare,"This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.",2018.0,"M. Ahmad, A. Teredesai, C. Eckert"
5fdc2223709079ba5c0f78661cdf66cec2173258,https://www.semanticscholar.org/paper/5fdc2223709079ba5c0f78661cdf66cec2173258,Current Applications and Future Impact of Machine Learning in Radiology.,"Recent advances and future perspectives of machine learning techniques offer promising applications in medical imaging. Machine learning has the potential to improve different steps of the radiology workflow including order scheduling and triage, clinical decision support systems, detection and interpretation of findings, postprocessing and dose estimation, examination quality control, and radiology reporting. In this article, the authors review examples of current applications of machine learning and artificial intelligence techniques in diagnostic radiology. In addition, the future impact and natural extension of these techniques in radiology practice are discussed.",2018.0,"G. Choy, O. Khalilzadeh, Mark H. Michalski, Synho Do, A. Samir, Oleg S. Pianykh, J. R. Geis, P. Pandharipande, J. Brink, K. Dreyer"
0f5476c9629f8093e8ba8c6a41868415c6a7f2f1,https://www.semanticscholar.org/paper/0f5476c9629f8093e8ba8c6a41868415c6a7f2f1,Stealing Hyperparameters in Machine Learning,"Hyperparameters are critical in machine learning, as different hyperparameters often result in models with significantly different performance. Hyperparameters may be deemed confidential because of their commercial value and the confidentiality of the proprietary algorithms that the learner uses to learn them. In this work, we propose attacks on stealing the hyperparameters that are learned by a learner. We call our attacks hyperparameter stealing attacks. Our attacks are applicable to a variety of popular machine learning algorithms such as ridge regression, logistic regression, support vector machine, and neural network. We evaluate the effectiveness of our attacks both theoretically and empirically. For instance, we evaluate our attacks on Amazon Machine Learning. Our results demonstrate that our attacks can accurately steal hyperparameters. We also study countermeasures. Our results highlight the need for new defenses against our hyperparameter stealing attacks for certain machine learning algorithms.",2018.0,"Binghui Wang, N. Gong"
70f6937b6253db8209d8fd6a4115e766946f04c5,https://www.semanticscholar.org/paper/70f6937b6253db8209d8fd6a4115e766946f04c5,eDoctor: machine learning and the future of medicine,"Machine learning (ML) is a burgeoning field of medicine with huge resources being applied to fuse computer science and statistics to medical problems. Proponents of ML extol its ability to deal with large, complex and disparate data, often found within medicine and feel that ML is the future for biomedical research, personalized medicine, computer‐aided diagnosis to significantly advance global health care. However, the concepts of ML are unfamiliar to many medical professionals and there is untapped potential in the use of ML as a research tool. In this article, we provide an overview of the theory behind ML, explore the common ML algorithms used in medicine including their pitfalls and discuss the potential future of ML in medicine.",2018.0,"G. Handelman, H. Kok, R. Chandra, A. H. Razavi, M. Lee, H. Asadi"
6d67ddd0855c60ada2fb4151f0f944feffdaf357,https://www.semanticscholar.org/paper/6d67ddd0855c60ada2fb4151f0f944feffdaf357,Machine Learning from Theory to Algorithms: An Overview,"The current SMAC (Social, Mobile, Analytic, Cloud) technology trend paves the way to a future in which intelligent machines, networked processes and big data are brought together. This virtual world has generated vast amount of data which is accelerating the adoption of machine learning solutions & practices. Machine Learning enables computers to imitate and adapt human-like behaviour. Using machine learning, each interaction, each action performed, becomes something the system can learn and use as experience for the next time. This work is an overview of this data analytics method which enables computers to learn and do what comes naturally to humans, i.e. learn from experience. It includes the preliminaries of machine learning, the definition, nomenclature and applications’ describing it’s what, how and why. The technology roadmap of machine learning is discussed to understand and verify its potential as a market & industry practice. The primary intent of this work is to give insight into why machine learning is the future.",2018.0,"J. Alzubi, A. Nayyar, Akshi Kumar"
2e6c570d277b0b4edd48e2054d5cede4c6bbb50f,https://www.semanticscholar.org/paper/2e6c570d277b0b4edd48e2054d5cede4c6bbb50f,Machine learning in acoustics: Theory and applications.,"Acoustic data provide scientific and engineering insights in fields ranging from biology and communications to ocean and Earth science. We survey the recent advances and transformative potential of machine learning (ML), including deep learning, in the field of acoustics. ML is a broad family of techniques, which are often based in statistics, for automatically detecting and utilizing patterns in data. Relative to conventional acoustics and signal processing, ML is data-driven. Given sufficient training data, ML can discover complex relationships between features and desired labels or actions, or between features themselves. With large volumes of training data, ML can discover models describing complex acoustic phenomena such as human speech and reverberation. ML in acoustics is rapidly developing with compelling results and significant future promise. We first introduce ML, then highlight ML developments in four acoustics research areas: source localization in speech processing, source localization in ocean acoustics, bioacoustics, and environmental sounds in everyday scenes.",2019.0,"Michael J. Bianco, P. Gerstoft, James Traer, Emma Ozanich, M. Roch, S. Gannot, Charles-Alban Deledalle"
a0390b8d4a82daa1d24bba341b317aa710e4ce4d,https://www.semanticscholar.org/paper/a0390b8d4a82daa1d24bba341b317aa710e4ce4d,Super-resolution reconstruction of turbulent flows with machine learning,"We use machine learning to perform super-resolution analysis of grossly under-resolved turbulent flow field data to reconstruct the high-resolution flow field. Two machine learning models are developed, namely, the convolutional neural network (CNN) and the hybrid downsampled skip-connection/multi-scale (DSC/MS) models. These machine learning models are applied to a two-dimensional cylinder wake as a preliminary test and show remarkable ability to reconstruct laminar flow from low-resolution flow field data. We further assess the performance of these models for two-dimensional homogeneous turbulence. The CNN and DSC/MS models are found to reconstruct turbulent flows from extremely coarse flow field images with remarkable accuracy. For the turbulent flow problem, the machine-leaning-based super-resolution analysis can greatly enhance the spatial resolution with as little as 50 training snapshot data, holding great potential to reveal subgrid-scale physics of complex turbulent flows. With the growing availability of flow field data from high-fidelity simulations and experiments, the present approach motivates the development of effective super-resolution models for a variety of fluid flows.",2018.0,"Kai Fukami, K. Fukagata, K. Taira"
790985a4bee821046992ff3d5322ff11dd1b4262,https://www.semanticscholar.org/paper/790985a4bee821046992ff3d5322ff11dd1b4262,Coresets for Data-efficient Training of Machine Learning Models,"Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.",2019.0,"Baharan Mirzasoleiman, J. Bilmes, J. Leskovec"
3e9a40a567c4a95b591530ff5771296b478a0f0c,https://www.semanticscholar.org/paper/3e9a40a567c4a95b591530ff5771296b478a0f0c,Machine Learning at the Network Edge: A Survey,"Resource-constrained IoT devices, such as sensors and actuators, have become ubiquitous in recent years. This has led to the generation of large quantities of data in real-time, which is an appealing target for AI systems. However, deploying machine learning models on such end-devices is nearly impossible. A typical solution involves offloading data to external computing systems (such as cloud servers) for further processing but this worsens latency, leads to increased communication costs, and adds to privacy concerns. To address this issue, efforts have been made to place additional computing devices at the edge of the network, i.e., close to the IoT devices where the data is generated. Deploying machine learning systems on such edge computing devices alleviates the above issues by allowing computations to be performed close to the data sources. This survey describes major research efforts where machine learning systems have been deployed at the edge of computer networks, focusing on the operational aspects including compression techniques, tools, frameworks, and hardware used in successful applications of intelligent edge systems.",2019.0,"M. G. Sarwar Murshed, Chris Murphy, Daqing Hou, Nazar Khan, Ganesh Ananthanarayanan, Faraz Hussain"
1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c,https://www.semanticscholar.org/paper/1ecc2bd0bc6ffa0a2f466a058589c20593e3e57c,The Marginal Value of Adaptive Gradient Methods in Machine Learning,"Adaptive optimization methods, which perform local optimization with a metric constructed from the history of iterates, are becoming increasingly popular for training deep neural networks. Examples include AdaGrad, RMSProp, and Adam. We show that for simple overparameterized problems, adaptive methods often find drastically different solutions than gradient descent (GD) or stochastic gradient descent (SGD). We construct an illustrative binary classification problem where the data is linearly separable, GD and SGD achieve zero test error, and AdaGrad, Adam, and RMSProp attain test errors arbitrarily close to half. We additionally study the empirical generalization capability of adaptive methods on several state-of-the-art deep learning models. We observe that the solutions found by adaptive methods generalize worse (often significantly worse) than SGD, even when these solutions have better training performance. These results suggest that practitioners should reconsider the use of adaptive methods to train neural networks.",2017.0,"Ashia C. Wilson, R. Roelofs, Mitchell Stern, N. Srebro, B. Recht"
a8797f1d253c75669d96e6fcceda2be3f8534e1d,https://www.semanticscholar.org/paper/a8797f1d253c75669d96e6fcceda2be3f8534e1d,Support Vector Machine Active Learning with Applications to Text Classification,"Support vector machines have met with signif-icant success in numerous real-world learning tasks. However, like most machine learning algorithms, they are generally applied using a randomly selected training set classiﬁed in advance. In many settings, we also have the option of using pool-based active learning . Instead of using a randomly selected training set, the learner has access to a pool of unlabeled instances and can request the labels for some number of them. We introduce an new algorithm for performing active learning with support vector machines, i.e., an algorithm for choosing which instances to request next. We provide a theoretical motivation for the algorithm. We present experimental results showing that employing our active learning method can signiﬁcantly reduce the need for labeled training instances in both the standard inductive and transductive settings.",2000.0,"Simon Tong, D. Koller"
330b5844d170b6b77f5f9fa4c2024150cef2af18,https://www.semanticscholar.org/paper/330b5844d170b6b77f5f9fa4c2024150cef2af18,Benchmark and Survey of Automated Machine Learning Frameworks,"Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to automatically build machine learning applications without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 different data sets.",2019.0,"M. Zöller, Marco F. Huber"
5ec6039389d448f24183084b503cf1ac899f45fc,https://www.semanticscholar.org/paper/5ec6039389d448f24183084b503cf1ac899f45fc,What Is Machine Learning: a Primer for the Epidemiologist.,"Machine learning is a branch of computer science that has the potential to transform epidemiological sciences. Amid a growing focus on ""Big Data,"" it offers epidemiologists new tools to tackle problems for which classical methods are not well-suited. In order to critically evaluate the value of integrating machine learning algorithms and existing methods, however, it is essential to address language and technical barriers between the two fields that can make it difficult for epidemiologists to read and assess machine learning studies. Here, we provide an overview of the concepts and terminology used in machine learning literature, which encompasses a diverse set of tools with goals ranging from prediction, to classification, to clustering. We provide a brief introduction to five common machine learning algorithms and four ensemble-based approaches. We then summarize epidemiological applications of machine learning techniques in the published literature. We recommend approaches to incorporate machine learning in epidemiological research and discuss opportunities and challenges for integrating machine learning and existing epidemiological research methods.",2019.0,"Qifang Bi, K. Goodman, J. Kaminsky, J. Lessler"
a8fadb33a38f1096f84f64bd66345717a5bc3241,https://www.semanticscholar.org/paper/a8fadb33a38f1096f84f64bd66345717a5bc3241,Machine Learning Made Easy: A Review of Scikit-learn Package in Python Programming Language,"Machine learning is a popular topic in data analysis and modeling. Many different machine learning algorithms have been developed and implemented in a variety of programming languages over the past 20 years. In this article, we first provide an overview of machine learning and clarify its difference from statistical inference. Then, we review Scikit-learn, a machine learning package in the Python programming language that is widely used in data science. The Scikit-learn package includes implementations of a comprehensive list of machine learning methods under unified data and modeling procedure conventions, making it a convenient toolkit for educational and behavior statisticians.",2019.0,"J. Hao, T. Ho"
5d500ff62baeac5a27ea7512a833e2a25dcb2354,https://www.semanticscholar.org/paper/5d500ff62baeac5a27ea7512a833e2a25dcb2354,Machine Learning for Survival Analysis,"Survival analysis is a subfield of statistics where the goal is to analyze and model data where the outcome is the time until an event of interest occurs. One of the main challenges in this context is the presence of instances whose event outcomes become unobservable after a certain time point or when some instances do not experience any event during the monitoring period. This so-called censoring can be handled most effectively using survival analysis techniques. Traditionally, statistical approaches have been widely developed in the literature to overcome the issue of censoring. In addition, many machine learning algorithms have been adapted to deal with such censored data and tackle other challenging problems that arise in real-world data. In this survey, we provide a comprehensive and structured review of the statistical methods typically used and the machine learning techniques developed for survival analysis, along with a detailed taxonomy of the existing methods. We also discuss several topics that are closely related to survival analysis and describe several successful applications in a variety of real-world application domains. We hope that this article will give readers a more comprehensive understanding of recent advances in survival analysis and offer some guidelines for applying these approaches to solve new problems arising in applications involving censored data.",2019.0,"Ping Wang, Yan Li, Chandan K. Reddy"
5b77625b30ab2fa8abf5c152831a6985a61516ee,https://www.semanticscholar.org/paper/5b77625b30ab2fa8abf5c152831a6985a61516ee,Can machine-learning improve cardiovascular risk prediction using routine clinical data?,"Background Current approaches to predict cardiovascular risk fail to identify many people who would benefit from preventive treatment, while others receive unnecessary intervention. Machine-learning offers opportunity to improve accuracy by exploiting complex interactions between risk factors. We assessed whether machine-learning can improve cardiovascular risk prediction. Methods Prospective cohort study using routine clinical data of 378,256 patients from UK family practices, free from cardiovascular disease at outset. Four machine-learning algorithms (random forest, logistic regression, gradient boosting machines, neural networks) were compared to an established algorithm (American College of Cardiology guidelines) to predict first cardiovascular event over 10-years. Predictive accuracy was assessed by area under the ‘receiver operating curve’ (AUC); and sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) to predict 7.5% cardiovascular risk (threshold for initiating statins). Findings 24,970 incident cardiovascular events (6.6%) occurred. Compared to the established risk prediction algorithm (AUC 0.728, 95% CI 0.723–0.735), machine-learning algorithms improved prediction: random forest +1.7% (AUC 0.745, 95% CI 0.739–0.750), logistic regression +3.2% (AUC 0.760, 95% CI 0.755–0.766), gradient boosting +3.3% (AUC 0.761, 95% CI 0.755–0.766), neural networks +3.6% (AUC 0.764, 95% CI 0.759–0.769). The highest achieving (neural networks) algorithm predicted 4,998/7,404 cases (sensitivity 67.5%, PPV 18.4%) and 53,458/75,585 non-cases (specificity 70.7%, NPV 95.7%), correctly predicting 355 (+7.6%) more patients who developed cardiovascular disease compared to the established algorithm. Conclusions Machine-learning significantly improves accuracy of cardiovascular risk prediction, increasing the number of patients identified who could benefit from preventive treatment, while avoiding unnecessary treatment of others.",2017.0,"S. Weng, J. Reps, J. Kai, J. Garibaldi, N. Qureshi"
4fb1202c313d6c221c51fc264e7f2a57b8bc4f1a,https://www.semanticscholar.org/paper/4fb1202c313d6c221c51fc264e7f2a57b8bc4f1a,Machine learning and soil sciences: a review aided by machine learning tools,"Abstract. The application of machine learning (ML) techniques in various fields of science has increased rapidly, especially in the last 10 years. The increasing availability of soil data that can be efficiently acquired remotely and proximally, and freely available open-source algorithms, have led to an accelerated adoption of ML techniques to analyse soil data. Given the large number of publications, it is an impossible task to manually review all papers on the application of ML in soil science without narrowing down a narrative of ML application in a specific research question. This paper aims to provide a comprehensive review of the application of ML techniques in soil science aided by a ML algorithm (latent Dirichlet allocation) to find patterns in a large collection of text corpora. The objective is to gain insight into publications of ML applications in soil science and to discuss the research gaps in this topic. We found that (a) there is an increasing usage of ML methods in soil sciences, mostly concentrated in developed countries,
(b) the reviewed publications can be grouped into 12 topics, namely remote sensing, soil organic carbon, water, contamination, methods (ensembles), erosion and parent material, methods (NN, neural networks, SVM, support vector machines), spectroscopy, modelling (classes), crops, physical, and modelling (continuous),
and (c) advanced ML methods usually perform better than simpler approaches thanks to their capability to capture non-linear relationships.
From these findings, we found research gaps, in particular, about the precautions that should be taken (parsimony) to avoid overfitting, and that the interpretability of the ML models is an important aspect to consider when applying advanced ML methods in order to improve our knowledge and understanding of soil. We foresee that a large number of studies will focus on the latter topic.
",2019.0,"J. Padarian, B. Minasny, A. McBratney"
dd41d656e21c30dd761bee2eba303d1aa014d120,https://www.semanticscholar.org/paper/dd41d656e21c30dd761bee2eba303d1aa014d120,Machine Learning Paradigms for Next-Generation Wireless Networks,"Next-generation wireless networks are expected to support extremely high data rates and radically new applications, which require a new wireless radio technology paradigm. The challenge is that of assisting the radio in intelligent adaptive learning and decision making, so that the diverse requirements of next-generation wireless networks can be satisfied. Machine learning is one of the most promising artificial intelligence tools, conceived to support smart radio terminals. Future smart 5G mobile terminals are expected to autonomously access the most meritorious spectral bands with the aid of sophisticated spectral efficiency learning and inference, in order to control the transmission power, while relying on energy efficiency learning/inference and simultaneously adjusting the transmission protocols with the aid of quality of service learning/inference. Hence we briefly review the rudimentary concepts of machine learning and propose their employment in the compelling applications of 5G networks, including cognitive radios, massive MIMOs, femto/small cells, heterogeneous networks, smart grid, energy harvesting, device-todevice communications, and so on. Our goal is to assist the readers in refining the motivation, problem formulation, and methodology of powerful machine learning algorithms in the context of future networks in order to tap into hitherto unexplored applications and services.",2017.0,"Chunxiao Jiang, Haijun Zhang, Yong Ren, Zhu Han, Kwang-Cheng Chen, L. Hanzo"
33d9d4593d44792e17a045e5f3407f0fe7a40dd1,https://www.semanticscholar.org/paper/33d9d4593d44792e17a045e5f3407f0fe7a40dd1,Machine learning of accurate energy-conserving molecular force fields,"The law of energy conservation is used to develop an efficient machine learning approach to construct accurate force fields. Using conservation of energy—a fundamental property of closed classical and quantum mechanical systems—we develop an efficient gradient-domain machine learning (GDML) approach to construct accurate molecular force fields using a restricted number of samples from ab initio molecular dynamics (AIMD) trajectories. The GDML implementation is able to reproduce global potential energy surfaces of intermediate-sized molecules with an accuracy of 0.3 kcal mol−1 for energies and 1 kcal mol−1 Å̊−1 for atomic forces using only 1000 conformational geometries for training. We demonstrate this accuracy for AIMD trajectories of molecules, including benzene, toluene, naphthalene, ethanol, uracil, and aspirin. The challenge of constructing conservative force fields is accomplished in our work by learning in a Hilbert space of vector-valued functions that obey the law of energy conservation. The GDML approach enables quantitative molecular dynamics simulations for molecules at a fraction of cost of explicit AIMD calculations, thereby allowing the construction of efficient force fields with the accuracy and transferability of high-level ab initio methods.",2016.0,"Stefan Chmiela, A. Tkatchenko, H. Sauceda, I. Poltavsky, Kristof T. Schütt, K. Müller"
ea7887fadc666d6faf92e569d4a10d994ee91297,https://www.semanticscholar.org/paper/ea7887fadc666d6faf92e569d4a10d994ee91297,iml: An R package for Interpretable Machine Learning,"Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.",2018.0,"Christoph Molnar, Giuseppe Casalicchio, B. Bischl"
8285e1b5536ce11d55462ae757f61c75ec6773c6,https://www.semanticscholar.org/paper/8285e1b5536ce11d55462ae757f61c75ec6773c6,The Frontiers of Fairness in Machine Learning,"The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.",2018.0,"A. Chouldechova, Aaron Roth"
50684b147b752a07c313cb73d864f7b21bd8b703,https://www.semanticscholar.org/paper/50684b147b752a07c313cb73d864f7b21bd8b703,Scaling Distributed Machine Learning with the Parameter Server,"We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. 
 
To demonstrate the scalability of the proposed framework, we show experimental results on petabytes of real data with billions of examples and parameters on problems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.",2014.0,"Mu Li, D. Andersen, J. Park, Alex Smola, Amr Ahmed, V. Josifovski, James Long, E. Shekita, Bor-Yiing Su"
3f22c9462f8e588ce4210a304133e2265f41d913,https://www.semanticscholar.org/paper/3f22c9462f8e588ce4210a304133e2265f41d913,Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods,"Increased interest in the opportunities provided by artificial intelligence and machine learning has spawned a new field of health-care research. The new tools under development are targeting many aspects of medical practice, including changes to the practice of pathology and laboratory medicine. Optimal design in these powerful tools requires cross-disciplinary literacy, including basic knowledge and understanding of critical concepts that have traditionally been unfamiliar to pathologists and laboratorians. This review provides definitions and basic knowledge of machine learning categories (supervised, unsupervised, and reinforcement learning), introduces the underlying concept of the bias-variance trade-off as an important foundation in supervised machine learning, and discusses approaches to the supervised machine learning study design along with an overview and description of common supervised machine learning algorithms (linear regression, logistic regression, Naive Bayes, k-nearest neighbor, support vector machine, random forest, convolutional neural networks).",2019.0,"H. Rashidi, N. Tran, E. Betts, L. Howell, Ralph Green"
be1496e9620089b377ef631692478f5034ee95b8,https://www.semanticscholar.org/paper/be1496e9620089b377ef631692478f5034ee95b8,Machine learning applications in epilepsy,"Machine learning leverages statistical and computer science principles to develop algorithms capable of improving performance through interpretation of data rather than through explicit instructions. Alongside widespread use in image recognition, language processing, and data mining, machine learning techniques have received increasing attention in medical applications, ranging from automated imaging analysis to disease forecasting. This review examines the parallel progress made in epilepsy, highlighting applications in automated seizure detection from electroencephalography (EEG), video, and kinetic data, automated imaging analysis and pre‐surgical planning, prediction of medication response, and prediction of medical and surgical outcomes using a wide variety of data sources. A brief overview of commonly used machine learning approaches, as well as challenges in further application of machine learning techniques in epilepsy, is also presented. With increasing computational capabilities, availability of effective machine learning algorithms, and accumulation of larger datasets, clinicians and researchers will increasingly benefit from familiarity with these techniques and the significant progress already made in their application in epilepsy.",2019.0,"B. Abbasi, D. Goldenholz"
bab1753d993c0c028a0fa729569b8c1528af4fe8,https://www.semanticscholar.org/paper/bab1753d993c0c028a0fa729569b8c1528af4fe8,TensorFlow.js: Machine Learning for the Web and Beyond,"TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.",2019.0,"D. Smilkov, Nikhil Thorat, Yannick Assogba, Ann Yuan, Nick Kreeger, Ping Yu, Kangyi Zhang, Shanqing Cai, Eric Nielsen, David Soergel, S. Bileschi, Michael Terry, Charles Nicholson, Sandeep N. Gupta, S. Sirajuddin, D. Sculley, R. Monga, G. Corrado, F. Viégas, M. Wattenberg"
22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd,https://www.semanticscholar.org/paper/22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd,DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning,"Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications.",2014.0,"Tianshi Chen, Zidong Du, Ninghui Sun, Jia Wang, Chengyong Wu, Yunji Chen, O. Temam"
741b11606c2fb3167c756b3d8fd2d39e060b11f9,https://www.semanticscholar.org/paper/741b11606c2fb3167c756b3d8fd2d39e060b11f9,Machine Learning for Medical Imaging.,"Machine learning is a technique for recognizing patterns that can be applied to medical images. Although it is a powerful tool that can help in rendering medical diagnoses, it can be misapplied. Machine learning typically begins with the machine learning algorithm system computing the image features that are believed to be of importance in making the prediction or diagnosis of interest. The machine learning algorithm system then identifies the best combination of these image features for classifying the image or computing some metric for the given image region. There are several methods that can be used, each with different strengths and weaknesses. There are open-source versions of most of these machine learning methods that make them easy to try and apply to images. Several metrics for measuring the performance of an algorithm exist; however, one must be aware of the possible associated pitfalls that can result in misleading metrics. More recently, deep learning has started to be used; this method has the benefit that it does not require image feature identification and calculation as a first step; rather, features are identified as part of the learning process. Machine learning has been used in medical imaging and will have a greater influence in the future. Those working in medical imaging must be aware of how machine learning works. ©RSNA, 2017.",2017.0,"B. Erickson, P. Korfiatis, Z. Akkus, T. Kline"
db4d0e45560ceda35b6212036513bd4ab59ce99d,https://www.semanticscholar.org/paper/db4d0e45560ceda35b6212036513bd4ab59ce99d,SARAH: A Novel Method for Machine Learning Problems Using Stochastic Recursive Gradient,"In this paper, we propose a StochAstic Recursive grAdient algoritHm (SARAH), as well as its practical variant SARAH+, as a novel approach to the finite-sum minimization problems. Different from the vanilla SGD and other modern stochastic methods such as SVRG, S2GD, SAG and SAGA, SARAH admits a simple recursive framework for updating stochastic gradient estimates; when comparing to SAG/SAGA, SARAH does not require a storage of past gradients. The linear convergence rate of SARAH is proven under strong convexity assumption. We also prove a linear convergence rate (in the strongly convex case) for an inner loop of SARAH, the property that SVRG does not possess. Numerical experiments demonstrate the efficiency of our algorithm.",2017.0,"Lam M. Nguyen, Jie Liu, K. Scheinberg, Martin Takác"
9d46dc975aeed3f96bddb144079b50238f746ecd,https://www.semanticscholar.org/paper/9d46dc975aeed3f96bddb144079b50238f746ecd,"Machine learning in manufacturing: advantages, challenges, and applications","The nature of manufacturing systems faces ever more complex, dynamic and at times even chaotic behaviors. In order to being able to satisfy the demand for high-quality products in an efficient manner, it is essential to utilize all means available. One area, which saw fast pace developments in terms of not only promising results but also usability, is machine learning. Promising an answer to many of the old and new challenges of manufacturing, machine learning is widely discussed by researchers and practitioners alike. However, the field is very broad and even confusing which presents a challenge and a barrier hindering wide application. Here, this paper contributes in presenting an overview of available machine learning techniques and structuring this rather complicated area. A special focus is laid on the potential benefit, and examples of successful applications in a manufacturing environment.",2016.0,"Thorsten Wuest, Daniel Weimer, C. Irgens, K. Thoben"
0fa45cfa88ee9ac93cb01ec159ad8713d0e32d93,https://www.semanticscholar.org/paper/0fa45cfa88ee9ac93cb01ec159ad8713d0e32d93,"Data Mining: Practical Machine Learning Tools and Techniques, 3/E","Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. 
 
 Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. 
 
 *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks—in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization",2014.0,"I. Witten, E. Frank, M. Hall"
441c31274f4535a4a50892c1ad6e19eacfd17f8c,https://www.semanticscholar.org/paper/441c31274f4535a4a50892c1ad6e19eacfd17f8c,Perspective: Machine learning potentials for atomistic simulations.,"Nowadays, computer simulations have become a standard tool in essentially all fields of chemistry, condensed matter physics, and materials science. In order to keep up with state-of-the-art experiments and the ever growing complexity of the investigated problems, there is a constantly increasing need for simulations of more realistic, i.e., larger, model systems with improved accuracy. In many cases, the availability of sufficiently efficient interatomic potentials providing reliable energies and forces has become a serious bottleneck for performing these simulations. To address this problem, currently a paradigm change is taking place in the development of interatomic potentials. Since the early days of computer simulations simplified potentials have been derived using physical approximations whenever the direct application of electronic structure methods has been too demanding. Recent advances in machine learning (ML) now offer an alternative approach for the representation of potential-energy surfaces by fitting large data sets from electronic structure calculations. In this perspective, the central ideas underlying these ML potentials, solved problems and remaining challenges are reviewed along with a discussion of their current applicability and limitations.",2016.0,J. Behler
3d6f1561f0bb29add7cbe56fac9040039a69eb92,https://www.semanticscholar.org/paper/3d6f1561f0bb29add7cbe56fac9040039a69eb92,A survey on evolutionary machine learning,"ABSTRACT Artificial intelligence (AI) emphasises the creation of intelligent machines/systems that function like humans. AI has been applied to many real-world applications. Machine learning is a branch of AI based on the idea that systems can learn from data, identify hidden patterns, and make decisions with little/minimal human intervention. Evolutionary computation is an umbrella of population-based intelligent/learning algorithms inspired by nature, where New Zealand has a good international reputation. This paper provides a review on evolutionary machine learning, i.e. evolutionary computation techniques for major machine learning tasks such as classification, regression and clustering, and emerging topics including combinatorial optimisation, computer vision, deep learning, transfer learning, and ensemble learning. The paper also provides a brief review of evolutionary learning applications, such as supply chain and manufacturing for milk/dairy, wine and seafood industries, which are important to New Zealand. Finally, the paper presents current issues with future perspectives in evolutionary machine learning.",2019.0,"Harith Al-Sahaf, Ying Bi, Qi Chen, Andrew Lensen, Yi Mei, Yanan Sun, Binh Tran, Bing Xue, Mengjie Zhang"
960ba564e9e598d864dff38d2f3d0bad1b319ead,https://www.semanticscholar.org/paper/960ba564e9e598d864dff38d2f3d0bad1b319ead,A Survey of Machine Learning for Big Code and Naturalness,"Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities.",2017.0,"Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, Charles Sutton"
5939ac3b5a9d64d8371ee179751351d7698637df,https://www.semanticscholar.org/paper/5939ac3b5a9d64d8371ee179751351d7698637df,Using Machine Learning to Advance Personality Assessment and Theory,"Machine learning has led to important advances in society. One of the most exciting applications of machine learning in psychological science has been the development of assessment tools that can powerfully predict human behavior and personality traits. Thus far, machine learning approaches to personality assessment have focused on the associations between social media and other digital records with established personality measures. The goal of this article is to expand the potential of machine learning approaches to personality assessment by embedding it in a more comprehensive construct validation framework. We review recent applications of machine learning to personality assessment, place machine learning research in the broader context of fundamental principles of construct validation, and provide recommendations for how to use machine learning to advance our understanding of personality.",2019.0,"W. Bleidorn, C. Hopwood"
4135da4cc0ef70917e45ae4436e7a6411077325c,https://www.semanticscholar.org/paper/4135da4cc0ef70917e45ae4436e7a6411077325c,A Review of Machine Learning and Deep Learning Applications,"Machine learning is one of the fields in the modern computing world. A plenty of research has been undertaken to make machines intelligent. Learning is a natural human behavior which has been made an essential aspect of the machines as well. There are various techniques devised for the same. Traditional machine learning algorithms have been applied in many application areas. Researchers have put many efforts to improve the accuracy of that machinelearning algorithms. Another dimension was given thought which leads to deep learning concept. Deep learning is a subset of machine learning. So far few applications of deep learning have been explored. This is definitely going to cater to solving issues in several new application domains, sub-domains using deep learning. A review of these past and future application domains, sub-domains, and applications of machine learning and deep learning are illustrated in this paper.",2018.0,"Pramila Shinde, Seema Shah"
b2e0b79e6f180af2e0e559f2b1faba66b2bd578a,https://www.semanticscholar.org/paper/b2e0b79e6f180af2e0e559f2b1faba66b2bd578a,Accelerating the Machine Learning Lifecycle with MLflow,"Machine learning development creates multiple new challenges that are not present in a traditional software development lifecycle. These include keeping track of the myriad inputs to an ML application (e.g., data versions, code and tuning parameters), reproducing results, and production deployment. In this paper, we summarize these challenges from our experience with Databricks customers, and describe MLﬂow, an open source platform we recently launched to streamline the machine learning lifecycle. MLﬂow covers three key challenges: experimentation, reproducibility, and model deployment, using generic APIs that work with any ML library, algorithm and programming language. The project has a rapidly growing open source community, with over 50 contributors since its launch in June 2018.",2018.0,"M. Zaharia, A. Chen, A. Davidson, A. Ghodsi, S. Hong, A. Konwinski, Siddharth Murching, Tomas Nykodym, Paul Ogilvie, Mani Parkhe, Fen Xie, Corey Zumar"
f615bd164110160e160c98f59d7bfcc931a3cdc1,https://www.semanticscholar.org/paper/f615bd164110160e160c98f59d7bfcc931a3cdc1,Theoretical Impediments to Machine Learning With Seven Sparks from the Causal Revolution,"Current machine learning systems operate, almost exclusively, in a statistical, or model-blind mode, which entails severe theoretical limits on their power and performance. Such systems cannot reason about interventions and retrospection and, therefore, cannot serve as the basis for strong AI. To achieve human level intelligence, learning machines need the guidance of a model of reality, similar to the ones used in causal inference. To demonstrate the essential role of such models, I will present a summary of seven tasks which are beyond reach of current machine learning systems and which have been accomplished using the tools of causal inference.",2018.0,J. Pearl
959c9dbfceda3825787f75f63fd7c87f332dc271,https://www.semanticscholar.org/paper/959c9dbfceda3825787f75f63fd7c87f332dc271,Machine Learning-Based Sentiment Analysis for Twitter Accounts,"Growth in the area of opinion mining and sentiment analysis has been rapid and aims to explore the opinions or text present on different platforms of social media through machine-learning techniques with sentiment, subjectivity analysis or polarity calculations. Despite the use of various machine-learning techniques and tools for sentiment analysis during elections, there is a dire need for a state-of-the-art approach. To deal with these challenges, the contribution of this paper includes the adoption of a hybrid approach that involves a sentiment analyzer that includes machine learning. Moreover, this paper also provides a comparison of techniques of sentiment analysis in the analysis of political views by applying supervised machine-learning algorithms such as Naive Bayes and support vector machines (SVM).",2018.0,"A. Hasan, Sana Moin, Ahmad Karim, Shahaboddin Shamshirband"
1c3752586e7d746b13eb5b2784ac9fe53756b7fd,https://www.semanticscholar.org/paper/1c3752586e7d746b13eb5b2784ac9fe53756b7fd,Attractor reconstruction by machine learning.,"A machine-learning approach called ""reservoir computing"" has been used successfully for short-term prediction and attractor reconstruction of chaotic dynamical systems from time series data. We present a theoretical framework that describes conditions under which reservoir computing can create an empirical model capable of skillful short-term forecasts and accurate long-term ergodic behavior. We illustrate this theory through numerical experiments. We also argue that the theory applies to certain other machine learning methods for time series prediction.",2018.0,"Zhixin Lu, B. Hunt, E. Ott"
d4414002f23c0f1c497166eb51c5b1d549ff75c8,https://www.semanticscholar.org/paper/d4414002f23c0f1c497166eb51c5b1d549ff75c8,Flux: Elegant machine learning with Julia,"Flux is library for machine learning (ML), written using the numerical computing language Julia (Bezanson et al. 2017). The package allows models to be written using Julia’s simple mathematical syntax, and applies automatic differentiation (AD) to seamlessly calculate derivatives and train the model. Meanwhile, it makes heavy use of Julia’s language and compiler features to carry out code analysis and make optimisations. For example, Julia’s GPU compilation support (Besard, Foket, and De Sutter 2017) can be used to JIT-compile custom GPU kernels for model layers (Innes and others 2017a).",2018.0,Mike Innes
c9887b9e16380884f2ba6568669883884163551a,https://www.semanticscholar.org/paper/c9887b9e16380884f2ba6568669883884163551a,Machine learning for image based species identification,"Accurate species identification is the basis for all aspects of taxonomic research and is an essential component of workflows in biological research. Biologists are asking for more efficient methods to meet the identification demand. Smart mobile devices, digital cameras as well as the mass digitisation of natural history collections led to an explosion of openly available image data depicting living organisms. This rapid increase in biological image data in combination with modern machine learning methods, such as deep learning, offers tremendous opportunities for automated species identification. In this paper, we focus on deep learning neural networks as a technology that enabled breakthroughs in automated species identification in the last 2 years. In order to stimulate more work in this direction, we provide a brief overview of machine learning frameworks applicable to the species identification problem. We review selected deep learning approaches for image based species identification and introduce publicly available applications. Eventually, this article aims to provide insights into the current state‐of‐the‐art in automated identification and to serve as a starting point for researchers willing to apply novel machine learning techniques in their biological studies. While modern machine learning approaches only slowly pave their way into the field of species identification, we argue that we are going to see a proliferation of these techniques being applied to the problem in the future. Artificial intelligence systems will provide alternative tools for taxonomic identification in the near future.",2018.0,"Jana Wäldchen, Patrick Mäder"
a7f8b8e6124901c1e22e940092e87b5b93776ab3,https://www.semanticscholar.org/paper/a7f8b8e6124901c1e22e940092e87b5b93776ab3,Machine Learning With Big Data: Challenges and Approaches,"The Big Data revolution promises to transform how we live, work, and think by enabling process optimization, empowering insight discovery and improving decision making. The realization of this grand potential relies on the ability to extract value from such massive data through data analytics; machine learning is at its core because of its ability to learn from data and provide data driven insights, decisions, and predictions. However, traditional machine learning approaches were developed in a different era, and thus are based upon multiple assumptions, such as the data set fitting entirely into memory, what unfortunately no longer holds true in this new context. These broken assumptions, together with the Big Data characteristics, are creating obstacles for the traditional techniques. Consequently, this paper compiles, summarizes, and organizes machine learning challenges with Big Data. In contrast to other research that discusses challenges, this work highlights the cause–effect relationship by organizing challenges according to Big Data Vs or dimensions that instigated the issue: volume, velocity, variety, or veracity. Moreover, emerging machine learning approaches and techniques are discussed in terms of how they are capable of handling the various challenges with the ultimate objective of helping practitioners select appropriate solutions for their use cases. Finally, a matrix relating the challenges and approaches is presented. Through this process, this paper provides a perspective on the domain, identifies research gaps and opportunities, and provides a strong foundation and encouragement for further research in the field of machine learning with Big Data.",2017.0,"Alexandra L’Heureux, Katarina Grolinger, H. ElYamany, Miriam A. M. Capretz"
2a944564c2466883ec14a6f6ef461f0e34d21b38,https://www.semanticscholar.org/paper/2a944564c2466883ec14a6f6ef461f0e34d21b38,Fairness in Machine Learning: Lessons from Political Philosophy,"What does it mean for a machine learning model to be `fair', in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise `fairness' in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.",2017.0,Reuben Binns
f4c3315684cfd474c3d13ae4954de0dead5e81a3,https://www.semanticscholar.org/paper/f4c3315684cfd474c3d13ae4954de0dead5e81a3,Machine learning in cardiovascular medicine: are we there yet?,"Artificial intelligence (AI) broadly refers to analytical algorithms that iteratively learn from data, allowing computers to find hidden insights without being explicitly programmed where to look. These include a family of operations encompassing several terms like machine learning, cognitive learning, deep learning and reinforcement learning-based methods that can be used to integrate and interpret complex biomedical and healthcare data in scenarios where traditional statistical methods may not be able to perform. In this review article, we discuss the basics of machine learning algorithms and what potential data sources exist; evaluate the need for machine learning; and examine the potential limitations and challenges of implementing machine in the context of cardiovascular medicine. The most promising avenues for AI in medicine are the development of automated risk prediction algorithms which can be used to guide clinical care; use of unsupervised learning techniques to more precisely phenotype complex disease; and the implementation of reinforcement learning algorithms to intelligently augment healthcare providers. The utility of a machine learning-based predictive model will depend on factors including data heterogeneity, data depth, data breadth, nature of modelling task, choice of machine learning and feature selection algorithms, and orthogonal evidence. A critical understanding of the strength and limitations of various methods and tasks amenable to machine learning is vital. By leveraging the growing corpus of big data in medicine, we detail pathways by which machine learning may facilitate optimal development of patient-specific models for improving diagnoses, intervention and outcome in cardiovascular medicine.",2018.0,"K. Shameer, Kipp W. Johnson, Benjamin S. Glicksberg, J. Dudley, P. Sengupta"
5c4dbf5b17e2729c570174f9099f35c247d1a889,https://www.semanticscholar.org/paper/5c4dbf5b17e2729c570174f9099f35c247d1a889,enchmark for molecular machine learning †,"Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.",2017.0,"Zhenqin Wu, Bharath Ramsundar, Evan N. Feinberg, Joseph Gomes, C. Geniesse, Aneesh S. Pappu, Karl Leswingd, V. Pande"
1ce15f4a83706b877e86f29549920651a888b144,https://www.semanticscholar.org/paper/1ce15f4a83706b877e86f29549920651a888b144,Data Mining and Analytics in the Process Industry: The Role of Machine Learning,"Data mining and analytics have played an important role in knowledge discovery and decision making/supports in the process industry over the past several decades. As a computational engine to data mining and analytics, machine learning serves as basic tools for information extraction, data pattern recognition and predictions. From the perspective of machine learning, this paper provides a review on existing data mining and analytics applications in the process industry over the past several decades. The state-of-the-art of data mining and analytics are reviewed through eight unsupervised learning and ten supervised learning algorithms, as well as the application status of semi-supervised learning algorithms. Several perspectives are highlighted and discussed for future researches on data mining and analytics in the process industry.",2017.0,"Zhiqiang Ge, Zhihuan Song, S. Ding, Biao Huang"
fdd025e077a36166b10120b448d0c4e4009824a9,https://www.semanticscholar.org/paper/fdd025e077a36166b10120b448d0c4e4009824a9,Model-Agnostic Interpretability of Machine Learning,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.",2016.0,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
02e2e79a77d8aabc1af1900ac80ceebac20abde4,https://www.semanticscholar.org/paper/02e2e79a77d8aabc1af1900ac80ceebac20abde4,Explanation and Justification in Machine Learning : A Survey Or,"We present a survey of the research concerning explanation and justiﬁcation in the Machine Learning literature and several adjacent ﬁelds. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justiﬁcation",2017.0,"Or Biran, Courtenay V. Cotton"
c0eb2d5d65ecc27cb00501bffdcc55167c61cfe0,https://www.semanticscholar.org/paper/c0eb2d5d65ecc27cb00501bffdcc55167c61cfe0,What can machine learning do? Workforce implications,"Profound change is coming, but roles for humans remain Digital computers have transformed work in almost every sector of the economy over the past several decades (1). We are now at the beginning of an even larger and more rapid transformation due to recent advances in machine learning (ML), which is capable of accelerating the pace of automation itself. However, although it is clear that ML is a “general purpose technology,” like the steam engine and electricity, which spawns a plethora of additional innovations and capabilities (2), there is no widely shared agreement on the tasks where ML systems excel, and thus little agreement on the specific expected impacts on the workforce and on the economy more broadly. We discuss what we see to be key implications for the workforce, drawing on our rubric of what the current generation of ML systems can and cannot do [see the supplementary materials (SM)]. Although parts of many jobs may be “suitable for ML” (SML), other tasks within these same jobs do not fit the criteria for ML well; hence, effects on employment are more complex than the simple replacement and substitution story emphasized by some. Although economic effects of ML are relatively limited today, and we are not facing the imminent “end of work” as is sometimes proclaimed, the implications for the economy and the workforce going forward are profound.",2017.0,"Erik Brynjolfsson, Tom M. Mitchell"
4ab891e6044695abb31c72048f654b3b205c60bb,https://www.semanticscholar.org/paper/4ab891e6044695abb31c72048f654b3b205c60bb,Survey of Machine Learning Algorithms for Disease Diagnostic,"In medical imaging, Computer Aided Diagnosis (CAD) is a rapidly growing dynamic area of research. In recent years, significant attempts are made for the enhancement of computer aided diagnosis applications because errors in medical diagnostic systems can result in seriously misleading medical treatments. Machine learning is important in Computer Aided Diagnosis. After using an easy equation, objects such as organs may not be indicated accurately. So, pattern recognition fundamentally involves learning from examples. In the field of bio-medical, pattern recognition and machine learning promise the improved accuracy of perception and diagnosis of disease. They also promote the objectivity of decision-making process. For the analysis of high-dimensional and multimodal bio-medical data, machine learning offers a worthy approach for making classy and automatic algorithms. This survey paper provides the comparative analysis of different machine learning algorithms for diagnosis of different diseases such as heart disease, diabetes disease, liver disease, dengue disease and hepatitis disease. It brings attention towards the suite of machine learning algorithms and tools that are used for the analysis of diseases and decision-making process accordingly.",2017.0,"M. Fatima, Maruf Pasha"
91f7848ea9045f29345467552496db97b037ae01,https://www.semanticscholar.org/paper/91f7848ea9045f29345467552496db97b037ae01,Machine Learning and Materials Informatics: Recent Applications and Prospects,"Propelled partly by the Materials Genome Initiative, and partly by the algorithmic developments and the resounding successes of data-driven efforts in other domains, informatics strategies are beginning to take shape within materials science. These approaches lead to surrogate machine learning models that enable rapid predictions based purely on past data rather than by direct experimentation or by computations/simulations in which fundamental equations are explicitly solved. Data-centric informatics methods are becoming useful to determine material properties that are hard to measure or compute using traditional methods--due to the cost, time or effort involved--but for which reliable data either already exists or can be generated for at least a subset of the critical cases. Predictions are typically interpolative, involving fingerprinting a material numerically first, and then following a mapping (established via a learning algorithm) between the fingerprint and the property of interest. Fingerprints may be of many types and scales, as dictated by the application domain and needs. Predictions may also be extrapolative--extending into new materials spaces--provided prediction uncertainties are properly taken into account. This article attempts to provide an overview of some of the recent successful data-driven ""materials informatics"" strategies undertaken in the last decade, and identifies some challenges the community is facing and those that should be overcome in the near future.",2017.0,"R. Ramprasad, Rohit Batra, G. Pilania, A. Mannodi‐Kanakkithodi, Chiho Kim"
b3e2629f412707889181ad650195c669777ebdea,https://www.semanticscholar.org/paper/b3e2629f412707889181ad650195c669777ebdea,Machine learning unifies the modeling of materials and molecules,"Statistical learning based on a local representation of atomic structures provides a universal model of chemical stability. Determining the stability of molecules and condensed phases is the cornerstone of atomistic modeling, underpinning our understanding of chemical and materials properties and transformations. We show that a machine-learning model, based on a local description of chemical environments and Bayesian statistical learning, provides a unified framework to predict atomic-scale properties. It captures the quantum mechanical effects governing the complex surface reconstructions of silicon, predicts the stability of different classes of molecules with chemical accuracy, and distinguishes active and inactive protein ligands with more than 99% reliability. The universality and the systematic nature of our framework provide new insight into the potential energy surface of materials and molecules.",2017.0,"A. Bartók, Sandip De, C. Poelking, N. Bernstein, J. Kermode, Gábor Csányi, Michele Ceriotti"
6724a5386dd7401d234f2d8b8715cb175fbe15bd,https://www.semanticscholar.org/paper/6724a5386dd7401d234f2d8b8715cb175fbe15bd,Unintended consequences of machine learning in medicine?,"Machine learning (ML) has the potential to significantly aid medical practice. However, a recent article highlighted some negative consequences that may arise from using ML decision support in medicine. We argue here that whilst the concerns raised by the authors may be appropriate, they are not specific to ML, and thus the article may lead to an adverse perception about this technique in particular. Whilst ML is not without its limitations like any methodology, a balanced view is needed in order to not hamper its use in potentially enabling better patient care.",2017.0,"L. McDonald, S. Ramagopalan, A. Cox, M. Oğuz"
4157ed3db4c656854e69931cb6089b64b08784b9,https://www.semanticscholar.org/paper/4157ed3db4c656854e69931cb6089b64b08784b9,DaDianNao: A Machine-Learning Supercomputer,"Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects.",2014.0,"Yunji Chen, Tao Luo, Shaoli Liu, Shijin Zhang, Liqiang He, Jia Wang, Ling Li, Tianshi Chen, Zhiwei Xu, Ninghui Sun, O. Temam"
c6850869aa5e78a107c378d2e8bfa39633158c0c,https://www.semanticscholar.org/paper/c6850869aa5e78a107c378d2e8bfa39633158c0c,Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation,"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (""wordpieces"") for both input and output. This method provides a good balance between the flexibility of ""character""-delimited models and the efficiency of ""word""-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.",2016.0,"Yonghui Wu, M. Schuster, Z. Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, M. Krikun, Yuan Cao, Qin Gao, Klaus Macherey, J. Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, H. Kazawa, K. Stevens, George Kurian, Nishant Patil, Wei Wang, C. Young, Jason R. Smith, Jason Riesa, Alex Rudnick, O. Vinyals, G. Corrado, Macduff Hughes, J. Dean"
7aaede70f5efcb1542a80707c1f0f8b01955a7d2,https://www.semanticscholar.org/paper/7aaede70f5efcb1542a80707c1f0f8b01955a7d2,Oblivious Multi-Party Machine Learning on Trusted Processors,"Privacy-preserving multi-party machine learning allows multiple organizations to perform collaborative data analytics while guaranteeing the privacy of their individual datasets. Using trusted SGX-processors for this task yields high performance, but requires a careful selection, adaptation, and implementation of machine-learning algorithms to provably prevent the exploitation of any side channels induced by data-dependent access patterns. 
 
We propose data-oblivious machine learning algorithms for support vector machines, matrix factorization, neural networks, decision trees, and k-means clustering. We show that our efficient implementation based on Intel Skylake processors scales up to large, realistic datasets, with overheads several orders of magnitude lower than with previous approaches based on advanced cryptographic multi-party computation schemes.",2016.0,"O. Ohrimenko, Felix Schuster, C. Fournet, Aastha Mehta, Sebastian Nowozin, K. Vaswani, Manuel Costa"
c87a4433c57ddabd50f32ca2c2d2197244692106,https://www.semanticscholar.org/paper/c87a4433c57ddabd50f32ca2c2d2197244692106,mlr: Machine Learning in R,"The MLR package provides a generic, object-oriented, and extensible framework for classification, regression, survival analysis and clustering for the R language. It provides a unified interface to more than 160 basic learners and includes meta-algorithms and model selection techniques to improve and extend the functionality of basic learners with, e.g., hyperparameter tuning, feature selection, and ensemble construction. Parallel high-performance computing is natively supported. The package targets practitioners who want to quickly apply machine learning algorithms, as well as researchers who want to implement, benchmark, and compare their new methods in a structured environment.",2016.0,"B. Bischl, Michel Lang, Lars Kotthoff, J. Schiffner, Jakob Richter, Erich Studerus, Giuseppe Casalicchio, Zachary M. Jones"
5ba3f64789fb54bee10a891bc21222964d83c687,https://www.semanticscholar.org/paper/5ba3f64789fb54bee10a891bc21222964d83c687,A Review of Challenges and Opportunities in Machine Learning for Health.,"Modern electronic health records (EHRs) provide data to answer clinically meaningful questions. The growing data in EHRs makes healthcare ripe for the use of machine learning. However, learning in a clinical setting presents unique challenges that complicate the use of common machine learning methodologies. For example, diseases in EHRs are poorly labeled, conditions can encompass multiple underlying endotypes, and healthy individuals are underrepresented. This article serves as a primer to illuminate these challenges and highlights opportunities for members of the machine learning community to contribute to healthcare.",2018.0,"M. Ghassemi, Tristan Naumann, Peter F. Schulam, A. Beam, I. Chen, R. Ranganath"
be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6,https://www.semanticscholar.org/paper/be1bb4e4aa1fcf70281b4bd24d8cd31c04864bb6,Matching Networks for One Shot Learning,"Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.",2016.0,"O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, D. Wierstra"
262c0e54370dfc03a7ad53d79930568d18dd448c,https://www.semanticscholar.org/paper/262c0e54370dfc03a7ad53d79930568d18dd448c,Speeding Up Distributed Machine Learning Using Codes,"Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=""LaTeX"">$\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=""LaTeX"">$\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=""LaTeX"">$\left({\alpha + \frac {1}{n}}\right)\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=""LaTeX"">$\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=""LaTeX"">$\gamma (n) \simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.",2015.0,"Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, K. Ramchandran"
448d13aae6ed21411d28887c550663973893f70c,https://www.semanticscholar.org/paper/448d13aae6ed21411d28887c550663973893f70c,Machine Learning in Healthcare: A Review,"Machine Learning is modern and highly sophisticated technological applications became a huge trend in the industry. Machine Learning is Omni present and is widely used in various applications. It is playing a vital role in many fields like finance, Medical science and in security. Machine learning is used to discover patterns from medical data sources and provide excellent capabilities to predict diseases. In this paper, we review various machine learning algorithms used for developing efficient decision support for healthcare applications. This paper helps in reducing the research gap for building efficient decision support system for medical applications.",2018.0,"K. Shailaja, B. Seetharamulu, M. Jabbar"
43d1fe40167c5f2ed010c8e06c8e008c774fd22b,https://www.semanticscholar.org/paper/43d1fe40167c5f2ed010c8e06c8e008c774fd22b,Non-convex Optimization for Machine Learning,"A vast majority of machine learning algorithms train their models and perform inference by solving optimization problems. In order to capture the learning and prediction problems accurately, structural constraints such as sparsity or low rank are frequently imposed or else the objective itself is designed to be a non-convex function. This is especially true of algorithms that operate in high-dimensional spaces or that train non-linear models such as tensor models and deep networks.  The freedom to express the learning problem as a non-convex optimization problem gives immense modeling power to the algorithm designer, but often such problems are NP-hard to solve.  A popular workaround to this has been to relax non-convex problems to convex ones and use traditional methods to solve the (convex) relaxed optimization problems. However this approach may be lossy and nevertheless presents significant challenges for large scale optimization.  On the other hand, direct approaches to non-convex optimization have met with resounding success in several domains and remain the methods of choice for the practitioner, as they frequently outperform relaxation-based techniques - popular heuristics include projected gradient descent and alternating minimization. However, these are often poorly understood in terms of their convergence and other properties.  This monograph presents a selection of recent advances that bridge a long-standing gap in our understanding of these heuristics. We hope that an insight into the inner workings of these methods will allow the reader to appreciate the unique marriage of task structure and generative models that allow these heuristic techniques to (provably) succeed. The monograph will lead the reader through several widely used non-convex optimization techniques, as well as applications thereof. The goal of this monograph is to both, introduce the rich literature in this area, as well as equip the reader with the tools and techniques needed to analyze these simple procedures for non-convex problems.",2017.0,"Prateek Jain, Purushottam Kar"
73811a7f8b89de1b8bdad6bb938e58059a9076d3,https://www.semanticscholar.org/paper/73811a7f8b89de1b8bdad6bb938e58059a9076d3,Introduction to machine learning: k-nearest neighbors.,"Machine learning techniques have been widely used in many scientific fields, but its use in medical literature is limited partly because of technical difficulties. k-nearest neighbors (kNN) is a simple method of machine learning. The article introduces some basic ideas underlying the kNN algorithm, and then focuses on how to perform kNN modeling with R. The dataset should be prepared before running the knn() function in R. After prediction of outcome with kNN algorithm, the diagnostic performance of the model should be checked. Average accuracy is the mostly widely used statistic to reflect the kNN algorithm. Factors such as k value, distance calculation and choice of appropriate predictors all have significant impact on the model performance.",2016.0,Wentao Bao
07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b,https://www.semanticscholar.org/paper/07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b,Advances and Open Problems in Federated Learning,"Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",2019.0,"P. Kairouz, H. B. McMahan, Brendan Avent, A. Bellet, M. Bennis, A. Bhagoji, Keith Bonawitz, Zachary B. Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, S. E. Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, M. Gruteser, Zaïd Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, T. Javidi, Gauri Joshi, M. Khodak, Jakub Konecný, A. Korolova, F. Koushanfar, Oluwasanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, M. Mohri, R. Nock, A. Özgür, R. Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, R. Raskar, D. Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, A. Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, Sen Zhao"
44479cc5266788c3bafcc0b12ef0758827741fe3,https://www.semanticscholar.org/paper/44479cc5266788c3bafcc0b12ef0758827741fe3,Guidelines for Developing and Reporting Machine Learning Predictive Models in Biomedical Research: A Multidisciplinary View,"Background As more and more researchers are turning to big data for new opportunities of biomedical discoveries, machine learning models, as the backbone of big data analysis, are mentioned more often in biomedical journals. However, owing to the inherent complexity of machine learning methods, they are prone to misuse. Because of the flexibility in specifying machine learning models, the results are often insufficiently reported in research articles, hindering reliable assessment of model validity and consistent interpretation of model outputs. Objective To attain a set of guidelines on the use of machine learning predictive models within clinical settings to make sure the models are correctly applied and sufficiently reported so that true discoveries can be distinguished from random coincidence. Methods A multidisciplinary panel of machine learning experts, clinicians, and traditional statisticians were interviewed, using an iterative process in accordance with the Delphi method. Results The process produced a set of guidelines that consists of (1) a list of reporting items to be included in a research article and (2) a set of practical sequential steps for developing predictive models. Conclusions A set of guidelines was generated to enable correct application of machine learning models and consistent reporting of model specifications and results in biomedical research. We believe that such guidelines will accelerate the adoption of big data analysis, particularly with machine learning methods, in the biomedical research community.",2016.0,"Wei Luo, Dinh Phung, T. Tran, Sunil Gupta, Santu Rana, C. Karmakar, A. Shilton, J. Yearwood, N. Dimitrova, T. Ho, S. Venkatesh, M. Berk"
46d71d947231f86e1f9d4581e61212385debbe14,https://www.semanticscholar.org/paper/46d71d947231f86e1f9d4581e61212385debbe14,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",2014.0,"J. Vanschoren, J. N. Rijn, B. Bischl, Luís Torgo"
e9a986c8ff6c2f381d026fe014f6aaa865f34da7,https://www.semanticscholar.org/paper/e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Deep Learning with Differential Privacy,"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",2016.0,"Martín Abadi, Andy Chu, I. Goodfellow, H. B. McMahan, Ilya Mironov, Kunal Talwar, Li Zhang"
1eb131a34fbb508a9dd8b646950c65901d6f1a5b,https://www.semanticscholar.org/paper/1eb131a34fbb508a9dd8b646950c65901d6f1a5b,Hidden Technical Debt in Machine Learning Systems,"Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.",2015.0,"D. Sculley, Gary Holt, D. Golovin, Eugene Davydov, Todd Phillips, D. Ebner, Vinay Chaudhary, Michael Young, Jean-François Crespo, Dan Dennison"
22a67be81ab3b778a1f39a6182cbab37e07d26ee,https://www.semanticscholar.org/paper/22a67be81ab3b778a1f39a6182cbab37e07d26ee,Diversity in Machine Learning,"Machine learning methods have achieved good performance and been widely applied in various real-world applications. They can learn the model adaptively and be better fit for special requirements of different tasks. Generally, a good machine learning system is composed of plentiful training data, a good model training process, and an accurate inference. Many factors can affect the performance of the machine learning process, among which the diversity of the machine learning process is an important one. The diversity can help each procedure to guarantee a totally good machine learning: diversity of the training data ensures that the training data can provide more discriminative information for the model, diversity of the learned model (diversity in parameters of each model or diversity among different base models) makes each parameter/model capture unique or complement information and the diversity in inference can provide multiple choices each of which corresponds to a specific plausible local optimal result. Even though diversity plays an important role in the machine learning process, there is no systematical analysis of the diversification in the machine learning system. In this paper, we systematically summarize the methods to make data diversification, model diversification, and inference diversification in the machine learning process. In addition, the typical applications where the diversity technology improved the machine learning performance have been surveyed including the remote sensing imaging tasks, machine translation, camera relocalization, image segmentation, object detection, topic modeling, and others. Finally, we discuss some challenges of the diversity technology in machine learning and point out some directions in future work. Our analysis provides a deeper understanding of the diversity technology in machine learning tasks and hence can help design and learn more effective models for real-world applications.",2018.0,"Z. Gong, P. Zhong, Weidong Hu"
f466157848d1a7772fb6d02cdac9a7a5e7ef982e,https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e,Neural Discrete Representation Learning,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",2017.0,"Aäron van den Oord, O. Vinyals, K. Kavukcuoglu"
92ace17730c2173e642934d64f96d359697b7a93,https://www.semanticscholar.org/paper/92ace17730c2173e642934d64f96d359697b7a93,Bayesian reasoning and machine learning,"Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.",2012.0,D. Barber
49bdeb07b045dd77f0bfe2b44436608770235a23,https://www.semanticscholar.org/paper/49bdeb07b045dd77f0bfe2b44436608770235a23,"Federated Learning: Challenges, Methods, and Future Directions","Federated learning involves training statistical models over remote devices or siloed data centers, such as mobile phones or hospitals, while keeping data localized. Training in heterogeneous and potentially massive networks introduces novel challenges that require a fundamental departure from standard approaches for large-scale machine learning, distributed optimization, and privacy-preserving data analysis. In this article, we discuss the unique characteristics and challenges of federated learning, provide a broad overview of current approaches, and outline several directions of future work that are relevant to a wide range of research communities.",2019.0,"Tian Li, Anit Kumar Sahu, Ameet Talwalkar, Virginia Smith"
53b55682222692323a3a0d546d9e1a3de29454f0,https://www.semanticscholar.org/paper/53b55682222692323a3a0d546d9e1a3de29454f0,A review of supervised machine learning algorithms,"Supervised machine learning is the construction of algorithms that are able to produce general patterns and hypotheses by using externally supplied instances to predict the fate of future instances. Supervised machine learning classification algorithms aim at categorizing data from prior information. Classification is carried out very frequently in data science problems. Various successful techniques have been proposed to solve such problems viz. Rule-based techniques, Logic-based techniques, Instance-based techniques, stochastic techniques. This paper discusses the efficacy of supervised machine learning algorithms in terms of the accuracy, speed of learning, complexity and risk of over fitting measures. The main objective of this paper is to provide a general comparison with state of art machine learning algorithms.",2016.0,"Amanpreet Singh, Narina Thakur, Aakanksha Sharma"
d4de528645fdfc6d954364a8e6eeeed9480ccfa2,https://www.semanticscholar.org/paper/d4de528645fdfc6d954364a8e6eeeed9480ccfa2,"Machine Learning for Networking: Workflow, Advances and Opportunities","Recently, machine learning has been used in every possible field to leverage its amazing power. For a long time, the networking and distributed computing system is the key infrastructure to provide efficient computational resources for machine learning. Networking itself can also benefit from this promising technology. This article focuses on the application of MLN, which can not only help solve the intractable old network questions but also stimulate new network applications. In this article, we summarize the basic workflow to explain how to apply machine learning technology in the networking domain. Then we provide a selective survey of the latest representative advances with explanations of their design principles and benefits. These advances are divided into several network design objectives and the detailed information of how they perform in each step of MLN workflow is presented. Finally, we shed light on the new opportunities in networking design and community building of this new inter-discipline. Our goal is to provide a broad research guideline on networking with machine learning to help motivate researchers to develop innovative algorithms, standards and frameworks.",2017.0,"Mowei Wang, Yong Cui, Xin Wang, Shihan Xiao, Junchen Jiang"
9ec1cb983d6d475bf0fc2879ea1a7e31201d8c37,https://www.semanticscholar.org/paper/9ec1cb983d6d475bf0fc2879ea1a7e31201d8c37,Python Machine Learning,"Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.",2015.0,
95615c6bce2123f12e39c3d9eb293ebb759501aa,https://www.semanticscholar.org/paper/95615c6bce2123f12e39c3d9eb293ebb759501aa,"Machine learning, social learning and the governance of self-driving cars","Self-driving cars, a quintessentially ‘smart’ technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking ‘Who is learning, what are they learning and how are they learning?’ Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. ‘Self-driving’ or ‘autonomous’ cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning.",2017.0,J. Stilgoe
18d026ec5d0eebd17ee2c762da89540c0b3d7bde,https://www.semanticscholar.org/paper/18d026ec5d0eebd17ee2c762da89540c0b3d7bde,A Comprehensive Survey on Transfer Learning,"Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target-domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning research studies, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey article reviews more than 40 representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over 20 representative transfer learning models are used for experiments. The models are performed on three different data sets, that is, Amazon Reviews, Reuters-21578, and Office-31, and the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.",2019.0,"Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, Qing He"
e1f9ef01ab55d53349096a58d76fd0cfa7bb051d,https://www.semanticscholar.org/paper/e1f9ef01ab55d53349096a58d76fd0cfa7bb051d,Quantum machine learning: a classical perspective,"Recently, increased computational power and data availability, as well as algorithmic advances, have led machine learning (ML) techniques to impressive results in regression, classification, data generation and reinforcement learning tasks. Despite these successes, the proximity to the physical limits of chip fabrication alongside the increasing size of datasets is motivating a growing number of researchers to explore the possibility of harnessing the power of quantum computation to speed up classical ML algorithms. Here we review the literature in quantum ML and discuss perspectives for a mixed readership of classical ML and quantum computation experts. Particular emphasis will be placed on clarifying the limitations of quantum algorithms, how they compare with their best classical counterparts and why quantum resources are expected to provide advantages for learning problems. Learning in the presence of noise and certain computationally hard problems in ML are identified as promising directions for the field. Practical questions, such as how to upload classical data into quantum form, will also be addressed.",2017.0,"C. Ciliberto, M. Herbster, Alessandro Davide Ialongo, M. Pontil, Andrea Rocchetto, S. Severini, Leonard Wossnig"
227e0591634cef50d0bcfc73fe6c5b34a2256e5f,https://www.semanticscholar.org/paper/227e0591634cef50d0bcfc73fe6c5b34a2256e5f,Radio Machine Learning Dataset Generation with GNU Radio,"This paper surveys emerging applications of Machine Learning (ML) to the Radio Signal Processing domain.  Provides some brief background on enabling methods and discusses some of the potential advancements for the field.  It discusses the critical importance of good datasets for model learning, testing, and evaluation and introduces several public open source synthetic datasets for various radio machine learning tasks.  These are intended to provide a robust common baselines for those working in the field and to provide a benchmark measure against which many techniques can be rapidly evaluated and compared.",2016.0,"Tim O'Shea, Nathan E. West"
05d20fda297c9afb347214bd1693bd049674e0c6,https://www.semanticscholar.org/paper/05d20fda297c9afb347214bd1693bd049674e0c6,Machine Learning for the Geosciences: Challenges and Opportunities,"Geosciences is a field of great societal relevance that requires solutions to several urgent problems facing our humanity and the planet. As geosciences enters the era of big data, machine learning (ML)—that has been widely successful in commercial domains—offers immense potential to contribute to problems in geosciences. However, geoscience applications introduce novel challenges for ML due to combinations of geoscience properties encountered in every problem, requiring novel research in machine learning. This article introduces researchers in the machine learning (ML) community to these challenges offered by geoscience problems and the opportunities that exist for advancing both machine learning and geosciences. We first highlight typical sources of geoscience data and describe their common properties. We then describe some of the common categories of geoscience problems where machine learning can play a role, discussing the challenges faced by existing ML methods and opportunities for novel ML research. We conclude by discussing some of the cross-cutting research themes in machine learning that are applicable across several geoscience problems, and the importance of a deep collaboration between machine learning and geosciences for synergistic advancements in both disciplines.",2017.0,"A. Karpatne, I. Ebert‐Uphoff, S. Ravela, H. Babaie, Vipin Kumar"
b191fc4294f6f067067e4e152bd4efc8bbb87afd,https://www.semanticscholar.org/paper/b191fc4294f6f067067e4e152bd4efc8bbb87afd,Unintended Consequences of Machine Learning in Medicine,"Over the past decade, machine learning techniques have made substantial advances in many domains. In health care, global interest in the potential of machine learning has increased; for example, a deep learning algorithm has shown high accuracy in detecting diabetic retinopathy.1 There have been suggestions that machine learning will drive changes in health care within a few years, specifically in medical disciplines that require more accurate prognostic models (eg, oncology) and those based on pattern recognition (eg, radiology and pathology). However, comparative studies on the effectiveness of machine learning–based decision support systems (ML-DSS) in medicine are lacking, especially regarding the effects on health outcomes. Moreover, the introduction of new technologies in health care has not always been straightforward or without unintended and adverse effects.2 In this Viewpoint we consider the potential unintended consequences that may result from the application of ML-DSS in clinical practice.",2017.0,"F. Cabitza, Raffaele Rasoini, G. Gensini"
a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e,https://www.semanticscholar.org/paper/a5d3a937e5ee43ab1542657ea2baf0c5cb139d6e,Faster and Better: A Machine Learning Approach to Corner Detection,"The repeatability and efficiency of a corner detector determines how likely it is to be useful in a real-world application. The repeatability is important because the same scene viewed from different positions should yield features which correspond to the same real-world 3D locations. The efficiency is important because this determines whether the detector combined with further processing can operate at frame rate. Three advances are described in this paper. First, we present a new heuristic for feature detection and, using machine learning, we derive a feature detector from this which can fully process live PAL video using less than 5 percent of the available processing time. By comparison, most other detectors cannot even operate at frame rate (Harris detector 115 percent, SIFT 195 percent). Second, we generalize the detector, allowing it to be optimized for repeatability, with little loss of efficiency. Third, we carry out a rigorous comparison of corner detectors based on the above repeatability criterion applied to 3D scenes. We show that, despite being principally constructed for speed, on these stringent tests, our heuristic detector significantly outperforms existing feature detectors. Finally, the comparison demonstrates that using machine learning produces significant improvements in repeatability, yielding a detector that is both very fast and of very high quality.",2008.0,"E. Rosten, R. Porter, T. Drummond"
b1059d25d092e0e872a1d2db01b24c73eb869ad9,https://www.semanticscholar.org/paper/b1059d25d092e0e872a1d2db01b24c73eb869ad9,Machine Learning for Neural Decoding,"Abstract Despite rapid advances in machine learning tools, the majority of neural decoding approaches still use traditional methods. Modern machine learning tools, which are versatile and easy to use, have the potential to significantly improve decoding performance. This tutorial describes how to effectively apply these algorithms for typical decoding problems. We provide descriptions, best practices, and code for applying common machine learning methods, including neural networks and gradient boosting. We also provide detailed comparisons of the performance of various methods at the task of decoding spiking activity in motor cortex, somatosensory cortex, and hippocampus. Modern methods, particularly neural networks and ensembles, significantly outperform traditional approaches, such as Wiener and Kalman filters. Improving the performance of neural decoding algorithms allows neuroscientists to better understand the information contained in a neural population and can help to advance engineering applications such as brain–machine interfaces. Our code package is available at github.com/kordinglab/neural_decoding.",2017.0,"Joshua I. Glaser, Raeed H. Chowdhury, M. Perich, L. Miller, Konrad Paul Kording"
31e12b8d558a7515f3a1e3337551f5f30e466cde,https://www.semanticscholar.org/paper/31e12b8d558a7515f3a1e3337551f5f30e466cde,Unified representation of molecules and crystals for machine learning,"Accurate simulations of atomistic systems from first principles are limited by computational cost. In high-throughput settings, machine learning can reduce these costs significantly by accurately interpolating between reference calculations. For this, kernel learning approaches crucially require a representation that accommodates arbitrary atomistic systems. We introduce a many-body tensor representation that is invariant to translations, rotations, and nuclear permutations of same elements, unique, differentiable, can represent molecules and crystals, and is fast to compute. Empirical evidence for competitive energy and force prediction errors is presented for changes in molecular structure, crystal chemistry, and molecular dynamics using kernel regression and symmetric gradient-domain machine learning as models. Applicability is demonstrated for phase diagrams of Pt-group/transition-metal binary systems.",2017.0,"Haoyan Huo, M. Rupp"
24e6c5bfe9bb0751e5708b501d04e860011b2953,https://www.semanticscholar.org/paper/24e6c5bfe9bb0751e5708b501d04e860011b2953,Applications of Support Vector Machine (SVM) Learning in Cancer Genomics.,"Machine learning with maximization (support) of separating margin (vector), called support vector machine (SVM) learning, is a powerful classification tool that has been used for cancer genomic classification or subtyping. Today, as advancements in high-throughput technologies lead to production of large amounts of genomic and epigenomic data, the classification feature of SVMs is expanding its use in cancer genomics, leading to the discovery of new biomarkers, new drug targets, and a better understanding of cancer driver genes. Herein we reviewed the recent progress of SVMs in cancer genomic studies. We intend to comprehend the strength of the SVM learning and its future perspective in cancer genomic applications.",2018.0,"Shujun Huang, Nianguang Cai, Pedro Penzuti Pacheco, Shavira Narrandes, Yang Wang, Wayne W. Xu"
9e6060316394393c226b5c86ce51b06c4c75bee1,https://www.semanticscholar.org/paper/9e6060316394393c226b5c86ce51b06c4c75bee1,Machine Learning Classification over Encrypted Data,"Machine learning classification is used for numerous tasks nowadays, such as medical or genomics predictions, spam detection, face recognition, and financial predictions. Due to privacy concerns, in some of these applications, it is important that the data and the classifier remain confidential. In this work, we construct three major classification protocols that satisfy this privacy constraint: hyperplane decision, Naïve Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis of these constructions is a new library of building blocks, which enables constructing a wide range of privacy-preserving classifiers; we demonstrate how this library can be used to construct other classifiers than the three mentioned above, such as a multiplexer and a face detection classifier. We implemented and evaluated our library and our classifiers. Our protocols are efficient, taking milliseconds to a few seconds to perform a classification when running on real medical datasets.",2015.0,"Raphael Bost, Raluca A. Popa, Stephen Tu, S. Goldwasser"
a5c0309b9895066ebd08acfe326b01ce2fdefdd4,https://www.semanticscholar.org/paper/a5c0309b9895066ebd08acfe326b01ce2fdefdd4,Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges,"Abstract Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.",2016.0,"B. Goldstein, A. Navar, R. Carter"
9e6060316394393c226b5c86ce51b06c4c75bee1,https://www.semanticscholar.org/paper/9e6060316394393c226b5c86ce51b06c4c75bee1,Machine Learning Classification over Encrypted Data,"Machine learning classification is used for numerous tasks nowadays, such as medical or genomics predictions, spam detection, face recognition, and financial predictions. Due to privacy concerns, in some of these applications, it is important that the data and the classifier remain confidential. In this work, we construct three major classification protocols that satisfy this privacy constraint: hyperplane decision, Naïve Bayes, and decision trees. We also enable these protocols to be combined with AdaBoost. At the basis of these constructions is a new library of building blocks, which enables constructing a wide range of privacy-preserving classifiers; we demonstrate how this library can be used to construct other classifiers than the three mentioned above, such as a multiplexer and a face detection classifier. We implemented and evaluated our library and our classifiers. Our protocols are efficient, taking milliseconds to a few seconds to perform a classification when running on real medical datasets.",2015.0,"Raphael Bost, Raluca A. Popa, Stephen Tu, S. Goldwasser"
a5c0309b9895066ebd08acfe326b01ce2fdefdd4,https://www.semanticscholar.org/paper/a5c0309b9895066ebd08acfe326b01ce2fdefdd4,Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges,"Abstract Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.",2016.0,"B. Goldstein, A. Navar, R. Carter"
a15067563a18378dac71a206c6cc2e2d8c871301,https://www.semanticscholar.org/paper/a15067563a18378dac71a206c6cc2e2d8c871301,Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning,"Algorithms for feature selection fall into two broad categories: wrappers that use the learning algorithm itself to evaluate the usefulness of features and filters that evaluate features according to heuristics based on general characteristics of the data. For application to large databases, filters have proven to be more practical than wrappers because they are much faster. However, most existing filter algorithms only work with discrete classification problems. This paper describes a fast, correlation-based filter algorithm that can be applied to continuous and discrete problems. The algorithm often outperforms the well-known ReliefF attribute estimator when used as a preprocessing step for naive Bayes, instance-based learning, decision trees, locally weighted regression, and model trees. It performs more feature selection than ReliefF does—reducing the data dimensionality by fifty percent in most cases. Also, decision and model trees built from the preprocessed data are often significantly smaller.",1999.0,M. Hall
77233d2f6fd10465a574ca33b869707822bf0c0b,https://www.semanticscholar.org/paper/77233d2f6fd10465a574ca33b869707822bf0c0b,A brief survey of machine learning methods and their sensor and IoT applications,"This paper provides a brief survey of the basic concepts and algorithms used for Machine Learning and its applications. We begin with a broader definition of machine learning and then introduce various learning modalities including supervised and unsupervised methods and deep learning paradigms. In the rest of the paper, we discuss applications of machine learning algorithms in various fields including pattern recognition, sensor networks, anomaly detection, Internet of Things (IoT) and health monitoring. In the final sections, we present some of the software tools and an extensive bibliography.",2017.0,"U. Shanthamallu, A. Spanias, C. Tepedelenlioğlu, M. Stanley"
f488e4a252c018f9391b0dc90036687a0b361844,https://www.semanticscholar.org/paper/f488e4a252c018f9391b0dc90036687a0b361844,Implementing Machine Learning in Radiology Practice and Research.,"OBJECTIVE
The purposes of this article are to describe concepts that radiologists should understand to evaluate machine learning projects, including common algorithms, supervised as opposed to unsupervised techniques, statistical pitfalls, and data considerations for training and evaluation, and to briefly describe ethical dilemmas and legal risk.


CONCLUSION
Machine learning includes a broad class of computer programs that improve with experience. The complexity of creating, training, and monitoring machine learning indicates that the success of the algorithms will require radiologist involvement for years to come, leading to engagement rather than replacement.",2017.0,"M. Kohli, L. Prevedello, Ross W. Filice, J. R. Geis"
c396ba3ce3c58591b8ee282e65ab5a6d610e16ed,https://www.semanticscholar.org/paper/c396ba3ce3c58591b8ee282e65ab5a6d610e16ed,Machine Teaching: A New Paradigm for Building Machine Learning Systems,"The current processes for building machine learning systems require practitioners with deep knowledge of machine learning. This significantly limits the number of machine learning systems that can be created and has led to a mismatch between the demand for machine learning systems and the ability for organizations to build them. We believe that in order to meet this growing demand for machine learning systems we must significantly increase the number of individuals that can teach machines. We postulate that we can achieve this goal by making the process of teaching machines easy, fast and above all, universally accessible. 
While machine learning focuses on creating new algorithms and improving the accuracy of ""learners"", the machine teaching discipline focuses on the efficacy of the ""teachers"". Machine teaching as a discipline is a paradigm shift that follows and extends principles of software engineering and programming languages. We put a strong emphasis on the teacher and the teacher's interaction with data, as well as crucial components such as techniques and design principles of interaction and visualization. 
In this paper, we present our position regarding the discipline of machine teaching and articulate fundamental machine teaching principles. We also describe how, by decoupling knowledge about machine learning algorithms from the process of teaching, we can accelerate innovation and empower millions of new uses for machine learning models.",2017.0,"Patrice Y. Simard, Saleema Amershi, D. M. Chickering, Alicia Edelman Pelton, S. Ghorashi, Christopher Meek, Gonzalo A. Ramos, Jina Suh, J. Verwey, Mo Wang, J. Wernsing"
ad0d1149875291d9b1177bc47e53e09237beeca0,https://www.semanticscholar.org/paper/ad0d1149875291d9b1177bc47e53e09237beeca0,Quantum-enhanced machine learning,"The emerging field of quantum machine learning has the potential to substantially aid in the problems and scope of artificial intelligence. This is only enhanced by recent successes in the field of classical machine learning. In this work we propose an approach for the systematic treatment of machine learning, from the perspective of quantum information. Our approach is general and covers all three main branches of machine learning: supervised, unsupervised, and reinforcement learning. While quantum improvements in supervised and unsupervised learning have been reported, reinforcement learning has received much less attention. Within our approach, we tackle the problem of quantum enhancements in reinforcement learning as well, and propose a systematic scheme for providing improvements. As an example, we show that quadratic improvements in learning efficiency, and exponential improvements in performance over limited time periods, can be obtained for a broad class of learning problems.",2016.0,"V. Dunjko, Jacob M. Taylor, H. Briegel"
f38513dc4350cfb987a8f0b774fc361c4d910a17,https://www.semanticscholar.org/paper/f38513dc4350cfb987a8f0b774fc361c4d910a17,"Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies","Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context. After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution. The book, informed by the authors' many years of teaching machine learning, and working on predictive data analytics projects, is suitable for use by undergraduates in computer science, engineering, mathematics, or statistics; by graduate students in disciplines with applications for predictive data analytics; and as a reference for professionals.",2015.0,"John D. Kelleher, Brian Mac Namee, Aoife D'Arcy"
9f86366feecbcfdf6c5be165fcf38c679164cc89,https://www.semanticscholar.org/paper/9f86366feecbcfdf6c5be165fcf38c679164cc89,Machine Learning and the Profession of Medicine.,"This Viewpoint discusses the opportunities and ethical implications of using machine learning technologies, which can rapidly collect and learn from large amounts of personal data, to provide individalized patient care.",2016.0,"Alison M Darcy, A. Louie, L. Roberts"
7fcb90f68529cbfab49f471b54719ded7528d0ef,https://www.semanticscholar.org/paper/7fcb90f68529cbfab49f471b54719ded7528d0ef,Federated Learning: Strategies for Improving Communication Efficiency,"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.",2016.0,"Jakub Konecný, H. B. McMahan, Felix X. Yu, Peter Richtárik, A. Suresh, D. Bacon"
3804ca5590a0829c5d56e84d860a2b2a456e3757,https://www.semanticscholar.org/paper/3804ca5590a0829c5d56e84d860a2b2a456e3757,Principles of Explanatory Debugging to Personalize Interactive Machine Learning,"How can end users efficiently influence the predictions that machine learning systems make on their behalf? This paper presents Explanatory Debugging, an approach in which the system explains to users how it made each of its predictions, and the user then explains any necessary corrections back to the learning system. We present the principles underlying this approach and a prototype instantiating it. An empirical evaluation shows that Explanatory Debugging increased participants' understanding of the learning system by 52% and allowed participants to correct its mistakes up to twice as efficiently as participants using a traditional learning system.",2015.0,"Todd Kulesza, M. Burnett, Weng-Keen Wong, S. Stumpf"
f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,https://www.semanticscholar.org/paper/f35de4f9b1a7c4d3fa96a0d2ab1bf8937671f6b6,Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning,"Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.",2015.0,"Y. Gal, Zoubin Ghahramani"
2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb,https://www.semanticscholar.org/paper/2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb,Deep Learning,"Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users’ interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress.",2015.0,"Yann LeCun, Yoshua Bengio, Geoffrey E. Hinton"
2878d9936f494ed7d0c8aec47e9bcc5e51609f9a,https://www.semanticscholar.org/paper/2878d9936f494ed7d0c8aec47e9bcc5e51609f9a,Extreme Learning Machine for Multilayer Perceptron,"Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via ℓ1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme.",2016.0,"Jiexiong Tang, Chenwei Deng, G. Huang"
f3ed7343727361a25e77fdef315850bdaf29d20e,https://www.semanticscholar.org/paper/f3ed7343727361a25e77fdef315850bdaf29d20e,Second-Order Stochastic Optimization for Machine Learning in Linear Time,"First-order stochastic methods are the state-of-the-art in large-scale machine learning optimization owing to efficient per-iteration complexity. Second-order methods, while able to provide faster convergence, have been much less explored due to the high cost of computing the second-order information. In this paper we develop second-order stochastic methods for optimization problems in machine learning that match the per-iteration cost of gradient based methods, and in certain settings improve upon the overall running time over popular first-order methods. Furthermore, our algorithm has the desirable property of being implementable in time linear in the sparsity of the input data.",2016.0,"Naman Agarwal, Brian Bullins, Elad Hazan"
139b2afafdaccba02c983d2f40f257db64860320,https://www.semanticscholar.org/paper/139b2afafdaccba02c983d2f40f257db64860320,Machine Learning Topological States,"Machine learning, the core of artificial intelligence and data science, is a very active field, with vast applications throughout science and technology. Recently, machine learning techniques have been adopted to tackle intricate quantum many-body problems and phase transitions. In this work, the authors construct exact mappings from exotic quantum states to machine learning network models. This work shows for the first time that the restricted Boltzmann machine can be used to study both symmetry-protected topological phases and intrinsic topological order. The exact results are expected to provide a substantial boost to the field of machine learning of phases of matter.",2016.0,"D. Deng, Xiaopeng Li, Xiaopeng Li, S. Sarma"
19a28325b68346f7715e17a97eb71db1b2b3c1af,https://www.semanticscholar.org/paper/19a28325b68346f7715e17a97eb71db1b2b3c1af,Machine Learning for Predictive Maintenance: A Multiple Classifier Approach,"In this paper, a multiple classifier machine learning (ML) methodology for predictive maintenance (PdM) is presented. PdM is a prominent strategy for dealing with maintenance issues given the increasing need to minimize downtime and associated costs. One of the challenges with PdM is generating the so-called “health factors,” or quantitative indicators, of the status of a system associated with a given maintenance issue, and determining their relationship to operating costs and failure risk. The proposed PdM methodology allows dynamical decision rules to be adopted for maintenance management, and can be used with high-dimensional and censored data problems. This is achieved by training multiple classification modules with different prediction horizons to provide different performance tradeoffs in terms of frequency of unexpected breaks and unexploited lifetime, and then employing this information in an operating cost-based maintenance decision system to minimize expected costs. The effectiveness of the methodology is demonstrated using a simulated example and a benchmark semiconductor manufacturing maintenance problem.",2015.0,"Gian Antonio Susto, A. Schirru, S. Pampuri, S. McLoone, A. Beghi"
3449b65008b27f6e60a73d80c1fd990f0481126b,https://www.semanticscholar.org/paper/3449b65008b27f6e60a73d80c1fd990f0481126b,Torch7: A Matlab-like Environment for Machine Learning,"Torch7 is a versatile numeric computing framework and machine learning library that extends Lua. Its goal is to provide a flexible environment to design and train learning machines. Flexibility is obtained via Lua, an extremely lightweight scripting language. High performance is obtained via efficient OpenMP/SSE and CUDA implementations of low-level numeric routines. Torch7 can easily be interfaced to third-party software thanks to Lua’s light interface.",2011.0,"R. Collobert, K. Kavukcuoglu, C. Farabet"
74d8094f967be96ac1d1212e1957b97ac0674d5d,https://www.semanticscholar.org/paper/74d8094f967be96ac1d1212e1957b97ac0674d5d,Machine Learning: The New AI,"Today, machine learning underlies a range of applications we use every day, from product recommendations to voice recognition -- as well as some we don't yet use everyday, including driverless cars. It is the basis of the new approach in computing where we do not write programs but collect data; the idea is to learn the algorithms for the tasks automatically from data. As computing devices grow more ubiquitous, a larger part of our lives and work is recorded digitally, and as ""Big Data"" has gotten bigger, the theory of machine learning -- the foundation of efforts to process that data into knowledge -- has also advanced. In this book, machine learning expert Ethem Alpaydin offers a concise overview of the subject for the general reader, describing its evolution, explaining important learning algorithms, and presenting example applications. Alpaydin offers an account of how digital technology advanced from number-crunching mainframes to mobile devices, putting today's machine learning boom in context. He describes the basics of machine learning and some applications; the use of machine learning algorithms for pattern recognition; artificial neural networks inspired by the human brain; algorithms that learn associations between instances, with such applications as customer segmentation and learning recommendations; and reinforcement learning, when an autonomous agent learns act so as to maximize reward and minimize penalty. Alpaydin then considers some future directions for machine learning and the new field of ""data science,"" and discusses the ethical and legal implications for data privacy and security.",2016.0,Ethem Alpaydin
aaa76e15235d937d093a7063c0be86ed84494dee,https://www.semanticscholar.org/paper/aaa76e15235d937d093a7063c0be86ed84494dee,Machine Learning: A Bayesian and Optimization Perspective,"This tutorial text gives a unifying perspective on machine learning by covering bothprobabilistic and deterministic approaches -which are based on optimization techniques together with the Bayesian inference approach, whose essence liesin the use of a hierarchy of probabilistic models. The book presents the major machine learning methods as they have been developed in different disciplines, such as statistics, statistical and adaptive signal processing and computer science. Focusing on the physical reasoning behind the mathematics, all the various methods and techniques are explained in depth, supported by examples and problems, giving an invaluable resource to the student and researcher for understanding and applying machine learning concepts. The book builds carefully from the basic classical methods to the most recent trends, with chapters written to be as self-contained as possible, making the text suitable for different courses: pattern recognition, statistical/adaptive signal processing, statistical/Bayesian learning, as well as short courses on sparse modeling, deep learning, and probabilistic graphical models. All major classical techniques: Mean/Least-Squares regression and filtering, Kalman filtering, stochastic approximation and online learning, Bayesian classification, decision trees, logistic regression and boosting methods. The latest trends: Sparsity, convex analysis and optimization, online distributed algorithms, learning in RKH spaces, Bayesian inference, graphical and hidden Markov models, particle filtering, deep learning, dictionary learning and latent variables modeling. Case studies - protein folding prediction, optical character recognition, text authorship identification, fMRI data analysis, change point detection, hyperspectral image unmixing, target localization, channel equalization and echo cancellation, show how the theory can be applied. MATLAB code for all the main algorithms are available on an accompanying website, enabling the reader to experiment with the code.",2015.0,
d37fc9e9c4fedc32865b08661e7fb950df1f8fbe,https://www.semanticscholar.org/paper/d37fc9e9c4fedc32865b08661e7fb950df1f8fbe,Kernel methods in machine learning,"We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.",2007.0,"Thomas Hofmann, B. Scholkopf, Alex Smola"
f04df4e20a18358ea2f689b4c129781628ef7fc1,https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1,A large annotated corpus for learning natural language inference,"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",2015.0,"Samuel R. Bowman, Gabor Angeli, Christopher Potts, Christopher D. Manning"
c62043a7d2537bbf40a84b9913957452a47fdb83,https://www.semanticscholar.org/paper/c62043a7d2537bbf40a84b9913957452a47fdb83,Dataset Shift in Machine Learning,"Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series",2009.0,"Joaquin Quionero-Candela, Masashi Sugiyama, A. Schwaighofer, Neil D. Lawrence"
df2a7756382540e92895f10703cec32d50c4f316,https://www.semanticscholar.org/paper/df2a7756382540e92895f10703cec32d50c4f316,Fast and accurate modeling of molecular atomization energies with machine learning.,"We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10  kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.",2011.0,"M. Rupp, A. Tkatchenko, K. Müller, O. A. von Lilienfeld"
93aa298b40bb3ec23c25239089284fdf61ded917,https://www.semanticscholar.org/paper/93aa298b40bb3ec23c25239089284fdf61ded917,Support vector machine learning for interdependent and structured output spaces,"Learning general functional dependencies is one of the main goals in machine learning. Recent progress in kernel-based methods has focused on designing flexible and powerful input representations. This paper addresses the complementary issue of problems involving complex outputs such as multiple dependent output variables and structured output spaces. We propose to generalize multiclass Support Vector Machine learning in a formulation that involves features extracted jointly from inputs and outputs. The resulting optimization problem is solved efficiently by a cutting plane algorithm that exploits the sparseness and structural decomposition of the problem. We demonstrate the versatility and effectiveness of our method on problems ranging from supervised grammar learning and named-entity recognition, to taxonomic text classification and sequence alignment.",2004.0,"Ioannis Tsochantaridis, Thomas Hofmann, T. Joachims, Y. Altun"
1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6,https://www.semanticscholar.org/paper/1cd7f2c74bd7ffb3a8b1527bec8795d0876a40b6,Transfer Learning for Low-Resource Neural Machine Translation,"The encoder-decoder framework for neural machine translation (NMT) has been shown effective in large data scenarios, but is much less effective for low-resource languages. We present a transfer learning method that significantly improves Bleu scores across a range of low-resource languages. Our key idea is to first train a high-resource language pair (the parent model), then transfer some of the learned parameters to the low-resource pair (the child model) to initialize and constrain training. Using our transfer learning method we improve baseline NMT models by an average of 5.6 Bleu on four low-resource language pairs. Ensembling and unknown word replacement add another 2 Bleu which brings the NMT performance on low-resource machine translation close to a strong syntax based machine translation (SBMT) system, exceeding its performance on one language pair. Additionally, using the transfer learning model for re-scoring, we can improve the SBMT system by an average of 1.3 Bleu, improving the state-of-the-art on low-resource machine translation.",2016.0,"Barret Zoph, Deniz Yuret, Jonathan May, Kevin Knight"
611544418ca53cdad254df444addc7814abcfddc,https://www.semanticscholar.org/paper/611544418ca53cdad254df444addc7814abcfddc,An introduction to statistical learning with applications in R,"The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site. This textbook considers statistical learning applications when interest centers on the conditional distribution of a response variable, given a set of predictors, and in the absence of a credible model that can be specified before the data analysis begins. Consistent with modern data analytics, it emphasizes that a proper statistical learning data analysis depends in an integrated fashion on sound data collection, intelligent data management, appropriate statistical procedures, and an",2021.0,"Fariha Sohil, Muhammad Umair Sohali, J. Shabbir"
8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92,https://www.semanticscholar.org/paper/8346b9a8e156d6e7a7012bcd47bc4f5d4be59e92,Outside the Closed World: On Using Machine Learning for Network Intrusion Detection,"In network intrusion detection research, one popular strategy for finding attacks is monitoring a network's activity for anomalies: deviations from profiles of normality previously learned from benign traffic, typically identified using tools borrowed from the machine learning community. However, despite extensive academic research one finds a striking gap in terms of actual deployments of such systems: compared with other intrusion detection approaches, machine learning is rarely employed in operational ""real world"" settings. We examine the differences between the network intrusion detection problem and other areas where machine learning regularly finds much more success. Our main claim is that the task of finding attacks is fundamentally different from these other applications, making it significantly harder for the intrusion detection community to employ machine learning effectively. We support this claim by identifying challenges particular to network intrusion detection, and provide a set of guidelines meant to strengthen future research on anomaly detection.",2010.0,"Robin Sommer, V. Paxson"
825ca26af5a2a510dbc1a7b97587212bc98ae968,https://www.semanticscholar.org/paper/825ca26af5a2a510dbc1a7b97587212bc98ae968,Power to the People: The Role of Humans in Interactive Machine Learning,"Intelligent systems that learn interactively from their end-users are quickly becoming widespread. Until recently, this progress has been fueled mostly by advances in machine learning; however, more and more researchers are realizing the importance of studying users of these systems. In this article we promote this approach and demonstrate how it can result in better user experiences and more effective learning systems. We present a number of case studies that characterize the impact of interactivity, demonstrate ways in which some existing systems fail to account for the user, and explore new ways for learning systems to interact with their users. We argue that the design process for interactive machine learning systems should involve users at all stages: explorations that reveal human interaction patterns and inspire novel interaction methods, as well as refinement stages to tune details of the interface and choose among alternatives. After giving a glimpse of the progress that has been made so far, we discuss the challenges that we face in moving the field forward.",2014.0,"Saleema Amershi, M. Cakmak, W. B. Knox, Todd Kulesza"
f743833c22961537791171ef1d3fb42db8f357a3,https://www.semanticscholar.org/paper/f743833c22961537791171ef1d3fb42db8f357a3,Machine Learning Methods for Attack Detection in the Smart Grid,"Attack detection problems in the smart grid are posed as statistical learning problems for different attack scenarios in which the measurements are observed in batch or online settings. In this approach, machine learning algorithms are used to classify measurements as being either secure or attacked. An attack detection framework is provided to exploit any available prior knowledge about the system and surmount constraints arising from the sparse structure of the problem in the proposed approach. Well-known batch and online learning algorithms (supervised and semisupervised) are employed with decision- and feature-level fusion to model the attack detection problem. The relationships between statistical and geometric properties of attack vectors employed in the attack scenarios and learning algorithms are analyzed to detect unobservable attacks using statistical learning methods. The proposed algorithms are examined on various IEEE test systems. Experimental analyses show that machine learning algorithms can detect attacks with performances higher than attack detection algorithms that employ state vector estimation methods in the proposed attack detection framework.",2015.0,"M. Ozay, I. Esnaola, Fatos Tunay, Yarman Vural, S. Kulkarni, H. Vincent Poor"
85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175,https://www.semanticscholar.org/paper/85d2d4cb2ca1dcd9da2d5a765cc2aa4911b0a175,An introduction to quantum machine learning,"Machine learning algorithms learn a desired input-output relation from examples in order to interpret new inputs. This is important for tasks such as image and speech recognition or strategy optimisation, with growing applications in the IT industry. In the last couple of years, researchers investigated if quantum computing can help to improve classical machine learning algorithms. Ideas range from running computationally costly algorithms or their subroutines efficiently on a quantum computer to the translation of stochastic methods into the language of quantum theory. This contribution gives a systematic overview of the emerging field of quantum machine learning. It presents the approaches as well as technical details in an accessible way, and discusses the potential of a future theory of quantum learning.",2014.0,"M. Schuld, I. Sinayskiy, Francesco Petruccione"
23b559b5ab27f2fca6f56c0a7b6478bcf69db509,https://www.semanticscholar.org/paper/23b559b5ab27f2fca6f56c0a7b6478bcf69db509,Dual Learning for Machine Translation,"While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English ↔ French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task.",2016.0,"Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, Wei-Ying Ma"
5888c776e38f39efb9b96d0ba2713981008a86b1,https://www.semanticscholar.org/paper/5888c776e38f39efb9b96d0ba2713981008a86b1,Structural Health Monitoring: A Machine Learning Perspective,"This book focuses on structural health monitoring in the context of machine learning. The authors review the technical literature and include case studies. Chapters include: operational evaluation, sensing and data acquisition, introduction to probability and statistics, machine learning and statistical pattern recognition, and data prognosis.",2012.0,"C. Farrar, K. Worden"
2dcef55a07f8607a819c21fe84131ea269cc2e3c,https://www.semanticscholar.org/paper/2dcef55a07f8607a819c21fe84131ea269cc2e3c,Deep Unsupervised Learning using Nonequilibrium Thermodynamics,"A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.",2015.0,"Jascha Narain Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, S. Ganguli"
cc242e85615d38ab166295753823180ec9099951,https://www.semanticscholar.org/paper/cc242e85615d38ab166295753823180ec9099951,Scikit-learn: Machine Learning Without Learning the Machinery,"Machine learning is a pervasive development at the intersection of statistics and computer science. While it can benefit many data-related applications, the technical nature of the research literature and the corresponding algorithms slows down its adoption. Scikit-learn is an open-source software project that aims at making machine learning accessible to all, whether it be in academia or in industry. It benefits from the general-purpose Python language, which is both broadly adopted in the scientific world, and supported by a thriving ecosystem of contributors. Here we give a quick introduction to scikit-learn as well as to machine-learning basics.",2015.0,"G. Varoquaux, L. Buitinck, Gilles Louppe, O. Grisel, Fabian Pedregosa, A. Mueller"
0173ca962e4ab3d084c89568345e06f67d3d7efc,https://www.semanticscholar.org/paper/0173ca962e4ab3d084c89568345e06f67d3d7efc,Hyperparameter Search in Machine Learning,"We introduce the hyperparameter search problem in the field of machine learning and discuss its main challenges from an optimization perspective. Machine learning methods attempt to build models that capture some element of interest based on given data. Most common learning algorithms feature a set of hyperparameters that must be determined before training commences. The choice of hyperparameters can significantly affect the resulting model's performance, but determining good values can be complex; hence a disciplined, theoretically sound search strategy is essential.",2015.0,"Marc Claesen, B. Moor"
7da323e7103245eeaed32367c46abe3f4913df86,https://www.semanticscholar.org/paper/7da323e7103245eeaed32367c46abe3f4913df86,A survey of techniques for internet traffic classification using machine learning,"The research community has begun looking for IP traffic classification techniques that do not rely on `well known¿ TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.",2008.0,"Thuy T. T. Nguyen, G. Armitage"
bdb5f7fc1608dd80e2d3fdb1575cec2d3c074ad6,https://www.semanticscholar.org/paper/bdb5f7fc1608dd80e2d3fdb1575cec2d3c074ad6,A survey of feature selection and feature extraction techniques in machine learning,"Dimensionality reduction as a preprocessing step to machine learning is effective in removing irrelevant and redundant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection and feature extraction methods with respect to efficiency and effectiveness. In the field of machine learning and pattern recognition, dimensionality reduction is important area, where many approaches have been proposed. In this paper, some widely used feature selection and feature extraction techniques have analyzed with the purpose of how effectively these techniques can be used to achieve high performance of learning algorithms that ultimately improves predictive accuracy of classifier. An endeavor to analyze dimensionality reduction techniques briefly with the purpose to investigate strengths and weaknesses of some widely used dimensionality reduction methods is presented.",2014.0,"Samina Khalid, Tehmina Khalil, Shamila Nasreen"
ed5ab1cff7dd3a902eea4a811b15aa5ea3a36b30,https://www.semanticscholar.org/paper/ed5ab1cff7dd3a902eea4a811b15aa5ea3a36b30,MLaaS: Machine Learning as a Service,"The demand for knowledge extraction has been increasing. With the growing amount of data being generated by global data sources (e.g., social media and mobile apps) and the popularization of context-specific data (e.g., the Internet of Things), companies and researchers need to connect all these data and extract valuable information. Machine learning has been gaining much attention in data mining, leveraging the birth of new solutions. This paper proposes an architecture to create a flexible and scalable machine learning as a service. An open source solution was implemented and presented. As a case study, a forecast of electricity demand was generated using real-world sensor and weather data by running different algorithms at the same time.",2015.0,"Mauro Ribeiro, Katarina Grolinger, Miriam A. M. Capretz"
b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e,https://www.semanticscholar.org/paper/b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e,MACHINE LEARNING An Artificial Intelligence Approach,"Research in the area of learning structural descriptions from examples is reviewed, giving primary attention to methods of learning characteristic descrip­ tions of single concepts. In particular, we examine methods for finding the maximally-specific conjunctive generalizations (MSC-generalizations) that cover all of the training examples of a given concept. Various important aspects of structural learning in general are examined, and several criteria for evaluating structural learning methods are presented. Briefly, these criteria include (i) ade­ quacy of the representation language, (ii) generalization rules employed, computational efficiency, and (iv) flexibility and extensibility. Selected learning methods developed by Buchanan, et al., Hayes-Roth, Vere, Winston, and the authors are analyzed according to these criteria. Finally, some goals are sug­ gested for future research.",2009.0,"R. Michalski, Tom Mitchell, Jack Mostow, Bernard Nudel, Michael Rychener, Ross Quinlan, Herbert Simon, Derek Sleeman, Robert Stepp, P. Utgoff"
29b9ff8f4a26acc90e6182e1e749f15f688bc7cf,https://www.semanticscholar.org/paper/29b9ff8f4a26acc90e6182e1e749f15f688bc7cf,Machine learning for quantum mechanics in a nutshell,"Models that combine quantum mechanics (QM) with machine learning (ML) promise to deliver the accuracy of QM at the speed of ML. This hands-on tutorial introduces the reader to QM/ML models based on kernel learning, an elegant, systematically nonlinear form of ML. Pseudocode and a reference implementation are provided, enabling the reader to reproduce results from recent publications where atomization energies of small organic molecules are predicted using kernel ridge regression. © 2015 Wiley Periodicals, Inc.",2015.0,M. Rupp
61ce67533d2dd6605c907146658ccdbc4778a5d8,https://www.semanticscholar.org/paper/61ce67533d2dd6605c907146658ccdbc4778a5d8,Learning a Multi-View Stereo Machine,"We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.",2017.0,"Abhishek Kar, Christian Häne, Jitendra Malik"
68837728232463651283edbb7ef0c93b2f502b2b,https://www.semanticscholar.org/paper/68837728232463651283edbb7ef0c93b2f502b2b,PuDianNao: A Polyvalent Machine Learning Accelerator,"Machine Learning (ML) techniques are pervasive tools in various emerging commercial applications, but have to be accommodated by powerful computer systems to process very large data. Although general-purpose CPUs and GPUs have provided straightforward solutions, their energy-efficiencies are limited due to their excessive supports for flexibility. Hardware accelerators may achieve better energy-efficiencies, but each accelerator often accommodates only a single ML technique (family). According to the famous No-Free-Lunch theorem in the ML domain, however, an ML technique performs well on a dataset may perform poorly on another dataset, which implies that such accelerator may sometimes lead to poor learning accuracy. Even if regardless of the learning accuracy, such accelerator can still become inapplicable simply because the concrete ML task is altered, or the user chooses another ML technique. In this study, we present an ML accelerator called PuDianNao, which accommodates seven representative ML techniques, including k-means, k-nearest neighbors, naive bayes, support vector machine, linear regression, classification tree, and deep neural network. Benefited from our thorough analysis on computational primitives and locality properties of different ML techniques, PuDianNao can perform up to 1056 GOP/s (e.g., additions and multiplications) in an area of 3.51 mm^2, and consumes 596 mW only. Compared with the NVIDIA K20M GPU (28nm process), PuDianNao (65nm process) is 1.20x faster, and can reduce the energy by 128.41x.",2015.0,"Dao-Fu Liu, Tianshi Chen, Shaoli Liu, Jinhong Zhou, Shengyuan Zhou, O. Temam, Xiaobing Feng, Xuehai Zhou, Yunji Chen"
7f8c7783a92d4c2f388902fffb3b378921f9e8ad,https://www.semanticscholar.org/paper/7f8c7783a92d4c2f388902fffb3b378921f9e8ad,"Machine Learning in Wireless Sensor Networks: Algorithms, Strategies, and Applications","Wireless sensor networks (WSNs) monitor dynamic environments that change rapidly over time. This dynamic behavior is either caused by external factors or initiated by the system designers themselves. To adapt to such conditions, sensor networks often adopt machine learning techniques to eliminate the need for unnecessary redesign. Machine learning also inspires many practical solutions that maximize resource utilization and prolong the lifespan of the network. In this paper, we present an extensive literature review over the period 2002-2013 of machine learning methods that were used to address common issues in WSNs. The advantages and disadvantages of each proposed algorithm are evaluated against the corresponding problem. We also provide a comparative guide to aid WSN designers in developing suitable machine learning solutions for their specific application challenges.",2014.0,"Mohammad Abu Alsheikh, Shaowei Lin, D. Niyato, H. Tan"
48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016,https://www.semanticscholar.org/paper/48a17d25d76f9bdf90fdd86d2b3e2739e5bb8016,Determinantal Point Processes for Machine Learning,"Determinantal point processes (DPPs) are elegant probabilistic models of repulsion that arise in quantum physics and random matrix theory. In contrast to traditional structured models like Markov random fields, which become intractable and hard to approximate in the presence of negative correlations, DPPs offer efficient and exact algorithms for sampling, marginalization, conditioning, and other inference tasks. While they have been studied extensively by mathematicians, giving rise to a deep and beautiful theory, DPPs are relatively new in machine learning. Determinantal Point Processes for Machine Learning provides a comprehensible introduction to DPPs, focusing on the intuitions, algorithms, and extensions that are most relevant to the machine learning community, and shows how DPPs can be applied to real-world applications like finding diverse sets of high-quality search results, building informative summaries by selecting diverse sentences from documents, modeling non-overlapping human poses in images or video, and automatically building timelines of important news stories. It presents the general mathematical background to DPPs along with a range of modeling extensions, efficient algorithms, and theoretical results that aim to enable practical modeling and learning.",2012.0,"Alex Kulesza, B. Taskar"
9cf9ba251aa8a41be50a8342839b1813cfc30370,https://www.semanticscholar.org/paper/9cf9ba251aa8a41be50a8342839b1813cfc30370,API design for machine learning software: experiences from the scikit-learn project,"Scikit-learn is an increasingly popular machine learning li- brary. Written in Python, it is designed to be simple and efficient, accessible to non-experts, and reusable in various contexts. In this paper, we present and discuss our design choices for the application programming interface (API) of the project. In particular, we describe the simple and elegant interface shared by all learning and processing units in the library and then discuss its advantages in terms of composition and reusability. The paper also comments on implementation details specific to the Python ecosystem and analyzes obstacles faced by users and developers of the library.",2013.0,"L. Buitinck, G. Louppe, Mathieu Blondel, F. Pedregosa, A. Mueller, O. Grisel, Vlad Niculae, P. Prettenhofer, A. Gramfort, Jaques Grobler, Robert Layton, Jacob VanderPlas, Arnaud Joly, Brian Holt, G. Varoquaux"
04ca5de59edbdd49a9c0502c58331524d220bc8c,https://www.semanticscholar.org/paper/04ca5de59edbdd49a9c0502c58331524d220bc8c,Communication Efficient Distributed Machine Learning with the Parameter Server,"This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from l1 -regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.",2014.0,"Mu Li, D. Andersen, Alex Smola, Kai Yu"
b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57,https://www.semanticscholar.org/paper/b3683f5b5bfa6533b2c0eacd653e19ac1ec33d57,Distributed GraphLab: A Framework for Machine Learning in the Cloud,"While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. 
 
We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.",2012.0,"Yucheng Low, Joseph Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, J. Hellerstein"
79cf9462a583e1889781868cbf8c31e43b36dd2f,https://www.semanticscholar.org/paper/79cf9462a583e1889781868cbf8c31e43b36dd2f,Towards Federated Learning at Scale: System Design,"Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.",2019.0,"Keith Bonawitz, Hubert Eichner, W. Grieskamp, Dzmitry Huba, A. Ingerman, Vladimir Ivanov, Chloé Kiddon, Jakub Konecný, S. Mazzocchi, H. B. McMahan, Timon Van Overveldt, David Petrou, Daniel Ramage, Jason Roselander"
ce615ae61d67db8537e981a0a08da7f0f2ff1cee,https://www.semanticscholar.org/paper/ce615ae61d67db8537e981a0a08da7f0f2ff1cee,Understanding Machine Learning: From Theory to Algorithms,"Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.",2014.0,"Shai Shalev-Shwartz, S. Ben-David"
6eabf6e67c29778265bc9fef3b58b2756c739c83,https://www.semanticscholar.org/paper/6eabf6e67c29778265bc9fef3b58b2756c739c83,Machine Learning for Aerial Image Labeling,"Information extracted from aerial photographs has found applications in a wide range of areas including urban planning, crop and forest management, disaster relief, and climate modeling. At present, much of the extraction is still performed by human experts, making the process slow, costly, and error prone. The goal of this thesis is to develop methods for automatically extracting the locations of objects such as roads, buildings, and trees directly from aerial images. 
We investigate the use of machine learning methods trained on aligned aerial images and possibly outdated maps for labeling the pixels of an aerial image with semantic labels. We show how deep neural networks implemented on modern GPUs can be used to efficiently learn highly discriminative image features. We then introduce new loss functions for training neural networks that are partially robust to incomplete and poorly registered target maps. Finally, we propose two ways of improving the predictions of our system by introducing structure into the outputs of the neural networks. 
We evaluate our system on the largest and most-challenging road and building detection datasets considered in the literature and show that it works reliably under a wide variety of conditions. Furthermore, we are releasing the first large-scale road and building detection datasets to the public in order to facilitate future comparisons with other methods.",2013.0,"Geoffrey E. Hinton, Volodymyr Mnih"
e3948c28d605e0d90e88e160556cfc14fbba57c8,https://www.semanticscholar.org/paper/e3948c28d605e0d90e88e160556cfc14fbba57c8,Incremental and Decremental Support Vector Machine Learning,"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental ""unlearning"" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.",2000.0,"G. Cauwenberghs, T. Poggio"
b4adef6c659ab62943ce1e68db4d9409d2ce3878,https://www.semanticscholar.org/paper/b4adef6c659ab62943ce1e68db4d9409d2ce3878,Machine learning methods in chemoinformatics,"Machine learning algorithms are generally developed in computer science or adjacent disciplines and find their way into chemical modeling by a process of diffusion. Though particular machine learning methods are popular in chemoinformatics and quantitative structure–activity relationships (QSAR), many others exist in the technical literature. This discussion is methods‐based and focused on some algorithms that chemoinformatics researchers frequently use. It makes no claim to be exhaustive. We concentrate on methods for supervised learning, predicting the unknown property values of a test set of instances, usually molecules, based on the known values for a training set. Particularly relevant approaches include Artificial Neural Networks, Random Forest, Support Vector Machine, k‐Nearest Neighbors and naïve Bayes classifiers. WIREs Comput Mol Sci 2014, 4:468–481.",2014.0,John B. O. Mitchell
e8dccfb88a6524a67b6239f6b38a8fdaf15f6b39,https://www.semanticscholar.org/paper/e8dccfb88a6524a67b6239f6b38a8fdaf15f6b39,Quantum Machine Learning: What Quantum Computing Means to Data Mining,"Quantum Machine Learning bridges the gap between abstract developments in quantum computing and the applied research on machine learning. Paring down the complexity of the disciplines involved, it ...",2014.0,P. Wittek
9b0aa51901f05278928bdfcb4e9826a429a81293,https://www.semanticscholar.org/paper/9b0aa51901f05278928bdfcb4e9826a429a81293,Quantum algorithms for supervised and unsupervised machine learning,"Machine-learning tasks frequently involve problems of manipulating and classifying large numbers of vectors in high-dimensional spaces. Classical algorithms for solving such problems typically take time polynomial in the number of vectors and the dimension of the space. Quantum computers are good at manipulating high-dimensional vectors in large tensor product spaces. This paper provides supervised and unsupervised quantum machine learning algorithms for cluster assignment and cluster finding. Quantum machine learning can take time logarithmic in both the number of vectors and their dimension, an exponential speed-up over classical algorithms.",2013.0,"S. Lloyd, M. Mohseni, P. Rebentrost"
819167ace2f0caae7745d2f25a803979be5fbfae,https://www.semanticscholar.org/paper/819167ace2f0caae7745d2f25a803979be5fbfae,The Limitations of Deep Learning in Adversarial Settings,"Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",2015.0,"Nicolas Papernot, P. Mcdaniel, S. Jha, Matt Fredrikson, Z. B. Celik, A. Swami"
01f702f8b1f9d1314587015f1f038af4d5735e77,https://www.semanticscholar.org/paper/01f702f8b1f9d1314587015f1f038af4d5735e77,Opposition-Based Learning: A New Scheme for Machine Intelligence,"Opposition-based learning as a new scheme for machine intelligence is introduced. Estimates and counter-estimates, weights and opposite weights, and actions versus counter-actions are the foundation of this new approach. Examples are provided. Possibilities for extensions of existing learning algorithms are discussed. Preliminary results are provided",2005.0,H. Tizhoosh
3bff76c25f7c416834655ba664553b14eb67a11c,https://www.semanticscholar.org/paper/3bff76c25f7c416834655ba664553b14eb67a11c,Sparse Bayesian Learning and the Relevance Vector Machine,"This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classi cation tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the `relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art `support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while o ering a number of additional advantages. These include the bene ts of probabilistic predictions, automatic estimation of `nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-`Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We o er some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.",2001.0,Michael E. Tipping
4efe53a9653d1481e50382a2c95bce6eb4f6de9d,https://www.semanticscholar.org/paper/4efe53a9653d1481e50382a2c95bce6eb4f6de9d,Kernel Methods and Machine Learning,"Part I. Machine Learning and Kernel Vector Spaces: 1. Fundamentals of machine learning 2. Kernel-induced vector spaces Part II. Dimension-Reduction: Feature Selection and PCA/KPCA: 3. Feature selection 4. PCA and Kernel-PCA Part III. Unsupervised Learning Models for Cluster Analysis: 5. Unsupervised learning for cluster discovery 6. Kernel methods for cluster discovery Part IV. Kernel Ridge Regressors and Variants: 7. Kernel-based regression and regularization analysis 8. Linear regression and discriminant analysis for supervised classification 9. Kernel ridge regression for supervised classification Part V. Support Vector Machines and Variants: 10. Support vector machines 11. Support vector learning models for outlier detection 12. Ridge-SVM learning models Part VI. Kernel Methods for Green Machine Learning Technologies: 13. Efficient kernel methods for learning and classifcation Part VII. Kernel Methods and Statistical Estimation Theory: 14. Statistical regression analysis and errors-in-variables models 15: Kernel methods for estimation, prediction, and system identification Part VIII. Appendices: Appendix A. Validation and test of learning models Appendix B. kNN, PNN, and Bayes classifiers References Index.",2014.0,S. Kung
d82f27f4a8dcee6cbab41ff954cc6c2b7709a693,https://www.semanticscholar.org/paper/d82f27f4a8dcee6cbab41ff954cc6c2b7709a693,Mastering Machine Learning With scikit-learn,"Apply effective learning algorithms to real-world problems using scikit-learn About This BookDesign and troubleshoot machine learning systems for common tasks including regression, classification, and clusteringAcquaint yourself with popular machine learning algorithms, including decision trees, logistic regression, and support vector machinesA practical example-based guide to help you gain expertise in implementing and evaluating machine learning systems using scikit-learnWho This Book Is ForIf you are a software developer who wants to learn how machine learning models work and how to apply them effectively, this book is for you. Familiarity with machine learning fundamentals and Python will be helpful, but is not essential. In Detail This book examines machine learning models including logistic regression, decision trees, and support vector machines, and applies them to common problems such as categorizing documents and classifying images. It begins with the fundamentals of machine learning, introducing you to the supervised-unsupervised spectrum, the uses of training and test data, and evaluating models. You will learn how to use generalized linear models in regression problems, as well as solve problems with text and categorical features.You will be acquainted with the use of logistic regression, regularization, and the various loss functions that are used by generalized linear models. The book will also walk you through an example project that prompts you to label the most uncertain training examples. You will also use an unsupervised Hidden Markov Model to predict stock prices.By the end of the book, you will be an expert in scikit-learn and will be well versed in machine learning",2014.0,Gavin Hackeling
9eca724e2b8e7a20fa1b05b8a9398f86a24b86d6,https://www.semanticscholar.org/paper/9eca724e2b8e7a20fa1b05b8a9398f86a24b86d6,The master algorithm: how the quest for the ultimate learning machine will remake our world,"Nowadays, “machine learning” is present in several aspects of the current world, internet advisors, advertisements and “smart” devices that seem to know what we need in a given moment. These are some examples of the problems solved by machine learning. This book presents the past, the present and the future of the different types of machine learning algorithms. At the beginning of the book, the author takes us to the first years of the computing science, where a programmer had to do absolutely everything by himself to make an algorithm do a certain task. As time passes, there appeared the first algorithms that were capable of programming themselves learning from the available data. The author presents what he himself calls the five “tribes” of machine learning, the essence that defends each one and the kind of problems that are able to solve without problems. With a great amount of simple examples, the author depicts which advantages and disadvantages of the “master” algorithms of each “tribes” are, saying that the problem that a tribe solves perfectly well, another one cannot do it, and the other way about. The author suggests to get the best out of each “tribe” and make a unique learning algorithm able to learn without caring about the problem: the master algorithm.",2015.0,W. Hasperué
06a81f63fc4ccfcf02934647a7c17454b91853b0,https://www.semanticscholar.org/paper/06a81f63fc4ccfcf02934647a7c17454b91853b0,Machine Learning - The Art and Science of Algorithms that Make Sense of Data,"As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.",2012.0,P. Flach
1d122a074c936fcfd95faf44608e377a9d1799c8,https://www.semanticscholar.org/paper/1d122a074c936fcfd95faf44608e377a9d1799c8,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,"Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep"" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",2017.0,"Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, Xiuqiang He"
ca011427853d34ce4ec9ccafde8a70c9eacc3e21,https://www.semanticscholar.org/paper/ca011427853d34ce4ec9ccafde8a70c9eacc3e21,Deep Learning for Computer Vision: A Brief Review,"Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.",2018.0,"A. Voulodimos, N. Doulamis, A. Doulamis, Eftychios E. Protopapadakis"
bf2827963894ed234db7d5a0d9474e1046531ad8,https://www.semanticscholar.org/paper/bf2827963894ed234db7d5a0d9474e1046531ad8,Ensemble Methods in Machine Learning,"Ensemble methods are learning algorithms that construct a set of classiiers and then classify new data points by taking a (weighted) vote of their predictions. The original ensemble method is Bayesian averaging , but more recent algorithms include error-correcting output coding, Bagging, and boosting. This paper reviews these methods and explains why ensembles can often perform better than any single classiier. Some previous studies comparing ensemble methods are reviewed, and some new experiments are presented to uncover the reasons that Adaboost does not overrt rapidly.",2007.0,Thomas G. Dietterich
fa25610fb8586c2b50a3654edc5bb42fa7fc4729,https://www.semanticscholar.org/paper/fa25610fb8586c2b50a3654edc5bb42fa7fc4729,"The Elements of Statistical Learning: Data Mining, Inference, and Prediction","In the words of the authors, the goal of this book was to “bring together many of the important new ideas in learning, and explain them in a statistical framework.” The authors have been quite successful in achieving this objective, and their work is a welcome addition to the statistics and learning literatures. Statistics has always been interdisciplinary, borrowing ideas from diverse  elds and repaying the debt with contributions, both theoretical and practical, to the other intellectual disciplines. For statistical learning, this cross-fertilization is especially noticeable. This book is a valuable resource, both for the statistician needing an introduction to machine learning and related  elds and for the computer scientist wishing to learn more about statistics. Statisticians will especially appreciate that it is written in their own language. The level of the book is roughly that of a second-year doctoral student in statistics, and it will be useful as a textbook for such students. In a stimulating article, Breiman (2001) argued that statistics has been focused too much on a “data modeling culture,” where the model is paramount. Breiman argued instead for an “algorithmic modeling culture,” with emphasis on black-box types of prediction. Breiman’s article is controversial, and in his discussion, Efron objects that “prediction is certainly an interesting subject, but Leo’s paper overstates both its role and our profession’s lack of interest in it.” Although I mostly agree with Efron, I worry that the courses offered by most statistics departments include little, if any, treatment of statistical learning and prediction. (Stanford, where Efron and the authors of this book teach, is an exception.) Graduate students in statistics certainly need to know more than they do now about prediction, machine learning, statistical learning, and data mining (not disjoint subjects). I hope that graduate courses covering the topics of this book will become more common in statistics curricula. Most of the book is focused on supervised learning, where one has inputs and outputs from some system and wishes to predict unknown outputs corresponding to known inputs. The methods discussed for supervised learning include linear and logistic regression; basis expansion, such as splines and wavelets; kernel techniques, such as local regression, local likelihood, and radial basis functions; neural networks; additive models; decision trees based on recursive partitioning, such as CART; and support vector machines. There is a  nal chapter on unsupervised learning, including association rules, cluster analysis, self-organizing maps, principal components and curves, and independent component analysis. Many statisticians will be unfamiliar with at least some of these algorithms. Association rules are popular for mining commercial data in what is called “market basket analysis.” The aim is to discover types of products often purchased together. Such knowledge can be used to develop marketing strategies, such as store or catalog layouts. Self-organizing maps (SOMs) involve essentially constrained k-means clustering, where prototypes are mapped to a two-dimensional curved coordinate system. Independent components analysis is similar to principal components analysis and factor analysis, but it uses higher-order moments to achieve independence, not merely zero correlation between components. A strength of the book is the attempt to organize a plethora of methods into a coherent whole. The relationships among the methods are emphasized. I know of no other book that covers so much ground. Of course, with such broad coverage, it is not possible to cover any single topic in great depth, so this book will encourage further reading. Fortunately, each chapter includes bibliographic notes surveying the recent literature. These notes and the extensive references provide a good introduction to the learning literature, including much outside of statistics. The book might be more suitable as a textbook if less material were covered in greater depth; however, such a change would compromise the book’s usefulness as a reference, and so I am happier with the book as it was written.",2004.0,D. Ruppert
38aff6df1accc456f6cda7d16d4b9ecf418ef21e,https://www.semanticscholar.org/paper/38aff6df1accc456f6cda7d16d4b9ecf418ef21e,Map-Reduce for Machine Learning on Multicore,"We are at the beginning of the multicore era. Computers will have increasingly many cores (processors), but there is still no good programming framework for these architectures, and thus no simple and unified way for machine learning to take advantage of the potential speed up. In this paper, we develop a broadly applicable parallel programming method, one that is easily applied to many different learning algorithms. Our work is in distinct contrast to the tradition in machine learning of designing (often ingenious) ways to speed up a single algorithm at a time. Specifically, we show that algorithms that fit the Statistical Query model [15] can be written in a certain ""summation form,"" which allows them to be easily parallelized on multicore computers. We adapt Google's map-reduce [7] paradigm to demonstrate this parallel speed up technique on a variety of learning algorithms including locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), SVM, ICA, PCA, gaussian discriminant analysis (GDA), EM, and backpropagation (NN). Our experimental results show basically linear speedup with an increasing number of processors.",2006.0,"Cheng-Tao Chu, Sang Kyun Kim, Yi-An Lin, YuanYuan Yu, Gary R. Bradski, A. Ng, K. Olukotun"
184ac0766262312ba76bbdece4e7ffad0aa8180b,https://www.semanticscholar.org/paper/184ac0766262312ba76bbdece4e7ffad0aa8180b,Representation Learning: A Review and New Perspectives,"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning.",2012.0,"Yoshua Bengio, Aaron C. Courville, Pascal Vincent"
a25fbcbbae1e8f79c4360d26aa11a3abf1a11972,https://www.semanticscholar.org/paper/a25fbcbbae1e8f79c4360d26aa11a3abf1a11972,A Survey on Transfer Learning,"A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.",2010.0,"Sinno Jialin Pan, Qiang Yang"
c0f7fcc2e03be4395625b6757b17b8834632b952,https://www.semanticscholar.org/paper/c0f7fcc2e03be4395625b6757b17b8834632b952,Ensemble Machine Learning: Methods and Applications,"It is common wisdom that gathering a variety of views and inputs improves the process of decision making, and, indeed, underpins a democratic society. Dubbed ensemble learning by researchers in computational intelligence and machine learning, it is known to improve a decision systems robustness and accuracy. Now, fresh developments are allowing researchers to unleash the power of ensemble learning in an increasing range of real-world applications. Ensemble learning algorithms such as boosting and random forest facilitate solutions to key computational issues such as face recognition and are now being applied in areas as diverse as object tracking and bioinformatics. Responding to a shortage of literature dedicated to the topic, this volume offers comprehensive coverage of state-of-the-art ensemble learning techniques, including the random forest skeleton tracking algorithm in the Xbox Kinect sensor, which bypasses the need for game controllers. At once a solid theoretical study and a practical guide, the volume is a windfall for researchers and practitioners alike.",2012.0,"Cha Zhang, Yunqian Ma"
63861fbeb7ec41986b85965b9780b428d919919e,https://www.semanticscholar.org/paper/63861fbeb7ec41986b85965b9780b428d919919e,Support vector machine active learning for image retrieval,"Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.",2001.0,"Simon Tong, E. Chang"
8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795,https://www.semanticscholar.org/paper/8e58dc63817a2a26e5a2ddad38d8b1d19d1c3795,Machine Unlearning,"Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36× in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",2019.0,"Lucas Bourtoule, Varun Chandrasekaran, Christopher A. Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, D. Lie, Nicolas Papernot"
ea58af907495e97c93997119db4a59fab5cd3683,https://www.semanticscholar.org/paper/ea58af907495e97c93997119db4a59fab5cd3683,Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier],"This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and ""weaknesses, depending on the application and context in ""which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.",2010.0,"I. Arel, Derek C. Rose, T. Karnowski"
c99179ca3784e3465fd9ed049d7f34b50d39393e,https://www.semanticscholar.org/paper/c99179ca3784e3465fd9ed049d7f34b50d39393e,Ensemble learning: A survey,"Ensemble methods are considered the state‐of‐the art solution for many machine learning challenges. Such methods improve the predictive performance of a single model by training multiple models and combining their predictions. This paper introduce the concept of ensemble learning, reviews traditional, novel and state‐of‐the‐art ensemble methods and discusses current challenges and trends in the field.",2018.0,"Omer Sagi, L. Rokach"
8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f,https://www.semanticscholar.org/paper/8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f,Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations,"Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.",2020.0,"M. Raissi, A. Yazdani, G. Karniadakis"
3803ea42e1fc773db3b1d0fa05f41b5ebf0a61d1,https://www.semanticscholar.org/paper/3803ea42e1fc773db3b1d0fa05f41b5ebf0a61d1,Toward Causal Representation Learning,"The two fields of machine learning and graphical causality arose and are developed separately. However, there is, now, cross-pollination and increasing interest in both fields to benefit from the advances of the other. In this article, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, that is, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.",2021.0,"Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, Y. Bengio"
6d431f835c06afdea45dff6b24486bf301ebdef0,https://www.semanticscholar.org/paper/6d431f835c06afdea45dff6b24486bf301ebdef0,An Overview of Multi-Task Learning in Deep Neural Networks,"Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",2017.0,Sebastian Ruder
85e4dbcff0b63773db298562ae3fff258eea195f,https://www.semanticscholar.org/paper/85e4dbcff0b63773db298562ae3fff258eea195f,Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers,"Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.",2011.0,"Stephen P. Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein"
4187caa4d0d329f47e18377a6cd31ef3f580cfcc,https://www.semanticscholar.org/paper/4187caa4d0d329f47e18377a6cd31ef3f580cfcc,GraphLab: A New Framework For Parallel Machine Learning,"Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.",2010.0,"Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, J. Hellerstein"
bc745811e231d1b4e37d2c56cbd2d67e37ba9032,https://www.semanticscholar.org/paper/bc745811e231d1b4e37d2c56cbd2d67e37ba9032,Machine Learning Paradigms for Speech Recognition: An Overview,"Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem - for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.",2013.0,"L. Deng, Xiao Li"
01e1fa7924b3eb76b73f1828c93805f3ba028bae,https://www.semanticscholar.org/paper/01e1fa7924b3eb76b73f1828c93805f3ba028bae,MLbase: A Distributed Machine-learning System,"Machine learning (ML) and statistical techniques are key to transforming big data into actionable knowledge. In spite of the modern primacy of data, the complexity of existing ML algorithms is often overwhelming|many users do not understand the trade-os and challenges of parameterizing and choosing between dierent learning techniques. Furthermore, existing scalable systems that support machine learning are typically not accessible to ML researchers without a strong background in distributed systems and low-level primitives. In this work, we present our vision for MLbase, a novel system harnessing the power of machine learning for both end-users and ML researchers. MLbase provides (1) a simple declarative way to specify ML tasks, (2) a novel optimizer to select and dynamically adapt the choice of learning algorithm, (3) a set of high-level operators to enable ML researchers to scalably implement a wide range of ML methods without deep systems knowledge, and (4) a new run-time optimized for the data-access patterns of these high-level operators.",2013.0,"Tim Kraska, Ameet Talwalkar, John C. Duchi, Rean Griffith, M. Franklin, Michael I. Jordan"
5776d0fea69d826519ee3649f620e8755a490efe,https://www.semanticscholar.org/paper/5776d0fea69d826519ee3649f620e8755a490efe,Lifelong Machine Learning Systems: Beyond Learning Algorithms,"Lifelong Machine Learning, or LML, considers systems that can learn many tasks from one or more domains over its lifetime. The goal is to sequentially retain learned knowledge and to selectively transfer that knowledge when learning a new task so as to develop more accurate hypotheses or policies. Following a review of prior work on LML, we propose that it is now appropriate for the AI community to move beyond learning algorithms to more seriously consider the nature of systems that are capable of learning over a lifetime. Reasons for our position are presented and potential counter-arguments are discussed. The remainder of the paper contributes by defining LML, presenting a reference framework that considers all forms of machine learning, and listing several key challenges for and benefits from LML research. We conclude with ideas for next steps to advance the field.",2013.0,"D. Silver, Qiang Yang, Lianghao Li"
0e779fd59353a7f1f5b559b9d65fa4bfe367890c,https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c,Geometric Deep Learning: Going beyond Euclidean data,"Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",2016.0,"M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, P. Vandergheynst"
e3d772986d176057aca2f5e3eb783da53b559134,https://www.semanticscholar.org/paper/e3d772986d176057aca2f5e3eb783da53b559134,Unsupervised Machine Translation Using Monolingual Corpora Only,"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.",2017.0,"Guillaume Lample, Ludovic Denoyer, Marc'Aurelio Ranzato"
b90d44f59fcb74c71d3e31f67a3f09efab187a4e,https://www.semanticscholar.org/paper/b90d44f59fcb74c71d3e31f67a3f09efab187a4e,Machine learning in cell biology – teaching computers to recognize phenotypes,"Summary Recent advances in microscope automation provide new opportunities for high-throughput cell biology, such as image-based screening. High-complex image analysis tasks often make the implementation of static and predefined processing rules a cumbersome effort. Machine-learning methods, instead, seek to use intrinsic data structure, as well as the expert annotations of biologists to infer models that can be used to solve versatile data analysis tasks. Here, we explain how machine-learning methods work and what needs to be considered for their successful application in cell biology. We outline how microscopy images can be converted into a data representation suitable for machine learning, and then introduce various state-of-the-art machine-learning algorithms, highlighting recent applications in image-based screening. Our Commentary aims to provide the biologist with a guide to the application of machine learning to microscopy assays and we therefore include extensive discussion on how to optimize experimental workflow as well as the data analysis pipeline.",2013.0,"Christoph Sommer, D. Gerlich"
2f7ad26514bce4df6c8ebc42c90383ef3a974df4,https://www.semanticscholar.org/paper/2f7ad26514bce4df6c8ebc42c90383ef3a974df4,Pylearn2: a machine learning research library,"Pylearn2 is a machine learning research library. This does not just mean that it is a collection of machine learning algorithms that share a common API; it means that it has been designed for flexibility and extensibility in order to facilitate research projects that involve new or unusual use cases. In this paper we give a brief history of the library, an overview of its basic philosophy, a summary of the library's architecture, and a description of how the Pylearn2 community functions socially.",2013.0,"I. Goodfellow, David Warde-Farley, Pascal Lamblin, Vincent Dumoulin, Mehdi Mirza, Razvan Pascanu, J. Bergstra, Frédéric Bastien, Yoshua Bengio"
02ccfc9b550d381b5df4365a2ae48bb5f7f7578e,https://www.semanticscholar.org/paper/02ccfc9b550d381b5df4365a2ae48bb5f7f7578e,Noise2Noise: Learning Image Restoration without Clean Data,"We apply basic statistical reasoning to signal reconstruction by machine learning -- learning to map corrupted observations to clean signals -- with a simple and powerful conclusion: under certain common circumstances, it is possible to learn to restore signals without ever observing clean ones, at performance close or equal to training using clean exemplars. We show applications in photographic noise removal, denoising of synthetic Monte Carlo images, and reconstruction of MRI scans from undersampled inputs, all based on only observing corrupted data.",2018.0,"J. Lehtinen, Jacob Munkberg, J. Hasselgren, S. Laine, Tero Karras, M. Aittala, Timo Aila"
78989616eeeac55b202e3e4205225e7135054185,https://www.semanticscholar.org/paper/78989616eeeac55b202e3e4205225e7135054185,An Introduction to Deep Learning for the Physical Layer,"We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.",2017.0,"Tim O'Shea, J. Hoydis"
260077710ef86047c582bbe505feca36962ca406,https://www.semanticscholar.org/paper/260077710ef86047c582bbe505feca36962ca406,Distributed GraphLab: A Framework for Machine Learning in the Cloud,"While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.",2012.0,"Yucheng Low, Joseph E. Gonzalez, Aapo Kyrola, Danny Bickson, Carlos Guestrin, J. Hellerstein"
e7e25fd534e9e024da329aea546484938df305a5,https://www.semanticscholar.org/paper/e7e25fd534e9e024da329aea546484938df305a5,Gaussian Processes for Machine Learning (GPML) Toolbox,"The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.",2010.0,"C. Rasmussen, H. Nickisch"
4e55e4f5352d843bcf71a046b211dc72d0da1e1c,https://www.semanticscholar.org/paper/4e55e4f5352d843bcf71a046b211dc72d0da1e1c,Human Decisions and Machine Predictions,"Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; and these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals. JEL Codes: C10 (Econometric and statistical methods and methodology), C55 (Large datasets: Modeling and analysis), K40 (Legal procedure, the legal system, and illegal behavior).",2017.0,"J. Kleinberg, Himabindu Lakkaraju, J. Leskovec, J. Ludwig, S. Mullainathan"
481dd25896ac531707870c9b8c179cce20013401,https://www.semanticscholar.org/paper/481dd25896ac531707870c9b8c179cce20013401,Towards Personalized Federated Learning,"In parallel with the rapid adoption of artificial intelligence (AI) empowered by advances in AI research, there has been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest toward privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges, opportunities, and envision promising future trajectories of research toward a new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches.",2021.0,"A. Tan, Han Yu, Li-zhen Cui, Qiang Yang"
2bf7c350a8280e7c593d46a60127f99b21517121,https://www.semanticscholar.org/paper/2bf7c350a8280e7c593d46a60127f99b21517121,On the Variance of the Adaptive Learning Rate and Beyond,"The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: this https URL.",2019.0,"Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, Jiawei Han"
a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3,https://www.semanticscholar.org/paper/a19a69cdb137e83ba4b8d5c99d187b9f44bbc2d3,Learning scikit-learn: Machine Learning in Python,"Experience the benefits of machine learning techniques by applying them to real-world problems using Python and the open source scikit-learn library Overview Use Python and scikit-learn to create intelligent applications Apply regression techniques to predict future behaviour and learn to cluster items in groups by their similarities Make use of classification techniques to perform image recognition and document classification In Detail Machine learning, the art of creating applications that learn from experience and data, has been around for many years. However, in the era of big data, huge amounts of information is being generated. This makes machine learning an unavoidable source of new data-based approximations for problem solving. With Learning scikit-learn: Machine Learning in Python, you will learn to incorporate machine learning in your applications. The book combines an introduction to some of the main concepts and methods in machine learning with practical, hands-on examples of real-world problems. Ranging from handwritten digit recognition to document classification, examples are solved step by step using Scikit-learn and Python. The book starts with a brief introduction to the core concepts of machine learning with a simple example. Then, using real-world applications and advanced features, it takes a deep dive into the various machine learning techniques. You will learn to evaluate your results and apply advanced techniques for preprocessing data. You will also be able to select the best set of features and the best methods for each problem. With Learning scikit-learn: Machine Learning in Python you will learn how to use the Python programming language and the scikit-learn library to build applications that learn from experience, applying the main concepts and techniques of machine learning. What you will learn from this book Set up scikit-learn inside your Python environment Classify objects (from documents to human faces and flower species) based on some of their features, using a variety of methods from Support Vector Machines to Nave Bayes Use Decision Trees to explain the main causes of certain phenomenon such as the Titanic passengers survival Predict house prices using regression techniques Display and analyse groups in your data using dimensionality reduction Make use of different tools to preprocess, extract, and select the learning features Select the best parameters for your models using model selection Improve the way you build your models using parallelization techniques Approach The book adopts a tutorial-based approach to introduce the user to Scikit-learn. Who this book is written for If you are a programmer who wants to explore machine learning and data-based methods to build intelligent applications and enhance your programming skills, this the book for you. No previous experience with machine-learning algorithms is required.",2013.0,"Ral Garreta, Guillermo Moncecchi"
5fb8ba2e3967e079c57aa703bf216c168ea8104f,https://www.semanticscholar.org/paper/5fb8ba2e3967e079c57aa703bf216c168ea8104f,Model-based machine learning,"Several decades of research in the field of machine learning have resulted in a multitude of different algorithms for solving a broad range of problems. To tackle a new application, a researcher typically tries to map their problem onto one of these existing methods, often influenced by their familiarity with specific algorithms and by the availability of corresponding software implementations. In this study, we describe an alternative methodology for applying machine learning, in which a bespoke solution is formulated for each new application. The solution is expressed through a compact modelling language, and the corresponding custom machine learning code is then generated automatically. This model-based approach offers several major advantages, including the opportunity to create highly tailored models for specific scenarios, as well as rapid prototyping and comparison of a range of alternative models. Furthermore, newcomers to the field of machine learning do not have to learn about the huge range of traditional methods, but instead can focus their attention on understanding a single modelling environment. In this study, we show how probabilistic graphical models, coupled with efficient inference algorithms, provide a very flexible foundation for model-based machine learning, and we outline a large-scale commercial application of this framework involving tens of millions of users. We also describe the concept of probabilistic programming as a powerful software environment for model-based machine learning, and we discuss a specific probabilistic programming language called Infer.NET, which has been widely used in practical applications.",2013.0,Charles M. Bishop
5e934c4d76b3c664149f7970ec11c3c8cabab91e,https://www.semanticscholar.org/paper/5e934c4d76b3c664149f7970ec11c3c8cabab91e,Machine learning with R : learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications,"Written as a tutorial to explore and understand the power of R for machine learning. This practical guide that covers all of the need to know topics in a very systematic way. For each machine learning approach, each step in the process is detailed, from preparing the data for analysis to evaluating the results. These steps will build the knowledge you need to apply them to your own data science tasks. Intended for those who want to learn how to use R's machine learning capabilities and gain insight from your data. Perhaps you already know a bit about machine learning, but have never used R; or perhaps you know a little R but are new to machine learning. In either case, this book will get you up and running quickly. It would be helpful to have a bit of familiarity with basic programming concepts, but no prior experience is required.",2013.0,Brett Lantz
ea11efe27e029e391ea52609468353f98d9f946b,https://www.semanticscholar.org/paper/ea11efe27e029e391ea52609468353f98d9f946b,Machine learning on Big Data,"Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.",2013.0,"Tyson Condie, Paul Mineiro, N. Polyzotis, Markus Weimer"
43dcda631f8a0d39949ba3f9e3e22101db4daba0,https://www.semanticscholar.org/paper/43dcda631f8a0d39949ba3f9e3e22101db4daba0,A Machine Learning Framework for Programming by Example,"Learning programs is a timely and interesting challenge. In Programming by Example (PBE), a system attempts to infer a program from input and output examples alone, by searching for a composition of some set of base functions. We show how machine learning can be used to speed up this seemingly hopeless search problem, by learning weights that relate textual features describing the provided input-output examples to plausible sub-components of a program. This generic learning framework lets us address problems beyond the scope of earlier PBE systems. Experiments on a prototype implementation show that learning improves search and ranking on a variety of text processing tasks found on help forums.",2013.0,"A. Menon, O. Tamuz, Sumit Gulwani, B. Lampson, A. Kalai"
e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da,https://www.semanticscholar.org/paper/e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da,Adaptive Federated Learning in Resource Constrained Edge Computing Systems,"Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.",2018.0,"Shiqiang Wang, Tiffany Tuor, Theodoros Salonidis, K. Leung, C. Makaya, T. He, K. Chan"
8729441d734782c3ed532a7d2d9611b438c0a09a,https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a,ADADELTA: An Adaptive Learning Rate Method,"We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",2012.0,Matthew D. Zeiler
da5c65b0ac8b525c3d3d4889bf44d8a48d254a07,https://www.semanticscholar.org/paper/da5c65b0ac8b525c3d3d4889bf44d8a48d254a07,Deep Bayesian Active Learning with Image Data,"Even though active learning forms an important pillar of machine learning, deep learning tools are not prevalent within it. Deep learning poses several difficulties when used in an active learning setting. First, active learning (AL) methods generally rely on being able to learn and update models from small amounts of data. Recent advances in deep learning, on the other hand, are notorious for their dependence on large amounts of data. Second, many AL acquisition functions rely on model uncertainty, yet deep learning methods rarely represent such model uncertainty. In this paper we combine recent advances in Bayesian deep learning into the active learning framework in a practical way. We develop an active learning framework for high dimensional data, a task which has been extremely challenging so far, with very sparse existing literature. Taking advantage of specialised models such as Bayesian convolutional neural networks, we demonstrate our active learning techniques with image data, obtaining a significant improvement on existing active learning approaches. We demonstrate this on both the MNIST dataset, as well as for skin cancer diagnosis from lesion images (ISIC2016 task).",2017.0,"Y. Gal, Riashat Islam, Zoubin Ghahramani"
bf8fe437f779f2098f9af82b534aa51dc9edb06f,https://www.semanticscholar.org/paper/bf8fe437f779f2098f9af82b534aa51dc9edb06f,Scaling Neural Machine Translation,"Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT’14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT’14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",2018.0,"Myle Ott, Sergey Edunov, David Grangier, Michael Auli"
8a0f17e0ee66ad5f50cd35932747e6a806ef03cf,https://www.semanticscholar.org/paper/8a0f17e0ee66ad5f50cd35932747e6a806ef03cf,Applications of Machine Learning in Cancer Prediction and Prognosis,"Machine learning is a branch of artificial intelligence that employs a variety of statistical, probabilistic and optimization techniques that allows computers to “learn” from past examples and to detect hard-to-discern patterns from large, noisy or complex data sets. This capability is particularly well-suited to medical applications, especially those that depend on complex proteomic and genomic measurements. As a result, machine learning is frequently used in cancer diagnosis and detection. More recently machine learning has been applied to cancer prognosis and prediction. This latter approach is particularly interesting as it is part of a growing trend towards personalized, predictive medicine. In assembling this review we conducted a broad survey of the different types of machine learning methods being used, the types of data being integrated and the performance of these methods in cancer prediction and prognosis. A number of trends are noted, including a growing dependence on protein biomarkers and microarray data, a strong bias towards applications in prostate and breast cancer, and a heavy reliance on “older” technologies such artificial neural networks (ANNs) instead of more recently developed or more easily interpretable machine learning methods. A number of published studies also appear to lack an appropriate level of validation or testing. Among the better designed and validated studies it is clear that machine learning methods can be used to substantially (15–25%) improve the accuracy of predicting cancer susceptibility, recurrence and mortality. At a more fundamental level, it is also evident that machine learning is also helping to improve our basic understanding of cancer development and progression.",2006.0,"Joseph A. Cruz, D. Wishart"
7ab0f0da686cd4094fd96f5a30e0b6072525fd09,https://www.semanticscholar.org/paper/7ab0f0da686cd4094fd96f5a30e0b6072525fd09,Deep Learning in Medical Image Analysis.,"This review covers computer-assisted analysis of images in the field of medical imaging. Recent advances in machine learning, especially with regard to deep learning, are helping to identify, classify, and quantify patterns in medical images. At the core of these advances is the ability to exploit hierarchical feature representations learned solely from data, instead of features designed by hand according to domain-specific knowledge. Deep learning is rapidly becoming the state of the art, leading to enhanced performance in various medical applications. We introduce the fundamentals of deep learning methods and review their successes in image registration, detection of anatomical and cellular structures, tissue segmentation, computer-aided disease diagnosis and prognosis, and so on. We conclude by discussing research issues and suggesting future directions for further improvement.",2017.0,"D. Shen, Guorong Wu, Heung-Il Suk"
0ecf8c56300c20622f317e1e6cefdeeb85c513e2,https://www.semanticscholar.org/paper/0ecf8c56300c20622f317e1e6cefdeeb85c513e2,Improving propensity score weighting using machine learning,"Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART‐based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non‐linear and non‐additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non‐linearity or non‐additivity alone. However, under conditions of both moderate non‐additivity and moderate non‐linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright © 2009 John Wiley & Sons, Ltd.",2010.0,"Brian K. Lee, J. Lessler, E. Stuart"
ecf6c42d84351f34e1625a6a2e4cc6526da45c74,https://www.semanticscholar.org/paper/ecf6c42d84351f34e1625a6a2e4cc6526da45c74,Representation Learning on Graphs: Methods and Applications,"Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.",2017.0,"William L. Hamilton, Rex Ying, J. Leskovec"
c088a41395a9f08d719be8479dc11ddecf047530,https://www.semanticscholar.org/paper/c088a41395a9f08d719be8479dc11ddecf047530,Density Ratio Estimation in Machine Learning,"Machine learning is an interdisciplinary field of science and engineering that studies mathematical theories and practical applications of systems that learn. This book introduces theories, methods, and applications of density ratio estimation, which is a newly emerging paradigm in the machine learning community. Various machine learning problems such as non-stationarity adaptation, outlier detection, dimensionality reduction, independent component analysis, clustering, classification, and conditional density estimation can be systematically solved via the estimation of probability density ratios. The authors offer a comprehensive introduction of various density ratio estimators including methods via density estimation, moment matching, probabilistic classification, density fitting, and density ratio fitting as well as describing how these can be applied to machine learning. The book also provides mathematical theories for density ratio estimation including parametric and non-parametric convergence analysis and numerical stability analysis to complete the first and definitive treatment of the entire framework of density ratio estimation in machine learning.",2012.0,"Masashi Sugiyama, Taiji Suzuki, T. Kanamori"
8dd53f10ca5fa14faeed2bd2951d247f1ac60f40,https://www.semanticscholar.org/paper/8dd53f10ca5fa14faeed2bd2951d247f1ac60f40,A State-of-the-Art Survey on Deep Learning Theory and Architectures,"In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models.",2019.0,"Md. Zahangir Alom, T. Taha, C. Yakopcic, Stefan Westberg, P. Sidike, Mst Shamima Nasrin, Mahmudul Hasan, Brian C. Van Essen, A. Awwal, V. Asari"
2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98,https://www.semanticscholar.org/paper/2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98,Deep learning for sentiment analysis: A survey,"Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state‐of‐the‐art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.",2018.0,"Lei Zhang, Shuai Wang, B. Liu"
c2a7afbb5609a723f8eea91bfde4b02579b048d6,https://www.semanticscholar.org/paper/c2a7afbb5609a723f8eea91bfde4b02579b048d6,Unsupervised Neural Machine Translation,"In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.",2017.0,"Mikel Artetxe, Gorka Labaka, Eneko Agirre, Kyunghyun Cho"
55635aac4cd439a00356f83dad52bd8d7b0ea87e,https://www.semanticscholar.org/paper/55635aac4cd439a00356f83dad52bd8d7b0ea87e,A Survey on Curriculum Learning,"Curriculum learning (CL) is a training strategy that trains a machine learning model from easier data to harder data, which imitates the meaningful learning order in human curricula. As an easy-to-use plug-in, the CL strategy has demonstrated its power in improving the generalization capacity and convergence rate of various models in a wide range of scenarios such as computer vision and natural language processing etc. In this survey article, we comprehensively review CL from various aspects including motivations, definitions, theories, and applications. We discuss works on curriculum learning within a general CL framework, elaborating on how to design a manually predefined curriculum or an automatic curriculum. In particular, we summarize existing CL designs based on the general framework of <italic>Difficulty Measurer <inline-formula><tex-math notation=""LaTeX"">$+$</tex-math><alternatives><mml:math><mml:mo>+</mml:mo></mml:math><inline-graphic xlink:href=""wang-ieq1-3069908.gif""/></alternatives></inline-formula> Training Scheduler</italic> and further categorize the methodologies for automatic CL into four groups, i.e., Self-paced Learning, Transfer Teacher, RL Teacher, and Other Automatic CL. We also analyze principles to select different CL designs that may benefit practical applications. Finally, we present our insights on the relationships connecting CL and other machine learning concepts including transfer learning, meta-learning, continual learning and active learning, etc., then point out challenges in CL as well as potential future research directions deserving further investigations.",2021.0,"Xin Wang, Yudong Chen, Wenwu Zhu"
7ad66cba3b7e3abae7ef33122588512a146f7f77,https://www.semanticscholar.org/paper/7ad66cba3b7e3abae7ef33122588512a146f7f77,A Survey on Multi-Task Learning,"Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.",2017.0,"Yu Zhang, Qiang Yang"
be9811f7e6019d5cd59ff97829a44bb5577bab00,https://www.semanticscholar.org/paper/be9811f7e6019d5cd59ff97829a44bb5577bab00,Machine Learning in Action,"SummaryMachine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification. About the BookA machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns. Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.Machine Learning in Action is a clearly written tutorial for developers. It avoids academic language and takes you straight to the techniques you'll use in your day-to-day work. Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse. You'll understand the concepts and how they fit in with tactical tasks like classification, forecasting, recommendations, and higher-level features like summarization and simplification.Readers need no prior experience with machine learning or statistical processing. Familiarity with Python is helpful.Purchase includes free PDF, ePub, and Kindle eBooks downloadable at manning.com. What's InsideA no-nonsense introduction Examples showing common ML tasks Everyday data analysis Implementing classic algorithms like Apriori and Adaboos=================================== Table of ContentsPART 1 CLASSIFICATION Machine learning basics Classifying with k-Nearest Neighbors Splitting datasets one feature at a time: decision trees Classifying with probability theory: nave Bayes Logistic regression Support vector machines Improving classification with the AdaBoost meta algorithm PART 2 FORECASTING NUMERIC VALUES WITH REGRESSION Predicting numeric values: regression Tree-based regression PART 3 UNSUPERVISED LEARNING Grouping unlabeled items using k-means clustering Association analysis with the Apriori algorithm Efficiently finding frequent itemsets with FP-growth PART 4 ADDITIONAL TOOLS Using principal component analysis to simplify data Simplifying data with the singular value decomposition Big data and MapReduce",2012.0,P. B. Harrington
276194e96ebd620b5cff35a9168bdda39a0be57b,https://www.semanticscholar.org/paper/276194e96ebd620b5cff35a9168bdda39a0be57b,Federated Multi-Task Learning,"Federated learning poses new statistical and systems challenges in training machine learning models over distributed networks of devices. In this work, we show that multi-task learning is naturally suited to handle the statistical challenges of this setting, and propose a novel systems-aware optimization method, MOCHA, that is robust to practical systems issues. Our method and theory for the first time consider issues of high communication cost, stragglers, and fault tolerance for distributed multi-task learning. The resulting method achieves significant speedups compared to alternatives in the federated setting, as we demonstrate through simulations on real-world federated datasets.",2017.0,"Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, Ameet Talwalkar"
642c1b4a9da95ea4239708afc5929a5007a1870d,https://www.semanticscholar.org/paper/642c1b4a9da95ea4239708afc5929a5007a1870d,Tensor2Tensor for Neural Machine Translation,Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.,2018.0,"Ashish Vaswani, Samy Bengio, E. Brevdo, François Chollet, Aidan N. Gomez, Stephan Gouws, Llion Jones, Lukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam M. Shazeer, Jakob Uszkoreit"
b87c0cf95208caacb025bf87d9ba451a87aacaca,https://www.semanticscholar.org/paper/b87c0cf95208caacb025bf87d9ba451a87aacaca,Machine Health Monitoring Using Local Feature-Based Gated Recurrent Unit Networks,"In modern industries, machine health monitoring systems (MHMS) have been applied wildly with the goal of realizing predictive maintenance including failures tracking, downtime reduction, and assets preservation. In the era of big machinery data, data-driven MHMS have achieved remarkable results in the detection of faults after the occurrence of certain failures (diagnosis) and prediction of the future working conditions and the remaining useful life (prognosis). The numerical representation for raw sensory data is the key stone for various successful MHMS. Conventional methods are the labor-extensive as they usually depend on handcrafted features, which require expert knowledge. Inspired by the success of deep learning methods that redefine representation learning from raw data, we propose local feature-based gated recurrent unit (LFGRU) networks. It is a hybrid approach that combines handcrafted feature design with automatic feature learning for machine health monitoring. First, features from windows of input time series are extracted. Then, an enhanced bidirectional GRU network is designed and applied on the generated sequence of local features to learn the representation. A supervised learning layer is finally trained to predict machine condition. Experiments on three machine health monitoring tasks: tool wear prediction, gearbox fault diagnosis, and incipient bearing fault detection verify the effectiveness and generalization of the proposed LFGRU.",2018.0,"Rui Zhao, Dongzhe Wang, Ruqiang Yan, K. Mao, Fei Shen, Jinjiang Wang"
adc61e21eafecfbf6ebecc570f9f913659a2bfb2,https://www.semanticscholar.org/paper/adc61e21eafecfbf6ebecc570f9f913659a2bfb2,Deep Learning--based Text Classification,"Deep learning--based models have surpassed classical machine learning--based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this article, we provide a comprehensive review of more than 150 deep learning--based models for text classification developed in recent years, and we discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and we discuss future research directions.",2020.0,"Shervin Minaee, E. Cambria, Jianfeng Gao"
cd49acefc8d51e324aa562e5337e1c2aff067053,https://www.semanticscholar.org/paper/cd49acefc8d51e324aa562e5337e1c2aff067053,An Overview of Multi-task Learning,"As a promising area in machine learning, multi-task learning (MTL) aims to improve the performance of multiple related learning tasks by leveraging useful information among them. In this paper, we give an overview of MTL by first giving a definition of MTL. Then several different settings of MTL are introduced, including multi-task supervised learning, multi-task unsupervised learning, multi-task semi-supervised learning, multi-task active learning, multi-task reinforcement learning, multi-task online learning and multi-task multi-view learning. For each setting, representative MTL models are presented. In order to speed up the learning process, parallel and distributed MTL models are introduced. Many areas, including computer vision, bioinformatics, health informatics, speech, natural language processing, web applications and ubiquitous computing, use MTL to improve the performance of the applications involved and some representative works are reviewed. Finally, recent theoretical analyses for MTL are presented.",2018.0,"Yu Zhang, Qiang Yang"
208cd4b25768f0096fb2e80e7690473da0e2a563,https://www.semanticscholar.org/paper/208cd4b25768f0096fb2e80e7690473da0e2a563,Meta-learning with differentiable closed-form solvers,"Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",2018.0,"Luca Bertinetto, João F. Henriques, Philip H. S. Torr, A. Vedaldi"
d2972fa779c91162f447d1e15540fba0df4cb547,https://www.semanticscholar.org/paper/d2972fa779c91162f447d1e15540fba0df4cb547,Deploying an interactive machine learning system in an evidence-based practice center: abstrackr,"Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.",2012.0,"Byron C. Wallace, Kevin Small, C. Brodley, J. Lau, T. Trikalinos"
3c8bf504ddc7db1829466b6e9da5251025dd48f1,https://www.semanticscholar.org/paper/3c8bf504ddc7db1829466b6e9da5251025dd48f1,Automatic analysis of malware behavior using machine learning,"Malicious software - so called malware - poses a major threat to the security of computer systems. The amount and diversity of its variants render classic security defenses ineffective, such that millions of hosts in the Internet are infected with malware in the form of computer viruses, Internet worms and Trojan horses. While obfuscation and polymorphism employed by malware largely impede detection at file level, the dynamic analysis of malware binaries during run-time provides an instrument for characterizing and defending against the threat of malicious software. 
 
In this article, we propose a framework for the automatic analysis of malware behavior using machine learning. The framework allows for automatically identifying novel classes of malware with similar behavior (clustering) and assigning unknown malware to these discovered classes (classification). Based on both, clustering and classification, we propose an incremental approach for behavior-based analysis, capable of processing the behavior of thousands of malware binaries on a daily basis. The incremental analysis significantly reduces the run-time overhead of current analysis methods, while providing accurate discovery and discrimination of novel malware variants.",2011.0,"Konrad Rieck, Philipp Trinius, Carsten Willems, Thorsten Holz"
71683e224ab91617950956b5005ed0439a733a71,https://www.semanticscholar.org/paper/71683e224ab91617950956b5005ed0439a733a71,Learning to learn by gradient descent by gradient descent,"The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.",2016.0,"Marcin Andrychowicz, Misha Denil, Sergio Gomez Colmenarejo, Matthew W. Hoffman, David Pfau, T. Schaul, Nando de Freitas"
a20bfec3c95aad003dcb45a21a220c19cca8bb66,https://www.semanticscholar.org/paper/a20bfec3c95aad003dcb45a21a220c19cca8bb66,A Machine Learning Approach to Coreference Resolution of Noun Phrases,"In this paper, we present a learning approach to coreference resolution of noun phrases in unrestricted text. The approach learns from a small, annotated corpus and the task includes resolving not just a certain type of noun phrase (e.g., pronouns) but rather general noun phrases. It also does not restrict the entity types of the noun phrases; that is, coreference is assigned whether they are of organization, person, or other types. We evaluate our approach on common data sets (namely, the MUC-6 and MUC-7 coreference corpora) and obtain encouraging results, indicating that on the general noun phrase coreference task, the learning approach holds promise and achieves accuracy comparable to that of nonlearning approaches. Our system is the first learning-based system that offers performance comparable to that of state-of-the-art nonlearning systems on these data sets.",2001.0,"Wee Meng Soon, H. Ng, Chung Yong Lim"
0294bd2e6638c9a3619d4baaa63202a3c511dccc,https://www.semanticscholar.org/paper/0294bd2e6638c9a3619d4baaa63202a3c511dccc,SplitFed: When Federated Learning Meets Split Learning,"Federated learning (FL) and split learning (SL) are two popular distributed machine learning approaches. Both follow a model-to-data scenario; clients train and test machine learning models without sharing raw data. SL provides better model privacy than FL due to the machine learning model architecture split between clients and the server. Moreover, the split model makes SL a better option for resource-constrained environments. However, SL performs slower than FL due to the relay-based training across multiple clients. In this regard, this paper presents a novel approach, named splitfed learning (SFL), that amalgamates the two approaches eliminating their inherent drawbacks, along with a refined architectural configuration incorporating differential privacy and PixelDP to enhance data privacy and model robustness. Our analysis and empirical results demonstrate that (pure) SFL provides similar test accuracy and communication efficiency as SL while significantly decreasing its computation time per global epoch than in SL for multiple clients. Furthermore, as in SL, its communication efficiency over FL improves with the number of clients. Besides, the performance of SFL with privacy and robustness measures is further evaluated under extended experimental settings.",2020.0,"Chandra Thapa, Pathum Chamikara Mahawaga Arachchige, S. Çamtepe"
ce70030c9d4e2ce2280cc15f50da42ea755d37d3,https://www.semanticscholar.org/paper/ce70030c9d4e2ce2280cc15f50da42ea755d37d3,Neural Networks and Learning Machines,"For graduate-level neural network courses offered in the departments of Computer Engineering, Electrical Engineering, and Computer Science. Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely upto-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists. Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/ Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.",2010.0,S. Haykin
12d1d070a53d4084d88a77b8b143bad51c40c38f,https://www.semanticscholar.org/paper/12d1d070a53d4084d88a77b8b143bad51c40c38f,Reinforcement Learning: A Survey,"This paper surveys the field of reinforcement learning from a computer-science perspective. It is written to be accessible to researchers familiar with machine learning. Both the historical basis of the field and a broad selection of current work are summarized. Reinforcement learning is the problem faced by an agent that learns behavior through trial-and-error interactions with a dynamic environment. The work described here has a resemblance to work in psychology, but differs considerably in the details and in the use of the word ""reinforcement."" The paper discusses central issues of reinforcement learning, including trading off exploration and exploitation, establishing the foundations of the field via Markov decision theory, learning from delayed reinforcement, constructing empirical models to accelerate learning, making use of generalization and hierarchy, and coping with hidden state. It concludes with a survey of some implemented systems and an assessment of the practical utility of current methods for reinforcement learning.",1996.0,"L. Kaelbling, M. Littman, A. Moore"
3904315e2eca50d0086e4b7273f7fd707c652230,https://www.semanticscholar.org/paper/3904315e2eca50d0086e4b7273f7fd707c652230,Meta-Learning with Memory-Augmented Neural Networks,"Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of ""one-shot learning."" Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.",2016.0,"Adam Santoro, Sergey Bartunov, M. Botvinick, D. Wierstra, T. Lillicrap"
0148bbc80ea2f2526ab019a317639b4fb357f399,https://www.semanticscholar.org/paper/0148bbc80ea2f2526ab019a317639b4fb357f399,A Machine Learning Approach to Twitter User Classification,"
 
 This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.
 
",2011.0,"M. Pennacchiotti, Ana-Maria Popescu"
856d5dcba4772328b8fb784494e3d41d39669b0d,https://www.semanticscholar.org/paper/856d5dcba4772328b8fb784494e3d41d39669b0d,Machine Theory of Mind,"Theory of mind (ToM; Premack & Woodruff, 1978) broadly refers to humans' ability to represent the mental states of others, including their desires, beliefs, and intentions. We propose to train a machine to build such models too. We design a Theory of Mind neural network -- a ToMnet -- which uses meta-learning to build models of the agents it encounters, from observations of their behaviour alone. Through this process, it acquires a strong prior model for agents' behaviour, as well as the ability to bootstrap to richer predictions about agents' characteristics and mental states using only a small number of behavioural observations. We apply the ToMnet to agents behaving in simple gridworld environments, showing that it learns to model random, algorithmic, and deep reinforcement learning agents from varied populations, and that it passes classic ToM tasks such as the ""Sally-Anne"" test (Wimmer & Perner, 1983; Baron-Cohen et al., 1985) of recognising that others can hold false beliefs about the world. We argue that this system -- which autonomously learns how to model other agents in its world -- is an important step forward for developing multi-agent AI systems, for building intermediating technology for machine-human interaction, and for advancing the progress on interpretable AI.",2018.0,"Neil C. Rabinowitz, Frank Perbet, H. F. Song, Chiyuan Zhang, S. Eslami, M. Botvinick"
818826f356444f3daa3447755bf63f171f39ec47,https://www.semanticscholar.org/paper/818826f356444f3daa3447755bf63f171f39ec47,Active Learning Literature Survey,"The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer labeled training instances if it is allowed to choose the data from which is learns. An active learner may ask queries in the form of unlabeled instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant but labels are difﬁcult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for active learning, a summary of several problem setting variants, and a discussion of related topics in machine learning research are also presented.",2009.0,Burr Settles
9f1e9e56d80146766bc2316efbc54d8b770a23df,https://www.semanticscholar.org/paper/9f1e9e56d80146766bc2316efbc54d8b770a23df,Deep Reinforcement Learning: An Overview,"We give an overview of recent exciting achievements of deep reinforcement learning (RL). We discuss six core elements, six important mechanisms, and twelve applications. We start with background of machine learning, deep learning and reinforcement learning. Next we discuss core RL elements, including value function, in particular, Deep Q-Network (DQN), policy, reward, model, planning, and exploration. After that, we discuss important mechanisms for RL, including attention and memory, unsupervised learning, transfer learning, multi-agent RL, hierarchical RL, and learning to learn. Then we discuss various applications of RL, including games, in particular, AlphaGo, robotics, natural language processing, including dialogue systems, machine translation, and text generation, computer vision, neural architecture design, business management, finance, healthcare, Industry 4.0, smart grid, intelligent transportation systems, and computer systems. We mention topics not reviewed yet, and list a collection of RL resources. After presenting a brief summary, we close with discussions. 
Please see Deep Reinforcement Learning, arXiv:1810.06339, for a significant update.",2017.0,Yuxi Li
4d931ea98be69882f547ec6c1b42b78c3e13c36d,https://www.semanticscholar.org/paper/4d931ea98be69882f547ec6c1b42b78c3e13c36d,Quantum circuit learning,"We propose a classical-quantum hybrid algorithm for machine learning on near-term quantum processors, which we call quantum circuit learning. A quantum circuit driven by our framework learns a given task by tuning parameters implemented on it. The iterative optimization of the parameters allows us to circumvent the high-depth circuit. Theoretical investigation shows that a quantum circuit can approximate nonlinear functions, which is further confirmed by numerical simulations. Hybridizing a low-depth quantum circuit and a classical computer for machine learning, the proposed framework paves the way toward applications of near-term quantum devices for quantum machine learning.",2018.0,"K. Mitarai, M. Negoro, M. Kitagawa, K. Fujii"
d837267b364b4dc97bb35facda235a19be5ed374,https://www.semanticscholar.org/paper/d837267b364b4dc97bb35facda235a19be5ed374,Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation,"As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.",2012.0,"S. Ben-David, S. Bickel, Karsten M. Borgwardt, Michael Brückner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Choon Hui, Teo, T. Kanamori, Klaus-Robert Müller, Sam Roweis, Neil Rubens, Tobias Scheffer, M. Schmittfull, Bernhard Schölkopf Hidetoshi Shimodaira, A. Smola, A. Storkey, Masashi Sugiyama"
441fbfdcc77187c9f9c41166b5fd42de04de1427,https://www.semanticscholar.org/paper/441fbfdcc77187c9f9c41166b5fd42de04de1427,Deep learning in remote sensing: a review,"Standing at the paradigm shift towards data-intensive science, machine learning techniques are becoming increasingly important. In particular, as a major breakthrough in the field, deep learning has proven as an extremely powerful tool in many fields. Shall we embrace deep learning as the key to all? Or, should we resist a 'black-box' solution? There are controversial opinions in the remote sensing community. In this article, we analyze the challenges of using deep learning for remote sensing data analysis, review the recent advances, and provide resources to make deep learning in remote sensing ridiculously simple to start with. More importantly, we advocate remote sensing scientists to bring their expertise into deep learning, and use it as an implicit general model to tackle unprecedented large-scale influential challenges, such as climate change and urbanization.",2017.0,"Xiaoxiang Zhu, D. Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, F. Fraundorfer"
d05d86db86a4ac0d95e6dcd951b42a9651939793,https://www.semanticscholar.org/paper/d05d86db86a4ac0d95e6dcd951b42a9651939793,Deep Learning Approach for Intelligent Intrusion Detection System,"Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01–0.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.",2019.0,"R. Vinayakumar, M. Alazab, I. K. P. S. Senior Member, P. Poornachandran, Ameer Al-Nemrat, S. Venkatraman"
0ef9ae1ce8c91ce671a211bdda792bf3752d1522,https://www.semanticscholar.org/paper/0ef9ae1ce8c91ce671a211bdda792bf3752d1522,A Deep Learning Approach for Intrusion Detection Using Recurrent Neural Networks,"Intrusion detection plays an important role in ensuring information security, and the key technology is to accurately identify various attacks in the network. In this paper, we explore how to model an intrusion detection system based on deep learning, and we propose a deep learning approach for intrusion detection using recurrent neural networks (RNN-IDS). Moreover, we study the performance of the model in binary classification and multiclass classification, and the number of neurons and different learning rate impacts on the performance of the proposed model. We compare it with those of J48, artificial neural network, random forest, support vector machine, and other machine learning methods proposed by previous researchers on the benchmark data set. The experimental results show that RNN-IDS is very suitable for modeling a classification model with high accuracy and that its performance is superior to that of traditional machine learning classification methods in both binary and multiclass classification. The RNN-IDS model improves the accuracy of the intrusion detection and provides a new research method for intrusion detection.",2017.0,"Chuanlong Yin, Yuefei Zhu, Jin-long Fei, Xin-Zheng He"
4b61c25a86083c20730c9b12737ac6ac4178c364,https://www.semanticscholar.org/paper/4b61c25a86083c20730c9b12737ac6ac4178c364,An Introduction to Deep Reinforcement Learning,"Deep reinforcement learning is the combination of reinforcement learning (RL) and deep learning. This field of research has been able to solve a wide range of complex decision-making tasks that were previously out of reach for a machine. Thus, deep RL opens up many new applications in domains such as healthcare, robotics, smart grids, finance, and many more. This manuscript provides an introduction to deep reinforcement learning models, algorithms and techniques. Particular focus is on the aspects related to generalization and how deep RL can be used for practical applications. We assume the reader is familiar with basic machine learning concepts.",2018.0,"Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G. Bellemare, Joelle Pineau"
8de174ab5419b9d3127695405efd079808e956e8,https://www.semanticscholar.org/paper/8de174ab5419b9d3127695405efd079808e956e8,Curriculum learning,"Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ""curriculum learning"". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",2009.0,"Yoshua Bengio, J. Louradour, R. Collobert, J. Weston"
35aebe08b34e5cb0d012a16563e5c3f6fd17a906,https://www.semanticscholar.org/paper/35aebe08b34e5cb0d012a16563e5c3f6fd17a906,Federated Learning with Personalization Layers,"The emerging paradigm of federated learning strives to enable collaborative training of machine learning models on the network edge without centrally aggregating raw data and hence, improving data privacy. This sharply deviates from traditional machine learning and necessitates the design of algorithms robust to various sources of heterogeneity. Specifically, statistical heterogeneity of data across user devices can severely degrade the performance of standard federated averaging for traditional machine learning applications like personalization with deep learning. This paper pro-posesFedPer, a base + personalization layer approach for federated training of deep feedforward neural networks, which can combat the ill-effects of statistical heterogeneity. We demonstrate effectiveness ofFedPerfor non-identical data partitions ofCIFARdatasetsand on a personalized image aesthetics dataset from Flickr.",2019.0,"Manoj Ghuhan Arivazhagan, V. Aggarwal, Aaditya Kumar Singh, Sunav Choudhary"
7fe7e80bf59a112386211b38ef2ea0b71ae76345,https://www.semanticscholar.org/paper/7fe7e80bf59a112386211b38ef2ea0b71ae76345,Machine Learning that Matters,"Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.",2012.0,K. Wagstaff
6ec7c724aa1d906e9e9f81c58497adddb22175b8,https://www.semanticscholar.org/paper/6ec7c724aa1d906e9e9f81c58497adddb22175b8,An Introduction to Support Vector Machines and Other Kernel-based Learning Methods,"From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software.",2000.0,"N. Cristianini, J. Shawe-Taylor"
ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da,https://www.semanticscholar.org/paper/ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da,Making machine learning models interpretable,"Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.",2012.0,"A. Vellido, J. Martín-Guerrero, P. Lisboa"
b57e6468740d9320f3f14c6079168b8e21366416,https://www.semanticscholar.org/paper/b57e6468740d9320f3f14c6079168b8e21366416,The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches,"Deep learning has demonstrated tremendous success in variety of application domains in the past few years. This new field of machine learning has been growing rapidly and applied in most of the application domains with some new modalities of applications, which helps to open new opportunity. There are different methods have been proposed on different category of learning approaches, which includes supervised, semi-supervised and un-supervised learning. The experimental results show state-of-the-art performance of deep learning over traditional machine learning approaches in the field of Image Processing, Computer Vision, Speech Recognition, Machine Translation, Art, Medical imaging, Medical information processing, Robotics and control, Bio-informatics, Natural Language Processing (NLP), Cyber security, and many more. This report presents a brief survey on development of DL approaches, including Deep Neural Network (DNN), Convolutional Neural Network (CNN), Recurrent Neural Network (RNN) including Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). In addition, we have included recent development of proposed advanced variant DL techniques based on the mentioned DL approaches. Furthermore, DL approaches have explored and evaluated in different application domains are also included in this survey. We have also comprised recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys have published on Deep Learning in Neural Networks [1, 38] and a survey on RL [234]. However, those papers have not discussed the individual advanced techniques for training large scale deep learning models and the recently developed method of generative models [1].",2018.0,"Md. Zahangir Alom, T. Taha, C. Yakopcic, Stefan Westberg, P. Sidike, Mst Shamima Nasrin, B. V. Essen, A. Awwal, V. Asari"
09c5931307cba3f80d3ecc14d02eecfa46463cfe,https://www.semanticscholar.org/paper/09c5931307cba3f80d3ecc14d02eecfa46463cfe,MLPACK: a scalable C++ machine learning library,"MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.",2012.0,"Ryan R. Curtin, J. R. Cline, N. P. Slagle, William B. March, Parikshit Ram, Nishant A. Mehta, Alexander G. Gray"
58209c6db7b321ea7c75395b23ddb5100cd9bf81,https://www.semanticscholar.org/paper/58209c6db7b321ea7c75395b23ddb5100cd9bf81,Machine Learning for the New York City Power Grid,"Power companies can benefit from the use of knowledge discovery methods and statistical machine learning for preventive maintenance. We introduce a general process for transforming historical electrical grid data into models that aim to predict the risk of failures for components and systems. These models can be used directly by power companies to assist with prioritization of maintenance and repair work. Specialized versions of this process are used to produce (1) feeder failure rankings, (2) cable, joint, terminator, and transformer rankings, (3) feeder Mean Time Between Failure (MTBF) estimates, and (4) manhole events vulnerability rankings. The process in its most general form can handle diverse, noisy, sources that are historical (static), semi-real-time, or real-time, incorporates state-of-the-art machine learning algorithms for prioritization (supervised ranking or MTBF), and includes an evaluation of results via cross-validation and blind test. Above and beyond the ranked lists and MTBF estimates are business management interfaces that allow the prediction capability to be integrated directly into corporate planning and decision support; such interfaces rely on several important properties of our general modeling approach: that machine learning features are meaningful to domain experts, that the processing of data is transparent, and that prediction results are accurate enough to support sound decision making. We discuss the challenges in working with historical electrical grid data that were not designed for predictive purposes. The “rawness” of these data contrasts with the accuracy of the statistical models that can be obtained from the process; these models are sufficiently accurate to assist in maintaining New York City's electrical grid.",2012.0,"C. Rudin, D. Waltz, R. Anderson, A. Boulanger, Ansaf Salleb-Aouissi, M. Chow, Haimonti Dutta, Philip Gross, Bert Huang, S. Ierome"
e3eaf3c461114bc34675b0aa33e48ac0be003451,https://www.semanticscholar.org/paper/e3eaf3c461114bc34675b0aa33e48ac0be003451,The random forest algorithm for statistical learning,"Random forests (Breiman, 2001, Machine Learning 45: 5–32) is a statistical- or machine-learning algorithm for prediction. In this article, we introduce a corresponding new command, rforest. We overview the random forest algorithm and illustrate its use with two examples: The first example is a classification problem that predicts whether a credit card holder will default on his or her debt. The second example is a regression problem that predicts the logscaled number of shares of online news articles. We conclude with a discussion that summarizes key points demonstrated in the examples.",2020.0,"Matthias Schonlau, Rosie Yuyan Zou"
43054544c4ff2e25513de8b1a655593b8ff89338,https://www.semanticscholar.org/paper/43054544c4ff2e25513de8b1a655593b8ff89338,Deep Learning for Classical Japanese Literature,"Much of machine learning research focuses on producing models which perform well on benchmark tasks, in turn improving our understanding of the challenges associated with those tasks. From the perspective of ML researchers, the content of the task itself is largely irrelevant, and thus there have increasingly been calls for benchmark tasks to more heavily focus on problems which are of social or cultural relevance. In this work, we introduce Kuzushiji-MNIST, a dataset which focuses on Kuzushiji (cursive Japanese), as well as two larger, more challenging datasets, Kuzushiji-49 and Kuzushiji-Kanji. Through these datasets, we wish to engage the machine learning community into the world of classical Japanese literature. Dataset available at this https URL",2018.0,"Tarin Clanuwat, Mikel Bober-Irizar, A. Kitamoto, Alex Lamb, Kazuaki Yamamoto, David Ha"
704aa23d0be8817dd0aa2d4794068fc167243b85,https://www.semanticscholar.org/paper/704aa23d0be8817dd0aa2d4794068fc167243b85,Findings of the 2017 Conference on Machine Translation (WMT17),"This paper presents the results of the WMT17 shared tasks, which included 
three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task.",2017.0,"Ondrej Bojar, Rajen Chatterjee, C. Federmann, Yvette Graham, B. Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, V. Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphaël Rubino, Lucia Specia, Marco Turchi"
4f2e84f1c5ea7a0d5d8ebfa12a78a869f13d7b59,https://www.semanticscholar.org/paper/4f2e84f1c5ea7a0d5d8ebfa12a78a869f13d7b59,Deep learning for neural networks,"Machine learning algorithms are designed to improve as they encounter more data, making them a versatile technology for understanding large sets of photos such as those accessible from Google Images. Elizabeth Holm, professor of materials science and engineering at Carnegie Mellon University, is leveraging this technology to better understand the enormous number of research images accumulated in the field of materials science. [13]",2018.0,"David Kauchak CS158 – Fall, 1. Admin, Adam Coates"
c58390a5563b672bf02f7fc3f8ca264babf3cc3d,https://www.semanticscholar.org/paper/c58390a5563b672bf02f7fc3f8ca264babf3cc3d,Foundations of Machine Learning,"This graduate-level textbook introduces fundamental concepts and methods in machine learning. It describes several important modern algorithms, provides the theoretical underpinnings of these algorithms, and illustrates key aspects for their application. The authors aim to present novel theoretical tools and concepts while giving concise proofs even for relatively advanced topics. Foundations of Machine Learning fills the need for a general textbook that also offers theoretical details and an emphasis on proofs. Certain topics that are often treated with insufficient attention are discussed in more detail here; for example, entire chapters are devoted to regression, multi-class classification, and ranking. The first three chapters lay the theoretical foundation for what follows, but each remaining chapter is mostly self-contained. The appendix offers a concise probability review, a short introduction to convex optimization, tools for concentration bounds, and several basic properties of matrices and norms used in the book. The book is intended for graduate students and researchers in machine learning, statistics, and related areas; it can be used either as a textbook or as a reference text for a research seminar.",2012.0,"M. Mohri, Afshin Rostamizadeh, Ameet Talwalkar"
9d3e0fce253a4ae4a4456b2f24c03329a2b74621,https://www.semanticscholar.org/paper/9d3e0fce253a4ae4a4456b2f24c03329a2b74621,Deep Learning for Health Informatics,"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.",2017.0,"D. Ravì, Charence Wong, F. Deligianni, M. Berthelot, Javier Andreu-Perez, Benny P. L. Lo, Guang-Zhong Yang"
93d6752f11d5db3687cc9f895f219b1bed7e1023,https://www.semanticscholar.org/paper/93d6752f11d5db3687cc9f895f219b1bed7e1023,"A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection","As data privacy increasingly becomes a critical societal concern, federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To understand the key design system components and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.",2019.0,"Q. Li, Zeyi Wen, Zhaomin Wu, Bingsheng He"
9d3e0fce253a4ae4a4456b2f24c03329a2b74621,https://www.semanticscholar.org/paper/9d3e0fce253a4ae4a4456b2f24c03329a2b74621,Deep Learning for Health Informatics,"With a massive influx of multimodality data, the role of data analytics in health informatics has grown rapidly in the last decade. This has also prompted increasing interests in the generation of analytical, data driven models based on machine learning in health informatics. Deep learning, a technique with its foundation in artificial neural networks, is emerging in recent years as a powerful tool for machine learning, promising to reshape the future of artificial intelligence. Rapid improvements in computational power, fast data storage, and parallelization have also contributed to the rapid uptake of the technology in addition to its predictive power and ability to generate automatically optimized high-level features and semantic interpretation from the input data. This article presents a comprehensive up-to-date review of research employing deep learning in health informatics, providing a critical analysis of the relative merit, and potential pitfalls of the technique as well as its future outlook. The paper mainly focuses on key applications of deep learning in the fields of translational bioinformatics, medical imaging, pervasive sensing, medical informatics, and public health.",2017.0,"D. Ravì, Charence Wong, F. Deligianni, M. Berthelot, Javier Andreu-Perez, Benny P. L. Lo, Guang-Zhong Yang"
93d6752f11d5db3687cc9f895f219b1bed7e1023,https://www.semanticscholar.org/paper/93d6752f11d5db3687cc9f895f219b1bed7e1023,"A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection","As data privacy increasingly becomes a critical societal concern, federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To understand the key design system components and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.",2019.0,"Q. Li, Zeyi Wen, Zhaomin Wu, Bingsheng He"
29e6b12d3c6cd55e04bdfb9c22201f99578f4080,https://www.semanticscholar.org/paper/29e6b12d3c6cd55e04bdfb9c22201f99578f4080,"Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges","The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithm for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Finally, we present several open research challenges with their possible solutions.",2020.0,"L. U. Khan, W. Saad, Zhu Han, E. Hossain, C. Hong"
90dd2cb51c396d283fe38701c8888a853e6d4fcf,https://www.semanticscholar.org/paper/90dd2cb51c396d283fe38701c8888a853e6d4fcf,Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation,"As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.",2012.0,"Masashi Sugiyama, M. Kawanabe"
a206216c3f67605ac6e25b0178c3f156dc0f7ba0,https://www.semanticscholar.org/paper/a206216c3f67605ac6e25b0178c3f156dc0f7ba0,WEKA: a machine learning workbench,"WEKA is a workbench for machine learning that is intended to aid in the application of machine learning techniques to a variety of real-world problems, in particular, those arising from agricultural and horticultural domains. Unlike other machine learning projects, the emphasis is on providing a working environment for the domain specialist rather than the machine learning expert. Lessons learned include the necessity of providing a wealth of interactive tools for data manipulation, result visualization, database linkage, and cross-validation and comparison of rule sets, to complement the basic machine learning tools.<<ETX>>",1994.0,"G. Holmes, A. Donkin, I. Witten"
60f43e763b370af0028317d7f6d94885cdfe390a,https://www.semanticscholar.org/paper/60f43e763b370af0028317d7f6d94885cdfe390a,Federated Learning,"How is it possible to allow multiple data owners to collaboratively train and use a shared prediction model while keeping all the local training data private? Traditional machine learning approaches need to combine all data at one location, typically a data center, which may very well violate the laws on user privacy and data confidentiality. Today, many parts of the world demand that technology companies treat user data carefully according to user-privacy laws. The European Union’s General Data Protection Regulation (GDPR) is a prime example. In this book, we describe how federated machine learning addresses this problem with novel solutions combining distributed machine learning, cryptography and security, and incentive mechanism design based on economic principles and game theory. We explain different types of privacypreserving machine learning solutions and their technological backgrounds, and highlight some representative practical use cases.We show how federated learning can become the foundation of next-generation machine learning that caters to technological and societal needs for responsible AI development and application.",2019.0,"Qiang Yang, Yang Liu, Yong Cheng, Yan Kang, Tianjian Chen, Han Yu"
48234756b7cf798bfeb47328f7c5d597fd4838c2,https://www.semanticscholar.org/paper/48234756b7cf798bfeb47328f7c5d597fd4838c2,ADASYN: Adaptive synthetic sampling approach for imbalanced learning,"This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.",2008.0,"Haibo He, Yang Bai, E. A. Garcia, Shutao Li"
44058a625cb64c311043145655645d8206e272c2,https://www.semanticscholar.org/paper/44058a625cb64c311043145655645d8206e272c2,Scalable Private Learning with PATE,"The rapid adoption of machine learning has increased concerns about the privacy implications of machine learning models trained on sensitive data, such as medical records or other personal information. To address those concerns, one promising approach is Private Aggregation of Teacher Ensembles, or PATE, which transfers to a ""student"" model the knowledge of an ensemble of ""teacher"" models, with intuitive privacy provided by training teachers on disjoint data and strong privacy guaranteed by noisy aggregation of teachers' answers. However, PATE has so far been evaluated only on simple classification tasks like MNIST, leaving unclear its utility when applied to larger-scale learning tasks and real-world datasets. 
In this work, we show how PATE can scale to learning tasks with large numbers of output classes and uncurated, imbalanced training data with errors. For this, we introduce new noisy aggregation mechanisms for teacher ensembles that are more selective and add less noise, and prove their tighter differential-privacy guarantees. Our new mechanisms build on two insights: the chance of teacher consensus is increased by using more concentrated noise and, lacking consensus, no answer need be given to a student. The consensus answers used are more likely to be correct, offer better intuitive privacy, and incur lower-differential privacy cost. Our evaluation shows our mechanisms improve on the original PATE on all measures, and scale to larger tasks with both high utility and very strong privacy ($\varepsilon$ < 1.0).",2018.0,"Nicolas Papernot, Shuang Song, Ilya Mironov, A. Raghunathan, Kunal Talwar, Ú. Erlingsson"
9c4da62e9e89e65ac78ee271e424e8b498053e8c,https://www.semanticscholar.org/paper/9c4da62e9e89e65ac78ee271e424e8b498053e8c,Advances in kernel methods: support vector learning,"Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al.",1999.0,"B. Scholkopf, C. Burges, Alex Smola"
64be9999b68e12d260ba7423f6b55ffd41552ad3,https://www.semanticscholar.org/paper/64be9999b68e12d260ba7423f6b55ffd41552ad3,Deep Learning Applications in Medical Image Analysis,"The tremendous success of machine learning algorithms at image recognition tasks in recent years intersects with a time of dramatically increased use of electronic medical records and diagnostic imaging. This review introduces the machine learning algorithms as applied to medical image analysis, focusing on convolutional neural networks, and emphasizing clinical aspects of the field. The advantage of machine learning in an era of medical big data is that significant hierarchal relationships within the data can be discovered algorithmically without laborious hand-crafting of features. We cover key research areas and applications of medical image classification, localization, detection, segmentation, and registration. We conclude by discussing research obstacles, emerging trends, and possible future directions.",2018.0,"Justin Ker, Lipo Wang, J. Rao, Tchoyoson C. C. Lim"
d7009d10dd80d556ac28becad1e035c7cc2cde90,https://www.semanticscholar.org/paper/d7009d10dd80d556ac28becad1e035c7cc2cde90,The MLIP package: moment tensor potentials with MPI and active learning,"The subject of this paper is the technology (the ‘how’) of constructing machine-learning interatomic potentials, rather than science (the ‘what’ and ‘why’) of atomistic simulations using machine-learning potentials. Namely, we illustrate how to construct moment tensor potentials using active learning as implemented in the MLIP package, focusing on the efficient ways to automatically sample configurations for the training set, how expanding the training set changes the error of predictions, how to set up ab initio calculations in a cost-effective manner, etc. The MLIP package (short for Machine-Learning Interatomic Potentials) is available at https://mlip.skoltech.ru/download/.",2020.0,"I. Novikov, Konstantin Gubaev, E. Podryabinkin, A. Shapeev"
1592fe924114866c1ac559bae33ea789930daa98,https://www.semanticscholar.org/paper/1592fe924114866c1ac559bae33ea789930daa98,Gaussian Processes for Machine Learning,"Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes.",2005.0,"Carl E. Rasmussen, Christopher K. I. Williams"
6c10262a5d4230c7c85fbe528b8bbd9444116bca,https://www.semanticscholar.org/paper/6c10262a5d4230c7c85fbe528b8bbd9444116bca,Finding Density Functionals with Machine Learning,"Machine learning is used to approximate density functionals. For the model problem of the kinetic energy of noninteracting fermions in 1D, mean absolute errors below 1 kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities. A predictor identifies if a test density is within the interpolation region. Via principal component analysis, a projected functional derivative finds highly accurate self-consistent densities. The challenges for application of our method to real electronic structure problems are discussed.",2011.0,"John C. Snyder, M. Rupp, K. Hansen, K. Müller, K. Burke"
90972e7394b5fc884470cf78a657aae3932a8d8a,https://www.semanticscholar.org/paper/90972e7394b5fc884470cf78a657aae3932a8d8a,Using Machine Learning to Detect Cyberbullying,"Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5% accuracy.",2011.0,"Kelly Reynolds, April Kontostathis, Lynne Edwards"
5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7,https://www.semanticscholar.org/paper/5f198e9f1a6cace1fcee5ec53f5d35d9d83af6b7,Can machine learning be secure?,"Machine learning systems offer unparalled flexibility in dealing with evolving input in a variety of applications, such as intrusion detection systems and spam e-mail filtering. However, machine learning algorithms themselves can be a target of attack by a malicious adversary. This paper provides a framework for answering the question, ""Can machine learning be secure?"" Novel contributions of this paper include a taxonomy of different types of attacks on machine learning techniques and systems, a variety of defenses against those attacks, a discussion of ideas that are important to security for machine learning, an analytical model giving a lower bound on attacker's work function, and a list of open problems.",2006.0,"M. Barreno, B. Nelson, Russell Sears, A. Joseph, J. D. Tygar"
0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5,https://www.semanticscholar.org/paper/0bfc6add0390f1b4cfbd0e90ac71475cca88b2d5,A Review on Multi-Label Learning Algorithms,"Multi-label learning studies the problem where each example is represented by a single instance while associated with a set of labels simultaneously. During the past decade, significant amount of progresses have been made toward this emerging machine learning paradigm. This paper aims to provide a timely review on this area with emphasis on state-of-the-art multi-label learning algorithms. Firstly, fundamentals on multi-label learning including formal definition and evaluation metrics are given. Secondly and primarily, eight representative multi-label learning algorithms are scrutinized under common notations with relevant analyses and discussions. Thirdly, several related learning settings are briefly summarized. As a conclusion, online resources and open research problems on multi-label learning are outlined for reference purposes.",2014.0,"Min-Ling Zhang, Zhi-Hua Zhou"
4f6487d61ba6c2afa44be0e870599bb292e27638,https://www.semanticscholar.org/paper/4f6487d61ba6c2afa44be0e870599bb292e27638,Uncovering social spammers: social honeypots + machine learning,"Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.",2010.0,"Kyumin Lee, James Caverlee, Steve Webb"
87a22424c8995f2d6f65b49a3f59eb1b712e8ed7,https://www.semanticscholar.org/paper/87a22424c8995f2d6f65b49a3f59eb1b712e8ed7,Scaling up machine learning: parallel and distributed approaches,"This tutorial gives a broad view of modern approaches for scaling up machine learning and data mining methods on parallel/distributed platforms. Demand for scaling up machine learning is task-specific: for some tasks it is driven by the enormous dataset sizes, for others by model complexity or by the requirement for real-time prediction. Selecting a task-appropriate parallelization platform and algorithm requires understanding their benefits, trade-offs and constraints. This tutorial focuses on providing an integrated overview of state-of-the-art platforms and algorithm choices. These span a range of hardware options (from FPGAs and GPUs to multi-core systems and commodity clusters), programming frameworks (including CUDA, MPI, MapReduce, and DryadLINQ), and learning settings (e.g., semi-supervised and online learning). The tutorial is example-driven, covering a number of popular algorithms (e.g., boosted trees, spectral clustering, belief propagation) and diverse applications (e.g., recommender systems and object recognition in vision).
 The tutorial is based on (but not limited to) the material from our upcoming Cambridge U. Press edited book which is currently in production.
 Visit the tutorial website at http://hunch.net/~large_scale_survey/",2011.0,"Ron Bekkerman, Mikhail Bilenko, J. Langford"
4c4cfa036de5f136e7cfe6fed69f58e3b9d309a0,https://www.semanticscholar.org/paper/4c4cfa036de5f136e7cfe6fed69f58e3b9d309a0,Transfer learning using VGG-16 with Deep Convolutional Neural Network for Classifying Images,"— Traditionally, data mining algorithms and machine learning algorithms are engineered to approach the problems in isolation. These algorithms are employed to train the model in separation on a specific feature space and same distribution. Depending on the business case, a model is trained by applying a machine learning algorithm for a specific task. A widespread assumption in the field of machine learning is that training data and test data must have identical feature spaces with the underlying distribution. On the contrary, in real world this assumption may not hold and thus models need to be rebuilt from the scratch if features and distribution changes. It is an arduous process to collect related training data and rebuild the models. In such cases, Transferring of Knowledge or transfer learning from disparate domains would be desirable. Transfer learning is a method of reusing a pre-trained model knowledge for another task. Transfer learning can be used for classification, regression and clustering problems. This paper uses one of the pre-trained models – VGG - 16 with Deep Convolutional Neural Network to classify images.",2019.0,Srikanth Tammina
2a4ba0c1699965381fb2ba802157a89edd217943,https://www.semanticscholar.org/paper/2a4ba0c1699965381fb2ba802157a89edd217943,Data Mining and Machine Learning in Cybersecurity,"With the rapid advancement of information discovery techniques, machine learning and data mining continue to play a significant role in cybersecurity. Although several conferences, workshops, and journals focus on the fragmented research topics in this area, there has been no single interdisciplinary resource on past and current works and possible paths for future research in this area. This book fills this need. From basic concepts in machine learning and data mining to advanced problems in the machine learning domain, Data Mining and Machine Learning in Cybersecurity provides a unified reference for specific machine learning solutions to cybersecurity problems. It supplies a foundation in cybersecurity fundamentals and surveys contemporary challengesdetailing cutting-edge machine learning and data mining techniques. It also: Unveils cutting-edge techniques for detectingnew attacks Contains in-depth discussions of machine learning solutions to detection problems Categorizes methods for detecting, scanning, and profiling intrusions and anomalies Surveys contemporary cybersecurity problems and unveils state-of-the-art machine learning and data mining solutions Details privacy-preserving data mining methods This interdisciplinary resource includes technique review tables that allow for speedy access to common cybersecurity problems and associated data mining methods. Numerous illustrative figures help readers visualize the workflow of complex techniques and more than forty case studies provide a clear understanding of the design and application of data mining and machine learning techniques in cybersecurity.",2011.0,"S. Dua, Xian Du"
1242d79573397094c5670f55e58c8333cced0beb,https://www.semanticscholar.org/paper/1242d79573397094c5670f55e58c8333cced0beb,Deep Learning: A Primer for Radiologists.,"Deep learning is a class of machine learning methods that are gaining success and attracting interest in many domains, including computer vision, speech recognition, natural language processing, and playing games. Deep learning methods produce a mapping from raw inputs to desired outputs (eg, image classes). Unlike traditional machine learning methods, which require hand-engineered feature extraction from inputs, deep learning methods learn these features directly from data. With the advent of large datasets and increased computing power, these methods can produce models with exceptional performance. These models are multilayer artificial neural networks, loosely inspired by biologic neural systems. Weighted connections between nodes (neurons) in the network are iteratively adjusted based on example pairs of inputs and target outputs by back-propagating a corrective error signal through the network. For computer vision tasks, convolutional neural networks (CNNs) have proven to be effective. Recently, several clinical applications of CNNs have been proposed and studied in radiology for classification, detection, and segmentation tasks. This article reviews the key concepts of deep learning for clinical radiologists, discusses technical requirements, describes emerging applications in clinical radiology, and outlines limitations and future directions in this field. Radiologists should become familiar with the principles and potential applications of deep learning in medical imaging. ©RSNA, 2017.",2017.0,"G. Chartrand, P. Cheng, Eugene Vorontsov, M. Drozdzal, S. Turcotte, C. Pal, S. Kadoury, A. Tang"
1f93588bb075eed40ffdfae2f7907c946e5974d9,https://www.semanticscholar.org/paper/1f93588bb075eed40ffdfae2f7907c946e5974d9,Deep Learning in Alzheimer's Disease: Diagnostic Classification and Prognostic Prediction Using Neuroimaging Data,"Deep learning, a state-of-the-art machine learning approach, has shown outstanding performance over traditional machine learning in identifying intricate structures in complex high-dimensional data, especially in the domain of computer vision. The application of deep learning to early detection and automated classification of Alzheimer's disease (AD) has recently gained considerable attention, as rapid progress in neuroimaging techniques has generated large-scale multimodal neuroimaging data. A systematic review of publications using deep learning approaches and neuroimaging data for diagnostic classification of AD was performed. A PubMed and Google Scholar search was used to identify deep learning papers on AD published between January 2013 and July 2018. These papers were reviewed, evaluated, and classified by algorithm and neuroimaging type, and the findings were summarized. Of 16 studies meeting full inclusion criteria, 4 used a combination of deep learning and traditional machine learning approaches, and 12 used only deep learning approaches. The combination of traditional machine learning for classification and stacked auto-encoder (SAE) for feature selection produced accuracies of up to 98.8% for AD classification and 83.7% for prediction of conversion from mild cognitive impairment (MCI), a prodromal stage of AD, to AD. Deep learning approaches, such as convolutional neural network (CNN) or recurrent neural network (RNN), that use neuroimaging data without pre-processing for feature selection have yielded accuracies of up to 96.0% for AD classification and 84.2% for MCI conversion prediction. The best classification performance was obtained when multimodal neuroimaging and fluid biomarkers were combined. Deep learning approaches continue to improve in performance and appear to hold promise for diagnostic classification of AD using multimodal neuroimaging data. AD research that uses deep learning is still evolving, improving performance by incorporating additional hybrid data types, such as—omics data, increasing transparency with explainable approaches that add knowledge of specific disease-related features and mechanisms.",2019.0,"T. Jo, K. Nho, A. Saykin"
3fa5f45ddbd5184f10bfb92e367493c5a344f207,https://www.semanticscholar.org/paper/3fa5f45ddbd5184f10bfb92e367493c5a344f207,Machine-Learning Research Four Current Directions,"Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.",1997.0,Thomas G. Dietterich
1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0,https://www.semanticscholar.org/paper/1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0,PANFIS: A Novel Incremental Learning Machine,"Most of the dynamics in real-world systems are compiled by shifts and drifts, which are uneasy to be overcome by omnipresent neuro-fuzzy systems. Nonetheless, learning in nonstationary environment entails a system owning high degree of flexibility capable of assembling its rule base autonomously according to the degree of nonlinearity contained in the system. In practice, the rule growing and pruning are carried out merely benefiting from a small snapshot of the complete training data to truncate the computational load and memory demand to the low level. An exposure of a novel algorithm, namely parsimonious network based on fuzzy inference system (PANFIS), is to this end presented herein. PANFIS can commence its learning process from scratch with an empty rule base. The fuzzy rules can be stitched up and expelled by virtue of statistical contributions of the fuzzy rules and injected datum afterward. Identical fuzzy sets may be alluded and blended to be one fuzzy set as a pursuit of a transparent rule base escalating human's interpretability. The learning and modeling performances of the proposed PANFIS are numerically validated using several benchmark problems from real-world or synthetic datasets. The validation includes comparisons with state-of-the-art evolving neuro-fuzzy methods and showcases that our new method can compete and in some cases even outperform these approaches in terms of predictive fidelity and model complexity.",2014.0,"Mahardhika Pratama, S. Anavatti, P. Angelov, E. Lughofer"
5a9e85af0bc45472e9c14e956819f1324085aaa1,https://www.semanticscholar.org/paper/5a9e85af0bc45472e9c14e956819f1324085aaa1,A Review of Machine Learning Algorithms for Text-Documents Classification,"With the increasing availability of electronic documents and the rapid growth of the World Wide Web, the task of automatic categorization of documents became the key method for organizing the information and know- ledge discovery. Proper classification of e-documents, online news, blogs, e-mails and digital libraries need text mining, machine learning and natural language processing tech- niques to get meaningful knowledge. The aim of this paper is to highlight the important techniques and methodologies that are employed in text documents classification, while at the same time making awareness of some of the interesting challenges that remain to be solved, focused mainly on text representation and machine learning techniques. This paper provides a review of the theory and methods of document classification and text mining, focusing on the existing litera- ture.",2010.0,"B. Baharudin, Lam Hong Lee, Khairullah Khan"
5a6b2b9bc3b51ff187826fc2dc21a967e04125ed,https://www.semanticscholar.org/paper/5a6b2b9bc3b51ff187826fc2dc21a967e04125ed,Model-based reinforcement learning: A survey,"Reinforcement learning is an important branch of machine learning and artificial intelligence. Compared with traditional reinforcement learning, model-based reinforcement learning obtains the action of the next state by the model that has been learned",2018.0,"Fengji Yi, Wenlong Fu, Huan Liang"
cb8a1b8d87a3fef15635eb4a32173f9c6f966055,https://www.semanticscholar.org/paper/cb8a1b8d87a3fef15635eb4a32173f9c6f966055,A Survey on Deep Learning,"The field of machine learning is witnessing its golden era as deep learning slowly becomes the leader in this domain. Deep learning uses multiple layers to represent the abstractions of data to build computational models. Some key enabler deep learning algorithms such as generative adversarial networks, convolutional neural networks, and model transfers have completely changed our perception of information processing. However, there exists an aperture of understanding behind this tremendously fast-paced domain, because it was never previously represented from a multiscope perspective. The lack of core understanding renders these powerful methods as black-box machines that inhibit development at a fundamental level. Moreover, deep learning has repeatedly been perceived as a silver bullet to all stumbling blocks in machine learning, which is far from the truth. This article presents a comprehensive review of historical and recent state-of-the-art approaches in visual, audio, and text processing; social network analysis; and natural language processing, followed by the in-depth analysis on pivoting and groundbreaking advances in deep learning applications. It was also undertaken to review the issues faced in deep learning such as unsupervised learning, black-box models, and online learning and to illustrate how these challenges can be transformed into prolific future research avenues.",2018.0,"Samira Pouyanfar, Saad Sadiq, Yilin Yan, Haiman Tian, Yudong Tao, Maria E. Presa-Reyes, Mei-Ling Shyu, Shu‐Ching Chen, S. S. Iyengar"
57e4afe9ca74414fa02f2e0a929b64dc9a03334d,https://www.semanticscholar.org/paper/57e4afe9ca74414fa02f2e0a929b64dc9a03334d,Application of Machine Learning To Epileptic Seizure Detection,"We present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp EEG, a non-invasive measure of the brain's electrical activity. This problem is challenging because the brain's electrical activity is composed of numerous classes with overlapping characteristics. The key steps involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework, and identifying the features critical to separating seizure from other types of brain activity. When trained on 2 or more seizures per patient and tested on 916 hours of continuous EEG from 24 patients, our algorithm detected 96% of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. We also provide information about how to download the CHB-MIT database, which contains the data used in this study.",2010.0,"Ali H. Shoeb, J. Guttag"
7550a05bf00f7b24aed9c1ac3ef000575388d21c,https://www.semanticscholar.org/paper/7550a05bf00f7b24aed9c1ac3ef000575388d21c,Making large scale SVM learning practical,"Training a support vector machine SVM leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples on the shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SVM light is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SVM light V 2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.",1998.0,T. Joachims
f34f684f7a4210db05a852da446e5aa3d8b0bd58,https://www.semanticscholar.org/paper/f34f684f7a4210db05a852da446e5aa3d8b0bd58,Adversarial Machine Learning,"The author briefly introduces the emerging field of adversarial machine learning, in which opponents can cause traditional machine learning algorithms to behave poorly in security applications. He gives a high-level overview and mentions several types of attacks, as well as several types of defenses, and theoretical limits derived from a study of near-optimal evasion.",2011.0,J. D. Tygar
19bb0dce99466077e9bc5a2ad4941607fc28b40c,https://www.semanticscholar.org/paper/19bb0dce99466077e9bc5a2ad4941607fc28b40c,Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples,We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.,2006.0,"M. Belkin, P. Niyogi, Vikas Sindhwani"
26b342333376fd7bf162f24f2e9a9de062f41d36,https://www.semanticscholar.org/paper/26b342333376fd7bf162f24f2e9a9de062f41d36,Optimization for machine learning,This is a draft containing only sra chapter.tex and an abbreviated front matter. Please check that the formatting and small changes have been performed correctly. Please verify the affiliation. Please use this version for sending us future modifications.,2010.0,"S. Sra, Sebastian Nowozin, Stephen J. Wright"
0ba86604228b555475496e200f31878df3aabd6e,https://www.semanticscholar.org/paper/0ba86604228b555475496e200f31878df3aabd6e,Never-Ending Learning,"Whereas people learn many different types of knowledge from diverse experiences over many years, most current machine learning systems acquire just a single function or data model from just a single data set. We propose a neverending learning paradigm for machine learning, to better reflect the more ambitious and encompassing type of learning performed by humans. As a case study, we describe the Never-Ending Language Learner (NELL), which achieves some of the desired properties of a never-ending learner, and we discuss lessons learned. NELL has been learning to read the web 24 hours/day since January 2010, and so far has acquired a knowledge base with over 80 million confidenceweighted beliefs (e.g., servedWith(tea, biscuits)). NELL has also learned millions of features and parameters that enable it to read these beliefs from the web. Additionally, it has learned to reason over these beliefs to infer new beliefs, and is able to extend its ontology by synthesizing new relational predicates. NELL can be tracked online at http://rtw.ml.cmu.edu, and followed on Twitter at @CMUNELL.",2015.0,"Tom Michael Mitchell, William W. Cohen, Estevam Hruschka, Partha P. Talukdar, Bo Yang, J. Betteridge, Andrew Carlson, Bhavana Dalvi, Matt Gardner, B. Kisiel, Jayant Krishnamurthy, N. Lao, Kathryn Mazaitis, Thahir Mohamed, Ndapandula Nakashole, Emmanouil Antonios Platanios, Alan Ritter, M. Samadi, Burr Settles, Richard C. Wang, Derry Tanti Wijaya, A. Gupta, Xinlei Chen, Abulhair Saparov, Malcolm Greaves, Joel Welling"
bcce96a2a074448953fc61a29a84afbdfc8db55a,https://www.semanticscholar.org/paper/bcce96a2a074448953fc61a29a84afbdfc8db55a,Online Learning and Online Convex Optimization,"Online learning is a well established learning paradigm which has both theoretical and practical appeals. The goal of online learning is to make a sequence of accurate predictions given knowledge of the correct answer to previous prediction tasks and possibly additional available information. Online learning has been studied in several research fields including game theory, information theory, and machine learning. It also became of great interest to practitioners due the recent emergence of large scale applications such as online advertisement placement and online web ranking. In this survey we provide a modern overview of online learning. Our goal is to give the reader a sense of some of the interesting ideas and in particular to underscore the centrality of convexity in deriving efficient online learning algorithms. We do not mean to be comprehensive but rather to give a high-level, rigorous yet easy to follow, survey.",2012.0,Shai Shalev-Shwartz
39f1cbef12f64dcdb3a7683f9e70f436a7742328,https://www.semanticscholar.org/paper/39f1cbef12f64dcdb3a7683f9e70f436a7742328,Applications of Deep Learning and Reinforcement Learning to Biological Data,"Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as omics, bioimaging, medical imaging, and (brain/body)–machine interfaces. These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.",2017.0,"M. Mahmud, M. S. Kaiser, A. Hussain, S. Vassanelli"
08c81389b3ac4b8253d718a7cebe04a5536efa78,https://www.semanticscholar.org/paper/08c81389b3ac4b8253d718a7cebe04a5536efa78,Improving Machine Learning Approaches to Coreference Resolution,"We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.",2002.0,"Vincent Ng, Claire Gardent"
c4ae802491724aee021f31f02327b9671cead3dc,https://www.semanticscholar.org/paper/c4ae802491724aee021f31f02327b9671cead3dc,Types of Machine Learning Algorithms,"• Supervised learning --where the algorithm generates a function that maps inputs to desired outputs. One standard formulation of the supervised learning task is the classification problem: the learner is required to learn (to approximate the behavior of) a function which maps a vector into one of several classes by looking at several input-output examples of the function. • Unsupervised learning --which models a set of inputs: labeled examples are not available. • Semi-supervised learning --which combines both labeled and unlabeled examples to generate an appropriate function or classifier. • Reinforcement learning --where the algorithm learns a policy of how to act given an observation of the world. Every action has some impact in the environment, and the environment provides feedback that guides the learning algorithm. • Transduction --similar to supervised learning, but does not explicitly construct a function: instead, tries to predict new outputs based on training inputs, training outputs, and new inputs. • Learning to learn --where the algorithm learns its own inductive bias based on previous experience.",2010.0,T. Ayodele
23dac921784d65530ba798109ded94996c533d47,https://www.semanticscholar.org/paper/23dac921784d65530ba798109ded94996c533d47,The SHOGUN Machine Learning Toolbox,"We have developed a machine learning toolbox, called SHOGUN, which is designed for unified large-scale learning for a broad range of feature types and learning settings. It offers a considerable number of machine learning models such as support vector machines, hidden Markov models, multiple kernel learning, linear discriminant analysis, and more. Most of the specific algorithms are able to deal with several different data classes. We have used this toolbox in several applications from computational biology, some of them coming with no less than 50 million training examples and others with 7 billion test examples. With more than a thousand installations worldwide, SHOGUN is already widely adopted in the machine learning community and beyond. 
 
SHOGUN is implemented in C++ and interfaces to MATLABTM, R, Octave, Python, and has a stand-alone command line interface. The source code is freely available under the GNU General Public License, Version 3 at http://www.shogun-toolbox.org.",2010.0,"S. Sonnenburg, G. Rätsch, S. Henschel, Christian Widmer, Jonas Behr, A. Zien, F. D. Bona, Alexander Binder, Christian Gehl, Vojtech Franc"
298af26244e3ad836c1aa5cf5855d05f5197063d,https://www.semanticscholar.org/paper/298af26244e3ad836c1aa5cf5855d05f5197063d,Machine Learning Methods Without Tears: A Primer for Ecologists,"Machine learning methods, a family of statistical techniques with origins in the field of artificial intelligence, are recognized as holding great promise for the advancement of understanding and prediction about ecological phenomena. These modeling techniques are flexible enough to handle complex problems with multiple interacting elements and typically outcompete traditional approaches (e.g., generalized linear models), making them ideal for modeling ecological systems. Despite their inherent advantages, a review of the literature reveals only a modest use of these approaches in ecology as compared to other disciplines. One potential explanation for this lack of interest is that machine learning techniques do not fall neatly into the class of statistical modeling approaches with which most ecologists are familiar. In this paper, we provide an introduction to three machine learning approaches that can be broadly used by ecologists: classification and regression trees, artificial neural networks, and evolutionary computation. For each approach, we provide a brief background to the methodology, give examples of its application in ecology, describe model development and implementation, discuss strengths and weaknesses, explore the availability of statistical software, and provide an illustrative example. Although the ecological application of machine learning approaches has increased, there remains considerable skepticism with respect to the role of these techniques in ecology. Our review encourages a greater understanding of machine learning approaches and promotes their future application and utilization, while also providing a basis from which ecologists can make informed decisions about whether to select or avoid these approaches in their future modeling endeavors.",2008.0,"J. Olden, J. Lawler, N. L. Poff"
2f2ade8c4944a96a44e6f70ef403b80b058d1725,https://www.semanticscholar.org/paper/2f2ade8c4944a96a44e6f70ef403b80b058d1725,Towards Making Systems Forget with Machine Unlearning,"Today's systems produce a rapidly exploding amount of data, and the data further derives more data, forming a complex data propagation network that we call the data's lineage. There are many reasons that users want systems to forget certain data including its lineage. From a privacy perspective, users who become concerned with new privacy risks of a system often want the system to forget their data and lineage. From a security perspective, if an attacker pollutes an anomaly detector by injecting manually crafted data into the training data set, the detector must forget the injected data to regain security. From a usability perspective, a user can remove noise and incorrect entries so that a recommendation engine gives useful recommendations. Therefore, we envision forgetting systems, capable of forgetting certain data and their lineages, completely and quickly. This paper focuses on making learning systems forget, the process of which we call machine unlearning, or simply unlearning. We present a general, efficient unlearning approach by transforming learning algorithms used by a system into a summation form. To forget a training data sample, our approach simply updates a small number of summations -- asymptotically faster than retraining from scratch. Our approach is general, because the summation form is from the statistical query learning in which many machine learning algorithms can be implemented. Our approach also applies to all stages of machine learning, including feature selection and modeling. Our evaluation, on four diverse learning systems and real-world workloads, shows that our approach is general, effective, fast, and easy to use.",2015.0,"Yinzhi Cao, Junfeng Yang"
293987e14d64dc768a432115c93171ab8653e3bb,https://www.semanticscholar.org/paper/293987e14d64dc768a432115c93171ab8653e3bb,Machine Learning in Medical Imaging,"This article will discuss very different ways of using machine learning that may be less familiar, and we will demonstrate through examples the role of these concepts in medical imaging. Although the term machine learning is relatively recent, the ideas of machine learning have been applied to medical imaging for decades, perhaps most notably in the areas of computer-aided diagnosis (CAD) and functional brain mapping. We will not attempt in this brief article to survey the rich literature of this field. Instead our goals will be 1) to acquaint the reader with some modern techniques that are now staples of the machine-learning field and 2) to illustrate how these techniques can be employed in various ways in medical imaging.",2010.0,"M. Wernick, Yongyi Yang, J. Brankov, G. Yourganov, S. Strother"
ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e,https://www.semanticscholar.org/paper/ccd1282aea3cc7c3d40300d82472fc5f9f54cb8e,Online Learning for Matrix Factorization and Sparse Coding,"Sparse coding--that is, modelling data vectors as sparse linear combinations of basis elements--is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on the large-scale matrix factorization problem that consists of learning the basis set in order to adapt it to specific data. Variations of this problem include dictionary learning in signal processing, non-negative matrix factorization and sparse principal component analysis. In this paper, we propose to address these tasks with a new online optimization algorithm, based on stochastic approximations, which scales up gracefully to large data sets with millions of training samples, and extends naturally to various matrix factorization formulations, making it suitable for a wide range of learning problems. A proof of convergence is presented, along with experiments with natural images and genomic data demonstrating that it leads to state-of-the-art performance in terms of speed and optimization for both small and large data sets.",2009.0,"J. Mairal, F. Bach, J. Ponce, G. Sapiro"
22feb6532228392457664becc48b3096d9858505,https://www.semanticscholar.org/paper/22feb6532228392457664becc48b3096d9858505,Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory,"In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.",2010.0,Sumio Watanabe
0948365ef39ef153e61e9569ade541cf881c7c2a,https://www.semanticscholar.org/paper/0948365ef39ef153e61e9569ade541cf881c7c2a,Learning the Kernel Matrix with Semidefinite Programming,"Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.",2002.0,"Gert R. G. Lanckriet, N. Cristianini, P. Bartlett, L. Ghaoui, Michael I. Jordan"
ff2c491ad5d6df1ac66a75ca4ce63c1cf79452c8,https://www.semanticscholar.org/paper/ff2c491ad5d6df1ac66a75ca4ce63c1cf79452c8,Weka: Practical machine learning tools and techniques with Java implementations,The Waikato Environment for Knowledge Analysis (Weka) is a comprehensive suite of Java class libraries that implement many state-of-the-art machine learning and data mining algorithms. Weka is freely available on the World-Wide Web and accompanies a new text on data mining [1] which documents and fully explains all the algorithms it contains. Applications written using the Weka class libraries can be run on any computer with a Web browsing capability; this allows users to apply machine learning techniques to their own data regardless of computer platform.,1999.0,"I. Witten, E. Frank, Leonard E. Trigg, M. Hall, G. Holmes, S. Cunningham"
fb829c5e6b406bb325fa5a02e05073df12b1772b,https://www.semanticscholar.org/paper/fb829c5e6b406bb325fa5a02e05073df12b1772b,Classes of Kernels for Machine Learning: A Statistics Perspective,"In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.",2002.0,M. Genton
12439a6ff384e95ee2262ee982bc055534e30487,https://www.semanticscholar.org/paper/12439a6ff384e95ee2262ee982bc055534e30487,Online dictionary learning for sparse coding,"Sparse coding---that is, modelling data vectors as sparse linear combinations of basis elements---is widely used in machine learning, neuroscience, signal processing, and statistics. This paper focuses on learning the basis set, also called dictionary, to adapt it to specific data, an approach that has recently proven to be very effective for signal reconstruction and classification in the audio and image processing domains. This paper proposes a new online optimization algorithm for dictionary learning, based on stochastic approximations, which scales up gracefully to large datasets with millions of training samples. A proof of convergence is presented, along with experiments with natural images demonstrating that it leads to faster performance and better dictionaries than classical batch algorithms for both small and large datasets.",2009.0,"J. Mairal, F. Bach, J. Ponce, G. Sapiro"
75e56ef7924972fde2ffc32d7071cd182d0f0f21,https://www.semanticscholar.org/paper/75e56ef7924972fde2ffc32d7071cd182d0f0f21,Selection of Relevant Features in Machine Learning,"In this paper, we review the problem of selecting rele- vant features for use in machine learning. We describe this problem in terms of heuristic search through a space of feature sets, and we identify four dimensions along which approaches to the problem can vary. We consider recent work on feature selection in terms of this framework, then close with some challenges for future work in the area. 1. The Problem of Irrelevant Features accuracy) to grow slowly with the number of irrele- vant attributes. Theoretical results for algorithms that search restricted hypothesis spaces are encouraging. For instance, the worst-case number of errors made by Littlestone's (1987) WINNOW method grows only logarithmically with the number of irrelevant features. Pazzani and Sarrett's (1992) average-case analysis for WHOLIST, a simple conjunctive algorithm, and Lang- ley and Iba's (1993) treatment of the naive Bayesian classifier, suggest that their sample complexities grow at most linearly with the number of irrelevant features. However, the theoretical results are less optimistic for induction methods that search a larger space of concept descriptions. For example, Langley and Iba's (1993) average-case analysis of simple nearest neighbor indicates that its sample complexity grows exponen- tially with the number of irrelevant attributes, even for conjunctive target concepts. Experimental stud- ies of nearest neighbor are consistent with this conclu- sion, and other experiments suggest that similar results hold even for induction algorithms that explicitly se- lect features. For example, the sample complexity for decision-tree methods appears to grow linearly with the number of irrelevants for conjunctive concepts, but exponentially for parity concepts, since the evaluation metric cannot distinguish relevant from irrelevant fea- tures in the latter situation (Langley & Sage, in press). Results of this sort have encouraged machine learn- ing researchers to explore more sophisticated methods for selecting relevant features. In the sections that fol- low, we present a general framework for this task, and then consider some recent examples of work on this important problem.",1994.0,P. Langley
50feafd2cdafdfb1eead14388f6210f6b467eaa0,https://www.semanticscholar.org/paper/50feafd2cdafdfb1eead14388f6210f6b467eaa0,Pareto-Based Multiobjective Machine Learning: An Overview and Case Studies,"Machine learning is inherently a multiobjective task. Traditionally, however, either only one of the objectives is adopted as the cost function or multiple objectives are aggregated to a scalar cost function. This can be mainly attributed to the fact that most conventional learning algorithms can only deal with a scalar cost function. Over the last decade, efforts on solving machine learning problems using the Pareto-based multiobjective optimization methodology have gained increasing impetus, particularly due to the great success of multiobjective optimization using evolutionary algorithms and other population-based stochastic search methods. It has been shown that Pareto-based multiobjective learning approaches are more powerful compared to learning algorithms with a scalar cost function in addressing various topics of machine learning, such as clustering, feature selection, improvement of generalization ability, knowledge extraction, and ensemble generation. One common benefit of the different multiobjective learning approaches is that a deeper insight into the learning problem can be gained by analyzing the Pareto front composed of multiple Pareto-optimal solutions. This paper provides an overview of the existing research on multiobjective machine learning, focusing on supervised learning. In addition, a number of case studies are provided to illustrate the major benefits of the Pareto-based approach to machine learning, e.g., how to identify interpretable models and models that can generalize on unseen data from the obtained Pareto-optimal solutions. Three approaches to Pareto-based multiobjective ensemble generation are compared and discussed in detail. Finally, potentially interesting topics in multiobjective machine learning are suggested.",2008.0,"Yaochu Jin, B. Sendhoff"
83cf4b2f39bcc802b09fd59b69e23068447b26b7,https://www.semanticscholar.org/paper/83cf4b2f39bcc802b09fd59b69e23068447b26b7,Multi-Task Learning for Multiple Language Translation,"In this paper, we investigate the problem of learning a machine translation model that can simultaneously translate sentences from one source language to multiple target languages. Our solution is inspired by the recently proposed neural machine translation model which generalizes machine translation as a sequence learning problem. We extend the neural machine translation to a multi-task learning framework which shares source language representation and separates the modeling of different target language translation. Our framework can be applied to situations where either large amounts of parallel data or limited parallel data is available. Experiments show that our multi-task learning model is able to achieve significantly higher translation quality over individually learned model in both situations on the data sets publicly available.",2015.0,"Daxiang Dong, Hua Wu, W. He, Dianhai Yu, Haifeng Wang"
1f135e98e867ffcde5b359e7b817bbe21f80cfce,https://www.semanticscholar.org/paper/1f135e98e867ffcde5b359e7b817bbe21f80cfce,Deep Learning and Its Application to LHC Physics,"Machine learning has played an important role in the analysis of high-energy physics data for decades. The emergence of deep learning in 2012 allowed for machine learning tools which could adeptly handle higher-dimensional and more complex problems than previously feasible. This review is aimed at the reader who is familiar with high-energy physics but not machine learning. The connections between machine learning and high-energy physics data analysis are explored, followed by an introduction to the core concepts of neural networks, examples of the key results demonstrating the power of deep learning for analysis of LHC data, and discussion of future prospects and concerns.",2018.0,"D. Guest, Kyle Cranmer, D. Whiteson"
bad620c25920edbaba8836032459b135669171c3,https://www.semanticscholar.org/paper/bad620c25920edbaba8836032459b135669171c3,Machine Learning and Its Applications to Biology,"The term machine learning refers to a set of topics dealing with the creation and evaluation of algorithms that facilitate pattern recognition, classification, and prediction, based on models derived from existing data. Two facets of mechanization should be acknowledged when considering machine learning in broad terms. Firstly, it is intended that the classification and prediction tasks can be accomplished by a suitably programmed computing machine. That is, the product of machine learning is a classifier that can be feasibly used on available hardware. Secondly, it is intended that the creation of the classifier should itself be highly mechanized, and should not involve too much human input. This second facet is inevitably vague, but the basic objective is that the use of automatic algorithm construction methods can minimize the possibility that human biases could affect the selection and performance of the algorithm. Both the creation of the algorithm and its operation to classify objects or predict events are to be based on concrete, observable data. 
 
The history of relations between biology and the field of machine learning is long and complex. An early technique [1] for machine learning called the perceptron constituted an attempt to model actual neuronal behavior, and the field of artificial neural network (ANN) design emerged from this attempt. Early work on the analysis of translation initiation sequences [2] employed the perceptron to define criteria for start sites in Escherichia coli. Further artificial neural network architectures such as the adaptive resonance theory (ART) [3] and neocognitron [4] were inspired from the organization of the visual nervous system. In the intervening years, the flexibility of machine learning techniques has grown along with mathematical frameworks for measuring their reliability, and it is natural to hope that machine learning methods will improve the efficiency of discovery and understanding in the mounting volume and complexity of biological data. 
 
This tutorial is structured in four main components. Firstly, a brief section reviews definitions and mathematical prerequisites. Secondly, the field of supervised learning is described. Thirdly, methods of unsupervised learning are reviewed. Finally, a section reviews methods and examples as implemented in the open source data analysis and visualization language R (http://www.r-project.org).",2007.0,"A. Tarca, V. Carey, Xue-wen Chen, R. Romero, S. Drăghici"
8df9c71f09eb0dabf5adf17bee0f6b36190b52b2,https://www.semanticscholar.org/paper/8df9c71f09eb0dabf5adf17bee0f6b36190b52b2,Representational Learning with Extreme Learning Machine for Big Data Liyanaarachchi,"Restricted Boltzmann Machines (RBM) and auto encoders, learns to represent features in a dataset meaningfully and used as the basic building blocks to create deep networks. This paper introduces Extreme Learning Machine based Auto Encoder (ELM-AE), which learns feature representations using singular values and is used as the basic building block for Multi Layer Extreme Learning Machine (ML-ELM). ML-ELM performance is better than auto encoders based deep networks and Deep Belief Networks (DBN), while in par with Deep Boltzmann Machines (DBM) for MNIST dataset. However MLELM is significantly faster than any state−of−the−art deep networks.",,"L. C. Kasun, Hongming Zhou, G. Huang, C. Vong"
9512facd37bba2ff1ece31900c08901bb011f1ce,https://www.semanticscholar.org/paper/9512facd37bba2ff1ece31900c08901bb011f1ce,Using Machine Teaching to Identify Optimal Training-Set Attacks on Machine Learners,"
 
 We investigate a problem at the intersection of machine learning and security: training-set attacks on machine learners. In such attacks an attacker contaminates the training data so that a specific learning algorithm would produce a model profitable to the attacker. Understanding training-set attacks is important as more intelligent agents (e.g. spam filters and robots) are equipped with learning capability and can potentially be hacked via data they receive from the environment. This paper identifies the optimal training-set attack on a broad family of machine learners. First we show that optimal training-set attack can be formulated as a bilevel optimization problem. Then we show that for machine learners with certain Karush-Kuhn-Tucker conditions we can solve the bilevel problem efficiently using gradient methods on an implicit function. As examples, we demonstrate optimal training-set attacks on Support VectorMachines, logistic regression, and linear regression with extensive experiments. Finally, we discuss potential defenses against such attacks.
 
",2015.0,"Shike Mei, Xiaojin Zhu"
302d64cebed714f453c6a1e63effb6fe884a7e80,https://www.semanticscholar.org/paper/302d64cebed714f453c6a1e63effb6fe884a7e80,Java-ML: A Machine Learning Library,"Java-ML is a collection of machine learning and data mining algorithms, which aims to be a readily usable and easily extensible API for both software developers and research scientists. The interfaces for each type of algorithm are kept simple and algorithms strictly follow their respective interface. Comparing different classifiers or clustering algorithms is therefore straightforward, and implementing new algorithms is also easy. The implementations of the algorithms are clearly written, properly documented and can thus be used as a reference. The library is written in Java and is available from http://java-ml.sourceforge.net/ under the GNU GPL license.",2009.0,"T. Abeel, Y. Peer, Y. Saeys"
d36815419c9abc7d185001ccd7cb1d101bd5f218,https://www.semanticscholar.org/paper/d36815419c9abc7d185001ccd7cb1d101bd5f218,Machine learning and data mining,"Over the past decade many organizations have begun to routinely capture huge volumes of historical data describing their operations, their products, and their customers. At the same time, scientists and engineers in many elds nd themselves capturing increasingly complex experimental datasets, such as the gigabytes of functional MRI data that describe brain activity in humans. The eld of data mining addresses the question of how best to use this historical data to discover general regularities and to improve future decisions.",1999.0,Tom M. Mitchell
00ec8123dd2ba03afab7c1fa02f774062f769181,https://www.semanticscholar.org/paper/00ec8123dd2ba03afab7c1fa02f774062f769181,Using Reward Machines for High-Level Task Specification and Decomposition in Reinforcement Learning,"In this paper we propose Reward Machines – a type of finite state machine that supports the specification of reward functions while exposing reward function structure to the learner and supporting decomposition. We then present Q-Learning for Reward Machines (QRM), an algorithm which appropriately decomposes the reward machine and uses off-policy q-learning to simultaneously learn subpolicies for the different components. QRM is guaranteed to converge to an optimal policy in the tabular case, in contrast to Hierarchical Reinforcement Learning methods which might converge to suboptimal policies. We demonstrate this behavior experimentally in two discrete domains. We also show how function approximation methods like neural networks can be incorporated into QRM, and that doing so can find better policies more quickly than hierarchical methods in a domain with a continuous state space.",2018.0,"Rodrigo Toro Icarte, Toryn Q. Klassen, R. Valenzano, Sheila A. McIlraith"
9874b4cfd9e8ef89fd0b753af18c14cbc7c42744,https://www.semanticscholar.org/paper/9874b4cfd9e8ef89fd0b753af18c14cbc7c42744,What do you mean by collaborative learning,"This book arises from a series of workshops on collaborative learning, that gathered together 20 scholars from the disciplines of psychology, education and computer science. The series was part of a research program entitled 'Learning in Humans and Machines' (LHM), launched by Peter Reimann and Hans Spada, and funded by the European Science Foundation. This program aimed to develop a multidisciplinary dialogue on learning, involving mainly scholars from cognitive psychology, educational science, and artificial intelligence (including machine learning). During the preparation of the program, Agnes Blaye, Claire O'Malley, Michael Baker and I developed a theme on collaborative learning. When the program officially began, 12 members were selected to work on this theme and formed the so-called 'task force 5'. I became the coordinator of the group. This group organised two workshops, in Sitges (Spain, 1994) and Aix-en-Provence (France, 1995). In 1996, the group was enriched with new members to reach its final size. Around 20 members met in the subsequent workshops, at Samoens (France, 1996), Houthalen (Belgium, 1996) and Mannheim (Germany, 1997). Several individuals joined the group for some time but have not written a chapter. I would nevertheless like to acknowledge their contributions to our activities: George Bilchev, Stevan Harnad, Calle Jansson and Claire O'Malley.",1999.0,P. Dillenbourg
d91ea30f4f9de817b29bb4ece00f43cb971822b4,https://www.semanticscholar.org/paper/d91ea30f4f9de817b29bb4ece00f43cb971822b4,Machine Learning Benchmarks and Random Forest Regression,"Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called ‘random forests’. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.",2004.0,M. Segal
403a730841c1e8e9e8062df22ff8f43537afd6ee,https://www.semanticscholar.org/paper/403a730841c1e8e9e8062df22ff8f43537afd6ee,Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper,"Feature selection is often an essential data processing step prior to applying a learning algorithm. The removal of irrelevant and redundant information often improves the performance of machine learning algorithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a fllter evaluates features according to heuristics based on general characteristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs much more slowly than a fllter. This paper describes a new fllter approach to feature selection that uses a correlation based heuristic to evaluate the worth of feature subsets When applied as a data preprocessing step for two common machine learning algorithms, the new method compares favourably with the wrapper but requires much less computation.",1999.0,"M. Hall, L. A. Smith"
0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c,https://www.semanticscholar.org/paper/0af81925ffade8b0ddaf84d5fb64a8fa5cbd4c5c,Statistical machine translation,"Statistical machine translation (SMT) treats the translation of natural language as a machine learning problem. By examining many samples of human-produced translation, SMT algorithms automatically learn how to translate. SMT has made tremendous strides in less than two decades, and new ideas are constantly introduced. This survey presents a tutorial overview of the state of the art. We describe the context of the current research and then move to a formal problem description and an overview of the main subproblems: translation modeling, parameter estimation, and decoding. Along the way, we present a taxonomy of some different approaches within these areas. We conclude with an overview of evaluation and a discussion of future directions.",2008.0,"Hany Hassan, Kareem Darwish"
fcb926027ba5001f8f69dc0f1de5ded7d003b6af,https://www.semanticscholar.org/paper/fcb926027ba5001f8f69dc0f1de5ded7d003b6af,A comparison of machine learning techniques for phishing detection,"There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.",2007.0,"Saeed Abu-Nimeh, D. Nappa, Xinlei Wang, S. Nair"
b0485fba23aabda526358f31cb5a382b66a08270,https://www.semanticscholar.org/paper/b0485fba23aabda526358f31cb5a382b66a08270,Text Classification Using Machine Learning Techniques,"Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question- answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.",2005.0,"M. Ikonomakis, S. Kotsiantis, Vassilis Tampakas"
db2d99084f3d25d3934976f9d9fdb6b882593a9d,https://www.semanticscholar.org/paper/db2d99084f3d25d3934976f9d9fdb6b882593a9d,Introduction to machine learning,"The goal of machine learning is to program computers to use example data or past experience to solve a given problem. Many successful applications of machine learning exist already, including systems that analyze past sales data to predict customer behavior, optimize robot behavior so that a task can be completed using minimum resources, and extract knowledge from bioinformatics data. Introduction to Machine Learning is a comprehensive textbook on the subject, covering a broad array of topics not usually included in introductory machine learning texts. In order to present a unified treatment of machine learning problems and solutions, it discusses many methods from different fields, including statistics, pattern recognition, neural networks, artificial intelligence, signal processing, control, and data mining. All learning algorithms are explained so that the student can easily move from the equations in the book to a computer program. The text covers such topics as supervised learning, Bayesian decision theory, parametric methods, multivariate methods, multilayer perceptrons, local models, hidden Markov models, assessing and comparing classification algorithms, and reinforcement learning. New to the second edition are chapters on kernel machines, graphical models, and Bayesian estimation; expanded coverage of statistical tests in a chapter on design and analysis of machine learning experiments; case studies available on the Web (with downloadable results for instructors); and many additional exercises. All chapters have been revised and updated. Introduction to Machine Learning can be used by advanced undergraduates and graduate students who have completed courses in computer programming, probability, calculus, and linear algebra. It will also be of interest to engineers in the field who are concerned with the application of machine learning methods. Adaptive Computation and Machine Learning series",2004.0,Ethem Alpaydin
08dc94471605308669c8d3d8284ba94fcc93e345,https://www.semanticscholar.org/paper/08dc94471605308669c8d3d8284ba94fcc93e345,Deep Learning in Microscopy Image Analysis: A Survey,"Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning.",2018.0,"Fuyong Xing, Yuanpu Xie, H. Su, Fujun Liu, Lin Yang"
a620d007603111ae263c5769c9dc9ac37efd2ddb,https://www.semanticscholar.org/paper/a620d007603111ae263c5769c9dc9ac37efd2ddb,TensorFlow: learning functions at scale,"TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Its computational model is based on dataflow graphs with mutable state. Graph nodes may be mapped to different machines in a cluster, and within each machine to CPUs, GPUs, and other devices. TensorFlow supports a variety of applications, but it particularly targets training and inference with deep neural networks. It serves as a platform for research and for deploying machine learning systems across many areas, such as speech recognition, computer vision, robotics, information retrieval, and natural language processing. In this talk, we describe TensorFlow and outline some of its applications. We also discuss the question of what TensorFlow and deep learning may have to do with functional programming. Although TensorFlow is not purely functional, many of its uses are concerned with optimizing functions (during training), then with applying those functions (during inference). These functions are defined as compositions of simple primitives (as is common in functional programming), with internal data representations that are learned rather than manually designed. TensorFlow is joint work with many other people in the Google Brain team and elsewhere. More information is available at tensorflow.org.",2016.0,Martín Abadi
ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6,https://www.semanticscholar.org/paper/ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6,Practical feature subset selection for machine learning,"Machine learning algorithms automatically extract knowledge from machine readable information. Unfortunately, their success is usually dependant on the quality of the data that they operate on. If the data is inadequate, or contains extraneous and irrelevant information, machine learning algorithms may produce less accurate and less understandable results, or may fail to discover anything of use at all. Feature subset selectors are algorithms that attempt to identify and remove as much irrelevant and redundant information as possible prior to learning. Feature subset selection can result in enhanced performance, a reduced hypothesis search space, and, in some cases, reduced storage requirement. This paper describes a new feature selection algorithm that uses a correlation based heuristic to determine the “goodness” of feature subsets, and evaluates its effectiveness with three common machine learning algorithms. Experiments using a number of standard machine learning data sets are presented. Feature subset selection gave significant improvement for all three algorithms.",1998.0,"M. Hall, L. A. Smith"
50a869bcd6d45ec7fdb317877c3d2a047c2cfc38,https://www.semanticscholar.org/paper/50a869bcd6d45ec7fdb317877c3d2a047c2cfc38,Overfitting and undercomputing in machine learning,"A central problem in machine learning is supervised learning—that is, learning from labeled training data. For example, a learning system for medical diagnosis might be trained with examples of patients whose case records (medical tests, clinical observations) and diagnoses were known. The task of the learning system is to infer a function that predicts the diagnosis of a patient from his or her case records. The function to be learned might be represented as a set of rules, a decision tree, a Bayes network, or a neural network. Learning algorithms essentially operate by searching some space of functions (usually called the hypothesis class) for a function that fits the given data. Because there are usually exponentially many functions, this search cannot actually examine individual hypothesis functions but instead must use some more direct method of constructing the hypothesis functions from the data. This search can usually be formalized by defining an objective function (e.g., number of data points predicted incorrectly) and applying various algorithms to find a function that minimizes this objective function is NP-hard. For example, fitting the weights of a neural network or finding the smallest decision tree are both NP-complete problems [Blum and Rivest, 1989; Quinlan and Rivest 1989]. Hence, heuristic algorithms such as gradient descent (for neural networks) and greedy search (for decision trees) have been applied with great success. Of course, the suboptimality of such heuristic algorithms ~mmediately suggests a reas&able line of research: find ~lgorithms that can search the hypothesis class better. Hence, there has been extensive research in applying secondorder methods to fit neural networks and in conducting much more thorough searches in learning decision trees and rule sets. Ironically, when these algorithms were tested on real datasets, it was found that their performance was often worse than simrde szradient descent or greedy sear~h [&inlan and Cameron-Jones 1995; Weigend 1994]. In short: it appears to be bet~er not to optimize! One of the other important trends in machine-learning research has been the establishment and nurturing of connections between various previously disparate fields, including computational learning theory, connectionist learning, symbolic learning. and statistics. The . connection to statistics was crucial in resolvins$ this naradox. The-key p~oblem arises from the structure of the machine-learning task, A learning algorithm is trained on a set of training data, but then it is applied to make predictions on new data points. The goal is to maximize its predictive accuracy on the new data points—not necessarily its accuracy on the trammg data. Indeed, if we work too hard to find the very best fit to the training data, there is a risk that we will fit the noise in the data by memorizing various peculiarities",1995.0,Thomas G. Dietterich
f455013a3e35fd660eab52f7f4dcb78d1faac266,https://www.semanticscholar.org/paper/f455013a3e35fd660eab52f7f4dcb78d1faac266,Elements of Machine Learning,Elements of Machine Learning by Pat Langley Preface 1. An overview of machine learning 1.1 The science of machine learning 1.2 Nature of the environment 1.3 Nature of representation and performance 1.4 Nature of the learning component 1.5 Five paradigms for machine learning 1.6 Summary of the chapter 2. The induction of logical conjunctions 2.1 General issues in logical induction 2.2 Nonincremental induction of logical conjunctions 2.3 Heuristic induction of logical conjunctions 2.4 Incremental induction of logical conjunctions 2.5 Incremental hill climbing for logical conjunctions 2.6 Genetic algorithms for logical concept induction 2.7 Summary of the chapter 3. The induction of threshold concepts 3.1 General issues for threshold concepts 3.2 Induction of criteria tables 3.3 Induction of linear threshold units 3.4 Induction of spherical threshold units 3.5 Summary of the chapter 4. The induction of competitive concepts 4.1 Instance-based learning 4.2 Learning probabilistic concept descriptions 4.3 Summary of the chapter 5. The construction of decision lists 5.1 General issues in disjunctive concept induction 5.2 Nonincremental learning using separate and conquer 5.3 Incremental induction using separate and conquer 5.4 Induction of decision lists through exceptions 5.5 Induction of competitive disjunctions 5.6 Instance-storing algorithms 5.7 Complementary beam search for disjunctive concepts 5.8 Summary of the chapter 6. Revision and extension of inference networks 6.1 General issues surrounding inference network 6.2 Extending an incomplete inference network 6.3 Inducing specialized concepts with inference networks 6.4 Revising an incorrect inference network 6.5 Network construction and term generation 6.6 Summary of the chapter 7. The formation of concept hierarchies 7.1 General issues concerning concept hierarchies 7.2 Nonincremental divisive formation of hierarchies 7.3 Incremental formation of concept hierarchies 7.4 Agglomerative formation of concept hierarchies 7.5 Variations on hierarchies into other structures 7.7 Summary of the chapter 8. Other issues in concept induction 8.1 Overfitting and pruning 8.2 Selecting useful features 8.3 Induction for numeric prediction 8.4 Unsupervised concept induction 8.5 Inducing relational concepts 8.6 Handling missing features 8.7 Summary of the chapter 9. The formation of transition networks 9.1 General issues for state-transition networks 9.2 Constructing finite-state transition networks 9.3 Forming recursive transition networks 9.4 Learning rules and networks for prediction 9.5 Summary of the chapter 10. The acquisition of search-control knowledge 10.1 General issues in search control 10.2 Reinforcement learning 10.3 Learning state-space heuristics from solution traces 10.4 Learning control knowledge for problem reduction 10.5 Learning control knowledge for means-ends analysis 10.6 The utility of search-control knowledge 10.7 Summary of the chapter 11. The formation of macro-operators 11.1 General issues related to macro-operators 11.2 The creation of simple macro-operators 11.3 The formation of flexible macro-operators 11.4 Problem solving by analogy 11.5 The utility of macro-operators 11.6 Summary of the chapter 12. Prospects for machine learning 12.1 Additional areas of machine learning 12.2 Methodological trends in machine learning 12.3 The future of machine learning References Index,1995.0,Pat Langley
3efcb97c1de1c87832a7a1d99e91801992a938ec,https://www.semanticscholar.org/paper/3efcb97c1de1c87832a7a1d99e91801992a938ec,Crafting Papers on Machine Learning,"This essay gives advice to authors of papers on machine learning, although much of it car-ries over to other computational disciplines. The issues covered include the material that should appear in a well-balanced paper, factors that arise in di(cid:11)erent approaches to evaluation, and ways to improve a submission's ability to communicate ideas to its readers.",2000.0,P. Langley
160a4786dd643d9f758b9cc0758bdd2581524941,https://www.semanticscholar.org/paper/160a4786dd643d9f758b9cc0758bdd2581524941,Machine learning for detection and diagnosis of disease.,"Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.",2006.0,P. Sajda
ab08f2a0b98fe7938d08875eb6125fa518620222,https://www.semanticscholar.org/paper/ab08f2a0b98fe7938d08875eb6125fa518620222,The Need for Open Source Software in Machine Learning,"Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.",2007.0,"S. Sonnenburg, M. Braun, Cheng Soon Ong, Samy Bengio, L. Bottou, G. Holmes, Yann LeCun, K. Müller, Fernando C Pereira, C. Rasmussen, Gunnar Rätsch, B. Scholkopf, Alex Smola, Pascal Vincent, J. Weston, R. C. Williamson"
47c862bf4542415702ff1084f2a3aac33d0e13ea,https://www.semanticscholar.org/paper/47c862bf4542415702ff1084f2a3aac33d0e13ea,Proceedings of the 24th international conference on Machine learning,"This volume contains the papers accepted to the 24th International Conference on Machine Learning (ICML 2007), which was held at Oregon State University in Corvalis, Oregon, from June 20th to 24th, 2007. ICML is the annual conference of the International Machine Learning Society (IMLS), and provides a venue for the presentation and discussion of current research in the field of machine learning. These proceedings can also be found online at: http://www.machinelearning.org. 
 
This year there were 522 submissions to ICML. There was a very thorough review process, in which each paper was reviewed by three program committee (PC) members. Authors were able to respond to the initial reviews, and the PC members could then modify their reviews based on online discussions and the content of this author response. For the first time this year there were two discussion periods led by the senior program committee (SPC), one just before and one after the submission of author responses. At the end of the second discussion period, the SPC members gave their recommendations and provided a summary review for each of their papers. Also for the first time, authors were asked to submit a list of changes with their final accepted papers, which was checked by the SPCs to ensure that reviewer comments had been addressed. Apart from the length restrictions on papers and the compressed time frame, the review process for ICML resembles that of many journal publications. In total, 150 papers were accepted to ICML this year, including a very small number of papers which were initially conditionally accepted, yielding an overall acceptance rate of 29%. 
 
ICML attracts submissions from machine learning researchers around the globe. The 150 accepted papers this year were geographically distributed as follows: 66 papers had a first author from the US, 32 from Europe, 19 from China or Hong Kong, 11 from Canada, 6 from India, 5 each from Australia and Japan, 3 from Israel, and 1 each from Korea, Russia and Taiwan. 
 
In addition to the main program of accepted papers, which includes both a talk and poster presentation for each paper, the ICML program included 3 workshops and 8 tutorials on machine learning topics which are currently of broad interest. We were also extremely pleased to have David Heckerman (Microsoft Research), Joshua Tenenbaum (Massachussetts Institute of Technology), and Bernhard Scholkopf (Max Planck Institute for Biological Cybernetics) as the invited speakers this year. Thanks to sponsorship by the Machine Learning Journal, we were able to award a number of outstanding student paper prizes. 
 
We were fortunate this year that ICML was co-located with the International Conference on Inductive Logic Programming (ILP 2007). ICML and ILP held joint sessions on the first day of ICML 2007.",2007.0,Zoubin Ghahramani
b8eb7da56dae58f77788c33a57b5b810ca930527,https://www.semanticscholar.org/paper/b8eb7da56dae58f77788c33a57b5b810ca930527,The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics,"Many different metrics are used in machine learning and data mining to build and evaluate models. However, there is no general theory of machine learning metrics, that could answer questions such as: When we simultaneously want to optimise two criteria, how can or should they be traded off? Some metrics are inherently independent of class and misclassification cost distributions, while other are not -- can this be made more precise? This paper provides a derivation of ROC space from first principles through 3D ROC space and the skew ratio, and redefines metrics in these dimensions. The paper demonstrates that the graphical depiction of machine learning metrics by means of ROC isometrics gives many useful insights into the characteristics of these metrics, and provides a foundation on which a theory of machine learning metrics can be built.",2003.0,Peter A. Flach
7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724,https://www.semanticscholar.org/paper/7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724,Machine Learning Approaches to Estimating Software Development Effort,"Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data. >",1995.0,"K. Srinivasan, D. Fisher"
a3db81e35f1890511c38d685a2f57de476820518,https://www.semanticscholar.org/paper/a3db81e35f1890511c38d685a2f57de476820518,Interactive machine learning,"Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers. We propose an interactive machine-learning (IML) model that allows users to train, classify/view and correct the classifications. The concept and implementation details of IML are discussed and contrasted with classical machine learning models. Evaluations of two algorithms are also presented. We also briefly describe Image Processing with Crayons (Crayons), which is a tool for creating new camera-based interfaces using a simple painting metaphor. The Crayons tool embodies our notions of interactive machine learning",2003.0,"J. A. Fails, D. Olsen"
28d2503b0f86dd3947bf745efdd609dee7975cd8,https://www.semanticscholar.org/paper/28d2503b0f86dd3947bf745efdd609dee7975cd8,TensorLy: Tensor Learning in Python,"Tensors are higher-order extensions of matrices. While matrix methods form the cornerstone of traditional machine learning and data analysis, tensor methods have been gaining increasing traction. However, software support for tensor operations is not on the same footing. In order to bridge this gap, we have developed TensorLy, a Python library that provides a high-level API for tensor methods and deep tensorized neural networks. TensorLy aims to follow the same standards adopted by the main projects of the Python scientific community, and to seamlessly integrate with them. Its BSD license makes it suitable for both academic and commercial applications. TensorLy's backend system allows users to perform computations with several libraries such as NumPy or PyTorch to name but a few. They can be scaled on multiple CPU or GPU machines. In addition, using the deep-learning frameworks as backend allows to easily design and train deep tensorized neural networks. TensorLy is available at https://github.com/tensorly/tensorly",2016.0,"Jean Kossaifi, Yannis Panagakis, M. Pantic"
463565c30b7a9c12c2ef0558a51cfc7b05055737,https://www.semanticscholar.org/paper/463565c30b7a9c12c2ef0558a51cfc7b05055737,Semi-Supervised Learning (Adaptive Computation and Machine Learning),"If searched for a ebook Semi-Supervised Learning (Adaptive Computation and Machine Learning series) in pdf format, then you have come on to right website. We presented utter variation of this ebook in DjVu, PDF, txt, doc, ePub forms. You may read Semi-Supervised Learning (Adaptive Computation and Machine Learning series) online or downloading. Further, on our site you can read the instructions and diverse artistic eBooks online, or downloading them. We like draw on your regard what our website does not store the eBook itself, but we give ref to the site wherever you can download or read online. If have necessity to downloading Semi-Supervised Learning (Adaptive Computation and Machine Learning series) pdf, in that case you come on to loyal website. We own Semi-Supervised Learning (Adaptive Computation and Machine Learning series) ePub, txt, PDF, DjVu, doc forms. We will be glad if you revert to us over.",2006.0,"O. Chapelle, B. Scholkopf, A. Zien"
fc82788021963ff8e318ffe955829bc68e48943a,https://www.semanticscholar.org/paper/fc82788021963ff8e318ffe955829bc68e48943a,Machine Learning of Temporal Relations,"This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.",2006.0,"I. Mani, M. Verhagen, Ben Wellner, Chong Min Lee, J. Pustejovsky"
7b7222ac076d211d7fcae7d012bebcc4ea71e952,https://www.semanticscholar.org/paper/7b7222ac076d211d7fcae7d012bebcc4ea71e952,"An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods","Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty; there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.",1989.0,"S. Weiss, I. Kapouleas"
adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1,https://www.semanticscholar.org/paper/adb4ea2c0f3eff8a17c97a67f28b923e8e5bdff1,Multimodal learning with deep Boltzmann machines,"Data often consists of multiple diverse modalities. For example, images are tagged with textual information and videos are accompanied by audio. Each modality is characterized by having distinct statistical properties. We propose a Deep Boltzmann Machine for learning a generative model of such multimodal data. We show that the model can be used to create fused representations by combining features across modalities. These learned representations are useful for classification and information retrieval. By sampling from the conditional distributions over each data modality, it is possible to create these representations even when some data modalities are missing. We conduct experiments on bimodal image-text and audio-video data. The fused representation achieves good classification results on the MIR-Flickr data set matching or outperforming other deep models as well as SVM based models that use Multiple Kernel Learning. We further demonstrate that this multimodal model helps classification and retrieval even when only unimodal data is available at test time.",2012.0,"Nitish Srivastava, R. Salakhutdinov"
675b93244f917d5dc2c79b89d2936d81d077e663,https://www.semanticscholar.org/paper/675b93244f917d5dc2c79b89d2936d81d077e663,Using Machine Learning to Break Visual Human Interaction Proofs (HIPs),"Machine learning is often used to automatically solve human tasks. In this paper, we look for tasks where machine learning algorithms are not as good as humans with the hope of gaining insight into their current limitations. We studied various Human Interactive Proofs (HIPs) on the market, because they are systems designed to tell computers and humans apart by posing challenges presumably too hard for computers. We found that most HIPs are pure recognition tasks which can easily be broken using machine learning. The harder HIPs use a combination of segmentation and recognition tasks. From this observation, we found that building segmentation tasks is the most effective way to confuse machine learning algorithms. This has enabled us to build effective HIPs (which we deployed in MSN Passport), as well as design challenging segmentation tasks for machine learning algorithms.",2004.0,"K. Chellapilla, Patrice Y. Simard"
b379c5eb2f8cc501e855d295fa5712294ca2b3ed,https://www.semanticscholar.org/paper/b379c5eb2f8cc501e855d295fa5712294ca2b3ed,Application of Machine Learning Algorithms to KDD Intrusion Detection Dataset within Misuse Detection Context,"A small subset of machine learning algorithms, mostly inductive learning based, applied to the KDD 1999 Cup intrusion detection dataset resulted in dismal performance for user-to-root and remote-to-local attack categories as reported in the recent literature. The uncertainty to explore if other machine learning algorithms can demonstrate better performance compared to the ones already employed constitutes the motivation for the study reported herein. Specifically, exploration of if certain algorithms perform better for certain attack classes and consequently, if a multi-expert classifier design can deliver desired performance measure is of high interest. This paper evaluates performance of a comprehensive set of pattern recognition and machine learning algorithms on four attack categories as found in the KDD 1999 Cup intrusion detection dataset. Results of simulation study implemented to that effect indicated that certain classification algorithms perform better for certain attack categories: a specific algorithm specialized for a given attack category . Consequently, a multi-classifier model, where a specific detection algorithm is associated with an attack category for which it is the most promising, was built. Empirical results obtained through simulation indicate that noticeable performance improvement was achieved for probing, denial of service, and user-to-root",2003.0,"Maheshkumar Sabhnani, G. Serpen"
5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c,https://www.semanticscholar.org/paper/5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c,Readings in Machine Learning,"From the Publisher: 
The ability to learn is a fundamental characteristic of intelligent behavior. Consequently, machine learning has been a focus of artificial intelligence since the beginnings of AI in the 1950s. The 1980s saw tremendous growth in the field, and this growth promises to continue with valuable contributions to science, engineering, and business. 
 
Readings in Machine Learning collects the best of the published machine learning literature, including papers that address a wide range of learning tasks, and that introduce a variety of techniques for giving machines the ability to learn. The editors, in cooperation with a group of expert referees, have chosen important papers that empirically study, theoretically analyze, or psychologically justify machine learning algorithms. The papers are grouped into a dozen categories, each of which is introduced by the editors.",1991.0,"J. Shavlik, T. Deitterich, Thomas G. Dietterich"
06757c457ec442eb35af6ea45d8d0e2339415178,https://www.semanticscholar.org/paper/06757c457ec442eb35af6ea45d8d0e2339415178,The Interplay of Optimization and Machine Learning Research,"The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.",2006.0,"Kristin P. Bennett, E. Parrado-Hernández"
a54657b8de38a18f30fd154d713f9522f705166c,https://www.semanticscholar.org/paper/a54657b8de38a18f30fd154d713f9522f705166c,Computational complexity of machine learning,"This thesis is a study of the computational complexity of machine learning from examples in the distribution-free model introduced by L. G. Valiant (V84). In the distribution-free model, a learning algorithm receives positive and negative examples of an unknown target set (or concept) that is chosen from some known class of sets (or concept class). These examples are generated randomly according to a fixed but unknown probability distribution representing Nature, and the goal of the learning algorithm is to infer an hypothesis concept that closely approximates the target concept with respect to the unknown distribution. This thesis is concerned with proving theorems about learning in this formal mathematical model. 
We are interested in the phenomenon of efficient learning in the distribution-free model, in the standard polynomial-time sense. Our results include general tools for determining the polynomial-time learnability of a concept class, an extensive study of efficient learning when errors are present in the examples, and lower bounds on the number of examples required for learning in our model. A centerpiece of the thesis is a series of results demonstrating the computational difficulty of learning a number of well-studied concept classes. These results are obtained by reducing some apparently hard number-theoretic problems from cryptography to the learning problems. The hard-to-learn concept classes include the sets represented by Boolean formulae, deterministic finite automata and a simplified form of neural networks. We also give algorithms for learning powerful concept classes under the uniform distribution, and give equivalences between natural models of efficient learnability. 
This thesis also includes detailed definitions and motivation for the distribution-free model, a chapter discussing past research in this model and related models, and a short list of important open problems.",1990.0,M. Kearns
f4971ff0f6ae626e78131bafa012eadfe8e238e2,https://www.semanticscholar.org/paper/f4971ff0f6ae626e78131bafa012eadfe8e238e2,Machine learning: an artificial intelligence approach volume III,This book reflects the expansion of machine learning research through presentation of recent advances in the field. The book provides an account of current research directions. Major topics covered include the following: learning concepts and rules from examples; cognitive aspects of learning; learning by analogy; learning by observation and discovery; and an exploration of general aspects of learning.,1990.0,"Yves Kodratoff, R. Michalski"
14f2b886678251cdd80dc9701c889bc55de7940d,https://www.semanticscholar.org/paper/14f2b886678251cdd80dc9701c889bc55de7940d,Student Modeling and Machine Learning,"After identifying essential student modeling issues and machine learning approaches, this paper examines how machine learning techniques have been used to automate the construction of student models as well as the background knowledge necessary for student modeling. In the process, the paper sheds light on the difficulty, suitability and potential of using machine learning for student modeling processes, and, to a lesser extent, the potential of using student modeling techniques in machine learning. (http://aied.inf.ed.ac.uk/members98/archive/vol_9/sison/full.html)",1998.0,"R. Sison, M. Shimura"
65df2d9b3c656ca85e4d66c327cfd8c8d1182df3,https://www.semanticscholar.org/paper/65df2d9b3c656ca85e4d66c327cfd8c8d1182df3,"Machine Learning: Neural Networks, Genetic Algorithms, and Fuzzy Systems",Perceptron Learning with a Hidden Layer An Object-Oriented Backpropagation Learning Model Concurrent Backpropagation Learning Algorithms An Adaptive Conjugate Gradient Learning Algorithm for Efficient Training of Neural Networks A Concurrent Adaptive Conjugate Gradient Learning Algorithm on MIMD Shared Memory Machines A Concurrent Genetic/Neural Network Learning Algorithm for MIMD Shared Memory Machines A Hybrid Learning Algorithm for Distributed Memory Multicomputers A Fuzzy Neural Network Learning Model Appendices References Index.,1994.0,"H. Adeli, S. Hung"
1671a665c636bec7d2eaff137d74e9b7f074892f,https://www.semanticscholar.org/paper/1671a665c636bec7d2eaff137d74e9b7f074892f,Learning Algorithms for the Classification Restricted Boltzmann Machine,"Recent developments have demonstrated the capacity of restricted Boltzmann machines (RBM) to be powerful generative models, able to extract useful features from input data or construct deep artificial neural networks. In such settings, the RBM only yields a preprocessing or an initialization for some other model, instead of acting as a complete supervised model in its own right. In this paper, we argue that RBMs can provide a self-contained framework for developing competitive classifiers. We study the Classification RBM (ClassRBM), a variant on the RBM adapted to the classification setting. We study different strategies for training the ClassRBM and show that competitive classification performances can be reached when appropriately combining discriminative and generative training objectives. Since training according to the generative objective requires the computation of a generally intractable gradient, we also compare different approaches to estimating this gradient and address the issue of obtaining such a gradient for problems with very high dimensional inputs. Finally, we describe how to adapt the ClassRBM to two special cases of classification problems, namely semi-supervised and multitask learning.",2012.0,"H. Larochelle, Michael I. Mandel, Razvan Pascanu, Yoshua Bengio"
7d89abfe87ed7d1b40391d37364560656d208117,https://www.semanticscholar.org/paper/7d89abfe87ed7d1b40391d37364560656d208117,Learning Memory Access Patterns,"The explosion in workload complexity and the recent slow-down in Moore's law scaling call for new approaches towards efficient computing. Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures. However, the space of machine learning for computer hardware architecture is only lightly explored. In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance. We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers. We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement. On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall. This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.",2018.0,"Milad Hashemi, Kevin Swersky, Jamie A. Smith, Grant Ayers, Heiner Litz, Jichuan Chang, Christos Kozyrakis, Parthasarathy Ranganathan"
298a09325dce98155779f9640ccae8fa5ddca62d,https://www.semanticscholar.org/paper/298a09325dce98155779f9640ccae8fa5ddca62d,Machine-Learning Applications of Algorithmic Randomness,"Machine-LearningApplicationsofAlgorithmicRandomnessVolodyaovk,AlexGammerman,CraigSaundersComputerLearningResearchCentreandDepartmentofScienceRoyalHollowa,UniversitofLondon,Egham,SurreyTW200EX,Englandfvovk,alex,craigg@dcs.rhbnc.ac.ukAbstractMostmachinelearningalgorithmssharethefollowingdrawback:theyonlyoutputbarepredictionsbutnotthecon denceinthosepredictions.Inthe1960salgorithmicinfor-mationtheorysupplieduniversalmeasuresofcon dencebuttheseare,unfortunately,non-computable.Inthispap erwecombinetheideasofalgorithmicinformationtheorywiththetheoryofSupp ortVectormachinestoobtainpracticableapproximationsuni-versalmeasuresofcon dence.Weshowthatinsomestandardproblemsofpatternrecog-nitionourapproximationsworkell.1INTRODUCTIONTwoimp ortantdi erencesofmostmo dernmetho dsmachinelearning(suchasstatisticaltheory,seeVapnik[21],1998,orPACtheory)fromclassicalstatisticalmetho dsarethat:machinelearningmetho dspro ducebarepredic-tions,withoutestimatingcon denceinthosepre-dictions(unlike,eg,predictionoffutureobser-vationsintraditionalstatistics(Guttman[5],1970));manymachinelearningmetho dsaredesignedtowork(andtheirp erformanceisanalysed)un-derthegeneraliidassumption(unlikeclas-sicalparametricstatistics)andtheyareabletodealwithextremelyhigh-dimensionalhyp othesisspaces;cfVapnik[21](1998).Inthispap erwewillfurtherdeveloptheapproachofGammermanetal[4](1998)andSaunders[17Figure1:Ifthetrainingsetonlycontainsclear2sand7s,weouldliktoattachmucloercon dencethemiddleimagethantorightandleftones(1999),wherethegoalistoobtaincon dencesforpredictionsunderthegeneraliidassumptioninhigh-dimensionalsituations.Figure1demonstratesthede-sirabilityofcon dences.Themaincontributionthispap erisemb eddingtheapproachesofGammermanetal[4](1998)andSaunderset[17(1999)intoagen-eralschemebasedonthenotionofalgorithmicran-domness.Aswillb ecomeclearlater,theproblemofassigningcon dencestopredictionsiscloselyconnectedtheproblemofde ningrandomsequences.ThelatterproblemwassolvedbyKolmogorov[8](1965),whobasedhisde nitionontheexistenceUniver-salTuringMachine(thoughitb ecameclearthatKol-mogorov'sde nitiondo essolvetheproblemofde ningrandomsequencesonlyafterMartin-Lof 'spap er[15],1966);Kolmogorov'sde nitionmovedthenotionofrandomnessfromthegreyareasurroundingprobabil-itytheoryandstatisticstomathematicalcomputersci-ence.Kolmogorovb elievedhisnotionofrandomnesstob easuitablebasisforapplicationsofprobability.Unfor-tunately,fateideaasdi erentfromKol-mogorov's1933axioms(Kolmogorov[7],1933),which",1999.0,"V. Vovk, A. Gammerman, C. Saunders"
d517b13f2b152c913b81ce534a149493517dbdad,https://www.semanticscholar.org/paper/d517b13f2b152c913b81ce534a149493517dbdad,Big Data Deep Learning: Challenges and Perspectives,"Deep learning is currently an extremely active research area in machine learning and pattern recognition society. It has gained huge successes in a broad area of applications such as speech recognition, computer vision, and natural language processing. With the sheer size of data available today, big data brings big opportunities and transformative potential for various sectors; on the other hand, it also presents unprecedented challenges to harnessing data and information. As the data keeps getting bigger, deep learning is coming to play a key role in providing big data predictive analytics solutions. In this paper, we provide a brief overview of deep learning, and highlight current research efforts and the challenges to big data, as well as the future trends.",2014.0,"Xue-wen Chen, Xiaotong Lin"
391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b,https://www.semanticscholar.org/paper/391b86cf16c2702dcc4beee55a6dd6d3bd7cf27b,Deep Learning for Content-Based Image Retrieval: A Comprehensive Study,"Learning effective feature representations and similarity measures are crucial to the retrieval performance of a content-based image retrieval (CBIR) system. Despite extensive research efforts for decades, it remains one of the most challenging open problems that considerably hinders the successes of real-world CBIR systems. The key challenge has been attributed to the well-known ``semantic gap'' issue that exists between low-level image pixels captured by machines and high-level semantic concepts perceived by human. Among various techniques, machine learning has been actively investigated as a possible direction to bridge the semantic gap in the long term. Inspired by recent successes of deep learning techniques for computer vision and other applications, in this paper, we attempt to address an open problem: if deep learning is a hope for bridging the semantic gap in CBIR and how much improvements in CBIR tasks can be achieved by exploring the state-of-the-art deep learning techniques for learning feature representations and similarity measures. Specifically, we investigate a framework of deep learning with application to CBIR tasks with an extensive set of empirical studies by examining a state-of-the-art deep learning method (Convolutional Neural Networks) for CBIR tasks under varied settings. From our empirical studies, we find some encouraging results and summarize some important insights for future research.",2014.0,"Ji Wan, Dayong Wang, S. Hoi, Pengcheng Wu, Jianke Zhu, Yongdong Zhang, Jintao Li"
e2d4321c99b74859b8aa57daac7df4f1c11291cb,https://www.semanticscholar.org/paper/e2d4321c99b74859b8aa57daac7df4f1c11291cb,The children's machine: rethinking school in the age of the computer,Yearners and Schoolers Personal Thinking School: Change and Resistance to Change Teachers A World for Learning An Anthology of Learning Stories Instructionism versus Constructionism Computerists Yearners and Schoolers Cybernetics What can be done?,1993.0,S. Papert
fc09717ba476ae2408c454e5557276a9fc4d093d,https://www.semanticscholar.org/paper/fc09717ba476ae2408c454e5557276a9fc4d093d,Machine Learning: A Theoretical Approach,Chapter 1 Introduction Chapter 2 Learning Concept on Countable Domains Chapter 3 Time Complexity of Concept Learning Chapter 4 Learning Concepts on Uncoutable Domains Chapter 5 Learning Functions Chapter 6 Finite Automata Chapter 7 Neural Networks Chapter 8 Generalizing the Learning Model Chapter 9 Conclusion,1992.0,B. Natarajan
884895a86fe15cb9601df4a15a1475c07f28da3c,https://www.semanticscholar.org/paper/884895a86fe15cb9601df4a15a1475c07f28da3c,Boosting for transfer learning,"Traditional machine learning makes a basic assumption: the training and test data should be under the same distribution. However, in many cases, this identical-distribution assumption does not hold. The assumption might be violated when a task from one new domain comes, while there are only labeled data from a similar old domain. Labeling the new data can be costly and it would also be a waste to throw away all the old data. In this paper, we present a novel transfer learning framework called TrAdaBoost, which extends boosting-based learning algorithms (Freund & Schapire, 1997). TrAdaBoost allows users to utilize a small amount of newly labeled data to leverage the old data to construct a high-quality classification model for the new data. We show that this method can allow us to learn an accurate model using only a tiny amount of new data and a large amount of old data, even when the new data are not sufficient to train a model alone. We show that TrAdaBoost allows knowledge to be effectively transferred from the old data to the new. The effectiveness of our algorithm is analyzed theoretically and empirically to show that our iterative algorithm can converge well to an accurate model.",2007.0,"Wenyuan Dai, Qiang Yang, Gui-Rong Xue, Yong Yu"
48e752c719d33ff55b3b3bec3538727f8ce69399,https://www.semanticscholar.org/paper/48e752c719d33ff55b3b3bec3538727f8ce69399,Ontology Learning for the Semantic Web,"The Semantic Web relies heavily on formal ontologies to structure data for comprehensive and transportable machine understanding. Thus, the proliferation of ontologies factors largely in the Semantic Web's success. The authors present an ontology learning framework that extends typical ontology engineering environments by using semiautomatic ontology construction tools. The framework encompasses ontology import, extraction, pruning, refinement and evaluation.",2002.0,"A. Maedche, Steffen Staab"
db64f424710d57025c5fb42a564551f093a4d111,https://www.semanticscholar.org/paper/db64f424710d57025c5fb42a564551f093a4d111,The Extreme Value Machine,"It is often desirable to be able to recognize when inputs to a recognition function learned in a supervised manner correspond to classes unseen at training time. With this ability, new class labels could be assigned to these inputs by a human operator, allowing them to be incorporated into the recognition function—ideally under an efficient incremental update mechanism. While good algorithms that assume inputs from a fixed set of classes exist, e.g. , artificial neural networks and kernel machines, it is not immediately obvious how to extend them to perform incremental learning in the presence of unknown query classes. Existing algorithms take little to no distributional information into account when learning recognition functions and lack a strong theoretical foundation. We address this gap by formulating a novel, theoretically sound classifier—the Extreme Value Machine (EVM). The EVM has a well-grounded interpretation derived from statistical Extreme Value Theory (EVT), and is the first classifier to be able to perform nonlinear kernel-free variable bandwidth incremental learning. Compared to other classifiers in the same deep network derived feature space, the EVM is accurate and efficient on an established benchmark partition of the ImageNet dataset.",2015.0,"Ethan M. Rudd, Lalit P. Jain, W. Scheirer, T. Boult"
d237de6e4974e6a34d2b35d7a3a223f6fb611219,https://www.semanticscholar.org/paper/d237de6e4974e6a34d2b35d7a3a223f6fb611219,Learning using privileged information: similarity control and knowledge transfer,"This paper describes a new paradigm of machine learning, in which Intelligent Teacher is involved. During training stage, Intelligent Teacher provides Student with information that contains, along with classification of each example, additional privileged information (for example, explanation) of this example. The paper describes two mechanisms that can be used for significantly accelerating the speed of Student's learning using privileged information: (1) correction of Student's concepts of similarity between examples, and (2) direct Teacher-Student knowledge transfer.",2015.0,"V. Vapnik, R. Izmailov"
85d727b119304dde458bcd8cf5cb87a906fb41ba,https://www.semanticscholar.org/paper/85d727b119304dde458bcd8cf5cb87a906fb41ba,Using AUC and accuracy in evaluating learning algorithms,"The area under the ROC (receiver operating characteristics) curve, or simply AUC, has been traditionally used in medical diagnosis since the 1970s. It has recently been proposed as an alternative single-number measure for evaluating the predictive ability of learning algorithms. However, no formal arguments were given as to why AUC should be preferred over accuracy. We establish formal criteria for comparing two different measures for learning algorithms and we show theoretically and empirically that AUC is a better measure (defined precisely) than accuracy. We then reevaluate well-established claims in machine learning based on accuracy using AUC and obtain interesting and surprising new results. For example, it has been well-established and accepted that Naive Bayes and decision trees are very similar in predictive accuracy. We show, however, that Naive Bayes is significantly better than decision trees in AUC. The conclusions drawn in this paper may make a significant impact on machine learning and data mining applications.",2005.0,"Jin Huang, C. Ling"
467568f1777bc51a15a5100516cd4fe8de62b9ab,https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab,Transfer Learning for Reinforcement Learning Domains: A Survey,"The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.",2009.0,"Matthew E. Taylor, P. Stone"
b3852f0113fcf8a3913c55ae92393ae6ccde347e,https://www.semanticscholar.org/paper/b3852f0113fcf8a3913c55ae92393ae6ccde347e,Self-taught learning: transfer learning from unlabeled data,"We present a new machine learning framework called ""self-taught learning"" for using unlabeled data in supervised classification tasks. We do not assume that the unlabeled data follows the same class labels or generative distribution as the labeled data. Thus, we would like to use a large number of unlabeled images (or audio samples, or text documents) randomly downloaded from the Internet to improve performance on a given image (or audio, or text) classification task. Such unlabeled data is significantly easier to obtain than in typical semi-supervised or transfer learning settings, making self-taught learning widely applicable to many practical learning problems. We describe an approach to self-taught learning that uses sparse coding to construct higher-level features using the unlabeled data. These features form a succinct input representation and significantly improve classification performance. When using an SVM for classification, we further show how a Fisher kernel can be learned for this representation.",2007.0,"Rajat Raina, Alexis Battle, Honglak Lee, Ben Packer, A. Ng"
5ed59f49c1bb7de06cfa2a9467d5efb535103277,https://www.semanticscholar.org/paper/5ed59f49c1bb7de06cfa2a9467d5efb535103277,Temporal difference learning and TD-Gammon,"Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.",1995.0,G. Tesauro
2c8ac3e1f0edeed1fbd76813e61efdc384c319c7,https://www.semanticscholar.org/paper/2c8ac3e1f0edeed1fbd76813e61efdc384c319c7,Learning Question Classifiers,"In order to respond correctly to a free form factual question given a large collection of texts, one needs to understand the question to a level that allows determining some of the constraints the question imposes on a possible answer. These constraints may include a semantic classification of the sought after answer and may even suggest using different strategies when looking for and verifying a candidate answer.This paper presents a machine learning approach to question classification. We learn a hierarchical classifier that is guided by a layered semantic hierarchy of answer types, and eventually classifies questions into fine-grained classes. We show accurate results on a large collection of free-form questions used in TREC 10.",2002.0,"Xin Li, D. Roth"
02e68b069d9cf13c082049429ffed18a5ca5f6d0,https://www.semanticscholar.org/paper/02e68b069d9cf13c082049429ffed18a5ca5f6d0,Support Vector Machines for Multiple-Instance Learning,"This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristic ally. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization.",2002.0,"Stuart Andrews, Ioannis Tsochantaridis, Thomas Hofmann"
4417f78b31546227784941bbd6f6532a177e60b8,https://www.semanticscholar.org/paper/4417f78b31546227784941bbd6f6532a177e60b8,Deep Learning using Linear Support Vector Machines,"Recently, fully-connected and convolutional neural networks have been trained to achieve state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics. For classification tasks, most of these ""deep learning"" models employ the softmax activation function for prediction and minimize cross-entropy loss. In this paper, we demonstrate a small but consistent advantage of replacing the softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. While there have been various combinations of neural nets and SVMs in prior art, our results using L2-SVMs show that by simply replacing softmax with linear SVMs gives significant gains on popular deep learning datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop's face expression recognition challenge.",2013.0,Yichuan Tang
2c5135a0531bc5ad7dd890f018e67a40529f5bcb,https://www.semanticscholar.org/paper/2c5135a0531bc5ad7dd890f018e67a40529f5bcb,A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data,"One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don't have a complete understanding of their effectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Specifically we consider learning predictive structures on hypothesis spaces (that is, what kind of classifiers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the effectiveness of the proposed algorithms in the semi-supervised learning setting.",2005.0,"R. Ando, Tong Zhang"
f2642db17084b14068d56f332de2f2d5a1622c5a,https://www.semanticscholar.org/paper/f2642db17084b14068d56f332de2f2d5a1622c5a,Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning,"One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.",2009.0,"Guorui Feng, G. Huang, Qingping Lin, R. Gay"
7b2dd79083a74699e4e0509ac3f0a8a302b4eabe,https://www.semanticscholar.org/paper/7b2dd79083a74699e4e0509ac3f0a8a302b4eabe,On the mathematical foundations of learning,"(1) A main theme of this report is the relationship of approximation to learning and the primary role of sampling (inductive inference). We try to emphasize relations of the theory of learning to the mainstream of mathematics. In particular, there are large roles for probability theory, for algorithms such as least squares, and for tools and ideas from linear algebra and linear analysis. An advantage of doing this is that communication is facilitated and the power of core mathematics is more easily brought to bear. We illustrate what we mean by learning theory by giving some instances. (a) The understanding of language acquisition by children or the emergence of languages in early human cultures. (b) In Manufacturing Engineering, the design of a new wave of machines is anticipated which uses sensors to sample properties of objects before, during, and after treatment. The information gathered from these samples is to be analyzed by the machine to decide how to better deal with new input objects (see [43]). (c) Pattern recognition of objects ranging from handwritten letters of the alphabet to pictures of animals, to the human voice. Understanding the laws of learning plays a large role in disciplines such as (Cognitive) Psychology, Animal Behavior, Economic Decision Making, all branches of Engineering, Computer Science, and especially the study of human thought processes (how the brain works). Mathematics has already played a big role towards the goal of giving a universal foundation of studies in these disciplines. We mention as examples the theory of Neural Networks going back to McCulloch and Pitts [25] and Minsky and Papert [27], the PAC learning of Valiant [40], Statistical Learning Theory as developed by Vapnik [42], and the use of reproducing kernels as in [17] among many other mathematical developments. We are heavily indebted to these developments. Recent discussions with a number of mathematicians have also been helpful. In",2001.0,"F. Cucker, S. Smale"
141e6c1dd532504611266d08458dbe2a0dbb4e98,https://www.semanticscholar.org/paper/141e6c1dd532504611266d08458dbe2a0dbb4e98,"Multiple kernel learning, conic duality, and the SMO algorithm","While classical kernel-based classifiers are based on a single kernel, in practice it is often desirable to base classifiers on combinations of multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for the support vector machine (SVM), and showed that the optimization of the coefficients of such a combination reduces to a convex optimization problem known as a quadratically-constrained quadratic program (QCQP). Unfortunately, current convex optimization toolboxes can solve this problem only for a small number of kernels and a small number of data points; moreover, the sequential minimal optimization (SMO) techniques that are essential in large-scale implementations of the SVM cannot be applied because the cost function is non-differentiable. We propose a novel dual formulation of the QCQP as a second-order cone programming problem, and show how to exploit the technique of Moreau-Yosida regularization to yield a formulation to which SMO techniques can be applied. We present experimental results that show that our SMO-based algorithm is significantly more efficient than the general-purpose interior point methods available in current optimization toolboxes.",2004.0,"F. Bach, Gert R. G. Lanckriet, Michael I. Jordan"
66f44806cd46a27f02ceb74bdfd9ad6e77e044ca,https://www.semanticscholar.org/paper/66f44806cd46a27f02ceb74bdfd9ad6e77e044ca,Toward an Online Anomaly Intrusion Detection System Based on Deep Learning,"In the past twenty years, progress in intrusion detection has been steady but slow. The biggest challenge is to detect new attacks in real time. In this work, a deep learning approach for anomaly detection using a Restricted Boltzmann Machine (RBM) and a deep belief network are implemented. Our method uses a one-hidden layer RBM to perform unsupervised feature reduction. The resultant weights from this RBM are passed to another RBM producing a deep belief network. The pre-trained weights are passed into a fine tuning layer consisting of a Logistic Regression (LR) classifier with multi-class soft-max. We have implemented the deep learning architecture in C++ in Microsoft Visual Studio 2013 and we use the DARPA KDDCUP'99 dataset to evaluate its performance. Our architecture outperforms previous deep learning methods implemented by Li and Salama in both detection speed and accuracy. We achieve a detection rate of 97.9% on the total 10% KDDCUP'99 test dataset. By improving the training process of the simulation, we are also able to produce a low false negative rate of 2.47%. Although the deficiencies in the KDDCUP'99 dataset are well understood, it still presents machine learning approaches for predicting attacks with a reasonable challenge. Our future work will include applying our machine learning strategy to larger and more challenging datasets, which include larger classes of attacks.",2016.0,"Khaled Alrawashdeh, C. Purdy"
0628fdf728d0aba31be803a7d834c7f4b569408d,https://www.semanticscholar.org/paper/0628fdf728d0aba31be803a7d834c7f4b569408d,"Imbalanced Learning: Foundations, Algorithms, and Applications","The first book of its kind to review the current status and future direction of the exciting new branch of machine learning/data mining called imbalanced learningImbalanced learning focuses on how an intelligent system can learn when it is provided with imbalanced data. Solving imbalanced learning problems is critical in numerous data-intensive networked systems, including surveillance, security, Internet, finance, biomedical, defense, and more. Due to the inherent complex characteristics of imbalanced data sets, learning from such data requires new understandings, principles, algorithms, and tools to transform vast amounts of raw data efficiently into information and knowledge representation. The first comprehensive look at this new branch of machine learning, this book offers a critical review of the problem of imbalanced learning, covering the state of the art in techniques, principles, and real-world applications. Featuring contributions from experts in both academia and industry, Imbalanced Learning: Foundations, Algorithms, and Applications provides chapter coverage on:Foundations of Imbalanced LearningImbalanced Datasets: From Sampling to ClassifiersEnsemble Methods for Class Imbalance LearningClass Imbalance Learning Methods for Support Vector MachinesClass Imbalance and Active LearningNonstationary Stream Data Learning with Imbalanced Class DistributionAssessment Metrics for Imbalanced LearningImbalanced Learning: Foundations, Algorithms, and Applications will help scientists and engineers learn how to tackle the problem of learning from imbalanced datasets, and gain insight into current developments in the field as well as future research directions.",2013.0,"Haibo He, Yunqian Ma"
0ca26f9a98dda0abb737692f72ffa682df14cb2f,https://www.semanticscholar.org/paper/0ca26f9a98dda0abb737692f72ffa682df14cb2f,Sparse Bayesian learning for basis selection,"Sparse Bayesian learning (SBL) and specifically relevance vector machines have received much attention in the machine learning literature as a means of achieving parsimonious representations in the context of regression and classification. The methodology relies on a parameterized prior that encourages models with few nonzero weights. In this paper, we adapt SBL to the signal processing problem of basis selection from overcomplete dictionaries, proving several results about the SBL cost function that elucidate its general behavior and provide solid theoretical justification for this application. Specifically, we have shown that SBL retains a desirable property of the /spl lscr//sub 0/-norm diversity measure (i.e., the global minimum is achieved at the maximally sparse solution) while often possessing a more limited constellation of local minima. We have also demonstrated that the local minima that do exist are achieved at sparse solutions. Later, we provide a novel interpretation of SBL that gives us valuable insight into why it is successful in producing sparse representations. Finally, we include simulation studies comparing sparse Bayesian learning with basis pursuit and the more recent FOCal Underdetermined System Solver (FOCUSS) class of basis selection algorithms. These results indicate that our theoretical insights translate directly into improved performance.",2004.0,"D. Wipf, B. Rao"
d1208ac421cf8ff67b27d93cd19ae42b8d596f95,https://www.semanticscholar.org/paper/d1208ac421cf8ff67b27d93cd19ae42b8d596f95,Deep learning with COTS HPC systems,"Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.",2013.0,"Adam Coates, Brody Huval, Tao Wang, David J. Wu, Bryan Catanzaro, A. Ng"
e4de0f69cd867dbcae88211ac05318be17615a66,https://www.semanticscholar.org/paper/e4de0f69cd867dbcae88211ac05318be17615a66,Regularized Extreme Learning Machine,"Extreme Learning Machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called Regularized Extreme Learning Machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.",2009.0,"W. Deng, Qinghua Zheng, Lin Chen"
fbc913faf39b1e369dfcdcfefb354d846a46573c,https://www.semanticscholar.org/paper/fbc913faf39b1e369dfcdcfefb354d846a46573c,"Learning With Kernels: Support Vector Machines, Regularization, Optimization, and Beyond","From the Publisher: 
In the 1990s, a new type of learning algorithm was developed, based on results from statistical learning theory: the Support Vector Machine (SVM). This gave rise to a new class of theoretically elegant learning machines that use a central concept of SVMs-kernels--for a number of learning tasks. Kernel machines provide a modular framework that can be adapted to different tasks and domains by the choice of the kernel function and the base algorithm. They are replacing neural networks in a variety of fields, including engineering, information retrieval, and bioinformatics. 
Learning with Kernels provides an introduction to SVMs and related kernel methods. Although the book begins with the basics, it also includes the latest research. It provides all of the concepts necessary to enable a reader equipped with some basic mathematical knowledge to enter the world of machine learning using theoretically well-founded yet easy-to-use kernel algorithms and to understand and apply the powerful algorithms that have been developed over the last few years.",2001.0,Christopher K. I Williams
177944150565195ee9c3e28dc6b032200cfda059,https://www.semanticscholar.org/paper/177944150565195ee9c3e28dc6b032200cfda059,Collaborative Learning: Cognitive and Computational Approaches,"Acknowledgement. Contributors. Introduction: what do you mean by 'collaborative learning'? (P. Dillenbourg). Learning together: understanding the processes of computer-based collaborative learning (K. Littleton, P. Hakkinen). The role of grounding in collaborative learning tasks (M. Baker et al.). What is ""multi"" in multi-agent learning? (G. Weiss, P. Dillenbourg). Comparing human-human and robot-robot interactions (R. Joiner et al.). Learning by explaining to oneself and to others (R. Ploetzner et al.). Knowledge transformations in agents and interactions: a comparison of machine learning and dialogue operators (E. Mephu Nguifo et al.). Can analytic models support learning in groups? (H.U. Hoppe, R. Ploetzner). Using telematics for collaborative knowledge construction (T. Hansen et al.). The productive agency that drives collaborative learning (D. Schwatrtz). References. Index.",1999.0,"Tia G. B. Hansen, L. Dirckinck-Holmfeld, R. Lewis, J. Rugelj"
d26a48aff2abc3460c1018d5b410766f698d696c,https://www.semanticscholar.org/paper/d26a48aff2abc3460c1018d5b410766f698d696c,Large Scale Multiple Kernel Learning,"While classical kernel-based learning algorithms are based on a single kernel, in practice it is often desirable to use multiple kernels. Lanckriet et al. (2004) considered conic combinations of kernel matrices for classification, leading to a convex quadratically constrained quadratic program. We show that it can be rewritten as a semi-infinite linear program that can be efficiently solved by recycling the standard SVM implementations. Moreover, we generalize the formulation and our method to a larger class of problems, including regression and one-class classification. Experimental results show that the proposed algorithm works for hundred thousands of examples or hundreds of kernels to be combined, and helps for automatic model selection, improving the interpretability of the learning result. In a second part we discuss general speed up mechanism for SVMs, especially when used with sparse feature maps as appear for string kernels, allowing us to train a string kernel SVM on a 10 million real-world splice data set from computational biology. We integrated multiple kernel learning in our machine learning toolbox SHOGUN for which the source code is publicly available at http://www.fml.tuebingen.mpg.de/raetsch/projects/shogun .",2006.0,"S. Sonnenburg, Gunnar Rätsch, C. Schäfer, B. Scholkopf"
6fdb77260fc83dff91c44fea0f31a2cb8ed13d04,https://www.semanticscholar.org/paper/6fdb77260fc83dff91c44fea0f31a2cb8ed13d04,Scaling learning algorithms towards AI,"One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.",2007.0,"Yoshua Bengio, Yann LeCun"
6f0cde3fcab0044f386b5b8a4244c371507bec15,https://www.semanticscholar.org/paper/6f0cde3fcab0044f386b5b8a4244c371507bec15,A Survey on Metric Learning for Feature Vectors and Structured Data,"The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.",2013.0,"A. Bellet, Amaury Habrard, M. Sebban"
6ce53f261fc6e115ff5723968e9b69be31a670f3,https://www.semanticscholar.org/paper/6ce53f261fc6e115ff5723968e9b69be31a670f3,Evaluating Learning Algorithms: A Classification Perspective,"The field of machine learning has matured to the point where many sophisticated learning approaches can be applied to practical applications. Thus it is of critical importance that researchers have the proper tools to evaluate learning approaches and understand the underlying issues. This book examines various aspects of the evaluation process with an emphasis on classification algorithms. The authors describe several techniques for classifier performance assessment, error estimation and resampling, obtaining statistical significance as well as selecting appropriate domains for evaluation. They also present a unified evaluation framework and highlight how different components of evaluation are both significantly interrelated and interdependent. The techniques presented in the book are illustrated using R and WEKA facilitating better practical insight as well as implementation.Aimed at researchers in the theory and applications of machine learning, this book offers a solid basis for conducting performance evaluations of algorithms in practical settings.",2011.0,"N. Japkowicz, Mohak Shah"
a13efb90f0b56417bf5dd5b6219681c4259ff355,https://www.semanticscholar.org/paper/a13efb90f0b56417bf5dd5b6219681c4259ff355,The Cerebellum: A Neuronal Learning Machine?,"Comparison of two seemingly quite different behaviors yields a surprisingly consistent picture of the role of the cerebellum in motor learning. Behavioral and physiological data about classical conditioning of the eyelid response and motor learning in the vestibulo-ocular reflex suggest that (i) plasticity is distributed between the cerebellar cortex and the deep cerebellar nuclei; (ii) the cerebellar cortex plays a special role in learning the timing of movement; and (iii) the cerebellar cortex guides learning in the deep nuclei, which may allow learning to be transferred from the cortex to the deep nuclei. Because many of the similarities in the data from the two systems typify general features of cerebellar organization, the cerebellar mechanisms of learning in these two systems may represent principles that apply to many motor systems.",1996.0,"J. Raymond, S. Lisberger, M. Mauk"
a92d7de31100a0fee816f0cb61b4a2a5a2d5e37a,https://www.semanticscholar.org/paper/a92d7de31100a0fee816f0cb61b4a2a5a2d5e37a,Semi-Supervised Learning,"In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series",2006.0,"O. Chapelle, Bernhard Schlkopf, A. Zien"
605402e235bd62437baf3c9ebefe77fb4d92ee95,https://www.semanticscholar.org/paper/605402e235bd62437baf3c9ebefe77fb4d92ee95,The Helmholtz Machine,"Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.",1995.0,"P. Dayan, Geoffrey E. Hinton, Radford M. Neal, R. Zemel"
73e1c4a1152a75ec7310adfb4b8daea16d627bc7,https://www.semanticscholar.org/paper/73e1c4a1152a75ec7310adfb4b8daea16d627bc7,Learning to learn with the informative vector machine,This paper describes an efficient method for learning the parameters of a Gaussian process (GP). The parameters are learned from multiple tasks which are assumed to have been drawn independently from the same GP prior. An efficient algorithm is obtained by extending the informative vector machine (IVM) algorithm to handle the multi-task learning case. The multi-task IVM (MTIVM) saves computation by greedily selecting the most informative examples from the separate tasks. The MT-IVM is also shown to be more efficient than random sub-sampling on an artificial data-set and more effective than the traditional IVM in a speaker dependent phoneme recognition task.,2004.0,"Neil D. Lawrence, John C. Platt"
efbeedfbf13db70878618553f0c4a0fec6f493fe,https://www.semanticscholar.org/paper/efbeedfbf13db70878618553f0c4a0fec6f493fe,Learning Collaborative Information Filters,"Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms.",1998.0,"Daniel Billsus, M. Pazzani"
b36a5bb1707bb9c70025294b3a310138aae8327a,https://www.semanticscholar.org/paper/b36a5bb1707bb9c70025294b3a310138aae8327a,Automatic differentiation in PyTorch,"In this article, we describe an automatic differentiation module of PyTorch — a library designed to enable rapid research on machine learning models. It builds upon a few projects, most notably Lua Torch, Chainer, and HIPS Autograd [4], and provides a high performance environment with easy access to automatic differentiation of models executed on different devices (CPU and GPU). To make prototyping easier, PyTorch does not follow the symbolic approach used in many other deep learning frameworks, but focuses on differentiation of purely imperative programs, with a focus on extensibility and low overhead. Note that this preprint is a draft of certain sections from an upcoming paper covering all PyTorch features.",2017.0,"Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, E. Yang, Zach DeVito, Zeming Lin, Alban Desmaison, L. Antiga, Adam Lerer"
3a288c63576fc385910cb5bc44eaea75b442e62e,https://www.semanticscholar.org/paper/3a288c63576fc385910cb5bc44eaea75b442e62e,UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,"UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",2018.0,"Leland McInnes, John Healy"
2e55ba6c97ce5eb55abd959909403fe8da7e9fe9,https://www.semanticscholar.org/paper/2e55ba6c97ce5eb55abd959909403fe8da7e9fe9,Overcoming catastrophic forgetting in neural networks,"Significance Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially. The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.",2016.0,"J. Kirkpatrick, Razvan Pascanu, Neil C. Rabinowitz, J. Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Kumaran, R. Hadsell"
c0883f5930a232a9c1ad601c978caede29155979,https://www.semanticscholar.org/paper/c0883f5930a232a9c1ad601c978caede29155979,“Why Should I Trust You?”: Explaining the Predictions of Any Classifier,"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",2016.0,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin"
26bc9195c6343e4d7f434dd65b4ad67efe2be27a,https://www.semanticscholar.org/paper/26bc9195c6343e4d7f434dd65b4ad67efe2be27a,XGBoost: A Scalable Tree Boosting System,"Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.",2016.0,"Tianqi Chen, Carlos Guestrin"
de42b848775f9fa1e4bff758ae04a54099c0c381,https://www.semanticscholar.org/paper/de42b848775f9fa1e4bff758ae04a54099c0c381,and Machine,"This editorial introduces the first part of CEJEME’s Special Issue on Artificial Intelligence and Machine Learning in Educational Measurement. As AI and ML technologies revolutionize education, they offer new opportunities for personalized learning and innovative assessment practices. This issue highlights the transformative impact of AI and ML on educational measurement, addressing both their potential and the ethical challenges they pose. This issue includes four articles that explore the opportunities and ethical challenges of AI in educational measurement, automated text scoring in the age of Generative AI for the GPU-poor, a novel approach using autoencoders and BERT to detect compromised items in computerized testing, and the use of ML packages in R. The issue provides valuable insights into the future of educational measurement. A second part of this special issue will be available in spring 2025.",1977.0,"Okan Bulut, Yi Zheng"
5ded2b8c64491b4a67f6d39ce473d4b9347a672e,https://www.semanticscholar.org/paper/5ded2b8c64491b4a67f6d39ce473d4b9347a672e,A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference,"This paper introduces the Multi-Genre Natural Language Inference (MultiNLI) corpus, a dataset designed for use in the development and evaluation of machine learning models for sentence understanding. At 433k examples, this resource is one of the largest corpora available for natural language inference (a.k.a. recognizing textual entailment), improving upon available resources in both its coverage and difficulty. MultiNLI accomplishes this by offering data from ten distinct genres of written and spoken English, making it possible to evaluate systems on nearly the full complexity of the language, while supplying an explicit setting for evaluating cross-genre domain adaptation. In addition, an evaluation using existing machine learning models designed for the Stanford NLI corpus shows that it represents a substantially more difficult task than does that corpus, despite the two showing similar levels of inter-annotator agreement.",2017.0,"Adina Williams, Nikita Nangia, Samuel R. Bowman"
f3de86aeb442216a8391befcacb49e58b478f512,https://www.semanticscholar.org/paper/f3de86aeb442216a8391befcacb49e58b478f512,Distributed Representations of Sentences and Documents,"Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, ""powerful,"" ""strong"" and ""Paris"" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperforms bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.",2014.0,"Quoc V. Le, Tomas Mikolov"
34f25a8704614163c4095b3ee2fc969b60de4698,https://www.semanticscholar.org/paper/34f25a8704614163c4095b3ee2fc969b60de4698,Dropout: a simple way to prevent neural networks from overfitting,"Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ""thinned"" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.",2014.0,"Nitish Srivastava, Geoffrey E. Hinton, A. Krizhevsky, I. Sutskever, R. Salakhutdinov"
bee044c8e8903fb67523c1f8c105ab4718600cdb,https://www.semanticscholar.org/paper/bee044c8e8903fb67523c1f8c105ab4718600cdb,Explaining and Harnessing Adversarial Examples,"Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.",2014.0,"I. Goodfellow, Jonathon Shlens, Christian Szegedy"
46bed4c578e96e05fa3e5704620c4ffa0746d78f,https://www.semanticscholar.org/paper/46bed4c578e96e05fa3e5704620c4ffa0746d78f,A Learning Machine: Part I,"Machines would be more useful if they could learn to perform tasks for which they were not given precise methods. Difficulties that attend giving a machine this ability are discussed. It is proposed that the program of a stored-program computer be gradually improved by a learning procedure which tries many programs and chooses, from the instructions that may occupy a given location, the one most often associated with a successful result. An experimental test of this principle is described in detail. Preliminary results, which show limited success, are reported and interpreted. Further results and conclusions will appear in the second part of the paper.",1958.0,R. Friedberg
b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b,https://www.semanticscholar.org/paper/b544ca32b66b4c9c69bcfa00d63ee4b799d8ab6b,Adversarial examples in the physical world,"Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.",2016.0,"Alexey Kurakin, I. Goodfellow, Samy Bengio"
8592e46a5435d18bba70557846f47290b34c1aa5,https://www.semanticscholar.org/paper/8592e46a5435d18bba70557846f47290b34c1aa5,Learning and relearning in Boltzmann machines,"This chapter contains sections titled: Relaxation Searches, Easy and Hard Learning, The Boltzmann Machine Learning Algorithm, An Example of Hard Learning, Achieving Reliable Computation with Unreliable Hardware, An Example of the Effects of Damage, Conclusion, Acknowledgments, Appendix: Derivation of the Learning Algorithm, References",1986.0,"Geoffrey E. Hinton, T. Sejnowski"
22d6d9c1b7ac2738b51d93be45ac8f753f81867c,https://www.semanticscholar.org/paper/22d6d9c1b7ac2738b51d93be45ac8f753f81867c,Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data,"Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.",2013.0,"Hoo-Chang Shin, M. Orton, D. Collins, S. Doran, M. Leach"
81a4fd3004df0eb05d6c1cef96ad33d5407820df,https://www.semanticscholar.org/paper/81a4fd3004df0eb05d6c1cef96ad33d5407820df,A Comprehensive Survey on Graph Neural Networks,"Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications, where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on the existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this article, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art GNNs into four categories, namely, recurrent GNNs, convolutional GNNs, graph autoencoders, and spatial–temporal GNNs. We further discuss the applications of GNNs across various domains and summarize the open-source codes, benchmark data sets, and model evaluation of GNNs. Finally, we propose potential research directions in this rapidly growing field.",2019.0,"Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, Philip S. Yu"
09879f7956dddc2a9328f5c1472feeb8402bcbcf,https://www.semanticscholar.org/paper/09879f7956dddc2a9328f5c1472feeb8402bcbcf,Density estimation using Real NVP,"Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",2016.0,"Laurent Dinh, Jascha Narain Sohl-Dickstein, Samy Bengio"
09879f7956dddc2a9328f5c1472feeb8402bcbcf,https://www.semanticscholar.org/paper/09879f7956dddc2a9328f5c1472feeb8402bcbcf,Density estimation using Real NVP,"Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.",2016.0,"Laurent Dinh, Jascha Narain Sohl-Dickstein, Samy Bengio"
273dfbcb68080251f5e9ff38b4413d7bd84b10a1,https://www.semanticscholar.org/paper/273dfbcb68080251f5e9ff38b4413d7bd84b10a1,LIBSVM: A library for support vector machines,"LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail.",2011.0,"Chih-Chung Chang, Chih-Jen Lin"
f216444d4f2959b4520c61d20003fa30a199670a,https://www.semanticscholar.org/paper/f216444d4f2959b4520c61d20003fa30a199670a,Siamese Neural Networks for One-Shot Image Recognition,"The process of learning good features for machine learning applications can be very computationally expensive and may prove difficult in cases where little data is available. A prototypical example of this is the one-shot learning setting, in which we must correctly make predictions given only a single example of each new class. In this paper, we explore a method for learning siamese neural networks which employ a unique structure to naturally rank similarity between inputs. Once a network has been tuned, we can then capitalize on powerful discriminative features to generalize the predictive power of the network not just to new data, but to entirely new classes from unknown distributions. Using a convolutional architecture, we are able to achieve strong results which exceed those of other deep learning models with near state-of-the-art performance on one-shot classification tasks.",2015.0,Gregory R. Koch
d516daff247f7157fccde6649ace91d969cd1973,https://www.semanticscholar.org/paper/d516daff247f7157fccde6649ace91d969cd1973,The mythos of model interpretability,"In machine learning, the concept of interpretability is both important and slippery.",2016.0,Zachary Chase Lipton
5cbe278b65a81602a864184bbca37de91448a5f5,https://www.semanticscholar.org/paper/5cbe278b65a81602a864184bbca37de91448a5f5,Competition-level code generation with AlphaCode,"Programming is a powerful and ubiquitous problem-solving tool. Systems that can assist programmers or even generate programs themselves could make programming more productive and accessible. Recent transformer-based neural network models show impressive code generation abilities yet still perform poorly on more complex tasks requiring problem-solving skills, such as competitive programming problems. Here, we introduce AlphaCode, a system for code generation that achieved an average ranking in the top 54.3% in simulated evaluations on recent programming competitions on the Codeforces platform. AlphaCode solves problems by generating millions of diverse programs using specially trained transformer-based networks and then filtering and clustering those programs to a maximum of just 10 submissions. This result marks the first time an artificial intelligence system has performed competitively in programming competitions. Description Machine learning systems can program too Computer programming competitions are popular tests among programmers that require critical thinking informed by experience and creating solutions to unforeseen problems, both of which are key aspects of human intelligence but challenging to mimic by machine learning models. Using self-supervised learning and an encoder-decoder transformer architecture, Li et al. developed AlphaCode, a deep-learning model that can achieve approximately human-level performance on the Codeforces platform, which regularly hosts these competitions and attracts numerous participants worldwide (see the Perspective by Kolter). The development of such coding platforms could have a huge impact on programmers’ productivity. It may even change the culture of programming by shifting human work to formulating problems, with machine learning being the main one responsible for generating and executing codes. —YS Modern machine learning systems can achieve average human-level performance in popular competitive programming contests.",2022.0,"Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Rémi Leblond, Tom, Eccles, James Keeling, Felix Gimeno, A. D. Lago, T. Hubert, Peter Choy, Cyprien de, Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy, D. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, K. Kavukcuoglu, O. Vinyals"
04b23f577c20d1a0e2a67aadda555f58e6d23d6e,https://www.semanticscholar.org/paper/04b23f577c20d1a0e2a67aadda555f58e6d23d6e,Support vector machines,"Support vector machines (SVMs) are a family of machine learning methods, originally introduced for the problem of classification and later generalized to various other situations. They are based on principles of statistical learning theory and convex optimization, and are currently used in various domains of application, including bioinformatics, text categorization, and computer vision. Copyright © 2009 John Wiley & Sons, Inc.",2008.0,"Ingo Steinwart, A. Christmann"
7e640729cca5a82c205bf95f346867097446838c,https://www.semanticscholar.org/paper/7e640729cca5a82c205bf95f346867097446838c,Statistical Comparisons of Classifiers over Multiple Data Sets,"While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.",2006.0,J. Demšar
68c1bfe375dde46777fe1ac8f3636fb651e3f0f8,https://www.semanticscholar.org/paper/68c1bfe375dde46777fe1ac8f3636fb651e3f0f8,Experiments with a New Boosting Algorithm,"In an earlier paper, we introduced a new ""boosting"" algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that con- sistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a ""pseudo-loss"" which is a method for forcing a learning algorithm of multi-label concepts to concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman's ""bagging"" method when used to aggregate various classifiers (including decision trees and single attribute- value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem.",1996.0,"Y. Freund, R. Schapire"
24281c886cd9339fe2fc5881faf5ed72b731a03e,https://www.semanticscholar.org/paper/24281c886cd9339fe2fc5881faf5ed72b731a03e,Spark: Cluster Computing with Working Sets,"MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However, most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms, as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals, Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs, and can be used to interactively query a 39 GB dataset with sub-second response time.",2010.0,"M. Zaharia, Mosharaf Chowdhury, Michael J. Franklin, S. Shenker, Ion Stoica"
08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1,https://www.semanticscholar.org/paper/08ad8fad21f6ec4cda4d56be1ca5e146b7c913a1,Understanding Black-box Predictions via Influence Functions,"How can we explain the predictions of a black-box model? In this paper, we use influence functions — a classic technique from robust statistics — to trace a model's prediction through the learning algorithm and back to its training data, thereby identifying training points most responsible for a given prediction. To scale up influence functions to modern machine learning settings, we develop a simple, efficient implementation that requires only oracle access to gradients and Hessian-vector products. We show that even on non-convex and non-differentiable models where the theory breaks down, approximations to influence functions can still provide valuable information. On linear models and convolutional neural networks, we demonstrate that influence functions are useful for multiple purposes: understanding model behavior, debugging models, detecting dataset errors, and even creating visually-indistinguishable training-set attacks.",2017.0,"Pang Wei Koh, Percy Liang"
a456265138c088a894301c0433dae938705a9bec,https://www.semanticscholar.org/paper/a456265138c088a894301c0433dae938705a9bec,Deep Sets,"In this paper, we study the problem of designing objective functions for machine learning problems defined on finite \emph{sets}. In contrast to traditional objective functions defined for machine learning problems operating on finite dimensional vectors, the new objective functions we propose are operating on finite sets and are invariant to permutations. Such problems are widespread, ranging from estimation of population statistics \citep{poczos13aistats}, via anomaly detection in piezometer data of embankment dams \citep{Jung15Exploration}, to cosmology \citep{Ntampaka16Dynamical,Ravanbakhsh16ICML1}. Our main theorem characterizes the permutation invariant objective functions and provides a family of functions to which any permutation invariant objective function must belong. This family of functions has a special structure which enables us to design a deep network architecture that can operate on sets and which can be deployed on a variety of scenarios including both unsupervised and supervised learning tasks. We demonstrate the applicability of our method on population statistic estimation, point cloud classification, set expansion, and image tagging.",2017.0,"M. Zaheer, Satwik Kottur, Siamak Ravanbakhsh, B. Póczos, R. Salakhutdinov, Alex Smola"
3a84214cb69ea0b34352285029f368b75718c32b,https://www.semanticscholar.org/paper/3a84214cb69ea0b34352285029f368b75718c32b,Understanding of a convolutional neural network,"The term Deep Learning or Deep Neural Network refers to Artificial Neural Networks (ANN) with multi layers. Over the last few decades, it has been considered to be one of the most powerful tools, and has become very popular in the literature as it is able to handle a huge amount of data. The interest in having deeper hidden layers has recently begun to surpass classical methods performance in different fields; especially in pattern recognition. One of the most popular deep neural networks is the Convolutional Neural Network (CNN). It take this name from mathematical linear operation between matrixes called convolution. CNN have multiple layers; including convolutional layer, non-linearity layer, pooling layer and fully-connected layer. The convolutional and fully-connected layers have parameters but pooling and non-linearity layers don't have parameters. The CNN has an excellent performance in machine learning problems. Specially the applications that deal with image data, such as largest image classification data set (Image Net), computer vision, and in natural language processing (NLP) and the results achieved were very amazing. In this paper we will explain and define all the elements and important issues related to CNN, and how these elements work. In addition, we will also state the parameters that effect CNN efficiency. This paper assumes that the readers have adequate knowledge about both machine learning and artificial neural network.",2017.0,"Saad Albawi, T. Mohammed, Saad Al-Zawi"
43d2ed5c3c55c1100450cd74dc1031afa24d37b2,https://www.semanticscholar.org/paper/43d2ed5c3c55c1100450cd74dc1031afa24d37b2,Collective Classification in Network Data,"Many real-world applications produce networked data such as the world-wide web (hypertext documents connected via hyperlinks), social networks (for example, people connected by friendship links), communication networks (computers connected via communication links) and biological networks (for example, protein interaction networks). A recent focus in machine learning research has been to extend traditional machine learning classification techniques to classify nodes in such networks. In this article, we provide a brief introduction to this area of research and how it has progressed during the past decade. We introduce four of the most widely used inference algorithms for classifying networked data and empirically compare them on both synthetic and real-world data.",2008.0,"Prithviraj Sen, Galileo Namata, M. Bilgic, L. Getoor, Brian Gallagher, Tina Eliassi-Rad"
0af75728bec67f698a8c619645165de13780c2fa,https://www.semanticscholar.org/paper/0af75728bec67f698a8c619645165de13780c2fa,Learning Multiple Tasks with Kernel Methods,"We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.",2005.0,"T. Evgeniou, C. Micchelli, M. Pontil"
7bb6bdf4ed609e5e72d4206d1b308323e73dceec,https://www.semanticscholar.org/paper/7bb6bdf4ed609e5e72d4206d1b308323e73dceec,An Introduction to Genetic Algorithms.,"An Introduction to Genetic Algorithms is one of the rare examples of a book in which every single page is worth reading. The author, Melanie Mitchell, manages to describe in depth many fascinating examples as well as important theoretical issues, yet the book is concise (200 pages) and readable. Although Mitchell explicitly states that her aim is not a complete survey, the essentials of genetic algorithms (GAs) are contained: theory and practice, problem solving and scientific models, a ""Brief History"" and ""Future Directions."" Her book is both an introduction for novices interested in GAs and a collection of recent research, including hot topics such as coevolution (interspecies and intraspecies), diploidy and dominance, encapsulation, hierarchical regulation, adaptive encoding, interactions of learning and evolution, self-adapting GAs, and more. Nevertheless, the book focused more on machine learning, artificial life, and modeling evolution than on optimization and engineering.",1997.0,D. Heiss-Czedik
0b2a2ed870d9e947aca41daf751d987ab3163d74,https://www.semanticscholar.org/paper/0b2a2ed870d9e947aca41daf751d987ab3163d74,Introduction to Statistical Pattern Recognition,"Annotation : Pattern recognition problem is briefly characterized as a process of machine learning. Its main stages (dimensionality reduction and classifier design) are stated. Statistical approach is given priority here. Two approaches to dimensionality reduction, namely feature selection (FS) and feature extraction (FE) are specified. Though FS is a special case of FE, they are very different from a practical viewpoint and thus must be considered separately.",2006.0,"P. Pudil, P. Somol, M. Haindl"
88816ae492956f3004daa41357166f1181c0c1bf,https://www.semanticscholar.org/paper/88816ae492956f3004daa41357166f1181c0c1bf,Laplacian Eigenmaps for Dimensionality Reduction and Data Representation,"One of the central problems in machine learning and pattern recognition is to develop appropriate representations for complex data. We consider the problem of constructing a representation for data lying on a low-dimensional manifold embedded in a high-dimensional space. Drawing on the correspondence between the graph Laplacian, the Laplace Beltrami operator on the manifold, and the connections to the heat equation, we propose a geometrically motivated algorithm for representing the high-dimensional data. The algorithm provides a computationally efficient approach to nonlinear dimensionality reduction that has locality-preserving properties and a natural connection to clustering. Some potential applications and illustrative examples are discussed.",2003.0,"M. Belkin, P. Niyogi"
7a59fde27461a3ef4a21a249cc403d0d96e4a0d7,https://www.semanticscholar.org/paper/7a59fde27461a3ef4a21a249cc403d0d96e4a0d7,Random Features for Large-Scale Kernel Machines,"To accelerate the training of kernel machines, we propose to map the input data to a randomized low-dimensional feature space and then apply existing fast linear methods. The features are designed so that the inner products of the transformed data are approximately equal to those in the feature space of a user specified shift-invariant kernel. We explore two sets of random features, provide convergence bounds on their ability to approximate various radial basis kernels, and show that in large-scale classification and regression tasks linear machine learning algorithms applied to these features outperform state-of-the-art large-scale kernel machines.",2007.0,"A. Rahimi, B. Recht"
8d49d34fff05285cb9a148261caff57775eb4453,https://www.semanticscholar.org/paper/8d49d34fff05285cb9a148261caff57775eb4453,ELLA: An Efficient Lifelong Learning Algorithm,"The problem of learning multiple consecutive tasks, known as lifelong learning, is of great importance to the creation of intelligent, general-purpose, and flexible machines. In this paper, we develop a method for online multi-task learning in the lifelong learning setting. The proposed Efficient Lifelong Learning Algorithm (ELLA) maintains a sparsely shared basis for all task models, transfers knowledge from the basis to learn each new task, and refines the basis over time to maximize performance across all tasks. We show that ELLA has strong connections to both online dictionary learning for sparse coding and state-of-the-art batch multitask learning methods, and provide robust theoretical performance guarantees. We show empirically that ELLA yields nearly identical performance to batch multi-task learning while learning tasks sequentially in three orders of magnitude (over 1,000x) less time.",2013.0,"P. Ruvolo, Eric Eaton"
6adf016e7531c91100d3cf4a74f5d4c87b26b528,https://www.semanticscholar.org/paper/6adf016e7531c91100d3cf4a74f5d4c87b26b528,Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,"Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",2015.0,"Nicolas Papernot, P. Mcdaniel, Xi Wu, S. Jha, A. Swami"
a1874aafa8730bdd4b28f29d025141c13ee28b58,https://www.semanticscholar.org/paper/a1874aafa8730bdd4b28f29d025141c13ee28b58,From Data Mining to Knowledge Discovery in Databases,"■ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field.",1996.0,"U. Fayyad, G. Piatetsky-Shapiro, Padhraic Smyth"
a883cacbc8f9b021b2a63f0453307855fa075d33,https://www.semanticscholar.org/paper/a883cacbc8f9b021b2a63f0453307855fa075d33,The relationship between Precision-Recall and ROC curves,"Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.",2006.0,"Jesse Davis, Mark H. Goadrich"
167e1359943b96b9e92ee73db1df69a1f65d731d,https://www.semanticscholar.org/paper/167e1359943b96b9e92ee73db1df69a1f65d731d,A Sentimental Education: Sentiment Analysis Using Subjectivity Summarization Based on Minimum Cuts,"Sentiment analysis seeks to identify the viewpoint(s) underlying a text span; an example application is classifying a movie review as ""thumbs up"" or ""thumbs down"". To determine this sentiment polarity, we propose a novel machine-learning method that applies text-categorization techniques to just the subjective portions of the document. Extracting these portions can be implemented using efficient techniques for finding minimum cuts in graphs; this greatly facilitates incorporation of cross-sentence contextual constraints.",2004.0,"B. Pang, Lillian Lee"
6b570069f14c7588e066f7138e1f21af59d62e61,https://www.semanticscholar.org/paper/6b570069f14c7588e066f7138e1f21af59d62e61,Theano: A Python framework for fast computation of mathematical expressions,"Theano is a Python library that allows to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. Since its introduction, it has been one of the most used CPU and GPU mathematical compilers - especially in the machine learning community - and has shown steady performance improvements. Theano is being actively and continuously developed since 2008, multiple frameworks have been built on top of it and it has been used to produce many state-of-the-art machine learning models. 
The present article is structured as follows. Section I provides an overview of the Theano software and its community. Section II presents the principal features of Theano and how to use them, and compares them with other similar projects. Section III focuses on recently-introduced functionalities and improvements. Section IV compares the performance of Theano against Torch7 and TensorFlow on several machine learning models. Section V discusses current limitations of Theano and potential ways of improving it.",2016.0,"Rami Al-Rfou, Guillaume Alain, Amjad Almahairi, Christof Angermüller, Dzmitry Bahdanau, Nicolas Ballas, Frédéric Bastien, Justin Bayer, A. Belikov, A. Belopolsky, Yoshua Bengio, Arnaud Bergeron, J. Bergstra, Valentin Bisson, Josh Bleecher Snyder, Nicolas Bouchard, Nicolas Boulanger-Lewandowski, Xavier Bouthillier, A. D. Brébisson, Olivier Breuleux, P. Carrier, Kyunghyun Cho, J. Chorowski, P. Christiano, Tim Cooijmans, Marc-Alexandre Côté, Myriam Côté, Aaron C. Courville, Yann Dauphin, Olivier Delalleau, Julien Demouth, Guillaume Desjardins, S. Dieleman, Laurent Dinh, Mélanie Ducoffe, Vincent Dumoulin, Samira Ebrahimi Kahou, D. Erhan, Ziye Fan, Orhan Firat, M. Germain, Xavier Glorot, I. Goodfellow, M. Graham, Çaglar Gülçehre, P. Hamel, Iban Harlouchet, J. Heng, Balázs Hidasi, S. Honari, Arjun Jain, Sébastien Jean, Kai Jia, Mikhail Korobov, Vivek Kulkarni, Alex Lamb, Pascal Lamblin, Eric Larsen, César Laurent, S. Lee, S. Lefrançois, S. Lemieux, Nicholas Léonard, Zhouhan Lin, J. Livezey, C. Lorenz, J. Lowin, Qianli Ma, Pierre-Antoine Manzagol, Olivier Mastropietro, R. McGibbon, R. Memisevic, B. V. Merrienboer, Vincent Michalski, Mehdi Mirza, A. Orlandi, C. Pal, Razvan Pascanu, M. Pezeshki, Colin Raffel, D. Renshaw, M. Rocklin, Adriana Romero, Markus Roth, Peter Sadowski, J. Salvatier, F. Savard, Jan Schlüter, John Schulman, Gabriel Schwartz, Iulian Serban, Dmitriy Serdyuk, S. Shabanian, Étienne Simon, Sigurd Spieckermann, S. Subramanyam, Jakub Sygnowski, Jérémie Tanguay, Gijs van Tulder, Joseph P. Turian, S. Urban, Pascal Vincent, Francesco Visin, H. D. Vries, David Warde-Farley, Dustin J. Webb, M. Willson, Kelvin Xu, Lijun Xue, Li Yao, Saizheng Zhang, Ying Zhang"
892f9a2f69241feec647856cd26bed37e04fd747,https://www.semanticscholar.org/paper/892f9a2f69241feec647856cd26bed37e04fd747,Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization,"Performance of machine learning algorithms depends critically on identifying a good set of hyperparameters. While recent approaches use Bayesian optimization to adaptively select configurations, we focus on speeding up random search through adaptive resource allocation and early-stopping. We formulate hyperparameter optimization as a pure-exploration non-stochastic infinite-armed bandit problem where a predefined resource like iterations, data samples, or features is allocated to randomly sampled configurations. We introduce a novel algorithm, Hyperband, for this framework and analyze its theoretical properties, providing several desirable guarantees. Furthermore, we compare Hyperband with popular Bayesian optimization methods on a suite of hyperparameter optimization problems. We observe that Hyperband can provide over an order-of-magnitude speedup over our competitor set on a variety of deep-learning and kernel-based learning problems.",2016.0,"Lisha Li, Kevin G. Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, Ameet Talwalkar"
98c25683fc8d6446448b734b1bcf08e1457f8d85,https://www.semanticscholar.org/paper/98c25683fc8d6446448b734b1bcf08e1457f8d85,A review of feature selection techniques in bioinformatics,"Feature selection techniques have become an apparent need in many bioinformatics applications. In addition to the large pool of techniques that have already been developed in the machine learning and data mining fields, specific applications in bioinformatics have led to a wealth of newly proposed techniques. In this article, we make the interested reader aware of the possibilities of feature selection, providing a basic taxonomy of feature selection techniques, and discussing their use, variety and potential in a number of both common as well as upcoming bioinformatics applications.",2007.0,"Yvan Saeys, Iñaki Inza, Pedro Larrañaga"
a34e35dbbc6911fa7b94894dffdc0076a261b6f0,https://www.semanticscholar.org/paper/a34e35dbbc6911fa7b94894dffdc0076a261b6f0,Neural Networks and the Bias/Variance Dilemma,"Feedforward neural networks trained by error backpropagation are examples of nonparametric regression estimators. We present a tutorial on nonparametric inference and its relation to neural networks, and we use the statistical viewpoint to highlight strengths and weaknesses of neural models. We illustrate the main points with some recognition experiments involving artificial data as well as handwritten numerals. In way of conclusion, we suggest that current-generation feedforward neural networks are largely inadequate for difficult problems in machine perception and machine learning, regardless of parallel-versus-serial hardware or other implementation issues. Furthermore, we suggest that the fundamental challenges in neural modeling are about representation rather than learning per se. This last point is supported by additional experiments with handwritten numerals.",1992.0,"S. Geman, E. Bienenstock, R. Doursat"
a3461eaf51016f9d6e85ea47173b27e019e801c4,https://www.semanticscholar.org/paper/a3461eaf51016f9d6e85ea47173b27e019e801c4,State of the Art,"We are concerned with the inference (induction) of theories (hypotheses) from observations (data). This problem is common to philosophy (Aristotle 1988), statistical inference (Casella & Berger 2001) and machine learning (Mitchell 1997, Agluin & Smith 1983). We constrain ourselves only to the latter two frameworks. Within machine-learning, we further concentrate on its subfield called inductive logic programming (Nienhuys-Cheng & de Wolf 1997). Whereas in statistics we namely concentrate on evaluating hypotheses, in machine learning we study ways of constructing the theories. From the theoretical viewpoint, however, the construction is also viewed as a selection of a hypothesis from an a priori given set. Unlike in statistics, however, the range of considered hypotheses is usually large so that hypotheses cannot by inspected individually by a human. Such a set of hypotheses may be conveniently viewed as (equivalent to) a language L H generated by a certain formal grammar. Every hypothesis H ∈ L H induces a mapping h : X → O where X is a predefined (usually countable) set of instances (which we also call the domain of L H) and O is a set usually assumed to be finite and its elements called classes. Very often, O has just two elements. The assigned mapping gives the hypothesis its meaning (semantics). The usual formalization of the concept learning task is then as follows. Let there be a hypothesis C ∈ L H called the target concept and let n examples (x 1 , c(x 1)),(x 2 , c(x 2)),... ,(x n , c(x n))= S drawn from a predefined distribution D X on X be provided to the algorithm L called the learner (S is called a sample). We ask L to output an hypothesis H ∈ L H such that a specified error function Err(H, C) is minimized with respect to D X. The error function may be defined as e.g. Err(H, C) = 0 if H ≡ C (i.e. h(x) = c(x) ∀x ∈ X) and Err(H, C) = 1 otherwise, that is, irrespectively of the distribution D X. We would thus require the learner to exactly identify the target concept. This would be close to the theoretical framework of identification in the limit (Gold 1967), which, roughly said, demands that the learner converges to the correct hypothesis in the limit as n → ∞. Such a requirement is however very rigid and does not comply to the …",1997.0,Markus Voelter
08b43d84e6747e370ef307e2ada50675b414514a,https://www.semanticscholar.org/paper/08b43d84e6747e370ef307e2ada50675b414514a,Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",2005.0,"Rui Xu, D. Wunsch"
f762cc39a824de1360e8223222739aaa4cd4168c,https://www.semanticscholar.org/paper/f762cc39a824de1360e8223222739aaa4cd4168c,Full regularization path for sparse principal component analysis,"Given a sample covariance matrix, we examine the problem of maximizing the variance explained by a particular linear combination of the input variables while constraining the number of nonzero coefficients in this combination. This is known as sparse principal component analysis and has a wide array of applications in machine learning and engineering. We formulate a new semidefinite relaxation to this problem and derive a greedy algorithm that computes a full set of good solutions for all numbers of non zero coefficients, with complexity O(n3), where n is the number of variables. We then use the same relaxation to derive sufficient conditions for global optimality of a solution, which can be tested in O(n3). We show on toy examples and biological data that our algorithm does provide globally optimal solutions in many cases.",2007.0,"A. d'Aspremont, Francis R. Bach, L. Ghaoui"
e86f71ca2948d17b003a5f068db1ecb2b77827f7,https://www.semanticscholar.org/paper/e86f71ca2948d17b003a5f068db1ecb2b77827f7,Concrete Problems in AI Safety,"Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (""avoiding side effects"" and ""avoiding reward hacking""), an objective function that is too expensive to evaluate frequently (""scalable supervision""), or undesirable behavior during the learning process (""safe exploration"" and ""distributional shift""). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.",2016.0,"Dario Amodei, C. Olah, J. Steinhardt, P. Christiano, John Schulman, Dandelion Mané"
10f919b1a5161b560504c225cfb2d1b3a4768f80,https://www.semanticscholar.org/paper/10f919b1a5161b560504c225cfb2d1b3a4768f80,"Artificial intelligence in healthcare: past, present and future","Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.",2017.0,"F. Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang, Q. Dong, Haipeng Shen, Yongjun Wang"
7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f,https://www.semanticscholar.org/paper/7b4f3d0e4e2486a8d5d3f8e00549cf9a117bf88f,Sequence Transduction with Recurrent Neural Networks,"Many machine learning tasks can be expressed as the transformation---or \emph{transduction}---of input sequences into output sequences: speech recognition, machine translation, protein secondary structure prediction and text-to-speech to name but a few. One of the key challenges in sequence transduction is learning to represent both the input and output sequences in a way that is invariant to sequential distortions such as shrinking, stretching and translating. Recurrent neural networks (RNNs) are a powerful sequence learning architecture that has proven capable of learning such representations. However RNNs traditionally require a pre-defined alignment between the input and output sequences to perform transduction. This is a severe limitation since \emph{finding} the alignment is the most difficult aspect of many sequence transduction problems. Indeed, even determining the length of the output sequence is often challenging. This paper introduces an end-to-end, probabilistic sequence transduction system, based entirely on RNNs, that is in principle able to transform any input sequence into any finite, discrete output sequence. Experimental results for phoneme recognition are provided on the TIMIT speech corpus.",2012.0,Alex Graves
bb144c04b9eb44579b19d21c3d5954401408440b,https://www.semanticscholar.org/paper/bb144c04b9eb44579b19d21c3d5954401408440b,Orange: data mining toolbox in python,"Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.",2013.0,"J. Demšar, Tomaž Curk, Ales Erjavec, C. Gorup, Tomaz Hocevar, Mitar Milutinovic, M. Mozina, M. Polajnar, Marko Toplak, A. Staric, Miha Stajdohar, Lan Umek, Lan Zagar, Jure Zbontar, M. Zitnik, B. Zupan"
d079a2f877f554e00f71a6975435d8325987bdf5,https://www.semanticscholar.org/paper/d079a2f877f554e00f71a6975435d8325987bdf5,Return of Frustratingly Easy Domain Adaptation,"
 
 Unlike human learning, machine learning often fails to handle changes between training (source) and test (target) input distributions. Such domain shifts, common in practical scenarios, severely damage the performance of conventional machine learning methods. Supervised domain adaptation methods have been proposed for the case when the target data have labels, including some that perform very well despite being ``frustratingly easy'' to implement. However, in practice, the target domain is often unlabeled, requiring unsupervised adaptation. We propose a simple, effective, and efficient method for unsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL minimizes domain shift by aligning the second-order statistics of source and target distributions, without requiring any target labels. Even though it is extraordinarily simple--it can be implemented in four lines of Matlab code--CORAL performs remarkably well in extensive evaluations on standard benchmark datasets.
 
",2015.0,"Baochen Sun, Jiashi Feng, Kate Saenko"
d1b9a3b11e6c9571a1553556f82b605b2b4baec3,https://www.semanticscholar.org/paper/d1b9a3b11e6c9571a1553556f82b605b2b4baec3,Model Inversion Attacks that Exploit Confidence Information and Basic Countermeasures,"Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classifiers in personalized medicine by Fredrikson et al., adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown. We develop a new class of model inversion attack that exploits confidence values revealed along with predictions. Our new attacks are applicable in a variety of settings, and we explore two in depth: decision trees for lifestyle surveys as used on machine-learning-as-a-service systems and neural networks for facial recognition. In both cases confidence values are revealed to those with the ability to make prediction queries to models. We experimentally show attacks that are able to estimate whether a respondent in a lifestyle survey admitted to cheating on their significant other and, in the other context, show how to recover recognizable images of people's faces given only their name and access to the ML model. We also initiate experimental exploration of natural countermeasures, investigating a privacy-aware decision tree training algorithm that is a simple variant of CART learning, as well as revealing only rounded confidence values. The lesson that emerges is that one can avoid these kinds of MI attacks with negligible degradation to utility.",2015.0,"Matt Fredrikson, S. Jha, Thomas Ristenpart"
f46714d200d69eb9cb5cce176297b89a3f5e3a2c,https://www.semanticscholar.org/paper/f46714d200d69eb9cb5cce176297b89a3f5e3a2c,An Introduction to Convolutional Neural Networks,"The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. 
This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.",2015.0,"K. O’Shea, Ryan Nash"
a42ca00fc188beb5586ad4c7108b70aeb5317da0,https://www.semanticscholar.org/paper/a42ca00fc188beb5586ad4c7108b70aeb5317da0,Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms,"Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.",2012.0,"C. Thornton, F. Hutter, H. Hoos, Kevin Leyton-Brown"
47c528344fedb6cb67a38e43d095b41c34715330,https://www.semanticscholar.org/paper/47c528344fedb6cb67a38e43d095b41c34715330,Adaptive Federated Optimization,"Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Due to the heterogeneity of the client datasets, standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.",2020.0,"Sashank J. Reddi, Zachary B. Charles, M. Zaheer, Zachary Garrett, Keith Rush, Jakub Konecný, Sanjiv Kumar, H. B. McMahan"
6af440915b8a0718c93be1cf61905e41e620484a,https://www.semanticscholar.org/paper/6af440915b8a0718c93be1cf61905e41e620484a,Deep One-Class Classification,"Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objec-tive. In this paper we introduce a new anomaly detection method—Deep Support Vector Data Description—, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GT-SRB stop signs.",2018.0,"Lukas Ruff, Nico Görnitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Robert A. Vandermeulen, Alexander Binder, Emmanuel Müller, M. Kloft"
043f084e379a44608c470059c2aa174a323e9774,https://www.semanticscholar.org/paper/043f084e379a44608c470059c2aa174a323e9774,Counterfactual Fairness,"Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.",2017.0,"Matt J. Kusner, Joshua R. Loftus, Chris Russell, Ricardo Silva"
e0cf9c51192f63c3a9e3b23aa07ee5654fc97b68,https://www.semanticscholar.org/paper/e0cf9c51192f63c3a9e3b23aa07ee5654fc97b68,Large Margin Methods for Structured and Interdependent Output Variables,"Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.",2005.0,"Ioannis Tsochantaridis, T. Joachims, Thomas Hofmann, Y. Altun"
c80d112ce59c72f943dc7b3e56e4c77dc3af1146,https://www.semanticscholar.org/paper/c80d112ce59c72f943dc7b3e56e4c77dc3af1146,CryptoNets: Applying Neural Networks to Encrypted Data with High Throughput and Accuracy,"Applying machine learning to a problem which involves medical, financial, or other types of sensitive data, not only requires accurate predictions but also careful attention to maintaining data privacy and security. Legal and ethical requirements may prevent the use of cloud-based machine learning solutions for such tasks. In this work, we will present a method to convert learned neural networks to CryptoNets, neural networks that can be applied to encrypted data. This allows a data owner to send their data in an encrypted form to a cloud service that hosts the network. The encryption ensures that the data remains confidential since the cloud does not have access to the keys needed to decrypt it. Nevertheless, we will show that the cloud service is capable of applying the neural network to the encrypted data to make encrypted predictions, and also return them in encrypted form. These encrypted predictions can be sent back to the owner of the secret key who can decrypt them. Therefore, the cloud service does not gain any information about the raw data nor about the prediction it made. We demonstrate CryptoNets on the MNIST optical character recognition tasks. CryptoNets achieve 99% accuracy and can make around 59000 predictions per hour on a single PC. Therefore, they allow high throughput, accurate, and private predictions.",2016.0,"Nathan Dowlin, Ran Gilad-Bachrach, Kim Laine, K. Lauter, M. Naehrig, J. Wernsing"
0c8ce4f11c00c78b04151d2526a812008ca7a764,https://www.semanticscholar.org/paper/0c8ce4f11c00c78b04151d2526a812008ca7a764,A review of classification algorithms for EEG-based brain–computer interfaces: a 10 year update,"Objective. Most current electroencephalography (EEG)-based brain–computer interfaces (BCIs) are based on machine learning algorithms. There is a large diversity of classifier types that are used in this field, as described in our 2007 review paper. Now, approximately ten years after this review publication, many new algorithms have been developed and tested to classify EEG signals in BCIs. The time is therefore ripe for an updated review of EEG classification algorithms for BCIs. Approach. We surveyed the BCI and machine learning literature from 2007 to 2017 to identify the new classification approaches that have been investigated to design BCIs. We synthesize these studies in order to present such algorithms, to report how they were used for BCIs, what were the outcomes, and to identify their pros and cons. Main results. We found that the recently designed classification algorithms for EEG-based BCIs can be divided into four main categories: adaptive classifiers, matrix and tensor classifiers, transfer learning and deep learning, plus a few other miscellaneous classifiers. Among these, adaptive classifiers were demonstrated to be generally superior to static ones, even with unsupervised adaptation. Transfer learning can also prove useful although the benefits of transfer learning remain unpredictable. Riemannian geometry-based methods have reached state-of-the-art performances on multiple BCI problems and deserve to be explored more thoroughly, along with tensor-based methods. Shrinkage linear discriminant analysis and random forests also appear particularly useful for small training samples settings. On the other hand, deep learning methods have not yet shown convincing improvement over state-of-the-art BCI methods. Significance. This paper provides a comprehensive overview of the modern classification algorithms used in EEG-based BCIs, presents the principles of these methods and guidelines on when and how to use them. It also identifies a number of challenges to further advance EEG classification in BCI.",2018.0,"F. Lotte, L. Bougrain, A. Cichocki, Maureen Clerc, M. Congedo, A. Rakotomamonjy, F. Yger"
7365f887c938ca21a6adbef08b5a520ebbd4638f,https://www.semanticscholar.org/paper/7365f887c938ca21a6adbef08b5a520ebbd4638f,Model Cards for Model Reporting,"Trained machine learning models are increasingly used to perform high-impact tasks in areas such as law enforcement, medicine, education, and employment. In order to clarify the intended use cases of machine learning models and minimize their usage in contexts for which they are not well suited, we recommend that released models be accompanied by documentation detailing their performance characteristics. In this paper, we propose a framework that we call model cards, to encourage such transparent model reporting. Model cards are short documents accompanying trained machine learning models that provide benchmarked evaluation in a variety of conditions, such as across different cultural, demographic, or phenotypic groups (e.g., race, geographic location, sex, Fitzpatrick skin type [15]) and intersectional groups (e.g., age and race, or sex and Fitzpatrick skin type) that are relevant to the intended application domains. Model cards also disclose the context in which models are intended to be used, details of the performance evaluation procedures, and other relevant information. While we focus primarily on human-centered machine learning models in the application fields of computer vision and natural language processing, this framework can be used to document any trained machine learning model. To solidify the concept, we provide cards for two supervised models: One trained to detect smiling faces in images, and one trained to detect toxic comments in text. We propose model cards as a step towards the responsible democratization of machine learning and related artificial intelligence technology, increasing transparency into how well artificial intelligence technology works. We hope this work encourages those releasing trained machine learning models to accompany model releases with similar detailed evaluation numbers and other relevant documentation.",2018.0,"Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer, Inioluwa Deborah Raji, Timnit Gebru"
eec471897375942fd690b736c2753bb19d907273,https://www.semanticscholar.org/paper/eec471897375942fd690b736c2753bb19d907273,"Gradient boosting machines, a tutorial","Gradient boosting machines are a family of powerful machine-learning techniques that have shown considerable success in a wide range of practical applications. They are highly customizable to the particular needs of the application, like being learned with respect to different loss functions. This article gives a tutorial introduction into the methodology of gradient boosting methods with a strong focus on machine learning aspects of modeling. A theoretical information is complemented with descriptive examples and illustrations which cover all the stages of the gradient boosting model design. Considerations on handling the model complexity are discussed. Three practical examples of gradient boosting applications are presented and comprehensively analyzed.",2013.0,"Alexey Natekin, A. Knoll"
7f57e9939560562727344c1c987416285ef76cda,https://www.semanticscholar.org/paper/7f57e9939560562727344c1c987416285ef76cda,Accessorize to a Crime: Real and Stealthy Attacks on State-of-the-Art Face Recognition,"Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.",2016.0,"Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, M. Reiter"
e4a85af3f5dc41e13dc2cae9ee851953709b764e,https://www.semanticscholar.org/paper/e4a85af3f5dc41e13dc2cae9ee851953709b764e,Solving the quantum many-body problem with artificial neural networks,"Machine learning and quantum physics Elucidating the behavior of quantum interacting systems of many particles remains one of the biggest challenges in physics. Traditional numerical methods often work well, but some of the most interesting problems leave them stumped. Carleo and Troyer harnessed the power of machine learning to develop a variational approach to the quantum many-body problem (see the Perspective by Hush). The method performed at least as well as state-of-the-art approaches, setting a benchmark for a prototypical two-dimensional problem. With further development, it may well prove a valuable piece in the quantum toolbox. Science, this issue p. 602; see also p. 580 A machine-learning approach sets a computational benchmark for a prototypical two-dimensional problem. The challenge posed by the many-body problem in quantum physics originates from the difficulty of describing the nontrivial correlations encoded in the exponential complexity of the many-body wave function. Here we demonstrate that systematic machine learning of the wave function can reduce this complexity to a tractable computational form for some notable cases of physical interest. We introduce a variational representation of quantum states based on artificial neural networks with a variable number of hidden neurons. A reinforcement-learning scheme we demonstrate is capable of both finding the ground state and describing the unitary time evolution of complex interacting quantum systems. Our approach achieves high accuracy in describing prototypical interacting spins models in one and two dimensions.",2016.0,"Giuseppe Carleo, M. Troyer"
52e2bd533323ddf97073d034bae40a46eda55f34,https://www.semanticscholar.org/paper/52e2bd533323ddf97073d034bae40a46eda55f34,Twitter Sentiment Classiﬁcation using Distant Supervision,"We introduce a novel approach for automatically classifying the sentiment of Twitter messages. These messages are classiﬁed as either positive or negative with respect to a query term. This is useful for consumers who want to research the sentiment of products before purchase, or companies that want to monitor the public sentiment of their brands. There is no previous research on classifying sentiment of messages on microblogging services like Twitter. We present the results of machine learning algorithms for classifying the sentiment of Twitter messages using distant supervision. Our training data consists of Twitter messages with emoticons, which are used as noisy labels. This type of training data is abundantly available and can be obtained through automated means. We show that machine learning algorithms (Naive Bayes, Maximum Entropy, and SVM) have accuracy above 80% when trained with emoticon data. This paper also describes the preprocessing steps needed in order to achieve high accuracy. The main contribution of this paper is the idea of using tweets with emoticons for distant supervised learning.",2009.0,Alec Go
6d5965a76f88a8ebab4fc9c43a3ae2630628966a,https://www.semanticscholar.org/paper/6d5965a76f88a8ebab4fc9c43a3ae2630628966a,Learning and evaluating classifiers under sample selection bias,"Classifier learning methods commonly assume that the training data consist of randomly drawn examples from the same distribution as the test examples about which the learned model is expected to make predictions. In many practical situations, however, this assumption is violated, in a problem known in econometrics as sample selection bias. In this paper, we formalize the sample selection bias problem in machine learning terms and study analytically and experimentally how a number of well-known classifier learning methods are affected by it. We also present a bias correction method that is particularly useful for classifier evaluation under sample selection bias.",2004.0,B. Zadrozny
0407b605b8f55db72e2545586bfe8e946b691b70,https://www.semanticscholar.org/paper/0407b605b8f55db72e2545586bfe8e946b691b70,An Empirical Investigation of Catastrophic Forgeting in Gradient-Based Neural Networks,"Catastrophic forgetting is a problem faced by many machine learning models and algorithms. When trained on one task, then trained on a second task, many machine learning models ""forget"" how to perform the first task. This is widely believed to be a serious problem for neural networks. Here, we investigate the extent to which the catastrophic forgetting problem occurs for modern neural networks, comparing both established and recent gradient-based training algorithms and activation functions. We also examine the effect of the relationship between the first task and the second task on catastrophic forgetting. We find that it is always best to train using the dropout algorithm--the dropout algorithm is consistently best at adapting to the new task, remembering the old task, and has the best tradeoff curve between these two extremes. We find that different tasks and relationships between tasks result in very different rankings of activation function performance. This suggests the choice of activation function should always be cross-validated.",2013.0,"I. Goodfellow, Mehdi Mirza, Xia Da, Aaron C. Courville, Yoshua Bengio"
9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746,https://www.semanticscholar.org/paper/9d4bb6ec511c5dd4c6c97224b59cf4cdf4dc0746,The class imbalance problem: A systematic study,"In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. This paper presents a systematic study aimed at answering three different questions. First, we attempt to understand the nature of the class imbalance problem by establishing a relationship between concept complexity, size of the training set and class imbalance level. Second, we discuss several basic re-sampling or cost-modifying methods previously proposed to deal with the class imbalance problem and compare their effectiveness. The results obtained by such methods on artificial domains are linked to results in real-world domains. Finally, we investigate the assumption that the class imbalance problem does not only affect decision tree systems but also affects other classification systems such as Neural Networks and Support Vector Machines.",2002.0,"N. Japkowicz, Shaju Stephen"
b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a,https://www.semanticscholar.org/paper/b8d7788f25dfaf0f9fe2e6c441d75ca7cd3bc09a,Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution,"Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality",2003.0,"Lei Yu, Huan Liu"
b57c54350769ffa59ff57f79ee5aad918844d298,https://www.semanticscholar.org/paper/b57c54350769ffa59ff57f79ee5aad918844d298,Differentially Private Empirical Risk Minimization,"Privacy-preserving machine learning algorithms are crucial for the increasingly common setting in which personal data, such as medical or financial records, are analyzed. We provide general techniques to produce privacy-preserving approximations of classifiers learned via (regularized) empirical risk minimization (ERM). These algorithms are private under the ε-differential privacy definition due to Dwork et al. (2006). First we apply the output perturbation ideas of Dwork et al. (2006), to ERM classification. Then we propose a new method, objective perturbation, for privacy-preserving machine learning algorithm design. This method entails perturbing the objective function before optimizing over classifiers. If the loss and regularizer satisfy certain convexity and differentiability criteria, we prove theoretical results showing that our algorithms preserve privacy, and provide generalization bounds for linear and nonlinear kernels. We further present a privacy-preserving technique for tuning the parameters in general machine learning algorithms, thereby providing end-to-end privacy guarantees for the training process. We apply these results to produce privacy-preserving analogues of regularized logistic regression and support vector machines. We obtain encouraging results from evaluating their performance on real demographic and benchmark data sets. Our results show that both theoretically and empirically, objective perturbation is superior to the previous state-of-the-art, output perturbation, in managing the inherent tradeoff between privacy and learning performance.",2009.0,"Kamalika Chaudhuri, C. Monteleoni, A. Sarwate"
602f31242e577d2d05f918a3080fd50095e7faed,https://www.semanticscholar.org/paper/602f31242e577d2d05f918a3080fd50095e7faed,Factors in automatic musical genre classification of audio signals,"Automatic musical genre classification is an important tool for organizing the large collections of music that are becoming available to the average user. In addition, it provides a structured way of evaluating musical content features that does not require extensive user studies. The paper provides a detailed comparative analysis of various factors affecting automatic classification performance, such as choice of features and classifiers. Using recent machine learning techniques, such as support vector machines, we improve on previously published results using identical data collections and features.",2003.0,"Tao Li, G. Tzanetakis"
e478fe3666e7cd630a40c81776fc2b55db61e085,https://www.semanticscholar.org/paper/e478fe3666e7cd630a40c81776fc2b55db61e085,An Efficient Boosting Algorithm for Combining Preferences,"We study the problem of learning to accurately rank a set of objects by combining a given collection of ranking or preference functions. This problem of combining preferences arises in several applications, such as that of combining the results of different search engines, or the “collaborative-ﬁltering” problem of ranking movies for a user based on the movie rankings provided by other users. In this work, we begin by presenting a formal framework for this general problem. We then describe and analyze an efﬁcient algorithm called RankBoost for combining preferences based on the boosting approach to machine learning. We give theoretical results describing the algorithm’s behavior both on the training data, and on new test data not seen during training. We also describe an efﬁcient implementation of the algorithm for a particular restricted but common case. We next discuss two experiments we carried out to assess the performance of RankBoost. In the ﬁrst exper-iment, we used the algorithm to combine different web search strategies, each of which is a query expansion for a given domain. The second experiment is a collaborative-ﬁltering task for making movie recommendations.",1998.0,"Y. Freund, Raj D. Iyer, R. Schapire, Y. Singer"
83040001210751239553269727b9ea53e152af71,https://www.semanticscholar.org/paper/83040001210751239553269727b9ea53e152af71,Building Machines that Learn and Think Like People,"Recent successes in artificial intelligence and machine learning have been largely driven by methods for sophisticated pattern recognition, including deep neural networks and other data-intensive methods. But human intelligence is more than just pattern recognition. And no machine system yet built has anything like the flexible, general-purpose commonsense grasp of the world that we can see in even a one-year-old human infant. I will consider how we might capture the basic learning and thinking abilities humans possess from early childhood, as one route to building more human-like forms of machine learning and thinking. At the heart of human common sense is our ability to model the physical and social environment around us: to explain and understand what we see, to imagine things we could see but haven't yet, to solve problems and plan actions to make these things real, and to build new models as we learn more about the world. I will focus on our recent work reverse-engineering these capacities using methods from probabilistic programming, program induction and program synthesis, which together with deep learning methods and video game simulation engines, provide a toolkit for the joint enterprise of modeling human intelligence and making AI systems smarter in more human-like ways.",2018.0,J. Tenenbaum
41fef1a197fab9684a4608b725d3ae72e1ab4b39,https://www.semanticscholar.org/paper/41fef1a197fab9684a4608b725d3ae72e1ab4b39,Sparse Feature Learning for Deep Belief Networks,"Unsupervised learning algorithms aim to discover the structure hidden in the data, and to learn representations that are more suitable as input to a supervised machine than the raw input. Many unsupervised methods are based on reconstructing the input from the representation, while constraining the representation to have certain desirable properties (e.g. low dimension, sparsity, etc). Others are based on approximating density by stochastically reconstructing the input from the representation. We describe a novel and efficient algorithm to learn sparse representations, and compare it theoretically and experimentally with a similar machine trained probabilistically, namely a Restricted Boltzmann Machine. We propose a simple criterion to compare and select different unsupervised machines based on the trade-off between the reconstruction error and the information content of the representation. We demonstrate this method by extracting features from a dataset of handwritten numerals, and from a dataset of natural image patches. We show that by stacking multiple levels of such machines and by training sequentially, high-order dependencies between the input observed variables can be captured.",2007.0,"Marc'Aurelio Ranzato, Y-Lan Boureau, Yann LeCun"
7333e127b62eb545d81830df2a66b98c0693a32b,https://www.semanticscholar.org/paper/7333e127b62eb545d81830df2a66b98c0693a32b,Quantile Regression Forests,"Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.",2006.0,N. Meinshausen
38d8230a7aeae6554497b253848ad5bf677e4fb3,https://www.semanticscholar.org/paper/38d8230a7aeae6554497b253848ad5bf677e4fb3,PennyLane: Automatic differentiation of hybrid quantum-classical computations,"PennyLane is a Python 3 software framework for optimization and machine learning of quantum and hybrid quantum-classical computations. The library provides a unified architecture for near-term quantum computing devices, supporting both qubit and continuous-variable paradigms. PennyLane's core feature is the ability to compute gradients of variational quantum circuits in a way that is compatible with classical techniques such as backpropagation. PennyLane thus extends the automatic differentiation algorithms common in optimization and machine learning to include quantum and hybrid computations. A plugin system makes the framework compatible with any gate-based quantum simulator or hardware. We provide plugins for Strawberry Fields, Rigetti Forest, Qiskit, Cirq, and ProjectQ, allowing PennyLane optimizations to be run on publicly accessible quantum devices provided by Rigetti and IBM Q. On the classical front, PennyLane interfaces with accelerated machine learning libraries such as TensorFlow, PyTorch, and autograd. PennyLane can be used for the optimization of variational quantum eigensolvers, quantum approximate optimization, quantum machine learning models, and many other applications.",2018.0,"V. Bergholm, J. Izaac, M. Schuld, C. Gogolin, Ankit Khandelwal, N. Killoran"
ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be,https://www.semanticscholar.org/paper/ba10b8f9ee40b68053af9e6c2383aa2c6e39e9be,Text Classification Algorithms: A Survey,"In recent years, there has been an exponential growth in the number of complex documentsand texts that require a deeper understanding of machine learning methods to be able to accuratelyclassify texts in many applications. Many machine learning approaches have achieved surpassingresults in natural language processing. The success of these learning algorithms relies on their capacityto understand complex models and non-linear relationships within data. However, finding suitablestructures, architectures, and techniques for text classification is a challenge for researchers. In thispaper, a brief overview of text classification algorithms is discussed. This overview covers differenttext feature extractions, dimensionality reduction methods, existing algorithms and techniques, andevaluations methods. Finally, the limitations of each technique and their application in real-worldproblems are discussed.",2019.0,"Kamran Kowsari, K. Meimandi, Mojtaba Heidarysafa, Sanjana Mendu, Laura E. Barnes, Donald E. Brown"
609e5cc1da126d7f760d1444b43b4fae41602841,https://www.semanticscholar.org/paper/609e5cc1da126d7f760d1444b43b4fae41602841,Less is More: Active Learning with Support Vector Machines,"We describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classiﬁcation tasks. We observe a number of beneﬁts, the most surprising of which is that a SVM trained on a well-chosen subset of the available corpus frequently performs better than one trained on all available data. The heuristic for choosing this subset is simple to compute, and makes no use of information about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better performance with fewer data, it frequently does so in less time than the naive approach of training on all available data.",2000.0,"Greg Schohn, David A. Cohn"
5e095981ebf4d389e9356bd56e59e0ade1b42e88,https://www.semanticscholar.org/paper/5e095981ebf4d389e9356bd56e59e0ade1b42e88,"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text","The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.",2011.0,"Özlem Uzuner, B. South, Shuying Shen, S. Duvall"
ade03d0c772c35dc8e865bdb41d7bc54d5b782d1,https://www.semanticscholar.org/paper/ade03d0c772c35dc8e865bdb41d7bc54d5b782d1,kernlab - An S4 Package for Kernel Methods in R,"kernlab is an extensible package for kernel-based machine learning methods in R. It takes advantage of R's new S4 ob ject model and provides a framework for creating and using kernel-based algorithms. The package contains dot product primitives (kernels), implementations of support vector machines and the relevance vector machine, Gaussian processes, a ranking algorithm, kernel PCA, kernel CCA, and a spectral clustering algorithm. Moreover it provides a general purpose quadratic programming solver, and an incomplete Cholesky decomposition method.",2004.0,"Alexandros Karatzoglou, A. Smola, K. Hornik, A. Zeileis"
2be40f5336afa68b49fef41e009b7172c2c9fdeb,https://www.semanticscholar.org/paper/2be40f5336afa68b49fef41e009b7172c2c9fdeb,POT: Python Optimal Transport,"Optimal transport has recently been reintroduced to the machine learning community thanks in part to novel eﬃcient optimization procedures allowing for medium to large scale applications. We propose a Python toolbox that implements several key optimal transport ideas for the machine learning community. The toolbox contains implementations of a number of founding works of OT for machine learning such as Sinkhorn algorithm and Wasserstein barycenters, but also provides generic solvers that can be used for conducting novel fundamental research. This toolbox, named POT for Python Optimal Transport, is open source with an MIT license.",2021.0,"Rémi Flamary, N. Courty, Alexandre Gramfort, Mokhtar Z. Alaya, Aurélie Boisbunon, Stanislas Chambon, Adrien Corenflos, Nemo Fournier, N. Gayraud, H. Janati, I. Redko, Antoine Rolet, A. Schutz, Danica J. Sutherland, R. Tavenard, Alexander Tong, Titouan Vayer, A. Mueller"
c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3,https://www.semanticscholar.org/paper/c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3,Generalizing from a Few Examples,"Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this article, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimizer is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications, and theories, are also proposed to provide insights for future research.1",2019.0,"Yaqing Wang, Quanming Yao, J. Kwok, L. Ni"
f7d997a640f2b804676cadb8030d8b2c7bd79d85,https://www.semanticscholar.org/paper/f7d997a640f2b804676cadb8030d8b2c7bd79d85,On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation,"Model selection strategies for machine learning algorithms typically involve the numerical optimisation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.",2010.0,"G. Cawley, N. L. C. Talbot"
7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3,https://www.semanticscholar.org/paper/7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3,"ALVINN, an autonomous land vehicle in a neural network",The support-vector network is a new leaming machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition.,2015.0,D. Pomerleau
79f2626046fdc56edfaca840874e355cac734b9a,https://www.semanticscholar.org/paper/79f2626046fdc56edfaca840874e355cac734b9a,Ad click prediction: a view from the trenches,"Predicting ad click-through rates (CTR) is a massive-scale learning problem that is central to the multi-billion dollar online advertising industry. We present a selection of case studies and topics drawn from recent experiments in the setting of a deployed CTR prediction system. These include improvements in the context of traditional supervised learning based on an FTRL-Proximal online learning algorithm (which has excellent sparsity and convergence properties) and the use of per-coordinate learning rates. We also explore some of the challenges that arise in a real-world system that may appear at first to be outside the domain of traditional machine learning research. These include useful tricks for memory savings, methods for assessing and visualizing performance, practical methods for providing confidence estimates for predicted probabilities, calibration methods, and methods for automated management of features. Finally, we also detail several directions that did not turn out to be beneficial for us, despite promising results elsewhere in the literature. The goal of this paper is to highlight the close relationship between theoretical advances and practical engineering in this industrial setting, and to show the depth of challenges that appear when applying traditional machine learning methods in a complex dynamic system.",2013.0,"H. B. McMahan, Gary Holt, D. Sculley, Michael Young, D. Ebner, Julian Grady, Lan Nie, Todd Phillips, Eugene Davydov, D. Golovin, S. Chikkerur, Dan Liu, M. Wattenberg, A. M. Hrafnkelsson, T. Boulos, J. Kubica"
0a5ff7336879c99513dca6fce6ef44984ebf3f55,https://www.semanticscholar.org/paper/0a5ff7336879c99513dca6fce6ef44984ebf3f55,Clipper: A Low-Latency Online Prediction Serving System,"Machine learning is being deployed in a growing number of applications which demand real-time, accurate, and robust predictions under heavy query load. However, most machine learning frameworks and systems only address model training and not deployment. 
In this paper, we introduce Clipper, a general-purpose low-latency prediction serving system. Interposing between end-user applications and a wide range of machine learning frameworks, Clipper introduces a modular architecture to simplify model deployment across frameworks and applications. Furthermore, by introducing caching, batching, and adaptive model selection techniques, Clipper reduces prediction latency and improves prediction throughput, accuracy, and robustness without modifying the underlying machine learning frameworks. We evaluate Clipper on four common machine learning benchmark datasets and demonstrate its ability to meet the latency, accuracy, and throughput demands of online serving applications. Finally, we compare Clipper to the TensorFlow Serving system and demonstrate that we are able to achieve comparable throughput and latency while enabling model composition and online learning to improve accuracy and render more robust predictions.",2016.0,"D. Crankshaw, Xin Wang, Giulio Zhou, M. Franklin, Joseph E. Gonzalez, Ion Stoica"
a538b05ebb01a40323997629e171c91aa28b8e2f,https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f,Rectified Linear Units Improve Restricted Boltzmann Machines,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ""Stepped Sigmoid Units"" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",2010.0,"Vinod Nair, Geoffrey E. Hinton"
223841a71f5bce4cb03040e229d13e9a71b78ec3,https://www.semanticscholar.org/paper/223841a71f5bce4cb03040e229d13e9a71b78ec3,Persistence Images: A Stable Vector Representation of Persistent Homology,"Many datasets can be viewed as a noisy sampling of an underlying space, and tools from topological data analysis can characterize this structure for the purpose of knowledge discovery. One such tool is persistent homology, which provides a multiscale description of the homological features within a dataset. A useful representation of this homological information is a persistence diagram (PD). Efforts have been made to map PDs into spaces with additional structure valuable to machine learning tasks. We convert a PD to a finite-dimensional vector representation which we call a persistence image (PI), and prove the stability of this transformation with respect to small perturbations in the inputs. The discriminatory power of PIs is compared against existing methods, showing significant performance gains. We explore the use of PIs with vector-based machine learning tools, such as linear sparse support vector machines, which identify features containing discriminating topological information. Finally, high accuracy inference of parameter values from the dynamic output of a discrete dynamical system (the linked twist map) and a partial differential equation (the anisotropic Kuramoto-Sivashinsky equation) provide a novel application of the discriminatory power of PIs.",2015.0,"Henry Adams, T. Emerson, M. Kirby, R. Neville, C. Peterson, Patrick D. Shipman, Sofya Chepushtanova, Eric M. Hanson, Francis C. Motta, Lori Ziegelmeier"
a9763afda62e960c35c80681f805ddecbef14a92,https://www.semanticscholar.org/paper/a9763afda62e960c35c80681f805ddecbef14a92,Images of Organization,"Preface Part I. An Overview Introduction Part II. Some Images of Organization 2. Mechanization Takes Command: Organizations as Machines Machines, Mechanical Thinking, and the Rise of Bureaucratic Organization The Origins of Mechanistic Organization Classical Management Theory: Designing bureaucratic organizations Scientific Management Strengths and Limitations of the Machine Metaphor 3. Nature Intervenes: Organizations as Organisms Discovering Organizational Needs Recognizing the Importance of Environment: Organizations as Open Systems Contingency Theory: Adapting Organization to Environment The Variety of the Species Contingency Theory: Promoting Organizational Health and Development Natural Selection: The Population-Ecology View of Organizations Organizational Ecology: The Creation of Shared Futures Strengths and Limitations of the Organismic Metaphor 4. Learning and Self-Organization: Organizations as Brains Images of the Brain Organizations as Information Processing Brains Creating Learning Organizations Cybernetics, Learning, and Learning to Learn Can Organizations Learn to Learn? Guidelines for ""Learning Organizations"" Organizations as Holographic Brains Principles of Holographic Design Strengths and Limitations of the Brain Metaphors 5. Creating Social Realty: Organizations as Cultures Culture and Organization Organization as a Cultural Phenomenon Organization and Cultural Context Corporate Cultures and Subcultures Creating Organizational Reality Culture: Rule Following or Enactment? Organization: The enactment of a Shared Reality Strengths and Limitations of the Cultural Metaphor 6. Interests, Conflict, and Power: Organizations as Political Systems Organizations as Systems of Government Organizations as Systems of Political Activity Analyzing Interests Understanding Conflict Exploring Power Managing Pluralist Organizations Strengths and Limitations of the Political Metaphor 7. Exploring Plato's Cave: Organizations as Psychic Prisons The Trap of Favored Ways of Thinking Organization and the Unconscious Organization and Repressed Sexuality Organization and the Patriarchal Family Organization, Death, and Immortality Organization and Anxiety Organization, Dolls, and Teddy Bears Organization, Shadow, and Archetype The Unconscious: A Creative and Destructive Force Strengths and Limitations of the Psychic Prison Metaphor 8. Unfolding Logics of Change: Organization as Flux and Transformation Autopoiesis: Rethinking Relations With the Environment Enactment as a Form of Narcissism: Organizations Interact With Projections of Themselves Identity and Closure: Egocentrism Versus Systemic Wisdom Shifting ""Attractors"": The Logic of Chaos and Complexity Managing in the Midst of Complexity Loops, Not Lines: The Logic of Mutual Causality Contradiction and Crisis: The Logic of Dialectical Change Dialectical Analysis: How Opposing Forces Drive Change The Dialectics of Management Strengths and Limitations of the Flux and Transformation Metaphor 9. The Ugly Face: Organizations as Instruments of Domination Organization as Domination How Organizations Use and Exploit Their Employees Organization, Class, and Control Work Hazards, Occupational Disease, and Industrial Accidents Workaholism and Social and Mental Stress Organizational Politics and the Radicalized Organization Multinationals and the World Economy The Multinationals as World Powers Multinationals: A Record of Exploitation? Strengths and Limitations of the Domination Metaphor Part III. Implications For Practice 10. The Challenge of Metaphor Metaphors Create Ways of Seeing and Shaping Organizational Life Seeing, Thinking, and Acting in New Ways 11. Reading and Shaping Organizational Life The Multicom Case Interpreting Multicom Developing and Detailed Reading and ""Storyline"" Multicom From Another View ""Reading"" and Emergent Intelligence 12. Postscript Bibliographic Notes Introduction The Machine Metaphor The Organismic Metaphor The Brain Metaphor The Culture Metaphor The Political Metaphor The Psychic Prison Metaphor The Flux and Transformation Metaphor The Domination Metaphor The Challenge of Metaphor Reading and Shaping Organizational Life Postscript Bibliography",1988.0,"J. Alexander, G. Morgan"
4a6e74d4bf4fd0106891e5518692a77c7aa8811d,https://www.semanticscholar.org/paper/4a6e74d4bf4fd0106891e5518692a77c7aa8811d,Outlier Detection in High Dimensional Data,"Artificial intelligence (AI) is the science that allows
computers to replicate human intelligence in areas such as
decision-making, text processing, visual perception. Artificial
Intelligence is the broader field that contains several subfields
such as machine learning, robotics, and computer vision.
Machine Learning is a branch of Artificial Intelligence that
allows a machine to learn and improve at a task over time. Deep
Learning is a subset of machine learning that makes use of deep
artificial neural networks for training. The paper proposed on
outlier detection for multivariate high dimensional data for
Autoencoder unsupervised model.",2021.0,"C. Aggarwal, Philip S. Yu"
38f23fe236b152cd4983c8f30d305a568afd0d3e,https://www.semanticscholar.org/paper/38f23fe236b152cd4983c8f30d305a568afd0d3e,A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.",2019.0,"Erico Tjoa, Cuntai Guan"
864e7db59f2ccfec1ee9f6eba79566ac7b0634df,https://www.semanticscholar.org/paper/864e7db59f2ccfec1ee9f6eba79566ac7b0634df,Convolutional Pose Machines,"Pose Machines provide a sequential prediction framework for learning rich implicit spatial models. In this work we show a systematic design for how convolutional networks can be incorporated into the pose machine framework for learning image features and image-dependent spatial models for the task of pose estimation. The contribution of this paper is to implicitly model long-range dependencies between variables in structured prediction tasks such as articulated pose estimation. We achieve this by designing a sequential architecture composed of convolutional networks that directly operate on belief maps from previous stages, producing increasingly refined estimates for part locations, without the need for explicit graphical model-style inference. Our approach addresses the characteristic difficulty of vanishing gradients during training by providing a natural learning objective function that enforces intermediate supervision, thereby replenishing back-propagated gradients and conditioning the learning procedure. We demonstrate state-of-the-art performance and outperform competing methods on standard benchmarks including the MPII, LSP, and FLIC datasets.",2016.0,"S. Wei, V. Ramakrishna, T. Kanade, Yaser Sheikh"
e75b3c12da067552fda910a5bbed8b4d0e82dbcb,https://www.semanticscholar.org/paper/e75b3c12da067552fda910a5bbed8b4d0e82dbcb, Neural Network Methods for Natural Language Processing,"Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries.

The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning.",2017.0,Yoav Goldberg
5966d7c7f60898d610812e24c64d4d57855ad86a,https://www.semanticscholar.org/paper/5966d7c7f60898d610812e24c64d4d57855ad86a,Semantics derived automatically from language corpora contain human-like biases,"Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",2016.0,"Aylin Caliskan, J. Bryson, Arvind Narayanan"
80196cdfcd0c6ce2953bf65a7f019971e2026386,https://www.semanticscholar.org/paper/80196cdfcd0c6ce2953bf65a7f019971e2026386,IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures,"In this work we aim to solve a large collection of tasks using a single reinforcement learning agent with a single set of parameters. A key challenge is to handle the increased amount of data and extended training time. We have developed a new distributed agent IMPALA (Importance Weighted Actor-Learner Architecture) that not only uses resources more efficiently in single-machine training but also scales to thousands of machines without sacrificing data efficiency or resource utilisation. We achieve stable learning at high throughput by combining decoupled acting and learning with a novel off-policy correction method called V-trace. We demonstrate the effectiveness of IMPALA for multi-task reinforcement learning on DMLab-30 (a set of 30 tasks from the DeepMind Lab environment (Beattie et al., 2016)) and Atari-57 (all available Atari games in Arcade Learning Environment (Bellemare et al., 2013a)). Our results show that IMPALA is able to achieve better performance than previous agents with less data, and crucially exhibits positive transfer between tasks as a result of its multi-task approach.",2018.0,"L. Espeholt, Hubert Soyer, R. Munos, K. Simonyan, Volodymyr Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, S. Legg, K. Kavukcuoglu"
990a02f20529f5ce3b382f1d54648afaab391179,https://www.semanticscholar.org/paper/990a02f20529f5ce3b382f1d54648afaab391179,Poisoning Attacks against Support Vector Machines,"We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. 
 
The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.",2012.0,"B. Biggio, B. Nelson, P. Laskov"
74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8,https://www.semanticscholar.org/paper/74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8,Transductive Inference for Text Classification using Support Vector Machines,"This paper introduces Transductive Support Vector Machines (TSVMs) for text classi(cid:12)-cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transduc-tive Support Vector Machines take into account a particular test set and try to minimize misclassi(cid:12)cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi(cid:12)cation. These theoretical (cid:12)ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, espe-ciallyfor smalltraining sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e(cid:14)-ciently, handling 10,000 examples and more.",1999.0,T. Joachims
0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff,https://www.semanticscholar.org/paper/0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff,Solving large scale linear prediction problems using stochastic gradient descent algorithms,"Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.",2004.0,Tong Zhang
7547fd7c5e4bc3b8b8bf714583684ff187e8a382,https://www.semanticscholar.org/paper/7547fd7c5e4bc3b8b8bf714583684ff187e8a382,An assessment of support vector machines for land cover classi(cid:142) cation,". The support vector machine (SVM) is a group of theoreticallysuperior machine learning algorithms. It was found competitive with the best available machine learning algorithms in classifying high-dimensionaldata sets. This paper gives an introduction to the theoretical development of the SVM and an experimental evaluation of its accuracy, stability and training speed in deriving land cover classi(cid:142) cations from satellite images. The SVM was compared to three other popular classi(cid:142) ers, including the maximum likelihood classi(cid:142) er (MLC), neural network classi(cid:142) ers (NNC) and decision tree classi(cid:142) ers (DTC). The impacts of kernel con(cid:142)guration on the performance of the SVM and of the selection of training data and input variables on the four classi(cid:142) ers were also evaluated in this experiment.",2002.0,"Chengquan Huang, L. Davis, J. Townshend"
b8012351bc5ebce4a4b3039bbbba3ce393bc3315,https://www.semanticscholar.org/paper/b8012351bc5ebce4a4b3039bbbba3ce393bc3315,An empirical evaluation of deep architectures on problems with many factors of variation,"Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks.",2007.0,"H. Larochelle, D. Erhan, Aaron C. Courville, J. Bergstra, Yoshua Bengio"
9691f67f5075bde2fd70da0135a4a70f25ef042b,https://www.semanticscholar.org/paper/9691f67f5075bde2fd70da0135a4a70f25ef042b,Pegasos: primal estimated sub-gradient solver for SVM,"We describe and analyze a simple and effective stochastic sub-gradient descent algorithm for solving the optimization problem cast by Support Vector Machines (SVM). We prove that the number of iterations required to obtain a solution of accuracy $${\epsilon}$$ is $${\tilde{O}(1 / \epsilon)}$$, where each iteration operates on a single training example. In contrast, previous analyses of stochastic gradient descent methods for SVMs require $${\Omega(1 / \epsilon^2)}$$ iterations. As in previously devised SVM solvers, the number of iterations also scales linearly with 1/λ, where λ is the regularization parameter of SVM. For a linear kernel, the total run-time of our method is $${\tilde{O}(d/(\lambda \epsilon))}$$, where d is a bound on the number of non-zero features in each example. Since the run-time does not depend directly on the size of the training set, the resulting algorithm is especially suited for learning from large datasets. Our approach also extends to non-linear kernels while working solely on the primal objective function, though in this case the runtime does depend linearly on the training set size. Our algorithm is particularly well suited for large text classification problems, where we demonstrate an order-of-magnitude speedup over previous SVM learning methods.",2007.0,"Shai Shalev-Shwartz, Y. Singer, N. Srebro, Andrew Cotter"
10b496ad48513f8585aa56f2c682159357858960,https://www.semanticscholar.org/paper/10b496ad48513f8585aa56f2c682159357858960,Understanding Data Augmentation for Classification: When to Warp?,"In this paper we investigate the benefit of augmenting data with synthetically created samples when training a machine learning classifier. Two approaches for creating additional training samples are data warping, which generates additional samples through transformations applied in the data-space, and synthetic over-sampling, which creates additional samples in feature-space. We experimentally evaluate the benefits of data augmentation for a convolutional backpropagation-trained neural network, a convolutional support vector machine and a convolutional extreme learning machine classifier, using the standard MNIST handwritten digit dataset. We found that while it is possible to perform generic augmentation in feature-space, if plausible transforms for the data are known then augmentation in data-space provides a greater benefit for improving performance and reducing overfitting.",2016.0,"S. Wong, Adam Gatt, V. Stamatescu, M. McDonnell"
1626c940a64ad96a7ed53d7d6c0df63c6696956b,https://www.semanticscholar.org/paper/1626c940a64ad96a7ed53d7d6c0df63c6696956b,Restricted Boltzmann machines for collaborative filtering,"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",2007.0,"R. Salakhutdinov, A. Mnih, Geoffrey E. Hinton"
ec76f55da5c6df30f6e4c9e4945bd3304d508ef7,https://www.semanticscholar.org/paper/ec76f55da5c6df30f6e4c9e4945bd3304d508ef7,Fuzzy support vector machines,"A support vector machine (SVM) learns the decision surface from two distinct classes of the input points. In many applications, each input point may not be fully assigned to one of these two classes. In this paper, we apply a fuzzy membership to each input point and reformulate the SVMs such that different input points can make different contributions to the learning of decision surface. We call the proposed method fuzzy SVMs (FSVMs).",2002.0,"Chun-fu Lin, Sheng-de Wang"
0ea2f1f5c470c4947b48bbd21245fb327282f3b4,https://www.semanticscholar.org/paper/0ea2f1f5c470c4947b48bbd21245fb327282f3b4,Stock market's price movement prediction with LSTM neural networks,"Predictions on stock market prices are a great challenge due to the fact that it is an immensely complex, chaotic and dynamic environment. There are many studies from various areas aiming to take on that challenge and Machine Learning approaches have been the focus of many of them. There are many examples of Machine Learning algorithms been able to reach satisfactory results when doing that type of prediction. This article studies the usage of LSTM networks on that scenario, to predict future trends of stock prices based on the price history, alongside with technical analysis indicators. For that goal, a prediction model was built, and a series of experiments were executed and theirs results analyzed against a number of metrics to assess if this type of algorithm presents and improvements when compared to other Machine Learning methods and investment strategies. The results that were obtained are promising, getting up to an average of 55.9% of accuracy when predicting if the price of a particular stock is going to go up or not in the near future.",2017.0,"David M. Q. Nelson, A. Pereira, Renato A. de Oliveira"
430c95aab5bc85404f9651eb2137a12e2c4d5fe7,https://www.semanticscholar.org/paper/430c95aab5bc85404f9651eb2137a12e2c4d5fe7,Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers,"We present a unifying framework for studying the solution of multiclass categorization problems by reducing them to multiple binary problems that are then solved using a margin-based binary learning algorithm. The proposed framework uniﬁes some of the most popular approaches in which each class is compared against all others, or in which all pairs of classes are compared to each other, or in which output codes with error-correcting properties are used. We propose a general method for combining the classiﬁers generated on the binary problems, and we prove a general empirical multiclass loss bound given the empirical loss of the individual binary learning algorithms. The scheme and the corresponding bounds apply to many popular classiﬁcation learning algorithms including support-vector machines, AdaBoost, regression, logistic regression and decision-tree algorithms. We also give a multiclass generalization error analysis for general output codes with AdaBoost as the binary learner. Experimental results with SVM and AdaBoost show that our scheme provides a viable alternative to the most commonly used multiclass algorithms.",2000.0,"Erin L. Allwein, Rob Schapire, Y. Singer"
b3b3c562a45d7710d6f62ad8f210ebca9a47d23f,https://www.semanticscholar.org/paper/b3b3c562a45d7710d6f62ad8f210ebca9a47d23f,Who should fix this bug?,"Open source development projects typically support an open bug repository to which both developers and users can report bugs. The reports that appear in this repository must be triaged to determine if the report is one which requires attention and if it is, which developer will be assigned the responsibility of resolving the report. Large open source developments are burdened by the rate at which new bug reports appear in the bug repository. In this paper, we present a semi-automated approach intended to ease one part of this process, the assignment of reports to a developer. Our approach applies a machine learning algorithm to the open bug repository to learn the kinds of reports each developer resolves. When a new report arrives, the classifier produced by the machine learning technique suggests a small number of developers suitable to resolve the report. With this approach, we have reached precision levels of 57% and 64% on the Eclipse and Firefox development projects respectively. We have also applied our approach to the gcc open source development with less positive results. We describe the conditions under which the approach is applicable and also report on the lessons we learned about applying machine learning to repositories used in open source development.",2006.0,"J. Anvik, L. Hiew, G. Murphy"
0c91d5305ad34814b631d4a642bb0535a2e066ea,https://www.semanticscholar.org/paper/0c91d5305ad34814b631d4a642bb0535a2e066ea,Feature selection based on mutual information,"The application of machine learning models such as support vector machine (SVM) and artificial neural networks (ANN) in predicting reservoir properties has been effective in the recent years when compared with the traditional empirical methods. Despite that the machine learning models suffer a lot in the faces of uncertain data which is common characteristics of well log dataset. The reason for uncertainty in well log dataset includes a missing scale, data interpretation and measurement error problems. Feature Selection aimed at selecting feature subset that is relevant to the predicting property. In this paper a feature selection based on mutual information criterion is proposed, the strong point of this method relies on the choice of threshold based on statistically sound criterion for the typical greedy feedforward method of feature selection. Experimental results indicate that the proposed method is capable of improving the performance of the machine learning models in terms of prediction accuracy and reduction in training time.",2015.0,"Muhammad Aliyu Sulaiman, J. Labadin"
29650544fded20dd5b2fc49f60f9a3ad30d0e275,https://www.semanticscholar.org/paper/29650544fded20dd5b2fc49f60f9a3ad30d0e275,Speech Recognition Using Deep Neural Networks: A Systematic Review,"Over the past decades, a tremendous amount of research has been done on the use of machine learning for speech processing applications, especially speech recognition. However, in the past few years, research has focused on utilizing deep learning for speech-related applications. This new area of machine learning has yielded far better results when compared to others in a variety of applications including speech, and thus became a very attractive area of research. This paper provides a thorough examination of the different studies that have been conducted since 2006, when deep learning first arose as a new area of machine learning, for speech applications. A thorough statistical analysis is provided in this review which was conducted by extracting specific information from 174 papers published between the years 2006 and 2018. The results provided in this paper shed light on the trends of research in this area as well as bring focus to new research topics.",2019.0,"Ali Bou Nassif, I. Shahin, Imtinan B. Attili, Mohammad Azzeh, Khaled Shaalan"
9257779eed46107bcdce9f4dc86298572ff466ce,https://www.semanticscholar.org/paper/9257779eed46107bcdce9f4dc86298572ff466ce,Automated learning of decision rules for text categorization,"We describe the results of extensive experiments using optimized rule-based induction methods on large document collections. The goal of these methods is to discover automatically classification patterns that can be used for general document categorization or personalized filtering of free text. Previous reports indicate that human-engineered rule-based systems, requiring many man-years of developmental efforts, have been successfully built to “read” documents and assign topics to them. We show that machine-generated decision rules appear comparable to human performance, while using the identical rule-based representation. In comparison with other machine-learning techniques, results on a key benchmark from the Reuters collection show a large gain in performance, from a previously reported 67% recall/precision breakeven point to 80.5%. In the context of a very high-dimensional feature space, several methodological alternatives are examined, including universal versus local dictionaries, and binary versus frequency-related features.",1994.0,"C. Apté, Fred J. Damerau, S. Weiss"
49b8dff62cccc26023c876460234bf29084a382f,https://www.semanticscholar.org/paper/49b8dff62cccc26023c876460234bf29084a382f,Transductive Learning via Spectral Graph Partitioning,"We present a new method for transductive learning, which can be seen as a transductive version of the k nearest-neighbor classifier. Unlike for many other transductive learning methods, the training problem has a meaningful relaxation that can be solved globally optimally using spectral methods. We propose an algorithm that robustly achieves good generalization performance and that can be trained efficiently. A key advantage of the algorithm is that it does not require additional heuristics to avoid unbalanced splits. Furthermore, we show a connection to transductive Support Vector Machines, and that an effective Co-Training algorithm arises as a special case.",2003.0,T. Joachims
b8e37682c847844a4b5c4851239fdc3357d5577b,https://www.semanticscholar.org/paper/b8e37682c847844a4b5c4851239fdc3357d5577b,Lecture Notes in Artificial Intelligence,"LNAI was established in the mid-1980s as a topical subseries of LNCS focusing on artificial intelligence. This subseries is devoted to the publication of state-of-the-art research results in artificial intelligence, at a high level and in both printed and electronic versions making use of the well-established LNCS publication machinery. As with the LNCS mother series, proceedings and postproceedings are at the core of LNAI; however, all other sublines are available for LNAI as well. The topics in LNAI include automated reasoning, automated programming, algorithms, knowledge representation, agent-based systems, intelligent systems, expert systems, machine learning, natural-language processing, machine vision, robotics, search systems, knowledge discovery, data mining, and related programming languages.",1999.0,"P. Brézillon, Paolo Bouquet"
77703a2783f64dfceb638aa9eebd9c9c501bb835,https://www.semanticscholar.org/paper/77703a2783f64dfceb638aa9eebd9c9c501bb835,The Case against Accuracy Estimation for Comparing Induction Algorithms,"We analyze critically the use of classi cation accuracy to compare classi ers on natural data sets, providing a thorough investigation using ROC analysis, standard machine learning algorithms, and standard benchmark data sets. The results raise serious concerns about the use of accuracy for comparing classi ers and draw into question the conclusions that can be drawn from such studies. In the course of the presentation, we describe and demonstrate what we believe to be the proper use of ROC analysis for comparative studies in machine learning research. We argue that this methodology is preferable both for making practical choices and for drawing scienti c conclusions.",1998.0,"F. Provost, Tom Fawcett, Ron Kohavi"
b97f9e43ec7b73ec1cb2eaf5a501ec51abd6ca92,https://www.semanticscholar.org/paper/b97f9e43ec7b73ec1cb2eaf5a501ec51abd6ca92,Linear Discriminant Analysis,"Linear discriminant analysis (LDA) and the related Fisher's linear discriminant are methods used in statistics, pattern recognition and machine learning to find a linear combination of features which characterize or separate two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.",2020.0,Hong Zhou
f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10,https://www.semanticscholar.org/paper/f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10,Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.,"Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.",2019.0,"Xin Yang, Yifei Wang, Ryan Byrne, G. Schneider, Sheng-yong Yang"
3335c340c20609b4e6de481c9eaf67ecd6c960dc,https://www.semanticscholar.org/paper/3335c340c20609b4e6de481c9eaf67ecd6c960dc,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",2016.0,"Randal S. Olson, Nathan Bartley, R. Urbanowicz, J. Moore"
cc1cad12521b5aab43fdda5b4dec67586aef1f87,https://www.semanticscholar.org/paper/cc1cad12521b5aab43fdda5b4dec67586aef1f87,Kernel Methods for Relation Extraction,"We present an application of kernel methods to extracting relations from unstructured natural language sources. We introduce kernels defined over shallow parse representations of text, and design efficient algorithms for computing the kernels. We use the devised kernels in conjunction with Support Vector Machine and Voted Perceptron learning algorithms for the task of extracting person-affiliation and organization-location relations from text. We experimentally evaluate the proposed methods and compare them with feature-based learning algorithms, with promising results.",2002.0,"D. Zelenko, Chinatsu Aone, A. Richardella"
ac12c9b9e35e58b55d85a97c47886a7371c14afa,https://www.semanticscholar.org/paper/ac12c9b9e35e58b55d85a97c47886a7371c14afa,Data mining in bioinformatics using Weka,"UNLABELLED
The Weka machine learning workbench provides a general-purpose environment for automatic classification, regression, clustering and feature selection-common data mining problems in bioinformatics research. It contains an extensive collection of machine learning algorithms and data pre-processing methods complemented by graphical user interfaces for data exploration and the experimental comparison of different machine learning techniques on the same problem. Weka can process data given in the form of a single relational table. Its main objectives are to (a) assist users in extracting useful information from data and (b) enable them to easily identify a suitable algorithm for generating an accurate predictive model from it.


AVAILABILITY
http://www.cs.waikato.ac.nz/ml/weka.",2004.0,"E. Frank, M. Hall, Leonard E. Trigg, G. Holmes, I. Witten"
1dbc1238409549ae6872a744b7b2ff1da5822053,https://www.semanticscholar.org/paper/1dbc1238409549ae6872a744b7b2ff1da5822053,A reliable effective terascale linear learning system,"We present a system and a set of techniques for learning linear predictors with convex losses on terascale data sets, with trillions of features, billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature. We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.",2011.0,"Alekh Agarwal, O. Chapelle, Miroslav Dudík, J. Langford"
55e36d6b45c91a0daa49234bd47b856470d6825c,https://www.semanticscholar.org/paper/55e36d6b45c91a0daa49234bd47b856470d6825c,Identifying Sarcasm in Twitter: A Closer Look,"Sarcasm transforms the polarity of an apparently positive or negative utterance into its opposite. We report on a method for constructing a corpus of sarcastic Twitter messages in which determination of the sarcasm of each message has been made by its author. We use this reliable corpus to compare sarcastic utterances in Twitter to utterances that express positive or negative attitudes without sarcasm. We investigate the impact of lexical and pragmatic factors on machine learning effectiveness for identifying sarcastic utterances and we compare the performance of machine learning techniques and human judges on this task. Perhaps unsurprisingly, neither the human judges nor the machine learning techniques perform very well.",2011.0,"Roberto I. González-Ibáñez, S. Muresan, Nina Wacholder"
b69df93991a1f5a712b20e832f5b0281acb3153b,https://www.semanticscholar.org/paper/b69df93991a1f5a712b20e832f5b0281acb3153b,Kernel Methods in Computational Biology,"Modern machine learning techniques are proving to be extremely valuable for the analysis of data in computational biology problems. One branch of machine learning, kernel methods, lends itself particularly well to the difficult aspects of biological data, which include high dimensionality (as in microarray measurements), representation as discrete and structured data (as in DNA or amino acid sequences), and the need to combine heterogeneous sources of information. This book provides a detailed overview of current research in kernel methods and their applications to computational biology.Following three introductory chapters -- an introduction to molecular and computational biology, a short review of kernel methods that focuses on intuitive concepts rather than technical details, and a detailed survey of recent applications of kernel methods in computational biology -- the book is divided into three sections that reflect three general trends in current research. The first part presents different ideas for the design of kernel functions specifically adapted to various biological data; the second part covers different approaches to learning from heterogeneous data; and the third part offers examples of successful applications of support vector machine methods.",2005.0,"B. Scholkopf, K. Tsuda, Jean-Philippe Vert"
8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6,https://www.semanticscholar.org/paper/8ce2c4a374e8b37e3eef080c956f22cfc6ea25d6,Time for a change: a tutorial for comparing multiple classifiers through Bayesian analysis,"The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better - more sound and useful - alternatives for it.",2016.0,"A. Benavoli, Giorgio Corani, J. Demšar, Marco Zaffalon"
a2403c1ce02120f7bd383e395b561ff7c64d52ec,https://www.semanticscholar.org/paper/a2403c1ce02120f7bd383e395b561ff7c64d52ec,A System for Massively Parallel Hyperparameter Tuning,"Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.",2018.0,"Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, B. Recht, Ameet Talwalkar"
797928abfe0d189383325fe6322ced2226bcd457,https://www.semanticscholar.org/paper/797928abfe0d189383325fe6322ced2226bcd457,Use of the Zero-Norm with Linear Models and Kernel Methods,"We explore the use of the so-called zero-norm of the parameters of linear models in learning. Minimization of such a quantity has many uses in a machine learning context: for variable or feature selection, minimizing training error and ensuring sparsity in solutions. We derive a simple but practical method for achieving these goals and discuss its relationship to existing techniques of minimizing the zero-norm. The method boils down to implementing a simple modification of vanilla SVM, namely via an iterative multiplicative rescaling of the training data. Applications we investigate which aid our discussion include variable and feature selection on biological microarray data, and multicategory classification.",2003.0,"J. Weston, A. Elisseeff, B. Scholkopf, Michael E. Tipping"
e044a4b5be1563fccc46e8f7552935b99365f90a,https://www.semanticscholar.org/paper/e044a4b5be1563fccc46e8f7552935b99365f90a,Learning with Support Vector Machines,"Support Vectors Machines have become a well established tool within machine learning. They work well in practice and have now been used across a wide range of applications from recognizing hand-written digits, to face identification, text categorisation, bioinformatics, and database marketing. In this book we give an introductory overview of this subject. We start with a simple Support Vector Machine for performing binary classification before considering multi-class classification and learning in the presence of noise. We show that this framework can be extended to many other scenarios such as prediction with real-valued outputs, novelty detection and the handling of complex output structures such as parse trees. Finally, we give an overview of the main types of kernels which are used in practice and how to learn and make predictions from multiple types of input data. Table of Contents: Support Vector Machines for Classification / Kernel-based Models / Learning with Kernels",2011.0,"C. Campbell, Yiming Ying"
f620ec7bc0632be5518718cb81e2bfb57c81e950,https://www.semanticscholar.org/paper/f620ec7bc0632be5518718cb81e2bfb57c81e950,Hubs in Space: Popular Nearest Neighbors in High-Dimensional Data,"Different aspects of the curse of dimensionality are known to present serious challenges to various machine-learning methods and tasks. This paper explores a new aspect of the dimensionality curse, referred to as hubness, that affects the distribution of k-occurrences: the number of times a point appears among the k nearest neighbors of other points in a data set. Through theoretical and empirical analysis involving synthetic and real data sets we show that under commonly used assumptions this distribution becomes considerably skewed as dimensionality increases, causing the emergence of hubs, that is, points with very high k-occurrences which effectively represent ""popular"" nearest neighbors. We examine the origins of this phenomenon, showing that it is an inherent property of data distributions in high-dimensional vector space, discuss its interaction with dimensionality reduction, and explore its influence on a wide range of machine-learning tasks directly or indirectly based on measuring distances, belonging to supervised, semi-supervised, and unsupervised learning families.",2010.0,"Miloš Radovanović, A. Nanopoulos, M. Ivanović"
a53da9916b87fa295837617c16ef2ca6462cafb8,https://www.semanticscholar.org/paper/a53da9916b87fa295837617c16ef2ca6462cafb8,Classification using discriminative restricted Boltzmann machines,"Recently, many applications for Restricted Boltzmann Machines (RBMs) have been developed for a large variety of learning problems. However, RBMs are usually used as feature extractors for another learning algorithm or to provide a good initialization for deep feed-forward neural network classifiers, and are not considered as a standalone solution to classification problems. In this paper, we argue that RBMs provide a self-contained framework for deriving competitive non-linear classifiers. We present an evaluation of different learning algorithms for RBMs which aim at introducing a discriminative component to RBM training and improve their performance as classifiers. This approach is simple in that RBMs are used directly to build a classifier, rather than as a stepping stone. Finally, we demonstrate how discriminative RBMs can also be successfully employed in a semi-supervised setting.",2008.0,"H. Larochelle, Yoshua Bengio"
8198e70878c907e1bd05e7a3fa4280d8c338df60,https://www.semanticscholar.org/paper/8198e70878c907e1bd05e7a3fa4280d8c338df60,Semi-Supervised Support Vector Machines,"We introduce a semi-supervised support vector machine (S3VM) method. Given a training set of labeled data and a working set of unlabeled data, S3VM constructs a support vector machine using both the training and working sets. We use S3VM to solve the transduction problem using overall risk minimization (ORM) posed by Vapnik. The transduction problem is to estimate the value of a classification function at the given points in the working set. This contrasts with the standard inductive learning problem of estimating the classification function at all possible values and then using the fixed function to deduce the classes of the working set data. We propose a general S3VM model that minimizes both the misclassification error and the function capacity based on all the available data. We show how the S3VM model for 1-norm linear support vector machines can be converted to a mixed-integer program and then solved exactly using integer programming. Results of S3VM and the standard 1-norm support vector machine approach are compared on ten data sets. Our computational results support the statistical learning theory results showing that incorporating working data improves generalization when insufficient training information is available. In every case, S3VM either improved or showed no significant difference in generalization compared to the traditional approach.",1998.0,"Kristin P. Bennett, A. Demiriz"
f84f2cc648a338a6b1317d039c018dbfb8989b9b,https://www.semanticscholar.org/paper/f84f2cc648a338a6b1317d039c018dbfb8989b9b,sbi: A toolkit for simulation-based inference,"e Equally contributing authors 1 Computational Neuroengineering, Department of Electrical and Computer Engineering, Technical University of Munich 2 School of Informatics, University of Edinburgh 3 Neural Systems Analysis, Center of Advanced European Studies and Research (caesar), Bonn 4 Model-Driven Machine Learning, Centre for Materials and Coastal Research, Helmholtz-Zentrum Geesthacht 5 Machine Learning in Science, University of Tübingen 6 Empirical Inference, Max Planck Institute for Intelligent Systems, Tübingen DOI: 10.21105/joss.02505",2020.0,"Álvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis Lueckmann, Conor Durkan, Pedro J. Gonccalves, David S. Greenberg, Jakob H. Macke Computational Neuroengineering, D. Electrical, Computer Engineering, T. U. Munich, School of Informatics, U. Edinburgh, Neural Systems Analysis, Center of Advanced European Studies, Research, Bonn, Model-Driven Machine Learning, Centre for Materials, Coastal Research, Helmholtz-Zentrum Geesthacht, Machine Learning in Science, U. Tubingen, Empirical Inference, Max Planck Institute for the Physics of Complex Systems, Tubingen"
2323173a0bddac0dd2586b17a2f3ac33f401c45c,https://www.semanticscholar.org/paper/2323173a0bddac0dd2586b17a2f3ac33f401c45c,Nearest-Neighbor Methods in Learning and Vision: Theory and Practice (Neural Information Processing),"Regression and classification methods based on similarity of the input to stored examples have not been widely used in applications involving very large sets of high-dimensional data. Recent advances in computational geometry and machine learning, however, may alleviate the problems in using these methods on large data sets. This volume presents theoretical and practical discussions of nearest-neighbor (NN) methods in machine learning and examines computer vision as an application domain in which the benefit of these advanced methods is often dramatic. It brings together contributions from researchers in theory of computation, machine learning, and computer vision with the goals of bridging the gaps between disciplines and presenting state-of-the-art methods for emerging applications.The contributors focus on the importance of designing algorithms for NN search, and for the related classification, regression, and retrieval tasks, that remain efficient even as the number of points or the dimensionality of the data grows very large. The book begins with two theoretical chapters on computational geometry and then explores ways to make the NN approach practicable in machine learning applications where the dimensionality of the data and the size of the data sets make the naive methods for NN search prohibitively expensive. The final chapters describe successful applications of an NN algorithm, locality-sensitive hashing (LSH), to vision tasks.",2006.0,"Gregory Shakhnarovich, Trevor Darrell, P. Indyk"
ec7c68427a26f812532b1c913c68fcf84b7de58e,https://www.semanticscholar.org/paper/ec7c68427a26f812532b1c913c68fcf84b7de58e,Beyond the point cloud: from transductive to semi-supervised learning,"Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.",2005.0,"Vikas Sindhwani, P. Niyogi, M. Belkin"
509c27bd45b5d9444949331ed8f5a3681c2ece68,https://www.semanticscholar.org/paper/509c27bd45b5d9444949331ed8f5a3681c2ece68,Incremental learning with support vector machines,"Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.",2001.0,S. Rüping
ea4feb953b86f6a099d61ffa70d21c59be99f76a,https://www.semanticscholar.org/paper/ea4feb953b86f6a099d61ffa70d21c59be99f76a,Enhancing Supervised Learning with Unlabeled Data,"In a wide variety of supervised learning scenarios, there is a small set of labeled data, along with a large pool of unlabeled data. In this thesis, we present a new semi-supervised learning method called co-learning that is designed to use unlabeled data to enhance standard supervised learning algorithms. The idea is that two or more standard supervised learning algorithms can leverage off the fact that they have different representations of the hypotheses and they are likely to detect different patterns in labeled data. We also design an active co-learning strategy to bootstrap our co-leaning procedure when the originally labeled data set is too small to provide accurate confidence estimate for the learned hypotheses. We provide a priority sampling technique as the selection component in our active co-learning method. We evaluate our co-learning algorithms on several datasets from a commonly used data repository in the machine learning community. We also test our co-learning method on text categorization. The contribution of this research is to put forward a new semi-supervised learning approach for learning with a small number of labeled examples, and explore the applicability of our co-learning strategy in real world applications.",2000.0,"S. Goldman, Yan Zhou"
0ca966cb390b442b10cb76aa3fddee6b613f4f0f,https://www.semanticscholar.org/paper/0ca966cb390b442b10cb76aa3fddee6b613f4f0f,Incorporating Diversity in Active Learning with Support Vector Machines,"In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h > 1 examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.",2003.0,K. Brinker
23d2d8b687d31b11573473a7c7792b7ec08d0745,https://www.semanticscholar.org/paper/23d2d8b687d31b11573473a7c7792b7ec08d0745,Learning Kernel Classifiers: Theory and Algorithms,"From the Publisher: 
Linear classifiers in kernel spaces have emerged as a major topic within the field of machine learning. The kernel technique takes the linear classifier--a limited, but well-established and comprehensively studied model--and extends its applicability to a wide range of nonlinear pattern-recognition tasks such as natural language processing, machine vision, and biological sequence analysis. This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments. It begins by describing the major algorithmic advances: kernel perceptron learning, kernel Fisher discriminants, support vector machines, relevance vector machines, Gaussian processes, and Bayes point machines. Then follows a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds. Throughout, the book emphasizes the interaction between theory and algorithms: how learning algorithms work and why. The book includes many examples, complete pseudo code of the algorithms presented, and an extensive source code library.",2001.0,R. Herbrich
356125478f5d06b564b420755a4944254045bbbe,https://www.semanticscholar.org/paper/356125478f5d06b564b420755a4944254045bbbe,Support vector learning,"Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology.",1997.0,B. Scholkopf
08745f22d0abbe66e486f0985c985ecf1eab4e9e,https://www.semanticscholar.org/paper/08745f22d0abbe66e486f0985c985ecf1eab4e9e,Question classification using support vector machines,"Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors (NN), Naive Bayes (NB), Decision Tree (DT), Sparse Network of Winnows (SNoW), and Support Vector Machines (SVM) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the SVM outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the SVM to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the TREC QA track.",2003.0,"Dell Zhang, Wee Sun Lee"
7b7f767edd532f1f156a31c2efc550e7a0b0279e,https://www.semanticscholar.org/paper/7b7f767edd532f1f156a31c2efc550e7a0b0279e,"Incremental Support Vector Learning: Analysis, Implementation and Applications","Incremental Support Vector Machines (SVM) are instrumental in practical applications of online learning. This work focuses on the design and analysis of efficient incremental SVM learning, with the aim of providing a fast, numerically stable and robust implementation. A detailed analysis of convergence and of algorithmic complexity of incremental SVM learning is carried out. Based on this analysis, a new design of storage and numerical operations is proposed, which speeds up the training of an incremental SVM by a factor of 5 to 20. The performance of the new algorithm is demonstrated in two scenarios: learning with limited resources and active learning. Various applications of the algorithm, such as in drug discovery, online monitoring of industrial devices and and surveillance of network traffic, can be foreseen.",2006.0,"P. Laskov, Christian Gehl, Stefan Krüger, K. Müller"
1889b9c3e8bc1118448b95fca38d6eff0bfca64d,https://www.semanticscholar.org/paper/1889b9c3e8bc1118448b95fca38d6eff0bfca64d,Learning the Kernel with Hyperkernels,"This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional.We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach.",2005.0,"Cheng Soon Ong, Alex Smola, R. C. Williamson"
008abebf4a9404db9050c9d2fbca769f4faf3ca6,https://www.semanticscholar.org/paper/008abebf4a9404db9050c9d2fbca769f4faf3ca6,Learning by Transduction,"We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.",1998.0,"A. Gammerman, V. Vovk, V. Vapnik"
5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398,https://www.semanticscholar.org/paper/5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398,Hidden Markov Support Vector Machines,"This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach.",2003.0,"Y. Altun, Ioannis Tsochantaridis, Thomas Hofmann"
d18f64aa830075ed3e10206907f32c8fb2aa189d,https://www.semanticscholar.org/paper/d18f64aa830075ed3e10206907f32c8fb2aa189d,INTRODUCTION TO STATISTICAL LEARNING THEORY AND SUPPORT VECTOR MACHINES,"Data based machine learning covers a wide range of topics from pattern recognition to function regression and density estimation. Most of the existing methods are based on traditional statistics, which provides conclusion only for the situation where sample size is tending to infinity. So they may not work in practical cases of limited samples. Statistical Learning Theory or SLT is a small sample statistics by Vapnik et al., which concerns mainly the statistic principles when samples are limited, especially the properties of learning procedure in such cases. SLT provides us a new framework for the general learning problem, and a novel powerful learning method called Support Vector Machine or SVM, which can solve small sample learning problems better. It is believed that the study of SLT and SVM is becoming a new hot area in the field of machine learning. This review introduces the basic ideas of SLT and SVM, their major characteristics and some current research trends.",2000.0,Xuegong Zhang
e37226a2f099c9a1ad13edce395ccca29225193c,https://www.semanticscholar.org/paper/e37226a2f099c9a1ad13edce395ccca29225193c,Support Vector Machines Under Adversarial Label Noise,"In adversarial classication tasks like spam ltering and intrusion detection, malicious adversaries may manipulate data to thwart the outcome of an automatic analysis. Thus, besides achieving good classication performances, machine learning algorithms have to be robust against adversarial data manipulation to successfully operate in these tasks. While support vector machines (SVMs) have shown to be a very successful approach in classication problems, their eectiveness in adversarial classication tasks has not been extensively investigated yet. In this paper we present a preliminary investigation of the robustness of SVMs against adversarial data manipulation. In particular, we assume that the adversary has control over some training data, and aims to subvert the SVM learning process. Within this assumption, we show that this is indeed possible, and propose a strategy to improve the robustness of SVMs to training data manipulation based on a simple kernel matrix correction.",2011.0,"B. Biggio, B. Nelson, P. Laskov"
6b0966c51d66e3097fc9f9d704bc43fdd963e90e,https://www.semanticscholar.org/paper/6b0966c51d66e3097fc9f9d704bc43fdd963e90e,Learning in Humans and Machines: Towards an Interdisciplinary Learning Science,"Chapter headings: Towards an Interdisciplinary Learning Science (P. Reimann, H. Spada). A Cognitive Psychological Approach to Learning (S. Vosniadou). Learning to Do and Learning to Understand: A Lesson and a Challenge for Cognitive Modeling (S. Ohlsson). Machine Learning: Case Studies of an Interdisciplinary Approach (W. Emde). Mental and Physical Artifacts in Cognitive Practices (R. Saljo). Learning Theory and Instructional Science (E. De Corte). Knowledge Representation Changes in Humans and Machines (L. Saitta and Task Force 1). Multi-Objective Learning with Multiple Representations (M. Van Someren, P. Reimann). Order Effects in Incremental Learning (P. Langley). Situated Learning and Transfer (H. Gruber et al.). The Evolution of Research on Collaborative Learning (P. Dillenbourg et al.). A Developmental Case Study on Sequential Learning: The Day-Night Cycle (K. Morik, S. Vosniadou). Subject index. Author index.",1995.0,"H. Spada, Reimann, P. Reimann"
9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb,https://www.semanticscholar.org/paper/9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb,WEKA: The Waikato Environment for Knowledge Analysis,"WEKA is a workbench designed to aid in the application of machine learning technology to real world data sets, in particular, data sets from New Zealand’s agricultural sector. In order to do this a range of machine learning techniques are presented to the user in such a way as to hide the idiosyncrasies of input and output formats, as well as allow an exploratory approach in applying the technology. The system presented is a component based one that also has application in machine learning research and education.",1996.0,Stephen R. Garner
480ddeb902c15861ef8f294f00c543dae508ee9b,https://www.semanticscholar.org/paper/480ddeb902c15861ef8f294f00c543dae508ee9b,Optimization Techniques for Semi-Supervised Support Vector Machines,"Due to its wide applicability, the problem of semi-supervised classification is attracting increasing attention in machine learning. Semi-Supervised Support Vector Machines (S3VMs) are based on applying the margin maximization principle to both labeled and unlabeled examples. Unlike SVMs, their formulation leads to a non-convex optimization problem. A suite of algorithms have recently been proposed for solving S3VMs. This paper reviews key ideas in this literature. The performance and behavior of various S3VMs algorithms is studied together, under a common experimental setting.",2008.0,"O. Chapelle, Vikas Sindhwani, S. Keerthi"
679bc3cf9f52b0a1d8ff2b5ed4718ce6e44f9a56,https://www.semanticscholar.org/paper/679bc3cf9f52b0a1d8ff2b5ed4718ce6e44f9a56,The Bayesian backfitting relevance vector machine,"Traditional non-parametric statistical learning techniques are often computationally attractive, but lack the same generalization and model selection abilities as state-of-the-art Bayesian algorithms which, however, are usually computationally prohibitive. This paper makes several important contributions that allow Bayesian learning to scale to more complex, real-world learning scenarios. Firstly, we show that backfitting --- a traditional non-parametric, yet highly efficient regression tool --- can be derived in a novel formulation within an expectation maximization (EM) framework and thus can finally be given a probabilistic interpretation. Secondly, we show that the general framework of sparse Bayesian learning and in particular the relevance vector machine (RVM), can be derived as a highly efficient algorithm using a Bayesian version of backfitting at its core. As we demonstrate on several regression and classification benchmarks, Bayesian backfitting offers a compelling alternative to current regression methods, especially when the size and dimensionality of the data challenge computational resources.",2004.0,"Aaron D'Souza, S. Vijayakumar, Stefan Schaal"
0fe97c88452d8d8603d9ba883a0721da46ba84f4,https://www.semanticscholar.org/paper/0fe97c88452d8d8603d9ba883a0721da46ba84f4,The Set Covering Machine,"We extend the classical algorithms of Valiant and Haussler for learning compact conjunctions and disjunctions of Boolean attributes to allow features that are constructed from the data and to allow a trade-off between accuracy and complexity. The result is a general-purpose learning machine, suitable for practical learning tasks, that we call the set covering machine. We present a version of the set covering machine that uses data-dependent balls for its set of features and compare its performance with the support vector machine. By extending a technique pioneered by Littlestone and Warmuth, we bound its generalization error as a function of the amount of data compression it achieves during training. In experiments with real-world learning tasks, the bound is shown to be extremely tight and to provide an effective guide for model selection.",2003.0,"M. Marchand, J. Shawe-Taylor"
47fc921add1421ff8adb730df7aa9e7f865bfdeb,https://www.semanticscholar.org/paper/47fc921add1421ff8adb730df7aa9e7f865bfdeb,Toward Practical Smile Detection,"Machine learning approaches have produced some of the highest reported performances for facial expression recognition. However, to date, nearly all automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled lighting conditions on a relatively small number of subjects. This paper explores whether current machine learning methods can be used to develop an expression recognition system that operates reliably in more realistic conditions. We explore the necessary characteristics of the training data set, image registration, feature representation, and machine learning algorithms. A new database, GENKI, is presented which contains pictures, photographed by the subjects themselves, from thousands of different people in many different real-world imaging conditions. Results suggest that human-level expression recognition accuracy in real-life illumination conditions is achievable with machine learning technology. However, the data sets currently used in the automatic expression recognition literature to evaluate progress may be overly constrained and could potentially lead research into locally optimal algorithmic solutions.",2009.0,"J. Whitehill, G. Littlewort, Ian R. Fasel, M. Bartlett, J. Movellan"
79a8334eb8393be100503b3d7b8f27dab2181528,https://www.semanticscholar.org/paper/79a8334eb8393be100503b3d7b8f27dab2181528,On the relation between multi-instance learning and semi-supervised learning,"Multi-instance learning and semi-supervised learning are different branches of machine learning. The former attempts to learn from a training set consists of labeled bags each containing many unlabeled instances; the latter tries to exploit abundant unlabeled instances when learning with a small number of labeled examples. In this paper, we establish a bridge between these two branches by showing that multi-instance learning can be viewed as a special case of semi-supervised learning. Based on this recognition, we propose the MissSVM algorithm which addresses multi-instance learning using a special semi-supervised support vector machine. Experiments show that solving multi-instance problems from the view of semi-supervised learning is feasible, and the MissSVM algorithm is competitive with state-of-the-art multi-instance learning algorithms.",2007.0,"Zhi-Hua Zhou, Jun-Ming Xu"
b7d89441fcf28ca1a365af4d739709a7075a5db2,https://www.semanticscholar.org/paper/b7d89441fcf28ca1a365af4d739709a7075a5db2,Knowledge Discovery with Support Vector Machines,"An easy-to-follow introduction to support vector machines This book provides an in-depth, easy-to-follow introduction to support vector machines drawing only from minimal, carefully motivated technical and mathematical background material. It begins with a cohesive discussion of machine learning and goes on to cover: Knowledge discovery environments Describing data mathematically Linear decision surfaces and functions Perceptron learning Maximum margin classifiers Support vector machines Elements of statistical learning theory Multi-class classification Regression with support vector machines Novelty detection Complemented with hands-on exercises, algorithm descriptions, and data sets, Knowledge Discovery with Support Vector Machines is an invaluable textbook for advanced undergraduate and graduate courses. It is also an excellent tutorial on support vector machines for professionals who are pursuing research in machine learning and related areas.",2009.0,L. Hamel
154507a7222c60380dd895d85171722548fbc81f,https://www.semanticscholar.org/paper/154507a7222c60380dd895d85171722548fbc81f,ISPRS Journal of Photogrammetry and Remote Sensing,"A wide range of methods for analysis of airborne-and satellite-derived imagery continues to be proposed and assessed. In this paper, we review remote sensing implementations of support vector machines (SVMs), a promising machine learning methodology. This review is timely due to the exponentially increasing number of works published in recent years. SVMs are particularly appealing in the remote sensing field due to their ability to generalize well even with limited training samples, a common limitation for remote sensing applications. However, they also suffer from parameter assignment issues that can significantly affect obtained results. A summary of empirical results is provided for various applications of over one hundred published works (as of April, 2010). It is our hope that this survey will provide guidelines for future applications of SVMs and possible areas of algorithm enhancement. © 2010 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by",,"G. Mountrakis, J. Im, Caesar Ogole"
02d82f970d201c631424af3c617ceb25884da4a7,https://www.semanticscholar.org/paper/02d82f970d201c631424af3c617ceb25884da4a7,PyBrain,"PyBrain is a versatile machine learning library for Python. Its goal is to provide flexible, easyto-use yet still powerful algorithms for machine learning t asks, including a variety of predefined environments and benchmarks to test and compare algorithms . I plemented algorithms include Long Short-Term Memory (LSTM), policy gradient methods, (m ultidimensional) recurrent neural networks and deep belief networks.",2010.0,"T. Schaul, Justin Bayer, D. Wierstra, Yi Sun, M. Felder, Frank Sehnke, Thomas Rückstieß, Jürgen Schmidhuber"
81d093393f7a62b7a72dfea7cb42c80f2d589d9a,https://www.semanticscholar.org/paper/81d093393f7a62b7a72dfea7cb42c80f2d589d9a,On Robustness Properties of Convex Risk Minimization Methods for Pattern Recognition,"The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have - besides other good properties - also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the influence function of the classifiers and for bounds on the influence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.",2004.0,"A. Christmann, Ingo Steinwart"
f50e54684086a91bb481f76f7180c1ef3c4cb312,https://www.semanticscholar.org/paper/f50e54684086a91bb481f76f7180c1ef3c4cb312,Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data,"The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.",2019.0,"Liang Guo, Y. Lei, Saibo Xing, Tao Yan, Naipeng Li"
fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4,https://www.semanticscholar.org/paper/fc8cda36a0972e7de1ac3a7bcb81dc32da79bee4,"Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond","Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.",2003.0,Christopher K. I Williams
5d90f06bb70a0a3dced62413346235c02b1aa086,https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086,Learning Multiple Layers of Features from Tiny Images,"Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.",2009.0,A. Krizhevsky
0e90a73f03902cbe915af1aff54ea7f0b3373680,https://www.semanticscholar.org/paper/0e90a73f03902cbe915af1aff54ea7f0b3373680,An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods,"This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.",2001.0,Tong Zhang
a675fe5a7d99ac6f7ff91fa084462faefe616148,https://www.semanticscholar.org/paper/a675fe5a7d99ac6f7ff91fa084462faefe616148,What video games have to teach us about learning and literacy,"Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.",2007.0,J. Gee
d04d6db5f0df11d0cff57ec7e15134990ac07a4f,https://www.semanticscholar.org/paper/d04d6db5f0df11d0cff57ec7e15134990ac07a4f,Learning Deep Architectures for AI,"Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",2007.0,Yoshua Bengio
4609f6bdc3beab00c9beceaa12dd8101fefe6f1c,https://www.semanticscholar.org/paper/4609f6bdc3beab00c9beceaa12dd8101fefe6f1c,An overview of statistical learning theory,Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).,1999.0,V. Vapnik
1fcbefeb0beae4470cf40df74cd116b1d4bdcae4,https://www.semanticscholar.org/paper/1fcbefeb0beae4470cf40df74cd116b1d4bdcae4,An introduction to kernel-based learning algorithms,"This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.",2001.0,"K. Müller, S. Mika, Gunnar Rätsch, K. Tsuda, B. Scholkopf"
51886def908b16d11685ea23eb2124dfe961754f,https://www.semanticscholar.org/paper/51886def908b16d11685ea23eb2124dfe961754f,Semi-Supervised and Unsupervised Extreme Learning Machines,"Extreme learning machines (ELMs) have proven to be efficient and effective learning mechanisms for pattern classification and regression. However, ELMs are primarily applied to supervised learning problems. Only a few existing research papers have used ELMs to explore unlabeled data. In this paper, we extend ELMs for both semi-supervised and unsupervised tasks based on the manifold regularization, thus greatly expanding the applicability of ELMs. The key advantages of the proposed algorithms are as follows: 1) both the semi-supervised ELM (SS-ELM) and the unsupervised ELM (US-ELM) exhibit learning capability and computational efficiency of ELMs; 2) both algorithms naturally handle multiclass classification or multicluster clustering; and 3) both algorithms are inductive and can handle unseen data at test time directly. Moreover, it is shown in this paper that all the supervised, semi-supervised, and unsupervised ELMs can actually be put into a unified framework. This provides new perspectives for understanding the mechanism of random feature mapping, which is the key concept in ELM theory. Empirical study on a wide range of data sets demonstrates that the proposed algorithms are competitive with the state-of-the-art semi-supervised or unsupervised learning algorithms in terms of accuracy and efficiency.",2014.0,"Gao Huang, Shiji Song, J. Gupta, Cheng Wu"
fd8ce955dc0c570b66305dfbc65e4ed5f37658d0,https://www.semanticscholar.org/paper/fd8ce955dc0c570b66305dfbc65e4ed5f37658d0,"Induction: Processes of Inference, Learning, and Discovery","Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.",1987.0,"J. H. Holland, K. Holyoak, R. Nisbett, Paul Thagard, S. Smoliar"
e219a61354d972a28954e655a7c53373508a08b6,https://www.semanticscholar.org/paper/e219a61354d972a28954e655a7c53373508a08b6,Regularized multi--task learning,"Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.",2004.0,"T. Evgeniou, M. Pontil"
9e2bb72595f7ff0a5a5d3dc98600b8c5342c3fb4,https://www.semanticscholar.org/paper/9e2bb72595f7ff0a5a5d3dc98600b8c5342c3fb4,Extreme Learning Machines [Trends & Controversies],"This special issue includes eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In ""Representational Learning with ELMs for Big Data,"" Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In ""A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing,"" Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C.M. Leung propose a method for handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In ""ELM-Guided Memetic Computation for Vehicle Routing,"" Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In ""ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs,"" Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In ""Combining ELMs with Random Projections,"" Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In ""Reduced ELMs for Causal Relation Extraction from Unstructured Text,"" Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In ""A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures,"" Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry for touchless applications without the need for handheld devices. Finally, in ""An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System,"" Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for touchless human-machine interaction.",2013.0,"Erik Cambria, G. Huang, L. L. C. Kasun, Hongming Zhou, C. Vong, Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, Victor C. M. Leung, Liang Feng, Y. Ong, Meng-Hiot Lim, Anton Akusok, A. Lendasse, F. Corona, Rui Nian, Y. Miché, P. Gastaldo, R. Zunino, S. Decherchi, Xuefeng Yang, Kezhi Mao, B. Oh, Je-Hyoung Jeon, Kar-Ann Toh, A. Teoh, Jaihie Kim, Hanchao Yu, Yiqiang Chen, Junfa Liu"
12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4,https://www.semanticscholar.org/paper/12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4,Online learning with kernels,"Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.",2001.0,"Jyrki Kivinen, Alex Smola, R. C. Williamson"
77e379fd57ea44638fc628623e383eccada82689,https://www.semanticscholar.org/paper/77e379fd57ea44638fc628623e383eccada82689,Kernel Methods for Deep Learning,"We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.",2009.0,"Youngmin Cho, L. Saul"
4673a47a4f6719e350196f4086a65d08f946df25,https://www.semanticscholar.org/paper/4673a47a4f6719e350196f4086a65d08f946df25,Learning by Design: Good Video Games as Learning Machines,"This article asks how good video and computer game designers manage to get new players to learn long, complex and difficult games. The short answer is that designers of good games have hit on excellent methods for getting people to learn and to enjoy learning. The longer answer is more complex. Integral to this answer are the good principles of learning built into successful games. The author discusses 13 such principles under the headings of ‘Empowered Learners’, ‘Problem Solving’ and ‘Understanding’ and concludes that the main impediment to implementing these principles in formal education is cost. This, however, is not only (or even so much) monetary cost. It is, importantly, the cost of changing minds about how and where learning is done and of changing one of our most profoundly change-resistant institutions: the school.",2005.0,J. Gee
52e2ac397f0c8d5f533959905df899bc328d9f85,https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85,Reinforcement Learning with Hierarchies of Machines,"We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and ""behavior-based"" or ""teleo-reactive"" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.",1997.0,"Ronald E. Parr, Stuart J. Russell"
424a6e62084d919bfc2e39a507c263e5991ebdad,https://www.semanticscholar.org/paper/424a6e62084d919bfc2e39a507c263e5991ebdad,Self-Normalizing Neural Networks,"Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ""scaled exponential linear units"" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL.",2017.0,"G. Klambauer, Thomas Unterthiner, Andreas Mayr, Sepp Hochreiter"
00cd1dab559a9671b692f39f14c1573ab2d1416b,https://www.semanticscholar.org/paper/00cd1dab559a9671b692f39f14c1573ab2d1416b,Efficient Learning of Deep Boltzmann Machines,"We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.",2010.0,"R. Salakhutdinov, H. Larochelle"
6bdb186ec4726e00a8051119636d4df3b94043b5,https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5,Caffe: Convolutional Architecture for Fast Feature Embedding,"Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",2014.0,"Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross B. Girshick, S. Guadarrama, Trevor Darrell"
3127190433230b3dc1abd0680bb58dced4bcd90e,https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e,Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",2012.0,"J. Dean, G. Corrado, R. Monga, Kai Chen, M. Devin, Quoc V. Le, Mark Z. Mao, Marc'Aurelio Ranzato, A. Senior, P. Tucker, Ke Yang, A. Ng"
e50f4d3316d13841c287dcdf5479d7820d593571,https://www.semanticscholar.org/paper/e50f4d3316d13841c287dcdf5479d7820d593571,Factorization Machines with libFM,"Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.
 Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.",2012.0,Steffen Rendle
ddc45fad8d15771d9f1f8579331458785b2cdd93,https://www.semanticscholar.org/paper/ddc45fad8d15771d9f1f8579331458785b2cdd93,Deep Boltzmann Machines,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",2009.0,"R. Salakhutdinov, Geoffrey E. Hinton"
62ed272e0e8b7be356c7f7595f5b7a22797a1c3e,https://www.semanticscholar.org/paper/62ed272e0e8b7be356c7f7595f5b7a22797a1c3e,Support vector machines for classification and regression.,"The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.",2010.0,"R. Brereton, G. Lloyd"
fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,https://www.semanticscholar.org/paper/fa5853fdef7d2f6bb68203d187ddacbbddc63a8b,High-Dimensional Probability: An Introduction with Applications in Data Science,"© 2018, Cambridge University Press Let us summarize our findings. A random projection of a set T in R n onto an m-dimensional subspace approximately preserves the geometry of T if m ⪆ d ( T ) . For...",2020.0,O. Papaspiliopoulos
aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,https://www.semanticscholar.org/paper/aeede2d75d7cb3e10bc3b732a897ca1a7bfc12c5,"BOOK REVIEW - Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you:",2024.0,Gabriel Neagu
01e863776846ebd1a9a7acc4a9ca24217f953aa2,https://www.semanticscholar.org/paper/01e863776846ebd1a9a7acc4a9ca24217f953aa2,Data Interpreter: An LLM Agent For Data Science,"Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.",2024.0,"Sirui Hong, Yizhang Lin, Bangbang Liu, Binhao Wu, Danyang Li, Jiaqi Chen, Jiayi Zhang, Jinlin Wang, Lingyao Zhang, Mingchen Zhuge, Taicheng Guo, Tuo Zhou, Wei Tao, Wenyi Wang, Xiangru Tang, Xiangtao Lu, Xinbing Liang, Yaying Fei, Yuheng Cheng, Zhibin Gou, Zongze Xu, Chenglin Wu, Li Zhang, Min Yang, Xiawu Zheng"
c082ccfcfe1afc696e371374146ba9380b84061e,https://www.semanticscholar.org/paper/c082ccfcfe1afc696e371374146ba9380b84061e,The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field,"ChatGPT, a conversational AI interface that utilizes natural language processing and machine learning algorithms, is taking the world by storm and is the buzzword across many sectors today. Given the likely impact of this model on data science, through this perspective article, we seek to provide an overview of the potential opportunities and challenges associated with using ChatGPT in data science, provide readers with a snapshot of its advantages, and stimulate interest in its use for data science projects. The paper discusses how ChatGPT can assist data scientists in automating various aspects of their workflow, including data cleaning and preprocessing, model training, and result interpretation. It also highlights how ChatGPT has the potential to provide new insights and improve decision-making processes by analyzing unstructured data. We then examine the advantages of ChatGPT’s architecture, including its ability to be fine-tuned for a wide range of language-related tasks and generate synthetic data. Limitations and issues are also addressed, particularly around concerns about bias and plagiarism when using ChatGPT. Overall, the paper concludes that the benefits outweigh the costs and ChatGPT has the potential to greatly enhance the productivity and accuracy of data science workflows and is likely to become an increasingly important tool for intelligence augmentation in the field of data science. ChatGPT can assist with a wide range of natural language processing tasks in data science, including language translation, sentiment analysis, and text classification. However, while ChatGPT can save time and resources compared to training a model from scratch, and can be fine-tuned for specific use cases, it may not perform well on certain tasks if it has not been specifically trained for them. Additionally, the output of ChatGPT may be difficult to interpret, which could pose challenges for decision-making in data science applications.",2023.0,"Hossein Hassani, E. Silva"
8a4fc5f00cd4aca61e148e46a2125c3a406719f1,https://www.semanticscholar.org/paper/8a4fc5f00cd4aca61e148e46a2125c3a406719f1,DS-1000: A Natural and Reliable Benchmark for Data Science Code Generation,"We introduce DS-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as NumPy and Pandas. Compared to prior works, DS-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from StackOverflow. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting API usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original StackOverflow source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.",2022.0,"Yuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, S. Yih, Daniel Fried, Si-yi Wang, Tao Yu"
7b5432e589b5f801d99bd27c00349ac651d308e4,https://www.semanticscholar.org/paper/7b5432e589b5f801d99bd27c00349ac651d308e4,A New Era of Learning: Considerations for ChatGPT as a Tool to Enhance Statistics and Data Science Education,"Abstract ChatGPT is one of many generative artificial intelligence (AI) tools that has emerged recently, creating controversy in the education community with concerns about its potential to be used for plagiarism and to undermine students’ ability to think independently. Recent publications have criticized the use of ChatGPT and other generative AI tools in the classroom, with little focus on the potential benefits. This article focuses on the potential of ChatGPT as an educational tool for statistics and data science. It encourages readers to consider the history of trepidation surrounding introducing new technology in the classroom, such as the calculator. We explore the possibility of leveraging ChatGPT’s capabilities in statistics and data science education, providing examples of how ChatGPT can aid in developing course materials and suggestions for how educators can prompt students to interact with ChatGPT responsibly. As educators, we can guide the use of generative AI tools in statistics and data science classrooms so that students and educators can leverage the benefits of this technology.",2023.0,"Amanda R. Ellis, E. Slade"
ed6473fd5294a0639d661e02092768f364d80f39,https://www.semanticscholar.org/paper/ed6473fd5294a0639d661e02092768f364d80f39,What is Data Science?,"The Communications website, https://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of Communications, we'll publish selected posts or excerpts. twitter Follow us on Twitter at http://twitter.com/blogCACM https://cacm.acm.org/blogs/blog-cacm Koby Mike and Orit Hazzan consider why multiple definitions are needed to pin down data science.",2023.0,Michael L. Brodie
9957908c33cba922bdbdfa6812b8db1b6ca4b7b3,https://www.semanticscholar.org/paper/9957908c33cba922bdbdfa6812b8db1b6ca4b7b3,Data Science and Applications,"This paper investigates the significance of data science as an indispensable instrument for decision-making across multiple domains. The study examines the history, concepts, methods, and applications of data science, as well as its impact on numerous industries, such as artificial intelligence, manufacturing, fintech, government, Astro-informatics, e-commerce, education, and biotechnology. ERP (Enterprise Resource Planning) software was first developed by SAP in the 1960s, with modern ERP systems emerging in the 1990s, according to the research. The paper highlights the significance of data science in enhancing the functionality of enterprise resource planning (ERP) systems, with AI-based solutions such as those offered by MahaaAi and other firms automating human tasks, chat-based ERP applications, and virtual assistant assistants support to avoid human efforts. The conclusion of the study emphasizes the significant benefits of data science in the ERP industry, including self-service analytics, predictions, and prescriptive analysis.",2023.0,"Narender Chinthamu, Manideep Karukuri"
fb29359d794265c0931d756858a70c9265b5693d,https://www.semanticscholar.org/paper/fb29359d794265c0931d756858a70c9265b5693d,The R Language: An Engine for Bioinformatics and Data Science,"The R programming language is approaching its 30th birthday, and in the last three decades it has achieved a prominent role in statistics, bioinformatics, and data science in general. It currently ranks among the top 10 most popular languages worldwide, and its community has produced tens of thousands of extensions and packages, with scopes ranging from machine learning to transcriptome data analysis. In this review, we provide an historical chronicle of how R became what it is today, describing all its current features and capabilities. We also illustrate the major tools of R, such as the current R editors and integrated development environments (IDEs), the R Shiny web server, the R methods for machine learning, and its relationship with other programming languages. We also discuss the role of R in science in general as a driver for reproducibility. Overall, we hope to provide both a complete snapshot of R today and a practical compendium of the major features and applications of this programming language.",2022.0,"F. Giorgi, Carmine Ceraolo, D. Mercatelli"
88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,https://www.semanticscholar.org/paper/88ca84ce36ddc1bf7b6593b7f73fe2663e2365ad,Foundations of Data Science,"Computer science as an academic discipline began in the 1960’s. Emphasis was on programming languages, compilers, operating systems, and the mathematical theory that supported these areas. Courses in theoretical computer science covered finite automata, regular expressions, context-free languages, and computability. In the 1970’s, the study of algorithms was added as an important component of theory. The emphasis was on making computers useful. Today, a fundamental change is taking place and the focus is more on applications. There are many reasons for this change. The merging of computing and communications has played an important role. The enhanced ability to observe, collect, and store data in the natural sciences, in commerce, and in other fields calls for a change in our understanding of data and how to handle it in the modern setting. The emergence of the web and social networks as central aspects of daily life presents both opportunities and challenges for theory.",2020.0,"Avrim Blum, J. Hopcroft, R. Kannan"
8bb6a6802027c7f2489accb0559e6f02984535c9,https://www.semanticscholar.org/paper/8bb6a6802027c7f2489accb0559e6f02984535c9,"Smart Health Intelligent Healthcare Systems in the Metaverse, Artificial Intelligence, and Data Science Era","In recent decades, healthcare organizations around the world have increasingly appreciated the value of information technologies for a variety of applications. Three of the new technological advancements that are impacting smart health are metaverse, artificial intelligence (AI), and data science. The metaverse is the intersection of three major technologies — AI, augmented reality (AR), and virtual reality (VR). Metaverse provides new possibilities and potential that are still emerging. The increased work efficiency enabled by artificial intelligence and data science in hospitals not only improves patient care but also cuts costs and workload for healthcare providers.The availability of big data enables data scientists to use the data for descriptive, predictive, and prescriptive analytics. This article reviews multiple case studies and the literature on AI and data science applications in hospital administration. The article also presents unresolved research questions and challenges in the applications of the metaverse, AI, and data science in the smart health context.",2022.0,"Yin Yang, K. Siau, Wen Xie, Yan Lindsay Sun"
55ad5e818cfed72317576027fb33a9609210d592,https://www.semanticscholar.org/paper/55ad5e818cfed72317576027fb33a9609210d592,Training and Evaluating a Jupyter Notebook Data Science Assistant,"We study the feasibility of a Data Science assistant powered by a sequence-to-sequence transformer by training a new model JuPyT5 on all publicly available Jupyter Notebook GitHub repositories and developing a new metric: Data Science Problems (DSP). DSP is a collection of 1119 problems curated from 306 pedagogical notebooks with 92 dataset dependencies, natural language and Markdown problem descriptions, and assert-based unit tests. These notebooks were designed to test university students' mastery of various Python implementations of Math and Data Science, and we now leverage them to study the ability of JuPyT5 to understand and pass the tests. We analyze the content of DSP, validate its quality, and we find that given 100 sampling attempts JuPyT5 is able to solve 77.5\% of the DSP problems. We further present various ablation and statistical analyses and compare DSP to other recent natural language to code benchmarks.",2022.0,"Shubham Chandel, Colin B. Clement, Guillermo Serrato, Neel Sundaresan"
72d3ddf1f7210d7e70144bbc09f770ec411fe909,https://www.semanticscholar.org/paper/72d3ddf1f7210d7e70144bbc09f770ec411fe909,"Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence","Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.",2020.0,"S. Raschka, Joshua Patterson, Corey J. Nolet"
4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,https://www.semanticscholar.org/paper/4c6e31458b0b44c1e8bd6e58f7d7e0767f7fde44,CRISP-DM Twenty Years Later: From Data Mining Processes to Data Science Trajectories,"CRISP-DM(CRoss-Industry Standard Process for Data Mining) has its origins in the second half of the nineties and is thus about two decades old. According to many surveys and user polls it is still the de facto standard for developing data mining and knowledge discovery projects. However, undoubtedly the field has moved on considerably in twenty years, with data science now the leading term being favoured over data mining. In this paper we investigate whether, and in what contexts, CRISP-DM is still fit for purpose for data science projects. We argue that if the project is goal-directed and process-driven the process model view still largely holds. On the other hand, when data science projects become more exploratory the paths that the project can take become more varied, and a more flexible model is called for. We suggest what the outlines of such a trajectory-based model might look like and how it can be used to categorise data science projects (goal-directed, exploratory or data management). We examine seven real-life exemplars where exploratory activities play an important role and compare them against 51 use cases extracted from the NIST Big Data Public Working Group. We anticipate this categorisation can help project planning in terms of time and cost characteristics.",2021.0,"Fernando Martínez-Plumed, Lidia Contreras-Ochando, C. Ferri, José Hernández Orallo, Meelis Kull, N. Lachiche, M. J. R. Quintana, P. Flach"
8bba999de25bfb288b3f7f88e1d907aab02638b6,https://www.semanticscholar.org/paper/8bba999de25bfb288b3f7f88e1d907aab02638b6,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",2020.0,"K. Jablonka, D. Ongari, S. M. Moosavi, B. Smit"
1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,https://www.semanticscholar.org/paper/1ba044d3d501dddd94b479aa9dbe55a93bfa9d5f,"QIIME 2: Reproducible, interactive, scalable, and extensible microbiome data science","We present QIIME 2, an open-source microbiome data science platform accessible to users spanning the microbiome research ecosystem, from scientists and engineers to clinicians and policy makers. QIIME 2 provides new features that will drive the next generation of microbiome research. These include interactive spatial and temporal analysis and visualization tools, support for metabolomics and shotgun metagenomics analysis, and automated data provenance tracking to ensure reproducible, transparent microbiome data science.",2018.0,"Evan Bolyen, J. Rideout, Matthew R. Dillon, N. Bokulich, C. Abnet, Gabriel A. Al-Ghalith, Harriet Alexander, Harriet Alexander, E. Alm, Manimozhiyan Arumugam, F. Asnicar, Yang Bai, J. Bisanz, K. Bittinger, A. Brejnrod, Colin J. Brislawn, C. T. Brown, B. Callahan, A. Caraballo-Rodríguez, John H. Chase, Emily K. Cope, R. R. Silva, P. Dorrestein, G. Douglas, D. Durall, C. Duvallet, C. Edwardson, Madeleine Ernst, M. Estaki, Jennifer T. Fouquier, J. Gauglitz, D. Gibson, Antonio Gonzalez, Kestrel Gorlick, Jiarong Guo, Benjamin M Hillmann, S. Holmes, H. Holste, C. Huttenhower, C. Huttenhower, G. Huttley, Stefan Janssen, A. Jarmusch, Lingjing Jiang, Benjamin D. Kaehler, K. Kang, K. Kang, Christopher R. Keefe, P. Keim, S. Kelley, D. Knights, I. Koester, I. Koester, T. Kosciólek, Jorden Kreps, M. Langille, Joslynn S. Lee, R. Ley, R. Ley, Yong-xin Liu, E. Loftfield, C. Lozupone, Massoud Maher, C. Marotz, Bryan D. Martin, Daniel McDonald, L. McIver, L. McIver, A. Melnik, J. Metcalf, S. C. Morgan, James T. Morton, Ahmad Turan Naimey, Jose A Navas-Molina, Jose A Navas-Molina, Louis-Félix Nothias, Stephanie B. Orchanian, Talima R. Pearson, Samuel L. Peoples, Samuel L. Peoples, D. Petráš, M. Preuss, E. Pruesse, Lasse Buur Rasmussen, A. Rivers, Ii Michael S Robeson, P. Rosenthal, N. Segata, Michael Shaffer, A. Shiffer, R. Sinha, Se Jin Song, J. Spear, Austin D. Swafford, Luke R. Thompson, Luke R. Thompson, P. Torres, Pauline Trinh, A. Tripathi, A. Tripathi, P. Turnbaugh, Sabah Ul-Hasan, J. J. Hooft, Fernando Vargas, Y. Vázquez-Baeza, E. Vogtmann, Max von Hippel, William Walters, Yunhu Wan, Mingxun Wang, Jonathan Warren, Kyle C. Weber, Kyle C. Weber, Chase Williamson, A. Willis, Z. Xu, Jesse R. Zaneveld, Yilong Zhang, R. Knight, J. Caporaso"
2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,https://www.semanticscholar.org/paper/2d6adb9636df5a8a5dbcbfaecd0c4d34d7c85034,Spectral Methods for Data Science: A Statistical Perspective,"Spectral methods have emerged as a simple yet surprisingly effective approach for extracting information from massive, noisy and incomplete data. In a nutshell, spectral methods refer to a collection of algorithms built upon the eigenvalues (resp. singular values) and eigenvectors (resp. singular vectors) of some properly designed matrices constructed from data. A diverse array of applications have been found in machine learning, data science, and signal processing. Due to their simplicity and effectiveness, spectral methods are not only used as a stand-alone estimator, but also frequently employed to initialize other more sophisticated algorithms to improve performance. 
While the studies of spectral methods can be traced back to classical matrix perturbation theory and methods of moments, the past decade has witnessed tremendous theoretical advances in demystifying their efficacy through the lens of statistical modeling, with the aid of non-asymptotic random matrix theory. This monograph aims to present a systematic, comprehensive, yet accessible introduction to spectral methods from a modern statistical perspective, highlighting their algorithmic implications in diverse large-scale applications. In particular, our exposition gravitates around several central questions that span various applications: how to characterize the sample efficiency of spectral methods in reaching a target level of statistical accuracy, and how to assess their stability in the face of random noise, missing data, and adversarial corruptions? In addition to conventional $\ell_2$ perturbation analysis, we present a systematic $\ell_{\infty}$ and $\ell_{2,\infty}$ perturbation theory for eigenspace and singular subspaces, which has only recently become available owing to a powerful ""leave-one-out"" analysis framework.",2020.0,"Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma"
108acf9a358512a40191d857e2456aeaaac3303b,https://www.semanticscholar.org/paper/108acf9a358512a40191d857e2456aeaaac3303b,Diversifying the genomic data science research community,"Over the past 20 years, the explosion of genomic data collection and the cloud computing revolution have made computational and data science research accessible to anyone with a web browser and an internet connection. However, students at institutions with limited resources have received relatively little exposure to curricula or professional development opportunities that lead to careers in genomic data science. To broaden participation in genomics research, the scientific community needs to support these programs in local education and research at underserved institutions (UIs). These include community colleges, historically Black colleges and universities, Hispanic-serving institutions, and tribal colleges and universities that support ethnically, racially, and socioeconomically underrepresented students in the United States. We have formed the Genomic Data Science Community Network to support students, faculty, and their networks to identify opportunities and broaden access to genomic data science. These opportunities include expanding access to infrastructure and data, providing UI faculty development opportunities, strengthening collaborations among faculty, recognizing UI teaching and research excellence, fostering student awareness, developing modular and open-source resources, expanding course-based undergraduate research experiences (CUREs), building curriculum, supporting student professional development and research, and removing financial barriers through funding programs and collaborator support.",2022.0,"The Genomic Observatories Network, Rosa M Alcazar, Maria Alvarez, Rachel Arnold, Mentewab Ayalew, L. Best, M. Campbell, K. Chowdhury, Katherine E. L. Cox, Christina R. Daulton, Youping Deng, Carla L. Easter, K. Fuller, S. Hakim, Ava M. Hoffman, N. Kucher, Andrew Lee, Joslynn S. Lee, J. Leek, R. Meller, Loyda B. M'endez, Miguel P. M'endez-Gonz'alez, Stephen L Mosher, M. Nishiguchi, S. Pratap, Tiffany Rolle, Sourav Roy, Rachel Saidi, M. Schatz, S. Sen, J. Sniezek, E. Martinez, F. Tan, Jennifer Vessio, K. Watson, W. Westbroek, Joseph Wilcox, Xianfa Xie Clovis Community College, Fresno, Ca, USA., Biology, El Paso Community College, E. Paso, Tx, U. Fish, Wildlife, Northwest Indian College, Onalaska, Wi, B. Department, Spelman College, Atlanta, Ga, Turtle Mountain Community College, Belcourt, Nd, Department of Mathematical Sciences, U. California, California LosAngeles, C. University, Orangeburg, Sc, D. Biostatistics, Johns Hopkins Bloomberg School of Public Health, Baltimore., Md, National Human Genome Research Institute, N. Health, Bethesda., Department of Mathematical Sciences, University of Hawai'i at Manoa, Honolulu, Hi, Smithsonian Institute National Museum of Natural History, Washington, Dc, Guttman Community College, New York., Ny, D. Microbiology, Biomedical Sciences, D. College, Tuba City, Az, Department of Medical Biology, J. University, Northern Virginia Community College - Alexandria, Alexandria, Va, D. Chemistry, Biochemistry, Fort Lewis College, Durango, Co, Department of Neurobiology, Morehouse School of Medicine, ScienceTechnology, Universidad Ana G. M'endez, Carolina, Pr, Natural Sciences Department, University of Puerto Rico at Aguadilla, Aguadilla, Department of Molecular, Cell Biology, U. California, Merced, School of Historical Studies, Research, M. M. College, Nashville, Tn, Border Biomedical Research Center, U. O. T. A. E. Paso, Department of Math, Statistics, Data Science, Montgomery College, Rockville, Department of Materials Science, Chemical, Biological Sciences, Germantown, University of Puerto Rico, Ponce, Department of Embryology, Carnegie Institution, Flathead Valley Community College, Kalispell, Mt, Nevada State College, Henderson, Nv, V. S. University, Petersburg"
7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,https://www.semanticscholar.org/paper/7282f5c9d84cd47c516a6a66c5a6b8f1e2cf44b6,AutoDS: Towards Human-Centered Automation of Data Science,"Data science (DS) projects often follow a lifecycle that consists of laborious tasks for data scientists and domain experts (e.g., data exploration, model training, etc.). Only till recently, machine learning(ML) researchers have developed promising automation techniques to aid data workers in these tasks. This paper introduces AutoDS, an automated machine learning (AutoML) system that aims to leverage the latest ML automation techniques to support data science projects. Data workers only need to upload their dataset, then the system can automatically suggest ML configurations, preprocess data, select algorithm, and train the model. These suggestions are presented to the user via a web-based graphical user interface and a notebook-based programming user interface. Our goal is to offer a systematic investigation of user interaction and perceptions of using an AutoDS system in solving a data science task. We studied AutoDS with 30 professional data scientists, where one group used AutoDS, and the other did not, to complete a data science project. As expected, AutoDS improves productivity; Yet surprisingly, we find that the models produced by the AutoDS group have higher quality and less errors, but lower human confidence scores. We reflect on the findings by presenting design implications for incorporating automation techniques into human work in the data science lifecycle.",2021.0,"Dakuo Wang, Josh Andres, Justin D. Weisz, Erick Oduor, Casey Dugan"
ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,https://www.semanticscholar.org/paper/ab8ba0f2d290a8e56eb61e10027d0b2e57d2d544,"How do Data Science Workers Collaborate? Roles, Workflows, and Tools","Today, the prominence of data science within organizations has given rise to teams of data science workers collaborating on extracting insights from data, as opposed to individual data scientists working alone. However, we still lack a deep understanding of how data science workers collaborate in practice. In this work, we conducted an online survey with 183 participants who work in various aspects of data science. We focused on their reported interactions with each other (e.g., managers with engineers) and with different tools (e.g., Jupyter Notebook). We found that data science teams are extremely collaborative and work with a variety of stakeholders and tools during the six common steps of a data science workflow (e.g., clean data and train model). We also found that the collaborative practices workers employ, such as documentation, vary according to the kinds of tools they use. Based on these findings, we discuss design implications for supporting data science team collaborations and future research directions.",2020.0,"Amy X. Zhang, Michael J. Muller, Dakuo Wang"
f9d403c58db99e2214f43e5b1740694b9c79002f,https://www.semanticscholar.org/paper/f9d403c58db99e2214f43e5b1740694b9c79002f,"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large","Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",2021.0,"Sumon Biswas, Mohammad Wardat, Hridesh Rajan"
0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,https://www.semanticscholar.org/paper/0751d2fa3a54cbbb4d594f2ee47c3aa7e4003a24,Leveraging Data Science to Combat COVID-19: A Comprehensive Review,"COVID-19, an infectious disease caused by the SARS-CoV-2 virus, was declared a pandemic by the World Health Organisation (WHO) in March 2020. By mid-August 2020, more than 21 million people have tested positive worldwide. Infections have been growing rapidly and tremendous efforts are being made to fight the disease. In this paper, we attempt to systematise the various COVID-19 research activities leveraging data science, where we define data science broadly to encompass the various methods and tools—including those from artificial intelligence (AI), machine learning (ML), statistics, modeling, simulation, and data visualization—that can be used to store, process, and extract insights from data. In addition to reviewing the rapidly growing body of recent research, we survey public datasets and repositories that can be used for further work to track COVID-19 spread and mitigation strategies. As part of this, we present a bibliometric analysis of the papers produced in this short span of time. Finally, building on these insights, we highlight common challenges and pitfalls observed across the surveyed works. We also created a live resource repository at https://github.com/Data-Science-and-COVID-19/Leveraging-Data-Science-To-Combat-COVID-19-A-Comprehensive-Review that we intend to keep updated with the latest resources including new papers and datasets.",2020.0,"S. Latif, Muhammad Usman, Sanaullah Manzoor, Waleed Iqbal, Junaid Qadir, Gareth Tyson, Ignacio Castro, Adeel Razi, M. Boulos, Adrian Weller, J. Crowcroft"
0405bfdc3f0ecb8e9d31ae68911731e61a65c01d,https://www.semanticscholar.org/paper/0405bfdc3f0ecb8e9d31ae68911731e61a65c01d,Surgical data science and artificial intelligence for surgical education,"Surgical data science (SDS) aims to improve the quality of interventional healthcare and its value through the capture, organization, analysis, and modeling of procedural data. As data capture has increased and artificial intelligence (AI) has advanced, SDS can help to unlock augmented and automated coaching, feedback, assessment, and decision support in surgery. We review major concepts in SDS and AI as applied to surgical education and surgical oncology.",2021.0,"Thomas M. Ward, P. Mascagni, A. Madani, N. Padoy, S. Perretta, D. Hashimoto"
f9b0b10713044c146caa84704b66804aa1e82d5e,https://www.semanticscholar.org/paper/f9b0b10713044c146caa84704b66804aa1e82d5e,Automating data science,"Given the complexity of data science projects and related demand for human expertise, automation has the potential to transform the data science process.",2021.0,"Tijl De Bie, Luc de Raedt, J. Hernández-Orallo, H. Hoos, Padhraic Smyth, C. Williams"
aaa952ea3ab8e2ac9839d52df9cea2d918f4d363,https://www.semanticscholar.org/paper/aaa952ea3ab8e2ac9839d52df9cea2d918f4d363,"CRISP-DM for Data Science: Strengths, Weaknesses and Potential Next Steps","This paper explores the strengths and weaknesses of CRISP-DM when used for data science projects. The paper then explores what key actions data science teams using CRISP-DM should consider that addresses CRISP-DM’s weaknesses. In brief, CRISP-DM, which is the most popular framework teams use to execute data science projects, provides an easy to understand description of the data science project workflow (i.e., the data science life cycle). However, CRISP-DM’s project phases miss some key aspects of the data science project life cycle. In addition, CRISP-DM’s task-focused approach fails to address how a team should prioritize tasks, and in general, collaborate and communicate. Hence, this paper also describes how CRISP-DM could be combined with a team coordination framework, such as Scrum or Data Driven Scrum, which is a newer collaboration framework developed to address the unique data science coordination challenges.",2021.0,J. Saltz
f42c69dbd792155fee6f4d2c525971f8d43f138b,https://www.semanticscholar.org/paper/f42c69dbd792155fee6f4d2c525971f8d43f138b,Finding Related Tables in Data Lakes for Interactive Data Science,"Many modern data science applications build on data lakes, schema-agnostic repositories of data files and data products that offer limited organization and management capabilities. There is a need to build data lake search capabilities into data science environments, so scientists and analysts can find tables, schemas, workflows, and datasets useful to their task at hand. We develop search and management solutions for the Jupyter Notebook data science platform, to enable scientists to augment training data, find potential features to extract, clean data, and find joinable or linkable tables. Our core methods also generalize to other settings where computational tasks involve execution of programs or scripts.",2020.0,"Yi Zhang, Z. Ives"
12f62537251cf8eb76fa11c59df68d2211008898,https://www.semanticscholar.org/paper/12f62537251cf8eb76fa11c59df68d2211008898,Big Earth Data science: an information framework for a sustainable planet,"ABSTRACT The digital transformation of our society coupled with the increasing exploitation of natural resources makes sustainability challenges more complex and dynamic than ever before. These changes will unlikely stop or even decelerate in the near future. There is an urgent need for a new scientific approach and an advanced form of evidence-based decision-making towards the benefit of society, the economy, and the environment. To understand the impacts and interrelationships between humans as a society and natural Earth system processes, we propose a new engineering discipline, Big Earth Data science. This science is called to provide the methodologies and tools to generate knowledge from diverse, numerous, and complex data sources necessary to ensure a sustainable human society essential for the preservation of planet Earth. Big Earth Data science aims at utilizing data from Earth observation and social sensing and develop theories for understanding the mechanisms of how such a social-physical system operates and evolves. The manuscript introduces the universe of discourse characterizing this new science, its foundational paradigms and methodologies, and a possible technological framework to be implemented by applying an ecosystem approach. CASEarth and GEOSS are presented as examples of international implementation attempts. Conclusions discuss important challenges and collaboration opportunities.",2020.0,"Huadong Guo, S. Nativi, Dong Liang, M. Craglia, Lizhe Wang, S. Schade, C. Corbane, G. He, M. Pesaresi, Jianhui Li, Zeeshan Shirazi, Jie Liu, A. Annoni"
a4b6f802b3f416fb1af6d723e0549c5e6d34faae,https://www.semanticscholar.org/paper/a4b6f802b3f416fb1af6d723e0549c5e6d34faae,Data science in economics: comprehensive review of advanced machine learning and deep learning methods,"This paper provides a state-of-the-art investigation of advances in data science in emerging economic applications. The analysis was performed on novel data science methods in four individual classes of deep learning models, hybrid deep learning models, hybrid machine learning, and ensemble models. Application domains include a wide and diverse range of economics research from the stock market, marketing, and e-commerce to corporate banking and cryptocurrency. Prisma method, a systematic literature review methodology, was used to ensure the quality of the survey. The findings reveal that the trends follow the advancement of hybrid models, which, based on the accuracy metric, outperform other learning algorithms. It is further expected that the trends will converge toward the advancements of sophisticated hybrid deep learning models.",2020.0,"Saeed Nosratabadi, A. Mosavi, Puhong Duan, Pedram Ghamisi, Ferdinánd Filip, Shahab S. Band, U. Reuter, João Gama, A. Gandomi"
deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,https://www.semanticscholar.org/paper/deb4e0c46f2e389ec5e4528f9dcee643bb6a15fa,Fixed Point Strategies in Data Science,"The goal of this article is to promote the use of fixed point strategies in data science by showing that they provide a simplifying and unifying framework to model, analyze, and solve a great variety of problems. They are seen to constitute a natural environment to explain the behavior of advanced convex optimization methods as well as of recent nonlinear methods in data science which are formulated in terms of paradigms that go beyond minimization concepts and involve constructs such as Nash equilibria or monotone inclusions. We review the pertinent tools of fixed point theory and describe the main state-of-the-art algorithms for provenly convergent fixed point construction. We also incorporate additional ingredients such as stochasticity, block-implementations, and non-Euclidean metrics, which provide further enhancements. Applications to signal and image processing, machine learning, statistics, neural networks, and inverse problems are discussed.",2020.0,"P. Combettes, J. Pesquet"
5b9ea2abf1c5a04b3024367409284edceb741ef2,https://www.semanticscholar.org/paper/5b9ea2abf1c5a04b3024367409284edceb741ef2,A new paradigm for accelerating clinical data science at Stanford Medicine,"Stanford Medicine is building a new data platform for our academic research community to do better clinical data science. Hospitals have a large amount of patient data and researchers have demonstrated the ability to reuse that data and AI approaches to derive novel insights, support patient care, and improve care quality. However, the traditional data warehouse and Honest Broker approaches that are in current use, are not scalable. We are establishing a new secure Big Data platform that aims to reduce time to access and analyze data. In this platform, data is anonymized to preserve patient data privacy and made available preparatory to Institutional Review Board (IRB) submission. Furthermore, the data is standardized such that analysis done at Stanford can be replicated elsewhere using the same analytical code and clinical concepts. Finally, the analytics data warehouse integrates with a secure data science computational facility to support large scale data analytics. The ecosystem is designed to bring the modern data science community to highly sensitive clinical data in a secure and collaborative big data analytics environment with a goal to enable bigger, better and faster science.",2020.0,"Somalee Datta, J. Posada, Garrick Olson, Wencheng Li, Ciaran O'Reilly, Deepa Balraj, Joseph Mesterhazy, Joseph Pallas, Priyamvada Desai, N. Shah"
648ba966b63975c6859e1948ae3ddc30053884e4,https://www.semanticscholar.org/paper/648ba966b63975c6859e1948ae3ddc30053884e4,Making data science systems work,"How are data science systems made to work? It may seem that whether a system works is a function of its technical design, but it is also accomplished through ongoing forms of discretionary work by many actors. Based on six months of ethnographic fieldwork with a corporate data science team, we describe how actors involved in a corporate project negotiated what work the system should do, how it should work, and how to assess whether it works. These negotiations laid the foundation for how, why, and to what extent the system ultimately worked. We describe three main findings. First, how already-existing technologies are essential reference points to determine how and whether systems work. Second, how the situated resolution of development challenges continually reshapes the understanding of how and whether systems work. Third, how business goals, and especially their negotiated balance with data science imperatives, affect a system’s working. We conclude with takeaways for critical data studies, orienting researchers to focus on the organizational and cultural aspects of data science, the third-party platforms underlying data science systems, and ways to engage with practitioners’ imagination of how systems can and should work.",2020.0,"Samir Passi, Phoebe Sengers"
68bee44cc58b1853c7ddcb41aa3c6d29f363637a,https://www.semanticscholar.org/paper/68bee44cc58b1853c7ddcb41aa3c6d29f363637a,Vamsa: Automated Provenance Tracking in Data Science Scripts,"There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.",2020.0,"M. Namaki, Avrilia Floratou, Fotis Psallidas, Subru Krishnan, Ashvin Agrawal, Yinghui Wu, Yiwen Zhu, Markus Weimer"
2d420c7f1675d41b01e694739a25b8b189f2c95f,https://www.semanticscholar.org/paper/2d420c7f1675d41b01e694739a25b8b189f2c95f,Data Science in the Food Industry.,"Food safety is one of the main challenges of the agri-food industry that is expected to be addressed in the current environment of tremendous technological progress, where consumers' lifestyles and preferences are in a constant state of flux. Food chain transparency and trust are drivers for food integrity control and for improvements in efficiency and economic growth. Similarly, the circular economy has great potential to reduce wastage and improve the efficiency of operations in multi-stakeholder ecosystems. Throughout the food chain cycle, all food commodities are exposed to multiple hazards, resulting in a high likelihood of contamination. Such biological or chemical hazards may be naturally present at any stage of food production, whether accidentally introduced or fraudulently imposed, risking consumers' health and their faith in the food industry. Nowadays, a massive amount of data is generated, not only from the next generation of food safety monitoring systems and along the entire food chain (primary production included) but also from the Internet of things, media, and other devices. These data should be used for the benefit of society, and the scientific field of data science should be a vital player in helping to make this possible.",2021.0,"George - John Nychas, E. Sims, P. Tsakanikas, F. Mohareb"
579b64d2179a58a8bc586c30850ea238d3c14164,https://www.semanticscholar.org/paper/579b64d2179a58a8bc586c30850ea238d3c14164,A Survey on Data Pricing: From Economics to Data Science,"Data are invaluable. How can we assess the value of data objectively, systematically and quantitatively? Pricing data, or information goods in general, has been studied and practiced in dispersed areas and principles, such as economics, marketing, electronic commerce, data management, data mining and machine learning. In this article, we present a unified, interdisciplinary and comprehensive overview of this important direction. We examine various motivations behind data pricing, understand the economics of data pricing and review the development and evolution of pricing models according to a series of fundamental principles. We discuss both digital products and data products. We also consider a series of challenges and directions for future work.",2020.0,J. Pei
840b60da93c2776230d3e6123d708e1c7e66ebc0,https://www.semanticscholar.org/paper/840b60da93c2776230d3e6123d708e1c7e66ebc0,Teaching Creative and Practical Data Science at Scale,"Abstract–Nolan and Temple Lang’s Computing in the Statistics Curricula (2010) advocated for a shift in statistical education to broadly include computing. In the time since, individuals with training in both computing and statistics have become increasingly employable in the burgeoning data science field. In response, universities have developed new courses and programs to meet the growing demand for data science education. To address this demand, we created Data Science in Practice, a large-enrollment undergraduate course. Here, we present our goals for teaching this course, including: (1) conceptualizing data science as creative problem solving, with a focus on project-based learning, (2) prioritizing practical application, teaching and using standardized tools and best practices, and (3) scaling education through coursework that enables hands-on and classroom learning in a large-enrollment course. Throughout this course we also emphasize social context and data ethics to best prepare students for the interdisciplinary and impactful nature of their work. We highlight creative problem solving and strategies for teaching automation-resilient skills, while providing students the opportunity to create a unique data science project that demonstrates their technical and creative capacities.",2020.0,"Thomas Donoghue, Bradley Voytek, Shannon E. Ellis"
8e234be4cdc34ea8deba609c31858198ad941797,https://www.semanticscholar.org/paper/8e234be4cdc34ea8deba609c31858198ad941797,Passing the Data Baton: A Retrospective Analysis on Data Science Work and Workers,"Data science is a rapidly growing discipline and organizations increasingly depend on data science work. Yet the ambiguity around data science, what it is, and who data scientists are can make it difficult for visualization researchers to identify impactful research trajectories. We have conducted a retrospective analysis of data science work and workers as described within the data visualization, human computer interaction, and data science literature. From this analysis we synthesis a comprehensive model that describes data science work and breakdown to data scientists into nine distinct roles. We summarise and reflect on the role that visualization has throughout data science work and the varied needs of data scientists themselves for tooling support. Our findings are intended to arm visualization researchers with a more concrete framing of data science with the hope that it will help them surface innovative opportunities for impacting data science work.",2020.0,"Anamaria Crisan, Brittany Fiore-Gartland, Melanie Tory"
41cf91ee13a1d15983ede066ddf6b67cc94a41f4,https://www.semanticscholar.org/paper/41cf91ee13a1d15983ede066ddf6b67cc94a41f4,The Role of Academia in Data Science Education,"As the demand for data scientists continues to grow, universities are trying to figure out how to best contribute to the training of a workforce. However, there does not appear to be a consensus on the fundamental principles, expertise, skills, or knowledge-base needed to define an academic discipline. We argue that data science is not a discipline but rather an umbrella term used to describe a complex process involving not one data scientist possessing all the necessary expertise, but a team of data scientists with nonoverlapping complementary skills. We provide some recommendations for how to take this into account when designing data science academic programs.",2020.0,R. Irizarry
4271faaa82eb722d079222211c30ab642bc734be,https://www.semanticscholar.org/paper/4271faaa82eb722d079222211c30ab642bc734be,The data science life cycle,A cycle that traces ways to define the landscape of data science.,2020.0,
4b5505a54799d796ae94115409b01ee33a7e2b20,https://www.semanticscholar.org/paper/4b5505a54799d796ae94115409b01ee33a7e2b20,Glossary for public health surveillance in the age of data science,"Public health surveillance is the ongoing systematic collection, analysis and interpretation of data, closely integrated with the timely dissemination of the resulting information to those responsible for preventing and controlling disease and injury. With the rapid development of data science, encompassing big data and artificial intelligence, and with the exponential growth of accessible and highly heterogeneous health-related data, from healthcare providers to user-generated online content, the field of surveillance and health monitoring is changing rapidly. It is, therefore, the right time for a short glossary of key terms in public health surveillance, with an emphasis on new data-science developments in the field.",2020.0,"A. Chiolero, D. Buckeridge"
b017bf6879e57077b4b4e180a02747b89878d7a1,https://www.semanticscholar.org/paper/b017bf6879e57077b4b4e180a02747b89878d7a1,A Fresh Look at Introductory Data Science,"ABSTRACT The proliferation of vast quantities of available datasets that are large and complex in nature has challenged universities to keep up with the demand for graduates trained in both the statistical and the computational set of skills required to effectively plan, acquire, manage, analyze, and communicate the findings of such data. To keep up with this demand, attracting students early on to data science as well as providing them a solid foray into the field becomes increasingly important. We present a case study of an introductory undergraduate course in data science that is designed to address these needs. Offered at Duke University, this course has no prerequisites and serves a wide audience of aspiring statistics and data science majors as well as humanities, social sciences, and natural sciences students. We discuss the unique set of challenges posed by offering such a course, and in light of these challenges, we present a detailed discussion into the pedagogical design elements, content, structure, computational infrastructure, and the assessment methodology of the course. We also offer a repository containing all teaching materials that are open-source, along with supplementary materials and the R code for reproducing the figures found in the article.",2020.0,"Mine Çetinkaya-Rundel, Victoria Ellison"
8ece479b5dfed4727d2d9b9763f777bb9a94096e,https://www.semanticscholar.org/paper/8ece479b5dfed4727d2d9b9763f777bb9a94096e,Human-AI Collaboration in Data Science,"The rapid advancement of artificial intelligence (AI) is changing our lives in many ways. One application domain is data science. New techniques in automating the creation of AI, known as AutoAI or AutoML, aim to automate the work practices of data scientists. AutoAI systems are capable of autonomously ingesting and pre-processing data, engineering new features, and creating and scoring models based on a target objectives (e.g. accuracy or run-time efficiency). Though not yet widely adopted, we are interested in understanding how AutoAI will impact the practice of data science. We conducted interviews with 20 data scientists who work at a large, multinational technology company and practice data science in various business settings. Our goal is to understand their current work practices and how these practices might change with AutoAI. Reactions were mixed: while informants expressed concerns about the trend of automating their jobs, they also strongly felt it was inevitable. Despite these concerns, they remained optimistic about their future job security due to a view that the future of data science work will be a collaboration between humans and AI systems, in which both automation and human expertise are indispensable.",2019.0,"Dakuo Wang, Justin D. Weisz, Michael J. Muller, Parikshit Ram, Werner Geyer, Casey Dugan, Y. Tausczik, Horst Samulowitz, Alexander G. Gray"
398b154013db9d8025bf60f910bc156dedd9b40e,https://www.semanticscholar.org/paper/398b154013db9d8025bf60f910bc156dedd9b40e,"How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation","With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.",2019.0,"Michael J. Muller, Ingrid Lange, Dakuo Wang, David Piorkowski, Jason Tsay, Q. Liao, Casey Dugan, T. Erickson"
a11e157cb828b800426223f0a3d79e8fb122c8cc,https://www.semanticscholar.org/paper/a11e157cb828b800426223f0a3d79e8fb122c8cc,Process Mining for Python (PM4Py): Bridging the Gap Between Process- and Data Science,"Process mining, i.e., a sub-field of data science focusing on the analysis of event data generated during the execution of (business) processes, has seen a tremendous change over the past two decades. Starting off in the early 2000's, with limited to no tool support, nowadays, several software tools, i.e., both open-source, e.g., ProM and Apromore, and commercial, e.g., Disco, Celonis, ProcessGold, etc., exist. The commercial process mining tools provide limited support for implementing custom algorithms. Moreover, both commercial and open-source process mining tools are often only accessible through a graphical user interface, which hampers their usage in large-scale experimental settings. Initiatives such as RapidProM provide process mining support in the scientific workflow-based data science suite RapidMiner. However, these offer limited to no support for algorithmic customization. In the light of the aforementioned, in this paper, we present a novel process mining library, i.e. Process Mining for Python (PM4Py) that aims to bridge this gap, providing integration with state-of-the-art data science libraries, e.g., pandas, numpy, scipy and scikit-learn. We provide a global overview of the architecture and functionality of PM4Py, accompanied by some representative examples of its usage.",2019.0,"A. Berti, S. V. Zelst, Wil M.P. van der Aalst"
28cc044d5ba938472bc53d87240583982ad21663,https://www.semanticscholar.org/paper/28cc044d5ba938472bc53d87240583982ad21663,Data Management for Data Science - Towards Embedded Analytics,"textabstractThe rise of Data Science has caused an influx of new usersin need of data management solutions. However, insteadof utilizing existing RDBMS solutions they are opting touse a stack of independent solutions for data storage andprocessing glued together by scripting languages. This is notbecause they do not need the functionality that an integratedRDBMS provides, but rather because existing RDBMS im-plementations do not cater to their use case. To solve theseissues, we propose a new class of data management systems:embedded analytical systems. These systems are tightlyintegrated with analytical tools, and provide fast and effi-cient access to the data stored within them. In this work,we describe the unique challenges and opportunities w.r.tworkloads, resilience and cooperation that are faced by thisnew class of systems and the steps we have taken towardsaddressing them in the DuckDB system.",2020.0,"Mark Raasveldt, H. Mühleisen"
38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,https://www.semanticscholar.org/paper/38f0c0f2567e074c775017e0e8dd1a43b1f6fcdd,"Data Science in 2020: Computing, Curricula, and Challenges for the Next 10 Years","Abstract In the past 10 years, new data science courses and programs have proliferated at the collegiate level. As faculty and administrators enter the race to provide data science training and attract new students, the road map for teaching data science remains elusive. In 2019, 69 college and university faculty teaching data science courses and developing data science curricula were surveyed to learn about their curricula, computing tools, and challenges they face in their classrooms. Faculty reported teaching a variety of computing skills in introductory data science (albeit fewer computing topics than statistics topics), and that one of the biggest challenges they face is teaching computing to a diverse audience with varying preparation. The ever-evolving nature of data science is a major hurdle for faculty teaching data science courses, and a call for more data science teaching resources was echoed in many responses.",2020.0,"Aimee Schwab-McCoy, C. Baker, Rebecca E. Gasper"
864a1791f5a0210abdc5f82152c4e6bc9b8ced63,https://www.semanticscholar.org/paper/864a1791f5a0210abdc5f82152c4e6bc9b8ced63,Web Scraping in the Statistics and Data Science Curriculum: Challenges and Opportunities,"Abstract Best practices in statistics and data science courses include the use of real and relevant data as well as teaching the entire data science cycle starting with importing data. A rich source of real and current data is the web, where data are often presented and stored in a structure that needs some wrangling and transforming before they can be ready for analysis. The web is a resource students naturally turn to for finding data for data analysis projects, but without formal instruction on how to get that data into a structured format, they often resort to copy-pasting or manual entry into a spreadsheet, which are both time consuming and error-prone. Teaching web scraping provides an opportunity to bring such data into the curriculum in an effective and efficient way. In this article, we explain how web scraping works and how it can be implemented in a pedagogically sound and technically executable way at various levels of statistics and data science curricula. We provide classroom activities where we connect this modern computing technique with traditional statistical topics. Finally, we share the opportunities web scraping brings to the classrooms as well as the challenges to instructors and tips for avoiding them.",2020.0,"M. Dogucu, Mine Çetinkaya-Rundel"
3746152e023e79b7d03cf12a560e473de2945d67,https://www.semanticscholar.org/paper/3746152e023e79b7d03cf12a560e473de2945d67,Interrogating Data Science,"Data science provides powerful tools and methods. CSCW researchers have contributed insightfulstudies of conventional work-practices in data science - and particularly machine learning. However,recent research has shown that human skills and collaborative decision-making, play important rolesin defining data, acquiring data, curating data, designing data, and creating data. This workshopgathers researchers and practitioners together to take a collective and critical look at data sciencework-practices, and at how those work-practices make crucial and often invisible impacts on theformal work of data science. When we understand the human and social contributions to data sciencepipelines, we can constructively redesign both work and technologies for new insights, theories, andchallenges.",2020.0,"Michael J. Muller, Cecilia M. Aragon, Shion Guha, M. Kogan, Gina Neff, Cathrine F. Seidelin, Katie Shilton, A. Tanweer"
d3edd5733dede8044de3f2e596b925eca4319eff,https://www.semanticscholar.org/paper/d3edd5733dede8044de3f2e596b925eca4319eff,Big Data Science on COVID-19 Data,"In the current era of big data, high volume of big data can be generated and collected from a wide variety of rich data sources at a rapid rate. Embedded in these big data are useful information and valuable knowledge. Examples include healthcare and epidemiological data such as data related to patients who suffered from viral diseases like the coronavirus disease 2019 (COVID-19). Knowledge discovered from these epidemiological data via data science helps researchers, epidemiologists and policy makers to get a better understanding of the disease, which may inspire them to come up ways to detect, control and combat the disease. In this paper, we present a data science solution for analyzing big COVID-19 epidemiological data. The solution helps users to get a better understanding of information about the confirmed cases of COVID-19. Evaluation results show the benefits of our data science solution in discovering useful knowledge from big COVID-19 data.",2020.0,"C. Leung, Yubo Chen, Siyuan Shang, Deyu Deng"
c209106b79366f16caec488852916c1f3c7d1dad,https://www.semanticscholar.org/paper/c209106b79366f16caec488852916c1f3c7d1dad,Ten Research Challenge Areas in Data Science,"Although data science builds on knowledge from computer science, mathematics, statistics, and other disciplines, data science is a unique field with many mysteries to unlock: challenging scientific questions and pressing questions of societal importance. This article starts with meta-questions about data science as a discipline and then elaborates on ten ideas for the basis of a research agenda for data science.",2020.0,Jeannette M. Wing
e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,https://www.semanticscholar.org/paper/e1c8f86668d3e37e430f187b7fd91d1643a0a0ff,Theory-Guided Data Science: A New Paradigm for Scientific Discovery from Data,"Data science models, although successful in a number of commercial domains, have had limited applicability in scientific problems involving complex physical phenomena. Theory-guided data science (TGDS) is an emerging paradigm that aims to leverage the wealth of scientific knowledge for improving the effectiveness of data science models in enabling scientific discovery. The overarching vision of TGDS is to introduce scientific consistency as an essential component for learning generalizable models. Further, by producing scientifically interpretable models, TGDS aims to advance our scientific understanding by discovering novel domain insights. Indeed, the paradigm of TGDS has started to gain prominence in a number of scientific disciplines such as turbulence modeling, material discovery, quantum chemistry, bio-medical science, bio-marker discovery, climate science, and hydrology. In this paper, we formally conceptualize the paradigm of TGDS and present a taxonomy of research themes in TGDS. We describe several approaches for integrating domain knowledge in different research themes using illustrative examples from different disciplines. We also highlight some of the promising avenues of novel research for realizing the full potential of theory-guided data science.",2016.0,"A. Karpatne, G. Atluri, James H. Faghmous, M. Steinbach, A. Banerjee, A. Ganguly, S. Shekhar, N. Samatova, Vipin Kumar"
27b9d1182e913decc7ef6a3509245fa6b6fd509d,https://www.semanticscholar.org/paper/27b9d1182e913decc7ef6a3509245fa6b6fd509d,Veridical data science,"Significance Predictability, computability, and stability (PCS) are three core principles of data science. They embed the scientific principles of prediction and replication in data-driven decision making while recognizing the central role of computation. Based on these principles, we propose the PCS framework, including workflow and documentation (in R Markdown or Jupyter Notebook). The PCS framework aims at responsible, reliable, reproducible, and transparent analysis across fields of science, social science, engineering, business, and government. It can be used as a recommendation system for scientific hypothesis generation and experimental design. In particular, we propose (basic) PCS inference for reliability measures on data results, extending statistical inference to a much broader scope as current data science practice entails. Building and expanding on principles of statistics, machine learning, and scientific inquiry, we propose the predictability, computability, and stability (PCS) framework for veridical data science. Our framework, composed of both a workflow and documentation, aims to provide responsible, reliable, reproducible, and transparent results across the data science life cycle. The PCS workflow uses predictability as a reality check and considers the importance of computation in data collection/storage and algorithm design. It augments predictability and computability with an overarching stability principle. Stability expands on statistical uncertainty considerations to assess how human judgment calls impact data results through data and model/algorithm perturbations. As part of the PCS workflow, we develop PCS inference procedures, namely PCS perturbation intervals and PCS hypothesis testing, to investigate the stability of data results relative to problem formulation, data cleaning, modeling decisions, and interpretations. We illustrate PCS inference through neuroscience and genomics projects of our own and others. Moreover, we demonstrate its favorable performance over existing methods in terms of receiver operating characteristic (ROC) curves in high-dimensional, sparse linear model simulations, including a wide range of misspecified models. Finally, we propose PCS documentation based on R Markdown or Jupyter Notebook, with publicly available, reproducible codes and narratives to back up human choices made throughout an analysis. The PCS workflow and documentation are demonstrated in a genomics case study available on Zenodo.",2019.0,Bin Yu
e2055b85dab66c922ccf25a28046e8e559074824,https://www.semanticscholar.org/paper/e2055b85dab66c922ccf25a28046e8e559074824,Algorithmic Government: Automating Public Services and Supporting Civil Servants in using Data Science Technologies,"The data science technologies of artificial intelligence (AI), Internet of Things (IoT), big data and behavioral/predictive analytics, and blockchain are poised to revolutionize government and create a new generation of GovTech start-ups. The impact from the ‘smartification’ of public services and the national infrastructure will be much more significant in comparison to any other sector given government's function and importance to every institution and individual. Potential GovTech systems include Chatbots and intelligent assistants for public engagement, Robo-advisors to support civil servants, real-time management of the national infrastructure using IoT and blockchain, automated compliance/regulation, public records securely stored in blockchain distributed ledgers, online judicial and dispute resolution systems, and laws/statutes encoded as blockchain smart contracts. Government is potentially the major ‘client’ and also ‘public champion’ for these new data technologies. This review paper uses our simple taxonomy of government services to provide an overview of data science automation being deployed by governments world-wide. The goal of this review paper is to encourage the Computer Science community to engage with government to develop these new systems to transform public services and support the work of civil servants.",2019.0,"Zeynep Engin, P. Treleaven"
f56425ec56586dcfd2694ab83643e9e76f314e91,https://www.semanticscholar.org/paper/f56425ec56586dcfd2694ab83643e9e76f314e91,50 Years of Data Science,"ABSTRACT More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a $100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.",2017.0,D. Donoho
590ead4aeddbf8fea8414998b2dc3b74576a71cb,https://www.semanticscholar.org/paper/590ead4aeddbf8fea8414998b2dc3b74576a71cb,A Second Chance to Get Causal Inference Right: A Classification of Data Science Tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term ""data science"" provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",2018.0,"M. Hernán, J. Hsu, B. Healy"
1ec4d0e29455e47245edaa17368257df3efb6562,https://www.semanticscholar.org/paper/1ec4d0e29455e47245edaa17368257df3efb6562,"Practitioners Teaching Data Science in Industry and Academia: Expectations, Workflows, and Challenges","Data science has been growing in prominence across both academia and industry, but there is still little formal consensus about how to teach it. Many people who currently teach data science are practitioners such as computational researchers in academia or data scientists in industry. To understand how these practitioner-instructors pass their knowledge onto novices and how that contrasts with teaching more traditional forms of programming, we interviewed 20 data scientists who teach in settings ranging from small-group workshops to large online courses. We found that: 1) they must empathize with a diverse array of student backgrounds and expectations, 2) they teach technical workflows that integrate authentic practices surrounding code, data, and communication, 3) they face challenges involving authenticity versus abstraction in software setup, finding and curating pedagogically-relevant datasets, and acclimating students to live with uncertainty in data analysis. These findings can point the way toward better tools for data science education and help bring data literacy to more people around the world.",2019.0,"Sean Kross, Philip J. Guo"
89f41c87c8849ce37e609c1010087291a4679a37,https://www.semanticscholar.org/paper/89f41c87c8849ce37e609c1010087291a4679a37,Outbreak analytics: a developing data science for informing the response to emerging pathogens,"Despite continued efforts to improve health systems worldwide, emerging pathogen epidemics remain a major public health concern. Effective response to such outbreaks relies on timely intervention, ideally informed by all available sources of data. The collection, visualization and analysis of outbreak data are becoming increasingly complex, owing to the diversity in types of data, questions and available methods to address them. Recent advances have led to the rise of outbreak analytics, an emerging data science focused on the technological and methodological aspects of the outbreak data pipeline, from collection to analysis, modelling and reporting to inform outbreak response. In this article, we assess the current state of the field. After laying out the context of outbreak response, we critically review the most common analytics components, their inter-dependencies, data requirements and the type of information they can provide to inform operations in real time. We discuss some challenges and opportunities and conclude on the potential role of outbreak analytics for improving our understanding of, and response to outbreaks of emerging pathogens. This article is part of the theme issue ‘Modelling infectious disease outbreaks in humans, animals and plants: epidemic forecasting and control‘. This theme issue is linked with the earlier issue ‘Modelling infectious disease outbreaks in humans, animals and plants: approaches and important themes’.",2019.0,"J. Polonsky, J. Polonsky, A. Baidjoe, Zhian N. Kamvar, A. Cori, K. Durski, W. Edmunds, R. Eggo, S. Funk, L. Kaiser, P. Keating, O. Waroux, O. Waroux, M. Marks, P. Moraga, O. Morgan, P. Nouvellet, P. Nouvellet, R. Ratnayake, C. Roberts, J. Whitworth, T. Jombart, T. Jombart"
2ad13329d44c74041626a60898ccf921b0bdacd3,https://www.semanticscholar.org/paper/2ad13329d44c74041626a60898ccf921b0bdacd3,SystemDS: A Declarative Machine Learning System for the End-to-End Data Science Lifecycle,"Machine learning (ML) applications become increasingly common in many domains. ML systems to execute these workloads include numerical computing frameworks and libraries, ML algorithm libraries, and specialized systems for deep neural networks and distributed ML. These systems focus primarily on efficient model training and scoring. However, the data science process is exploratory, and deals with underspecified objectives and a wide variety of heterogeneous data sources. Therefore, additional tools are employed for data engineering and debugging, which requires boundary crossing, unnecessary manual effort, and lacks optimization across the lifecycle. In this paper, we introduce SystemDS, an open source ML system for the end-to-end data science lifecycle from data integration, cleaning, and preparation, over local, distributed, and federated ML model training, to debugging and serving. To this end, we aim to provide a stack of declarative languages with R-like syntax for the different lifecycle tasks, and users with different expertise. We describe the overall system architecture, explain major design decisions (motivated by lessons learned from Apache SystemML), and discuss key features and research directions. Finally, we provide preliminary results that show the potential of end-to-end lifecycle optimization.",2019.0,"Matthias Boehm, I. Antonov, Mark Dokter, Robert Ginthoer, Kevin Innerebner, Florijan Klezin, Stefanie N. Lindstaedt, Arnab Phani, Benjamin Rath"
e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,https://www.semanticscholar.org/paper/e564e3656395782d0ab9f801bfbe9f9f1a5d34a7,Data science in data librarianship: Core competencies of a data librarian,"Currently, data are stored in an always-on condition, and can be globally accessed at any point, by any user. Data librarianship has its origins in the social sciences. In particular, the creation of data services and data archives, in the United Kingdom (Data Archives Services) and in the United States and Canada (Data Library Services), is a key factor for the emergence of data librarianship. The focus of data librarianship nowadays is on the creation of new library services. Data librarians are concerned with the proposition of services for data management and curation in academic libraries and other research organizations. The purpose of this paper is to understand how the complexity of the data can serve as the basis for identifying the technical skills required by data librarians. This essay is systematically divided, first introducing the concepts of data and research data in data librarianship, followed by an overview of data science as a theory, method, and technology to assess data. Next, the identification of the competencies and skills required by data scientists and data librarians are discussed. Our final remarks highlight that data librarians should understand that the complexity and novelty associated with data science praxis. Data science provides new methods and practices for data librarianship. A data librarian need not become a programmer, statistician, or database manager, but should be interested in learning about the languages and programming logic of computers, databases, and information retrieval tools. We believe that numerous kinds of scientific data research provide opportunities for a data librarian to engage with data science.",2019.0,"Alexandre Ribas Semeler, A. Pinto, Helen Beatriz Frota Rozados"
2081ed6854290a479f796f2432c7951ff24232fe,https://www.semanticscholar.org/paper/2081ed6854290a479f796f2432c7951ff24232fe,Human-Centered Study of Data Science Work Practices,"With the rise of big data, there has been an increasing need to understand who is working in data science and how they are doing their work. HCI and CSCW researchers have begun to examine these questions. In this workshop, we invite researchers to share their observations, experiences, hypotheses, and insights, in the hopes of developing a taxonomy of work practices and open issues in the behavioral and social study of data science and data science workers.",2019.0,"Michael J. Muller, Melanie Feinberg, Timothy George, S. Jackson, Bonnie E. John, Mary Beth Kery, Samir Passi"
0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,https://www.semanticscholar.org/paper/0e23ff1f915b6af32bf1a1107ee7e15ebe10efe8,The Challenge of Big Data and Data Science,"Big data and data science are transforming the world in ways that spawn new concerns for social scientists, such as the impacts of the internet on citizens and the media, the repercussions of smart cities, the possibilities of cyber-warfare and cyber-terrorism, the implications of precision medicine, and the consequences of artificial intelligence and automation. Along with these changes in society, powerful new data science methods support research using administrative, internet, textual, and sensor-audio-video data. Burgeoning data and innovative methods facilitate answering previously hard-to-tackle questions about society by offering new ways to form concepts from data, to do descriptive inference, to make causal inferences, and to generate predictions. They also pose challenges as social scientists must grasp the meaning of concepts and predictions generated by convoluted algorithms, weigh the relative value of prediction versus causal inference, and cope with ethical challenges as their methods, such as algorithms for mobilizing voters or determining bail, are adopted by policy makers.",2019.0,Henry E. Brady
2ab2796390ac12df283e218907ed0ffef232dbc7,https://www.semanticscholar.org/paper/2ab2796390ac12df283e218907ed0ffef232dbc7,Situating Data Science: Exploring How Relationships to Data Shape Learning,"The emerging field of Data Science has had a large impact on science and society. This has led to over a decade of calls to establish a corresponding field of Data Science Education. There is still a need, however, to more deeply conceptualize what a field of Data Science Education might entail in terms of scope, responsibility, and execution. This special issue explores how one distinguishing feature of Data Science—its focus on data collected from social and environmental contexts within which learners often find themselves deeply embedded—suggests serious implications for learning and education. The learning sciences is uniquely positioned to investigate how such contextual embeddings impact learners’ engagement with data including conceptual, experiential, communal, racialized, spatial, and political dimensions. This special issue demonstrates the richly layered relationships learners build with data and reveals them to be not merely utilitarian mechanisms for learning about data, but a critical part of navigating data as social text and understanding Data Science as a discipline. Together, the contributions offer a vision of how the learning sciences can contribute to a more expansive, agentive and socially aware Data Science Education.",2019.0,"Michelle Wilkerson, J. Polman"
5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,https://www.semanticscholar.org/paper/5c8b7127ad0b5257f81ce1aa70b89faa97bbc211,Data Science of the Natural Environment: A Research Roadmap,"Data science is the science of extracting meaning from potentially complex data. This is a fast moving field, drawing principles and techniques from a number of different disciplinary areas including computer science, statistics and complexity science. Data science is having a profound impact on a number of areas including commerce, health and smart cities. This paper argues that data science can have an equal if not greater impact in the area of earth and environmental sciences, offering a rich tapestry of new techniques to support both a deeper understanding of the natural environment in all its complexities, as well as the development of well-founded mitigation and adaptation strategies in the face of climate change. The paper argues that data science for the natural environment brings about new challenges for data science, particularly around complexity, spatial and temporal reasoning, and managing uncertainty. The paper also describes a case study in environmental data science which offers up insights into the promise of the area. The paper concludes with a research roadmap highlighting ten top challenges of environmental data science and also an invitation to become part of an international community working collaboratively on these problems.",2019.0,"G. Blair, P. Henrys, A. Leeson, J. Watkins, E. Eastoe, S. Jarvis, P. Young"
e799d31e1c2d80a971c1f956d62b98c0a9f27031,https://www.semanticscholar.org/paper/e799d31e1c2d80a971c1f956d62b98c0a9f27031,Big Data and data science: A critical review of issues for educational research,"Big Data refers to large and disparate volumes of data generated by people, applications and machines. It is gaining increasing attention from a variety of domains, including education. What are the challenges of engaging with Big Data research in education? This paper identifies a wide range of critical issues that researchers need to consider when working with Big Data in education. The issues identified include diversity in the conception and meaning of Big Data in education, ontological, epistemological disparity, technical challenges, ethics and privacy, digital divide and digital dividend, lack of expertise and academic development opportunities to prepare educational researchers to leverage opportunities afforded by Big Data. The goal of this paper is to raise awareness on these issues and initiate a dialogue. The paper was inspired partly by insights drawn from the literature but mostly informed by experience researching into Big Data in education. [ABSTRACT FROM AUTHOR]",2019.0,B. Daniel
b00f836c62d0ea7678d0f20aeec3397138633060,https://www.semanticscholar.org/paper/b00f836c62d0ea7678d0f20aeec3397138633060,Health Care and Precision Medicine Research: Analysis of a Scalable Data Science Platform,"Background Health care data are increasing in volume and complexity. Storing and analyzing these data to implement precision medicine initiatives and data-driven research has exceeded the capabilities of traditional computer systems. Modern big data platforms must be adapted to the specific demands of health care and designed for scalability and growth. Objective The objectives of our study were to (1) demonstrate the implementation of a data science platform built on open source technology within a large, academic health care system and (2) describe 2 computational health care applications built on such a platform. Methods We deployed a data science platform based on several open source technologies to support real-time, big data workloads. We developed data-acquisition workflows for Apache Storm and NiFi in Java and Python to capture patient monitoring and laboratory data for downstream analytics. Results Emerging data management approaches, along with open source technologies such as Hadoop, can be used to create integrated data lakes to store large, real-time datasets. This infrastructure also provides a robust analytics platform where health care and biomedical research data can be analyzed in near real time for precision medicine and computational health care use cases. Conclusions The implementation and use of integrated data science platforms offer organizations the opportunity to combine traditional datasets, including data from the electronic health record, with emerging big data sources, such as continuous patient monitoring and real-time laboratory results. These platforms can enable cost-effective and scalable analytics for the information that will be key to the delivery of precision medicine initiatives. Organizations that can take advantage of the technical advances found in data science platforms will have the opportunity to provide comprehensive access to health care data for computational health care and precision medicine research.",2019.0,"J. McPadden, T. Durant, Dustin R. Bunch, A. Coppi, Nathaniel Price, Kris Rodgerson, C. Torre, William Byron, A. Hsiao, H. Krumholz, W. Schulz"
747359803e9a734fa4f1338a83121a942f3da60e,https://www.semanticscholar.org/paper/747359803e9a734fa4f1338a83121a942f3da60e,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",2019.0,"A. Singleton, Daniel Arribas-Bel"
4082dbd56578313f219fbd88c87899b1ffe1600a,https://www.semanticscholar.org/paper/4082dbd56578313f219fbd88c87899b1ffe1600a,“You Social Scientists Love Mind Games”: Experimenting in the “divide” between data science and critical algorithm studies,"In recent years, many qualitative sociologists, anthropologists, and social theorists have critiqued the use of algorithms and other automated processes involved in data science on both epistemological and political grounds. Yet, it has proven difficult to bring these important insights into the practice of data science itself. We suggest that part of this problem has to do with under-examined or unacknowledged assumptions about the relationship between the two fields—ideas about how data science and its critics can and should relate. Inspired by recent work in Science and Technology Studies on interventions, we attempted to stage an encounter in which practicing data scientists were asked to analyze a corpus of critical social science literature about their work, using tools of textual analysis such as co-word and topic modelling. The idea was to provoke discussion both about the content of these texts and the possible limits of such analyses. In this commentary, we reflect on the planning stages of the experiment and how responses to the exercise, from both data scientists and qualitative social scientists, revealed some of the tensions and interactions between the normative positions of the different fields. We argue for further studies which can help us understand what these interdisciplinary tensions turn on—which do not paper over them but also do not take them as given.",2019.0,"David Moats, Nick Seaver"
747359803e9a734fa4f1338a83121a942f3da60e,https://www.semanticscholar.org/paper/747359803e9a734fa4f1338a83121a942f3da60e,Geographic Data Science,"It is widely acknowledged that the emergence of “Big Data” is having a profound and often controversial impact on the production of knowledge. In this context, Data Science has developed as an interdisciplinary approach that turns such “Big Data” into information. This article argues for the positive role that Geography can have on Data Science when being applied to spatially explicit problems; and inversely, makes the case that there is much that Geography and Geographical Analysis could learn from Data Science. We propose a deeper integration through an ambitious research agenda, including systems engineering, new methodological development, and work toward addressing some acute challenges around epistemology. We argue that such issues must be resolved in order to realize a Geographic Data Science, and that such goal would be a desirable one.",2019.0,"A. Singleton, Daniel Arribas-Bel"
4082dbd56578313f219fbd88c87899b1ffe1600a,https://www.semanticscholar.org/paper/4082dbd56578313f219fbd88c87899b1ffe1600a,“You Social Scientists Love Mind Games”: Experimenting in the “divide” between data science and critical algorithm studies,"In recent years, many qualitative sociologists, anthropologists, and social theorists have critiqued the use of algorithms and other automated processes involved in data science on both epistemological and political grounds. Yet, it has proven difficult to bring these important insights into the practice of data science itself. We suggest that part of this problem has to do with under-examined or unacknowledged assumptions about the relationship between the two fields—ideas about how data science and its critics can and should relate. Inspired by recent work in Science and Technology Studies on interventions, we attempted to stage an encounter in which practicing data scientists were asked to analyze a corpus of critical social science literature about their work, using tools of textual analysis such as co-word and topic modelling. The idea was to provoke discussion both about the content of these texts and the possible limits of such analyses. In this commentary, we reflect on the planning stages of the experiment and how responses to the exercise, from both data scientists and qualitative social scientists, revealed some of the tensions and interactions between the normative positions of the different fields. We argue for further studies which can help us understand what these interdisciplinary tensions turn on—which do not paper over them but also do not take them as given.",2019.0,"David Moats, Nick Seaver"
702cd9a7a128706b8a6ec88e7424e06c326021e5,https://www.semanticscholar.org/paper/702cd9a7a128706b8a6ec88e7424e06c326021e5,Upscaling urban data science for global climate solutions,"Non-technical summary Manhattan, Berlin and New Delhi all need to take action to adapt to climate change and to reduce greenhouse gas emissions. While case studies on these cities provide valuable insights, comparability and scalability remain sidelined. It is therefore timely to review the state-of-the-art in data infrastructures, including earth observations, social media data, and how they could be better integrated to advance climate change science in cities and urban areas. We present three routes for expanding knowledge on global urban areas: mainstreaming data collections, amplifying the use of big data and taking further advantage of computational methods to analyse qualitative data to gain new insights. These data-based approaches have the potential to upscale urban climate solutions and effect change at the global scale. Technical summary Cities have an increasingly integral role in addressing climate change. To gain a common understanding of solutions, we require adequate and representative data of urban areas, including data on related greenhouse gas emissions, climate threats and of socio-economic contexts. Here, we review the current state of urban data science in the context of climate change, investigating the contribution of urban metabolism studies, remote sensing, big data approaches, urban economics, urban climate and weather studies. We outline three routes for upscaling urban data science for global climate solutions: 1) Mainstreaming and harmonizing data collection in cities worldwide; 2) Exploiting big data and machine learning to scale solutions while maintaining privacy; 3) Applying computational techniques and data science methods to analyse published qualitative information for the systematization and understanding of first-order climate effects and solutions. Collaborative efforts towards a joint data platform and integrated urban services would provide the quantitative foundations of the emerging global urban sustainability science.",2019.0,"F. Creutzig, S. Lohrey, X. Bai, A. Baklanov, R. Dawson, S. Dhakal, W. Lamb, T. McPhearson, J. Minx, Esteban Muñoz, Brenna Walsh"
3335c340c20609b4e6de481c9eaf67ecd6c960dc,https://www.semanticscholar.org/paper/3335c340c20609b4e6de481c9eaf67ecd6c960dc,Evaluation of a Tree-based Pipeline Optimization Tool for Automating Data Science,"As the field of data science continues to grow, there will be an ever-increasing demand for tools that make machine learning accessible to non-experts. In this paper, we introduce the concept of tree-based pipeline optimization for automating one of the most tedious parts of machine learning--pipeline design. We implement an open source Tree-based Pipeline Optimization Tool (TPOT) in Python and demonstrate its effectiveness on a series of simulated and real-world benchmark data sets. In particular, we show that TPOT can design machine learning pipelines that provide a significant improvement over a basic machine learning analysis while requiring little to no input nor prior knowledge from the user. We also address the tendency for TPOT to design overly complex pipelines by integrating Pareto optimization, which produces compact pipelines without sacrificing classification accuracy. As such, this work represents an important step toward fully automating machine learning pipeline design.",2016.0,"Randal S. Olson, Nathan Bartley, R. Urbanowicz, J. Moore"
ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,https://www.semanticscholar.org/paper/ffdb6039a5d82f8edd70b2d177074c2f2c89e97f,Data Science as Political Action: Grounding Data Science in a Politics of Justice,"In response to recent controversies, the field of data science has rushed to adopt codes of ethics. Such professional codes, however, are ill-equipped to address broad matters of social justice. Instead of ethics codes, I argue, the field must embrace politics. Data scientists must recognize themselves as political actors engaged in normative constructions of society and, as befits political work, evaluate their work according to its downstream material impacts on people's lives. I justify this notion in two parts: first, by articulating why data scientists must recognize themselves as political actors, and second, by describing how the field can evolve toward a deliberative and rigorous grounding in a politics of social justice. Part 1 responds to three arguments that are commonly invoked by data scientists when they are challenged to take political positions regarding their work. In confronting these arguments, I will demonstrate why attempting to remain apolitical is itself a political stance--a fundamentally conservative one--and why the field's current attempts to promote ""social good"" dangerously rely on vague and unarticulated political assumptions. Part 2 proposes a framework for what a politically-engaged data science could look like and how to achieve it, recognizing the challenge of reforming the field in this manner. I conceptualize the process of incorporating politics into data science in four stages: becoming interested in directly addressing social issues, recognizing the politics underlying these issues, redirecting existing methods toward new applications, and, finally, developing new practices and methods that orient data science around a mission of social justice. The path ahead does not require data scientists to abandon their technical expertise, but it does entail expanding their notions of what problems to work on and how to engage with society.",2018.0,Ben Green
bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,https://www.semanticscholar.org/paper/bf12943b1862cbdf556ba1ddcdbc685d4f38a6c3,Realizing the potential of data science,"Data science promises new insights, helping transform information into knowledge that can drive science and industry.",2018.0,"F. Berman, Rob A. Rutenbar, B. Hailpern, Henrik Christensen, S. Davidson, D. Estrin, Michael J. Franklin, M. Martonosi, Padma Raghavan, V. Stodden, A. Szalay"
bb44d1472bb281c699ef556f6eb6ccc66889f2d3,https://www.semanticscholar.org/paper/bb44d1472bb281c699ef556f6eb6ccc66889f2d3,Data Science and Machine Learning,"The purpose of Data Science and Machine Learning: Mathematical and Statistical Methods is to provide an accessible, yet comprehensive textbook intended for students interested in gaining a better understanding of the mathematics and statistics that underpin the rich variety of ideas and machine learning algorithms in data science.",2019.0,"Dirk P. Kroese, Z. Botev, T. Taimre, Radislav Vaisman"
6bec0106bebc93fc30ec47af9779d7e327639034,https://www.semanticscholar.org/paper/6bec0106bebc93fc30ec47af9779d7e327639034,Machine learning and data science in soft materials engineering,"In many branches of materials science it is now routine to generate data sets of such large size and dimensionality that conventional methods of analysis fail. Paradigms and tools from data science and machine learning can provide scalable approaches to identify and extract trends and patterns within voluminous data sets, perform guided traversals of high-dimensional phase spaces, and furnish data-driven strategies for inverse materials design. This topical review provides an accessible introduction to machine learning tools in the context of soft and biological materials by ‘de-jargonizing’ data science terminology, presenting a taxonomy of machine learning techniques, and surveying the mathematical underpinnings and software implementations of popular tools, including principal component analysis, independent component analysis, diffusion maps, support vector machines, and relative entropy. We present illustrative examples of machine learning applications in soft matter, including inverse design of self-assembling materials, nonlinear learning of protein folding landscapes, high-throughput antimicrobial peptide design, and data-driven materials design engines. We close with an outlook on the challenges and opportunities for the field.",2018.0,Andrew L. Ferguson
bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,https://www.semanticscholar.org/paper/bb6adeeb3a21479cc45490a5c2ff6d8dd5e77603,Knowledge-based Biomedical Data Science 2019,"Knowledge-based biomedical data science involves the design and implementation of computer systems that act as if they knew about biomedicine. Such systems depend on formally represented knowledge in computer systems, often in the form of knowledge graphs. Here we survey recent progress in systems that use formally represented knowledge to address data science problems in both clinical and biological domains, as well as progress on approaches for creating knowledge graphs. Major themes include the relationships between knowledge graphs and machine learning, the use of natural language processing to construct knowledge graphs, and the expansion of novel knowledge-based approaches to clinical and biological domains.",2019.0,"T. Callahan, Harrison Pielke-Lombardo, Ignacio J. Tripodi, L. Hunter"
ff37c528ed2ed5b952acb68a09673290fa6d7c99,https://www.semanticscholar.org/paper/ff37c528ed2ed5b952acb68a09673290fa6d7c99,Data Science,"The Master of Data Science (Non-Thesis) program is designed to give candidates a foundation in statistics and computer science and also provide knowledge in a particular application domain of science or engineering. The balance between these three elements is a strength of the program and can prepare candidates for Data Science careers in industry, government, or for further study at the PhD level. Throughout is an emphasis on working in teams, creative problem solving, and professional development.",2020.0,Field Cady
140a6476f7b8dde9e7bbcd199d248fc629721faa,https://www.semanticscholar.org/paper/140a6476f7b8dde9e7bbcd199d248fc629721faa,Trust in Data Science,"The trustworthiness of data science systems in applied and real-world settings emerges from the resolution of specific tensions through situated, pragmatic, and ongoing forms of work. Drawing on research in CSCW, critical data studies, and history and sociology of science, and six months of immersive ethnographic fieldwork with a corporate data science team, we describe four common tensions in applied data science work: (un)equivocal numbers, (counter)intuitive knowledge, (in)credible data, and (in)scrutable models. We show how organizational actors establish and re-negotiate trust under messy and uncertain analytic conditions through practices of skepticism, assessment, and credibility. Highlighting the collaborative and heterogeneous nature of real-world data science, we show how the management of trust in applied corporate data science settings depends not only on pre-processing and quantification, but also on negotiation and translation. We conclude by discussing the implications of our findings for data science research and practice, both within and beyond CSCW.",2018.0,"Samir Passi, S. Jackson"
daec8baf1740a09725b375729d95caebc42f61c8,https://www.semanticscholar.org/paper/daec8baf1740a09725b375729d95caebc42f61c8,ACM Task Force on Data Science Education: Draft Report and Opportunity for Feedback,"The ACM Data Science Task Force was established by the ACM Education Council and tasked with articulating the role of computing discipline-specific contributions to this emerging field. This special session seeks to introduce the work of the ACM Data Science Task Force as well as to engage the SIGCSE community in this effort. Members of the task force will introduce key components of a draft report, including a summary of data science curricular efforts to date, results of ACM academic and industry surveys on data science, as well as the initial articulation of computing competencies for undergraduate programs in data science. This session should be of interest to all SIGCSE attendees, but especially faculty developing college-level curricula in Data Science.",2019.0,"A. Danyluk, Paul M. Leidig, L. Cassel, C. Servin"
31de1d23b2f1f1c14ce7025489d2892aa10fb1ef,https://www.semanticscholar.org/paper/31de1d23b2f1f1c14ce7025489d2892aa10fb1ef,A review of data science in business and industry and a future view,"The aim of this paper is to frame Data Science, a fashion and emerging topic nowadays in the context of business and industry. We open with a discussion about the origin of Data Science, and its requirement for a challenging mix of capability in data analytics, information technology and business know-how. The mission of Data Science is to provide new or revised computational theory able to extract useful information from the massive volumes of data collected at an accelerating pace. In fact, besides the traditional measurements, digital data obtained from images, text, audio, sensors, etc complement the survey. Then we review the different and most popular methodologies amongst the practitioners of Data Science research and applications. And since the emerging field requires personnel with new competences, we attempt to describe the Data Scientist profile, one of the sexiest jobs of the 21st Century according to Davenport and Patil. Most people are aware of the need to embrace Data Science, but they feel intimidated that they don’t understand it and they worry that their jobs will disappear. We want to encourage them: Data Science is more likely to add value to jobs and enrich the lives of working people by helping them make better, more informed business decisions. We conclude the paper by presenting examples of Data Science in action in business and industry, to demonstrate the collection of specialist skills that must come together for this new science to be effective.",2019.0,"Grazia Vicario, S. Coleman"
305600f3cba8a63bad1bedeab34a299bf748754b,https://www.semanticscholar.org/paper/305600f3cba8a63bad1bedeab34a299bf748754b,Northstar: An Interactive Data Science System,"In order to democratize data science, we need to fundamentally rethink the current analytics stack, from the user interface to the ""guts."" Most importantly, enabling a broader range of users to unfold the potential of (their) data requires a change in the interface and the ""protection"" we offer them. On the one hand, visual interfaces for data science have to be intuitive, easy, and interactive to reach users without a strong background in computer science or statistics. On the other hand, we need to protect users from making false discoveries. Furthermore, it requires that technically involved (and often boring) tasks have to be automatically done by the system so that the user can focus on contributing their domain expertise to the problem. In this paper, we present Northstar, the Interactive Data Science System, which we have developed over the last 4 years to explore designs that make advanced analytics and model building more accessible.",2018.0,Tim Kraska
6bf9d589f80823735084956f056728ae1a7bcfa8,https://www.semanticscholar.org/paper/6bf9d589f80823735084956f056728ae1a7bcfa8,"Situating Ecology as a Big-Data Science: Current Advances, Challenges, and Solutions","Ecology has joined a world of big data. Two complementary frameworks define big data: data that exceed the analytical capacities of individuals or disciplines or the “Four Vs” axes of volume, variety, veracity, and velocity. Variety predominates in ecoinformatics and limits the scalability of ecological science. Volume varies widely. Ecological velocity is low but growing as data throughput and societal needs increase. Ecological big-data systems include in situ and remote sensors, community data resources, biodiversity databases, citizen science, and permanent stations. Technological solutions include the development of open code- and data-sharing platforms, flexible statistical models that can handle heterogeneous data and sources of uncertainty, and cloud-computing delivery of high-velocity computing to large-volume analytics. Cultural solutions include training targeted to early and current scientific workforce and strengthening collaborations among ecologists and data scientists. The broader goal is to maximize the power, scalability, and timeliness of ecological insights and forecasting.",2018.0,"S. Farley, A. Dawson, S. Goring, John W. Williams"
b85ac20631159ca3e370afa9c1f81a4618242b4f,https://www.semanticscholar.org/paper/b85ac20631159ca3e370afa9c1f81a4618242b4f,The Democratization of Data Science Education,"Abstract Over the last three decades, data have become ubiquitous and cheap. This transition has accelerated over the last five years and training in statistics, machine learning, and data analysis has struggled to keep up. In April 2014, we launched a program of nine courses, the Johns Hopkins Data Science Specialization, which has now had more than 4 million enrollments over the past five years. Here, the program is described and compared to standard data science curricula as they were organized in 2014 and 2015. We show that novel pedagogical and administrative decisions introduced in our program are now standard in online data science programs. The impact of the Data Science Specialization on data science education in the U.S. is also discussed. Finally, we conclude with some thoughts about the future of data science education in a data democratized world.",2019.0,"Sean Kross, R. Peng, B. Caffo, Ira Gooding, J. Leek"
36708c11c2fde2efb50e75d81f174b2c205082c8,https://www.semanticscholar.org/paper/36708c11c2fde2efb50e75d81f174b2c205082c8,What is responsible and sustainable data science?,"In the expansion of health ecosystems, issues of responsibility and sustainability of the data science involved are central. The idea that these values should be central to the practice of data science is increasingly gaining traction, yet there is no agreement on what exactly makes data science responsible or sustainable because these concepts prove slippery when applied to a global field involving commercial, academic and governmental actors. This lack of clarity is causing problems in setting goals and boundaries for data scientific practice, and risks fundamental disagreement on governance principles for this emerging field. We will argue in this commentary for a commons analytical framework as one approach to this problem, since it offers useful signposts for how to establish governance principles for shared resources.",2019.0,"L. Taylor, Nadezhda Purtova"
46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,https://www.semanticscholar.org/paper/46f1c45c62b7dbf77af405f5ddcf137b5e1ddde9,Data science from a library and information science perspective,"
Purpose
Data science is a relatively new field which has gained considerable attention in recent years. This new field requires a wide range of knowledge and skills from different disciplines including mathematics and statistics, computer science and information science. The purpose of this paper is to present the results of the study that explored the field of data science from the library and information science (LIS) perspective.


Design/methodology/approach
Analysis of research publications on data science was made on the basis of papers published in the Web of Science database. The following research questions were proposed: What are the main tendencies in publication years, document types, countries of origin, source titles, authors of publications, affiliations of the article authors and the most cited articles related to data science in the field of LIS? What are the main themes discussed in the publications from the LIS perspective?


Findings
The highest contribution to data science comes from the computer science research community. The contribution of information science and library science community is quite small. However, there has been continuous increase in articles from the year 2015. The main document types are journal articles, followed by conference proceedings and editorial material. The top three journals that publish data science papers from the LIS perspective are the Journal of the American Medical Informatics Association, the International Journal of Information Management and the Journal of the Association for Information Science and Technology. The top five countries publishing are USA, China, England, Australia and India. The most cited article has got 112 citations. The analysis revealed that the data science field is quite interdisciplinary by nature. In addition to the field of LIS the papers belonged to several other research areas. The reviewed articles belonged to the six broad categories: data science education and training; knowledge and skills of the data professional; the role of libraries and librarians in the data science movement; tools, techniques and applications of data science; data science from the knowledge management perspective; and data science from the perspective of health sciences.


Research limitations/implications
The limitations of this research are that this study only analyzed research papers in the Web of Science database and therefore only covers a certain amount of scientific papers published in the field of LIS. In addition, only publications with the term “data science” in the topic area of the Web of Science database were analyzed. Therefore, several relevant studies are not discussed in this paper that are not reflected in the Web of Science database or were related to other keywords such as “e-science,” “e-research,” “data service,” “data curation” or “research data management.”


Originality/value
The field of data science has not been explored using bibliographic analysis of publications from the perspective of the LIS. This paper helps to better understand the field of data science and the perspectives for information professionals.
",2019.0,"S. Virkus, E. Garoufallou"
1b620faa43b4f812f3c5b09feb18c9943eda6ff3,https://www.semanticscholar.org/paper/1b620faa43b4f812f3c5b09feb18c9943eda6ff3,DATA SCIENCE: CHALLENGES AND TRENDS,"The technological revolution has led to an explosion of data in domains of knowledge ranging from medicine to social science and from commerce to high energy physics. Data science is the study of extracting value from data. It combines insights, techniques, and tools from computer science, statistics, social science, and elsewhere. The minor program in data science is intended to equip students with computational and analytical comprehension and tools that will allow them to work on a variety of data-driven problems in any discipline. The program also emphasizes important issues in data privacy, ethics, and communication.",2019.0,"Katia Rasheva-Yordanova, Stefka Toleva-Stoimenova, D. Christozov"
a2d7efb8b174702111e713765cbf741dff2bf9b8,https://www.semanticscholar.org/paper/a2d7efb8b174702111e713765cbf741dff2bf9b8,Searching for Hidden Perovskite Materials for Photovoltaic Systems by Combining Data Science and First Principle Calculations,"Undiscovered perovskite materials for applications in capturing solar lights are explored through the implementation of data science. In particular, 15000 perovskite materials data is analyzed where visualization of the data reveals hidden trends and clustering of data. Random forest classification within machine learning is used in order to predict the band gap of perovskite materials where 18 physical descriptors are revealed to determine the band gap. With trained random forest, 9328 perovskite materials with potential for applications in solar cell materials are predicted. The selected Li and Na based perovskite materials within predicted 9328 perovskite materials are evaluated with first principle calculations where 11 undiscovered Li(Na) based perovskite materials fall into the ideal band gap and formation energy ranges for solar cell applications. Thus, the implementation of data science accelerates the discovery of hidden perovskite materials and the approach can be applied to the materials scienc...",2018.0,"Keisuke Takahashi, Lauren Takahashi, Itsuki Miyazato, Yuzuru Tanaka"
08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,https://www.semanticscholar.org/paper/08468bac470e5c2cbbd2b66e8e7cf2ab65f38e02,Data Science for Local Government,"The Data Science for Local Government project was about understanding how the growth of ‘data science’ is changing the way that local government works in the UK. We define data science as a dual shift which involves both bringing in new decision making and analytical techniques to local government work (e.g. machine learning and predictive analytics, artificial intelligence and A/B testing) and also expanding the types of data local government makes use of (for example, by repurposing administrative data, harvesting social media data, or working with mobile phone companies). The emergence of data science is facilitated by the growing availability of free, open-source tools for both collecting data and performing analysis. Based on extensive documentary review, a nationwide survey of local authorities, and in-depth interviews with over 30 practitioners, we have sought to produce a comprehensive guide to the different types of data science being undertaken in the UK, the types of opportunities and benefits created, and also some of the challenges and difficulties being encountered. Our aim was to provide a basis for people working in local government to start on their own data science projects, both by providing a library of dozens of ideas which have been tried elsewhere and also by providing hints and tips for overcoming key problems and challenges.",2019.0,"Jonathan Bright, B. Ganesh, Cathrine F. Seidelin, Thomas M. Vogl"
4aeda303fa0b9beae3f6d65e052dace9d4540116,https://www.semanticscholar.org/paper/4aeda303fa0b9beae3f6d65e052dace9d4540116,Data Science Support at the Academic Library,"Abstract Data science is a rapidly growing field with applications across all scientific domains. The demand for support in data science literacy is outpacing available resources at college campuses. The academic library is uniquely positioned to provide training and guidance in a number of areas relevant to data science. The University of Arizona Libraries has built a successful data science support program, focusing on computational literacy, geographic information systems, and reproducible science. Success of the program has largely been due to the strength of library personnel and strategic partnerships with units outside of the library. Academic libraries can support campus data science needs through professional development of current staff and recruitment of new personnel with expertise in data-intensive domains.",2019.0,"J. Oliver, Chris Kollen, B. Hickson, F. Rios"
2146edb37621d80f53c1261c8a53c94d3dda84c8,https://www.semanticscholar.org/paper/2146edb37621d80f53c1261c8a53c94d3dda84c8,Smart Blockchain Badges for Data Science Education,"Blockchain technology has the potential to revolutionise education in a number of ways. In this paper, we explore the applications of Smart Blockchain Badges on data science education. In particular, we investigate how Smart Blockchain Badges can support learners that want to advance their careers in data science, by offering them personalised recommendations based on their learning achievements. This work aims at enhancing data science accreditation by introducing a robust system based on the Blockchain technology. Learners will benefit from a sophisticated, open and transparent accreditation system, as well as from receiving job recommendations that match their skills and can potentially progress their careers. As a result, this work contributes towards closing the data science skills gap by linking data science education to the industry.",2018.0,"A. Mikroyannidis, John Domingue, Michelle Bachler, K. Quick"
e4c66275e46a66586365c851f0974a3c88baf3d7,https://www.semanticscholar.org/paper/e4c66275e46a66586365c851f0974a3c88baf3d7,Network embedding in biomedical data science,"Owning to the rapid development of computer technologies, an increasing number of relational data have been emerging in modern biomedical research. Many network-based learning methods have been proposed to perform analysis on such data, which provide people a deep understanding of topology and knowledge behind the biomedical networks and benefit a lot of applications for human healthcare. However, most network-based methods suffer from high computational and space cost. There remain challenges on handling high dimensionality and sparsity of the biomedical networks. The latest advances in network embedding technologies provide new effective paradigms to solve the network analysis problem. It converts network into a low-dimensional space while maximally preserves structural properties. In this way, downstream tasks such as link prediction and node classification can be done by traditional machine learning methods. In this survey, we conduct a comprehensive review of the literature on applying network embedding to advance the biomedical domain. We first briefly introduce the widely used network embedding models. After that, we carefully discuss how the network embedding approaches were performed on biomedical networks as well as how they accelerated the downstream tasks in biomedical science. Finally, we discuss challenges the existing network embedding applications in biomedical domains are faced with and suggest several promising future directions for a better improvement in human healthcare.",2018.0,"Chang Su, Jie Tong, Yongjun Zhu, Peng Cui, Fei Wang"
5a44f70130875b212452ad777ab02a4eb5cd35d9,https://www.semanticscholar.org/paper/5a44f70130875b212452ad777ab02a4eb5cd35d9,A Position Statement on Population Data Science: The Science of Data about People,"Information is increasingly digital, creating opportunities to respond to pressing issues about human populations using linked datasets that are large, complex, and diverse. The potential social and individual benefits that can come from data-intensive science are large, but raise challenges of balancing individual privacy and the public good, building appropriate socio-technical systems to support data-intensive science, and determining whether defining a new field of inquiry might help move those collective interests and activities forward. A combination of expert engagement, literature review, and iterative conversations led to our conclusion that defining the field of Population Data Science (challenge 3) will help address the other two challenges as well. We define Population Data Science succinctly as the science of data about people and note that it is related to but distinct from the fields of data science and informatics. A broader definition names four characteristics of: data use for positive impact on citizens and society; bringing together and analyzing data from multiple sources; finding population-level insights; and developing safe, privacy-sensitive and ethical infrastructure to support research. One implication of these characteristics is that few people possess all of the requisite knowledge and skills of Population Data Science, so this is by nature a multi-disciplinary field. Other implications include the need to advance various aspects of science, such as data linkage technology, various forms of analytics, and methods of public engagement. These implications are the beginnings of a research agenda for Population Data Science, which if approached as a collective field, can catalyze significant advances in our understanding of trends in society, health, and human behavior.",2018.0,"K. McGrail, K. Jones, A. Akbari, T. Bennett, A. Boyd, F. Carinci, Xinjie Cui, S. Denaxas, N. Dougall, D. Ford, R. Kirby, Hye‐Chung Kum, R. Moorin, R. Moran, C. O'Keefe, D. Preen, H. Quan, C. Sanmartin, M. Schull, Mark Smith, Christine Williams, T. Williamson, Grant MA Wyper, M. Kotelchuck"
94c52a7516ef8955f76c3ee1319ff4fd8bf071fd,https://www.semanticscholar.org/paper/94c52a7516ef8955f76c3ee1319ff4fd8bf071fd,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",2016.0,
a1dbdc2ce338d694a720163f591e4eb5c4070140,https://www.semanticscholar.org/paper/a1dbdc2ce338d694a720163f591e4eb5c4070140,Deep Learning in Biomedical Data Science,"Since the 1980s, deep learning and biomedical data have been coevolving and feeding each other. The breadth, complexity, and rapidly expanding size of biomedical data have stimulated the development of novel deep learning methods, and application of these methods to biomedical data have led to scientific discoveries and practical solutions. This overview provides technical and historical pointers to the field, and surveys current applications of deep learning to biomedical data organized around five subareas, roughly of increasing spatial scale: chemoinformatics, proteomics, genomics and transcriptomics, biomedical imaging, and health care. The black box problem of deep learning methods is also briefly discussed.",2018.0,P. Baldi
3c51a892ce5a8fc78d57ea290c6e5144ee9db579,https://www.semanticscholar.org/paper/3c51a892ce5a8fc78d57ea290c6e5144ee9db579,Key Concepts for a Data Science Ethics Curriculum,"Data science is a new field that integrates aspects of computer science, statistics and information management. As a new field, ethical issues a data scientist may encounter have received little attention to date, and ethics training within a data science curriculum has received even less attention. To address this gap, this article explores the different codes of conduct and ethics frameworks related to data science. We compare this analysis with the results of a systematic literature review focusing on ethics in data science. Our analysis identified twelve key ethics areas that should be included within a data science ethics curriculum. Our research notes that none of the existing codes or frameworks covers all of the identified themes. Data science educators and program coordinators can use our results as a way to identify key ethical concepts that can be introduced within a data science program.",2018.0,"J. Saltz, Neil I. Dewar, Robert Heckman"
843793928e308b5414d2883ac869e813ec16f65d,https://www.semanticscholar.org/paper/843793928e308b5414d2883ac869e813ec16f65d,Progressive Data Science: Potential and Challenges,"Data science requires time-consuming iterative manual activities. In particular, activities such as data selection, preprocessing, transformation, and mining, highly depend on iterative trial-and-error processes that could be sped up significantly by providing quick feedback on the impact of changes. The idea of progressive data science is to compute the results of changes in a progressive manner, returning a first approximation of results quickly and allow iterative refinements until converging to a final result. Enabling the user to interact with the intermediate results allows an early detection of erroneous or suboptimal choices, the guided definition of modifications to the pipeline and their quick assessment. In this paper, we discuss the progressiveness challenges arising in different steps of the data science pipeline. We describe how changes in each step of the pipeline impact the subsequent steps and outline why progressive data science will help to make the process more effective. Computing progressive approximations of outcomes resulting from changes creates numerous research challenges, especially if the changes are made in the early steps of the pipeline. We discuss these challenges and outline first steps towards progressiveness, which, we argue, will ultimately help to significantly speed-up the overall data science process.",2018.0,"C. Turkay, Nicola Pezzotti, Carsten Binnig, Hendrik Strobelt, B. Hammer, D. Keim, Jean-Daniel Fekete, Themis Palpanas, Yunhai Wang, Florin Rusu"
1901a26945ecb5445a9d58b4c32a0dc6dbd12f1a,https://www.semanticscholar.org/paper/1901a26945ecb5445a9d58b4c32a0dc6dbd12f1a,"STS, Meet Data Science, Once Again","Science and technology studies (STS) and the emerging field of data science share surprising elective affinities. At the growing intersections of these fields, there will be many opportunities and not a few thorny difficulties for STS scholars. First, I discuss how both fields frame the rollout of data science as a simultaneously social and technical endeavor, even if in distinct ways and for diverging purposes. Second, I discuss the logic of domains in contemporary computer, information, and data science circles. While STS is often agnostic about the borders between the sciences or with industry and state—occasionally taking those boundaries as an object of study—data science takes those boundaries as its target to overcome. These two elective affinities present analytic and practical challenges for STS but also opportunities for engagement. Overall, in addition to these typifications, I urge STS scholars to strategically position themselves to investigate and contribute to the breadth of transformations that seek to touch virtually every science and newly bind spheres of academy, industry, and state.",2018.0,David Ribes
d2f83aa22def149095f1dd89b4cf36d09a748a87,https://www.semanticscholar.org/paper/d2f83aa22def149095f1dd89b4cf36d09a748a87,Data science is science's second chance to get causal inference right: A classification of data science tasks,"Causal inference from observational data is the goal of many data analyses in the health and social sciences. However, academic statistics has often frowned upon data analyses with a causal objective. The introduction of the term""data science""provides a historic opportunity to redefine data analysis in such a way that it naturally accommodates causal inference from observational data. Like others before, we organize the scientific contributions of data science into three classes of tasks: Description, prediction, and counterfactual prediction (which includes causal inference). An explicit classification of data science tasks is necessary to discuss the data, assumptions, and analytics required to successfully accomplish each task. We argue that a failure to adequately describe the role of subject-matter expert knowledge in data analysis is a source of widespread misunderstandings about data science. Specifically, causal analyses typically require not only good data and algorithms, but also domain expert knowledge. We discuss the implications for the use of data science to guide decision-making in the real world and to train data scientists.",2018.0,"M. Hernán, J. Hsu, B. Healy"
b34b9758b36c92c023c3c10f3a39aeb8f5c83927,https://www.semanticscholar.org/paper/b34b9758b36c92c023c3c10f3a39aeb8f5c83927,Exploring Project Management Methodologies Used Within Data Science Teams,"There are many reasons data science teams should use a well-defined process to manage and coordinate their efforts, such as improved collaboration, efficiency and stakeholder communication. This paper explores the current methodology data science teams use to manage and coordinate their efforts. Unfortunately, based on our survey results, most data science teams currently use an ad hoc project management approach. In fact, 82% of the data scientists surveyed did not follow an explicit process. However, it is encouraging to note that 85% of the respondents thought that adopting an improved process methodology would improve the teams’ outcomes. Based on these results, we described six possible process methodologies teams could use. To conclude, we outlined plans to describe best practices for data science team processes and to develop a process evaluation framework.",2018.0,"J. Saltz, Nicholas Hotz, D. Wild, Kyle Stirling"
5de20ffb7852ae0665c382084c8a56918f23dc0b,https://www.semanticscholar.org/paper/5de20ffb7852ae0665c382084c8a56918f23dc0b,Drafting a Data Science Curriculum for Secondary Schools,"Data science as the art of generating information and knowledge from data is increasingly becoming an important part of most operational processes. But up to now, data science is hardly an issue in German computer science education at secondary schools. For this reason, we are developing a data science curriculum for German secondary schools, which first guidelines and ideas we present in this paper. The curriculum is designed as interdisciplinary approach between maths and computer science education, with also a strong focus on societal aspects. After a brief discussion of important concepts and challenges in data science, a first draft of the curriculum and an outline of a data science course for upper secondary schools accompanying the development are presented.",2018.0,"B. Heinemann, Simone Opel, Lea Budde, Carsten Schulte, Daniel Frischemeier, Rolf Biehler, Susanne Podworny, Thomas Wassong"
dd1f93c3faae464d50d2e97c2bf4ac8d43681cb1,https://www.semanticscholar.org/paper/dd1f93c3faae464d50d2e97c2bf4ac8d43681cb1,Twinning data science with information science in schools of library and information science,"As an emerging discipline, data science represents a vital new current of school of library and information science (LIS) education. However, it remains unclear how it relates to information science within LIS schools. The purpose of this paper is to clarify this issue.,Mission statement and nature of both data science and information science are analyzed by reviewing existing work in the two disciplines and drawing DIKW hierarchy. It looks at the ways in which information science theories bring new insights and shed new light on fundamentals of data science.,Data science and information science are twin disciplines by nature. The mission, task and nature of data science are consistent with those of information science. They greatly overlap and share similar concerns. Furthermore, they can complement each other. LIS school should integrate both sciences and develop organizational ambidexterity. Information science can make unique contributions to data science research, including conception of data, data quality control, data librarianship and theory dualism. Document theory, as a promising direction of unified information science, should be introduced to data science to solve the disciplinary divide.,The results of this paper may contribute to the integration of data science and information science within LIS schools and iSchools. It has particular value for LIS school development and reform in the age of big data.",2018.0,L. Wang
04d7b3457dc78b2d2282e6af2c787308f75c9b26,https://www.semanticscholar.org/paper/04d7b3457dc78b2d2282e6af2c787308f75c9b26,Care and the Practice of Data Science for Social Good,"Data science is an interdisciplinary field that extracts insights from data through a multi-stage process of data collection, analysis and use. When data science is applied for social good, a variety of stakeholders are introduced to the process with an intention to inform policies or programs to improve well-being. Our goal in this paper is to propose an orientation to care in the practice of data science for social good. When applied to data science, a logic of care can improve the data science process and reveal outcomes of ""good"" throughout. Consideration of care in practice has its origins in Science and Technology Studies (STS) and has recently been applied by Human Computer Interaction (HCI) researchers to understand technology repair and use in under-served environments as well as care in remote health monitoring. We bring care to the practice of data science through a detailed examination of our engaged research with a community group that uses data as a strategy to advocate for permanently affordable housing. We identify opportunities and experiences of care throughout the stages of the data science process. We bring greater detail to the notion of human-centered systems for data science and begin to describe what these look like.",2018.0,"E. Zegura, C. Disalvo, Amanda Meng"
2a85f034ae7a6119ae6b718c8f73a58dc1fbd7b4,https://www.semanticscholar.org/paper/2a85f034ae7a6119ae6b718c8f73a58dc1fbd7b4,Curriculum Guidelines for Undergraduate Programs in Data Science,"The Park City Math Institute (PCMI) 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in Data Science. The group consisted of 25 undergraduate faculty from a variety of institutions in the U.S., primarily from the disciplines of mathematics, statistics and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in Data Science.",2017.0,"R. D. Veaux, Mahesh Agarwal, Maia Averett, B. Baumer, Andrew Bray, T. Bressoud, Lance Bryant, Lei Cheng, Amanda Francis, R. Gould, Albert Y. Kim, Matt Kretchmar, Qin Lu, Ann Moskol, D. Nolan, Roberto Pelayo, Sean Raleigh, Ricky J. Sethi, Mutiara Sondjaja, Neelesh Tiruviluamala, P. Uhlig, Talitha M. Washington, Curtis L. Wesley, David L. White, Ping Ye"
e78be911203960b3b2a417465d726734367f8e30,https://www.semanticscholar.org/paper/e78be911203960b3b2a417465d726734367f8e30,Counter‐mapping data science,"Counter-mapping is a combination of critical ideas and practices for social change that offers a productive and promising approach for grassroots data science initiatives. Current information technologies collect, store, and analyze data with new degrees of size, speed, heterogeneity, and detail. While much work utilizing data science technologies is dedicated to generating profit or to national security, some data science projects explicitly attempt to facilitate new social relations, though with inconsistent results and consequences. This paper reviews counter-mapping's particular combination of theory and practice as a potential point of reference for such initiatives. Counter-mapping takes the tools of institutional map-making at government agencies and corporations and applies them in situated, bottom-up ways. Moreover, counter-mapping's multiple theoretical approaches and polyglot practices offer a variety of inspirations and avenues for future work in identifying and realizing alternative, ideally better, possibilities. This paper defines counter-mapping; outlines its multiple theorizations; briefly describes three relevant case studies, The Detroit Geographical Expedition and Institute, Mapping Police Violence, and the Counter-Cartographies Collective; and concludes with a few hard-learned considerations from counter-mapping that are directly pertinent for data-oriented projects focused on change.",2018.0,"Craig M. Dalton, Tim Stallmann"
ea07f64ad84542e04acc41db6b171007f344efd7,https://www.semanticscholar.org/paper/ea07f64ad84542e04acc41db6b171007f344efd7,Milo: A visual programming environment for Data Science Education,"Most courses on Data Science offered at universities or online require students to have familiarity with at least one programming language. In this paper, we present, “Milo”, a web-based visual programming environment for Data Science Education, designed as a pedagogical tool that can be used by students without prior-programming experience. To that end, Milo uses graphical blocks as abstractions of language specific implementations of Data Science and Machine Learning(ML) concepts along with creation of interactive visualizations. Using block definitions created by a user, Milo generates equivalent source code in JavaScript to run entirely in the browser. Based on a preliminary user study with a focus group of undergraduate computer science students, Milo succeeds as an effective tool for novice learners in the field of Data Science.",2018.0,"A. Rao, Ayush Bihani, Mydhili K. Nair"
a0ef3467c09acc3106b915258b7b8db7bb663b77,https://www.semanticscholar.org/paper/a0ef3467c09acc3106b915258b7b8db7bb663b77,Data Science Methodology for Cybersecurity Projects,"Cyber-security solutions are traditionally static and signature-based. The traditional solutions along with the use of analytic models, machine learning and big data could be improved by automatically trigger mitigation or provide relevant awareness to control or limit consequences of threats. This kind of intelligent solutions is covered in the context of Data Science for Cyber-security. Data Science provides a significant role in cyber-security by utilising the power of data (and big data), high-performance computing and data mining (and machine learning) to protect users against cyber-crimes. For this purpose, a successful data science project requires an effective methodology to cover all issues and provide adequate resources. In this paper, we are introducing popular data science methodologies and will compare them in accordance with cyber-security challenges. A comparison discussion has also delivered to explain methodologies strengths and weaknesses in case of cyber-security projects.",2018.0,"F. Foroughi, P. Luksch"
79be83a308a9a75ef4e64f63a938b201531c0bbf,https://www.semanticscholar.org/paper/79be83a308a9a75ef4e64f63a938b201531c0bbf,Data Science,"The 21st century has ushered in the age of big data and data economy, in which data DNA, which carries important knowledge, insights, and potential, has become an intrinsic constituent of all data-based organisms. An appropriate understanding of data DNA and its organisms relies on the new field of data science and its keystone, analytics. Although it is widely debated whether big data is only hype and buzz, and data science is still in a very early phase, significant challenges and opportunities are emerging or have been inspired by the research, innovation, business, profession, and education of data science. This article provides a comprehensive survey and tutorial of the fundamental aspects of data science: the evolution from data analysis to data science, the data science concepts, a big picture of the era of data science, the major challenges and directions in data innovation, the nature of data analytics, new industrialization and service opportunities in the data economy, the profession and competency of data education, and the future of data science. This article is the first in the field to draw a comprehensive big picture, in addition to offering rich observations, lessons, and thinking about data science and analytics.",2017.0,Longbing Cao
c0225f99c9b1619c3be74b63241faffe02d275d7,https://www.semanticscholar.org/paper/c0225f99c9b1619c3be74b63241faffe02d275d7,Science and data science,"Data science has attracted a lot of attention, promising to turn vast amounts of data into useful predictions and insights. In this article, we ask why scientists should care about data science. To answer, we discuss data science from three perspectives: statistical, computational, and human. Although each of the three is a critical component of data science, we argue that the effective combination of all three components is the essence of what data science is about.",2017.0,"D. Blei, Padhraic Smyth"
010f65dd2fa979892a8229db825954871652fb8f,https://www.semanticscholar.org/paper/010f65dd2fa979892a8229db825954871652fb8f,Defining Data Science by a Data-Driven Quantification of the Community,"Data science is a new academic field that has received much attention in recent years. One reason for this is that our increasingly digitalized society generates more and more data in all areas of our lives and science and we are desperately seeking for solutions to deal with this problem. In this paper, we investigate the academic roots of data science. We are using data of scientists and their citations from Google Scholar, who have an interest in data science, to perform a quantitative analysis of the data science community. Furthermore, for decomposing the data science community into its major defining factors corresponding to the most important research fields, we introduce a statistical regression model that is fully automatic and robust with respect to a subsampling of the data. This statistical model allows us to define the ‘importance’ of a field as its predictive abilities. Overall, our method provides an objective answer to the question ‘What is data science?’.",2018.0,"F. Emmert-Streib, M. Dehmer"
da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,https://www.semanticscholar.org/paper/da63f30bd5b3a1b16c261f75ca1b1daddfc5b44d,Big Data and Data Science Methods for Management Research,"The recent advent of remote sensing, mobile technologies, novel transaction systems, and highperformance computing offers opportunities to understand trends, behaviors, and actions in a manner that has not been previously possible. Researchers can thus leverage “big data” that are generated from a plurality of sources including mobile transactions, wearable technologies, social media, ambient networks, andbusiness transactions.An earlierAcademy of Management Journal (AMJ) editorial explored the potential implications for data science inmanagement research and highlighted questions for management scholarship as well as the attendant challenges of data sharing and privacy (George, Haas, & Pentland, 2014). This nascent field is evolving rapidly and at a speed that leaves scholars and practitioners alike attempting to make sense of the emergent opportunities that big datahold.With thepromiseof bigdata comequestions about the analytical value and thus relevance of these data for theory development—including concerns over the context-specific relevance, its reliability and its validity. To address this challenge, data science is emerging as an interdisciplinary field that combines statistics, data mining, machine learning, and analytics to understand and explainhowwecan generate analytical insights and prediction models from structured and unstructured big data. Data science emphasizes the systematic study of the organization, properties, and analysis of data and their role in inference, including our confidence in the inference (Dhar, 2013).Whereas both big data and data science terms are often used interchangeably, “big data” refer to large and varied data that can be collected and managed, whereas “data science” develops models that capture, visualize, andanalyze theunderlyingpatterns in thedata. In this editorial, we address both the collection and handling of big data and the analytical tools provided by data science for management scholars. At the current time, practitioners suggest that data science applications tackle the three core elements of big data: volume, velocity, and variety (McAfee & Brynjolfsson, 2012; Zikopoulos & Eaton, 2011). “Volume” represents the sheer size of the dataset due to the aggregation of a large number of variables and an even larger set of observations for each variable. “Velocity” reflects the speed atwhich these data are collected and analyzed, whether in real time or near real time from sensors, sales transactions, social media posts, and sentiment data for breaking news and social trends. “Variety” in big data comes from the plurality of structured and unstructured data sources such as text, videos, networks, and graphics among others. The combinations of volume, velocity, and variety reveal the complex task of generating knowledge from big data, which often runs into millions of observations, and deriving theoretical contributions from such data. In this editorial, we provide a primer or a “starter kit” for potential data science applications inmanagement research. We do so with a caveat that emerging fields outdate and improve uponmethodologies while often supplanting them with new applications. Nevertheless, this primer can guide management scholars who wish to use data science techniques to reach better answers to existing questions or explore completely new research questions.",2016.0,"G. George, E. Osinga, Dovev Lavie, Brent A. Scott"
82feed9f0f8d077046b9b8be36e664483a66e33b,https://www.semanticscholar.org/paper/82feed9f0f8d077046b9b8be36e664483a66e33b,Teaching Stats for Data Science,"ABSTRACT “Data science” is a useful catchword for methods and concepts original to the field of statistics, but typically being applied to large, multivariate, observational records. Such datasets call for techniques not often part of an introduction to statistics: modeling, consideration of covariates, sophisticated visualization, and causal reasoning. This article re-imagines introductory statistics as an introduction to data science and proposes a sequence of 10 blocks that together compose a suitable course for extracting information from contemporary data. Recent extensions to the mosaic packages for R together with tools from the “tidyverse” provide a concise and readable notation for wrangling, visualization, model-building, and model interpretation: the fundamental computational tasks of data science.",2018.0,Daniel T. Kaplan
89535aa63bc5dac6f3beb60b813abb77aa4309d1,https://www.semanticscholar.org/paper/89535aa63bc5dac6f3beb60b813abb77aa4309d1,Critique and Contribute: A Practice-Based Framework for Improving Critical Data Studies and Data Science,"Abstract What would data science look like if its key critics were engaged to help improve it, and how might critiques of data science improve with an approach that considers the day-to-day practices of data science? This article argues for scholars to bridge the conversations that seek to critique data science and those that seek to advance data science practice to identify and create the social and organizational arrangements necessary for a more ethical data science. We summarize four critiques that are commonly made in critical data studies: data are inherently interpretive, data are inextricable from context, data are mediated through the sociomaterial arrangements that produce them, and data serve as a medium for the negotiation and communication of values. We present qualitative research with academic data scientists, “data for good” projects, and specialized cross-disciplinary engineering teams to show evidence of these critiques in the day-to-day experience of data scientists as they acknowledge and grapple with the complexities of their work. Using ethnographic vignettes from two large multiresearcher field sites, we develop a set of concepts for analyzing and advancing the practice of data science and improving critical data studies, including (1) communication is central to the data science endeavor; (2) making sense of data is a collective process; (3) data are starting, not end points, and (4) data are sets of stories. We conclude with two calls to action for researchers and practitioners in data science and critical data studies alike. First, creating opportunities for bringing social scientific and humanistic expertise into data science practice simultaneously will advance both data science and critical data studies. Second, practitioners should leverage the insights from critical data studies to build new kinds of organizational arrangements, which we argue will help advance a more ethical data science. Engaging the insights of critical data studies will improve data science. Careful attention to the practices of data science will improve scholarly critiques. Genuine collaborative conversations between these different communities will help push for more ethical, and better, ways of knowing in increasingly datum-saturated societies.",2017.0,"Gina Neff, A. Tanweer, Brittany Fiore-Gartland, Laura Osburn"
afe79672aa99b7f606cbff234ec2454cf2295554,https://www.semanticscholar.org/paper/afe79672aa99b7f606cbff234ec2454cf2295554,Big Data Science: Opportunities and Challenges to Address Minority Health and Health Disparities in the 21st Century.,"Addressing minority health and health disparities has been a missing piece of the puzzle in Big Data science. This article focuses on three priority opportunities that Big Data science may offer to the reduction of health and health care disparities. One opportunity is to incorporate standardized information on demographic and social determinants in electronic health records in order to target ways to improve quality of care for the most disadvantaged populations over time. A second opportunity is to enhance public health surveillance by linking geographical variables and social determinants of health for geographically defined populations to clinical data and health outcomes. Third and most importantly, Big Data science may lead to a better understanding of the etiology of health disparities and understanding of minority health in order to guide intervention development. However, the promise of Big Data needs to be considered in light of significant challenges that threaten to widen health disparities. Care must be taken to incorporate diverse populations to realize the potential benefits. Specific recommendations include investing in data collection on small sample populations, building a diverse workforce pipeline for data science, actively seeking to reduce digital divides, developing novel ways to assure digital data privacy for small populations, and promoting widespread data sharing to benefit under-resourced minority-serving institutions and minority researchers. With deliberate efforts, Big Data presents a dramatic opportunity for reducing health disparities but without active engagement, it risks further widening them.",2017.0,"Xinzhi Zhang, E. Pérez-Stable, P. Bourne, E. Peprah, O. K. Duru, N. Breen, D. Berrigan, Frederick B. Wood, J. Jackson, David W. S. Wong, J. Denny"
f4e66bd035e195f539f1b65a5aaec0e873cdee29,https://www.semanticscholar.org/paper/f4e66bd035e195f539f1b65a5aaec0e873cdee29,Data science in education: Big data and learning analytics,This paper considers the data science and the summaries significance of Big Data and Learning Analytics in education. The widespread platform of making high‐quality benefits that could be achieved by exhausting big data techniques in the field of education is considered. One principal architecture framework to support education research is proposed.,2017.0,"Aleksandra Klašnja-Milićević, M. Ivanović, Z. Budimac"
fde0b586e3bc9e5139a14493044bce9ff61706d4,https://www.semanticscholar.org/paper/fde0b586e3bc9e5139a14493044bce9ff61706d4,Inverse statistical problems: from the inverse Ising problem to data science,"Inverse problems in statistical physics are motivated by the challenges of ‘big data’ in different fields, in particular high-throughput experiments in biology. In inverse problems, the usual procedure of statistical physics needs to be reversed: Instead of calculating observables on the basis of model parameters, we seek to infer parameters of a model based on observations. In this review, we focus on the inverse Ising problem and closely related problems, namely how to infer the coupling strengths between spins given observed spin correlations, magnetizations, or other data. We review applications of the inverse Ising problem, including the reconstruction of neural connections, protein structure determination, and the inference of gene regulatory networks. For the inverse Ising problem in equilibrium, a number of controlled and uncontrolled approximate solutions have been developed in the statistical mechanics community. A particularly strong method, pseudolikelihood, stems from statistics. We also review the inverse Ising problem in the non-equilibrium case, where the model parameters must be reconstructed based on non-equilibrium statistics.",2017.0,"H. C. Nguyen, R. Zecchina, J. Berg"
427a613d349d305726e1c4c7935b33c79de5850a,https://www.semanticscholar.org/paper/427a613d349d305726e1c4c7935b33c79de5850a,Python Data Science Handbook: Essential Tools for Working with Data,"For many researchers, Python is a first-class tool mainly because of its libraries for storing, manipulating, and gaining insight from data. Several resources exist for individual pieces of this data science stack, but only with the Python Data Science Handbook do you get them all IPython, NumPy, Pandas, Matplotlib, Scikit-Learn, and other related tools. Working scientists and data crunchers familiar with reading and writing Python code will find this comprehensive desk reference ideal for tackling day-to-day issues: manipulating, transforming, and cleaning data; visualizing different types of data; and using data to build statistical or machine learning models. Quite simply, this is the must-have reference for scientific computing in Python. With this handbook, youll learn how to use:IPython and Jupyter: provide computational environments for data scientists using PythonNumPy: includes the ndarray for efficient storage and manipulation of dense data arrays in PythonPandas: features the DataFrame for efficient storage and manipulation of labeled/columnar data in PythonMatplotlib: includes capabilities for a flexible range of data visualizations in PythonScikit-Learn: for efficient and clean Python implementations of the most important and established machine learning algorithms",2016.0,J. Vanderplas
b0fbdffb9733e7857afbb21ccbcd9cd74803ca1d,https://www.semanticscholar.org/paper/b0fbdffb9733e7857afbb21ccbcd9cd74803ca1d,"Data Science and symbolic AI: Synergies, challenges and opportunities","Symbolic approaches to Artificial Intelligence (AI) represent things within a domain of knowledge through physical symbols, combine symbols into symbol expressions, and manipulate symbols and symbol expressions through inference processes. While a large part of Data Science relies on statistics and applies statistical approaches to AI, there is an increasing potential for successfully applying symbolic approaches as well. Symbolic representations and symbolic inference are close to human cognitive representations and therefore comprehensible and interpretable; they are widely used to represent data and metadata, and their specific semantic content must be taken into account for analysis of such information; and human communication largely relies on symbols, making symbolic representations a crucial part in the analysis of natural language. Here we discuss the role symbolic representations and inference can play in Data Science, highlight the research challenges from the perspective of the data scientist, and argue that symbolic methods should become a crucial component of the data scientists’ toolbox.",2017.0,"R. Hoehndorf, N. Queralt-Rosinach"
88761dffd173cd0e75e88c02d68f866f8cc43c14,https://www.semanticscholar.org/paper/88761dffd173cd0e75e88c02d68f866f8cc43c14,Knowledge-based biomedical Data Science,"Computational manipulation of knowledge is an important, and often under-appreciated, aspect of biomedical Data Science. The first Data Science initiative from the US National Institutes of Health was entitled “Big Data to Knowledge (BD2K).” The main emphasis of the more than $200M allocated to that program has been on “Big Data;” the “Knowledge” component has largely been the implicit assumption that the work will lead to new biomedical knowledge. However, there is long-standing and highly productive work in computational knowledge representation and reasoning, and computational processing of knowledge has a role in the world of Data Science. Knowledge-based biomedical Data Science involves the design and implementation of computer systems that act as if they knew about biomedicine. There are many ways in which a computational approach might act as if it knew something: for example, it might be able to answer a natural language question about a biomedical topic, or pass an exam; it might be able to use existing biomedical knowledge to rank or evaluate hypotheses; it might explain or interpret data in light of prior knowledge, either in a Bayesian or other sort of framework. These are all examples of automated reasoning that act on computational representations of knowledge. After a brief survey of existing approaches to knowledge-based data science, this position paper argues that such research is ripe for expansion, and expanded application.",2017.0,Lawrence E. Hunter
ce0b7ee60920f9b37f88cab785cb8b4dc337e89f,https://www.semanticscholar.org/paper/ce0b7ee60920f9b37f88cab785cb8b4dc337e89f,Educational data science in massive open online courses,"The current massive open online course (MOOC) euphoria is revolutionizing online education. Despite its expediency, there is considerable skepticism over various concerns. In order to resolve some of these problems, educational data science (EDS) has been used with success. MOOCs provide a wealth of information about the way in which a large number of learners interact with educational platforms and engage with the courses offered. This extensive amount of data provided by MOOCs concerning students' usage information is a gold mine for EDS. This paper aims to provide the reader with a complete and comprehensive review of the existing literature that helps us understand the application of EDS in MOOCs. The main works in this area are described and grouped by task or issue to be solved, along with the techniques used. WIREs Data Mining Knowl Discov 2017, 7:e1187. doi: 10.1002/widm.1187",2016.0,"C. Romero, Sebastián Ventura"
e36022198f21f46d066007ee5cf901ea55080e21,https://www.semanticscholar.org/paper/e36022198f21f46d066007ee5cf901ea55080e21,"Introduction to Data Science: A Python Approach to Concepts, Techniques and Applications","This accessible and classroom-tested textbook/reference presents an introduction to the fundamentals of the emerging and interdisciplinary field of data science. The coverage spans key concepts adopted from statistics and machine learning, useful techniques for graph analysis and parallel programming, and the practical application of data science for such tasks as building recommender systems or performing sentiment analysis. Topics and features: provides numerous practical case studies using real-world data throughout the book; supports understanding through hands-on experience of solving data science problems using Python; describes techniques and tools for statistical analysis, machine learning, graph analysis, and parallel programming; reviews a range of applications of data science, including recommender systems and sentiment analysis of text data; provides supplementary code resources and data at an associated website.",2017.0,"L. Igual, S. Seguí"
00b1fa3c7170563567fb22a9bb6ff4c7b2e8853e,https://www.semanticscholar.org/paper/00b1fa3c7170563567fb22a9bb6ff4c7b2e8853e,Comparing Data Science Project Management Methodologies via a Controlled Experiment,"Data Science is an emerging field with a significant research focus on improving the techniques available to analyze data. However, there has been much less focus on how people should work together on a data science project. In this paper, we report on the results of an experiment comparing four different methodologies to manage and coordinate a data science project. We first introduce a model to compare different project management methodologies and then report on the results of our experiment. The results from our experiment demonstrate that there are significant differences based on the methodology used, with an Agile Kanban methodology being the most effective and surprisingly, an Agile Scrum methodology being the least effective.",2017.0,"J. Saltz, Ivan Shamshurin, Kevin Crowston"
843149b649b888fdb3649b8d4852263b62356799,https://www.semanticscholar.org/paper/843149b649b888fdb3649b8d4852263b62356799,Democratizing data science through data science training,"The biomedical sciences have experienced an explosion of data which promises to overwhelm many current practitioners. Without easy access to data science training resources, biomedical researchers may find themselves unable to wrangle their own datasets. In 2014, to address the challenges posed such a data onslaught, the National Institutes of Health (NIH) launched the Big Data to Knowledge (BD2K) initiative. To this end, the BD2K Training Coordinating Center (TCC; bigdatau.org) was funded to facilitate both in-person and online learning, and open up the concepts of data science to the widest possible audience. Here, we describe the activities of the BD2K TCC and its focus on the construction of the Educational Resource Discovery Index (ERuDIte), which identifies, collects, describes, and organizes online data science materials from BD2K awardees, open online courses, and videos from scientific lectures and tutorials. ERuDIte now indexes over 9,500 resources. Given the richness of online training materials and the constant evolution of biomedical data science, computational methods applying information retrieval, natural language processing, and machine learning techniques are required - in effect, using data science to inform training in data science. In so doing, the TCC seeks to democratize novel insights and discoveries brought forth via large-scale data science training.",2018.0,"J. D. Horn, Lily Fierro, Jeana Kamdar, Jonathan Gordon, Crystal Stewart, Avnish Bhattrai, Sumiko Abe, Xiaoxia Lei, Caroline O'Driscoll, Aakanchha Sinha, Priyambada Jain, Gully A. Burns, Kristina Lerman, J. Ambite"
31485e1213dd886fa2b668eefcd9b13533d8a9fe,https://www.semanticscholar.org/paper/31485e1213dd886fa2b668eefcd9b13533d8a9fe,Big data and data science: what should we teach?,"The era of big data has arrived. Big data bring us the data‐driven paradigm and enlighten us to challenge new classes of problems we were not able to solve in the past. We are beginning to see the impacts of big data in every aspect of our lives and society. We need a science that can address these big data problems. Data science is a new emerging discipline that was termed to address challenges that we are facing and going to face in the big data era. Thus, education in data science is the key to success, and we need concrete strategies and approaches to better educate future data scientists. In this paper, we discuss general concepts on big data, data science, and data scientists and show the results of an extensive survey on current data science education in United States. Finally, we propose various approaches that data science education should aim to accomplish.",2016.0,"I. Song, Yongjun Zhu"
bfd6caddec8a98d531ee9f1f7ebf5833797cd5e3,https://www.semanticscholar.org/paper/bfd6caddec8a98d531ee9f1f7ebf5833797cd5e3,Introducing Data Science to School Kids,"Data-driven decision making is fast becoming a necessary skill in jobs across the board. The industry today uses analytics and machine learning to get useful insights from a wealth of digital information in order to make decisions. With data science becoming an important skill needed in varying degrees of complexity by the workforce of the near future, we felt the need to expose school-goers to its power through a hands-on exercise. We organized a half-day long data science tutorial for kids in grades 5 through 9 (10-15 years old). Our aim was to expose them to the full cycle of a typical supervised learning approach - data collection, data entry, data visualization, feature engineering, model building, model testing and data permissions. We discuss herein the design choices made while developing the dataset, the method and the pedagogy for the tutorial. These choices aimed to maximize student engagement while ensuring minimal pre-requisite knowledge. This was a challenging task given that we limited the pre-requisites for the kids to the knowledge of counting, addition, percentages, comparisons and a basic exposure to operating computers. By designing an exercise with the stated principles, we were able to provide to kids an exciting, hands-on introduction to data science, as confirmed by their experiences. To the best of the authors' knowledge, the tutorial was the first of its kind. Considering the positive reception of such a tutorial, we hope that educators across the world are encouraged to introduce data science in their respective curricula for high-schoolers and are able to use the principles laid out in this work to build full-fledged courses.",2017.0,"Shashank Srikant, V. Aggarwal"
3b963487cbf944d51f33c2a0b41eb2aed7c68b89,https://www.semanticscholar.org/paper/3b963487cbf944d51f33c2a0b41eb2aed7c68b89,Locating ethics in data science: responsibility and accountability in global and distributed knowledge production systems,"The distributed and global nature of data science creates challenges for evaluating the quality, import and potential impact of the data and knowledge claims being produced. This has significant consequences for the management and oversight of responsibilities and accountabilities in data science. In particular, it makes it difficult to determine who is responsible for what output, and how such responsibilities relate to each other; what ‘participation’ means and which accountabilities it involves, with regard to data ownership, donation and sharing as well as data analysis, re-use and authorship; and whether the trust placed on automated tools for data mining and interpretation is warranted (especially as data processing strategies and tools are often developed separately from the situations of data use where ethical concerns typically emerge). To address these challenges, this paper advocates a participative, reflexive management of data practices. Regulatory structures should encourage data scientists to examine the historical lineages and ethical implications of their work at regular intervals. They should also foster awareness of the multitude of skills and perspectives involved in data science, highlighting how each perspective is partial and in need of confrontation with others. This approach has the potential to improve not only the ethical oversight for data science initiatives, but also the quality and reliability of research outputs. This article is part of the themed issue ‘The ethical impact of data science’.",2016.0,S. Leonelli
589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,https://www.semanticscholar.org/paper/589ebdd0d7b4a58f7fdfb07f116f62681bb9a915,Hack weeks as a model for data science education and collaboration,"Significance As scientific disciplines grapple with more datasets of rapidly increasing complexity and size, new approaches are urgently required to introduce new statistical and computational tools into research communities and improve the cross-disciplinary exchange of ideas. In this paper, we introduce a type of scientific workshop, called a hack week, which allows for fast dissemination of new methodologies into scientific communities and fosters exchange and collaboration within and between disciplines. We present implementations of this concept in astronomy, neuroscience, and geoscience and show that hack weeks produce positive learning outcomes, foster lasting collaborations, yield scientific results, and promote positive attitudes toward open science. Across many scientific disciplines, methods for recording, storing, and analyzing data are rapidly increasing in complexity. Skillfully using data science tools that manage this complexity requires training in new programming languages and frameworks as well as immersion in new modes of interaction that foster data sharing, collaborative software development, and exchange across disciplines. Learning these skills from traditional university curricula can be challenging because most courses are not designed to evolve on time scales that can keep pace with rapidly shifting data science methods. Here, we present the concept of a hack week as an effective model offering opportunities for networking and community building, education in state-of-the-art data science methods, and immersion in collaborative project work. We find that hack weeks are successful at cultivating collaboration and facilitating the exchange of knowledge. Participants self-report that these events help them in both their day-to-day research as well as their careers. Based on our results, we conclude that hack weeks present an effective, easy-to-implement, fairly low-cost tool to positively impact data analysis literacy in academic disciplines, foster collaboration, and cultivate best practices.",2017.0,"D. Huppenkothen, A. Arendt, D. Hogg, Karthik Ram, J. Vanderplas, Ariel S. Rokem"
61b3ce156347a7f107df75924a45f81f12a0ef14,https://www.semanticscholar.org/paper/61b3ce156347a7f107df75924a45f81f12a0ef14,Surgical data science: the new knowledge domain,"Abstract Healthcare in general, and surgery/interventional care in particular, is evolving through rapid advances in technology and increasing complexity of care, with the goal of maximizing the quality and value of care. Whereas innovations in diagnostic and therapeutic technologies have driven past improvements in the quality of surgical care, future transformation in care will be enabled by data. Conventional methodologies, such as registry studies, are limited in their scope for discovery and research, extent and complexity of data, breadth of analytical techniques, and translation or integration of research findings into patient care. We foresee the emergence of surgical/interventional data science (SDS) as a key element to addressing these limitations and creating a sustainable path toward evidence-based improvement of interventional healthcare pathways. SDS will create tools to measure, model, and quantify the pathways or processes within the context of patient health states or outcomes and use information gained to inform healthcare decisions, guidelines, best practices, policy, and training, thereby improving the safety and quality of healthcare and its value. Data are pervasive throughout the surgical care pathway; thus, SDS can impact various aspects of care, including prevention, diagnosis, intervention, or postoperative recovery. The existing literature already provides preliminary results, suggesting how a data science approach to surgical decision-making could more accurately predict severe complications using complex data from preoperative, intraoperative, and postoperative contexts, how it could support intraoperative decision-making using both existing knowledge and continuous data streams throughout the surgical care pathway, and how it could enable effective collaboration between human care providers and intelligent technologies. In addition, SDS is poised to play a central role in surgical education, for example, through objective assessments, automated virtual coaching, and robot-assisted active learning of surgical skill. However, the potential for transforming surgical care and training through SDS may only be realized through a cultural shift that not only institutionalizes technology to seamlessly capture data but also assimilates individuals with expertise in data science into clinical research teams. Furthermore, collaboration with industry partners from the inception of the discovery process promotes optimal design of data products as well as their efficient translation and commercialization. As surgery continues to evolve through advances in technology that enhance delivery of care, SDS represents a new knowledge domain to engineer surgical care of the future.",2017.0,"S. Vedula, Gregory Hager"
021865bb9fcc59814d2ce84d086554e5e0259779,https://www.semanticscholar.org/paper/021865bb9fcc59814d2ce84d086554e5e0259779,"Big Metadata, Smart Metadata, and Metadata Capital: Toward Greater Synergy Between Data Science and Metadata","Abstract Purpose The purpose of the paper is to provide a framework for addressing the disconnect between metadata and data science. Data science cannot progress without metadata research. This paper takes steps toward advancing the synergy between metadata and data science, and identifies pathways for developing a more cohesive metadata research agenda in data science. Design/methodology/approach This paper identifies factors that challenge metadata research in the digital ecosystem, defines metadata and data science, and presents the concepts big metadata, smart metadata, and metadata capital as part of a metadata lingua franca connecting to data science. Findings The “utilitarian nature” and “historical and traditional views” of metadata are identified as two intersecting factors that have inhibited metadata research. Big metadata, smart metadata, and metadata capital are presented as part of a metadata lingua franca to help frame research in the data science research space. Research limitations There are additional, intersecting factors to consider that likely inhibit metadata research, and other significant metadata concepts to explore. Practical implications The immediate contribution of this work is that it may elicit response, critique, revision, or, more significantly, motivate research. The work presented can encourage more researchers to consider the significance of metadata as a research worthy topic within data science and the larger digital ecosystem. Originality/value Although metadata research has not kept pace with other data science topics, there is little attention directed to this problem. This is surprising, given that metadata is essential for data science endeavors. This examination synthesizes original and prior scholarship to provide new grounding for metadata research in data science.",2017.0,J. Greenberg
f447afeccbdb9ed5df15c44011aec9c018d4b2c4,https://www.semanticscholar.org/paper/f447afeccbdb9ed5df15c44011aec9c018d4b2c4,Big Data and Data Science: Opportunities and Challenges of iSchools,"Abstract Due to the recent explosion of big data, our society has been rapidly going through digital transformation and entering a new world with numerous eye-opening developments. These new trends impact the society and future jobs, and thus student careers. At the heart of this digital transformation is data science, the discipline that makes sense of big data. With many rapidly emerging digital challenges ahead of us, this article discusses perspectives on iSchools’ opportunities and suggestions in data science education. We argue that iSchools should empower their students with “information computing” disciplines, which we define as the ability to solve problems and create values, information, and knowledge using tools in application domains. As specific approaches to enforcing information computing disciplines in data science education, we suggest the three foci of user-based, tool-based, and application-based. These three foci will serve to differentiate the data science education of iSchools from that of computer science or business schools. We present a layered Data Science Education Framework (DSEF) with building blocks that include the three pillars of data science (people, technology, and data), computational thinking, data-driven paradigms, and data science lifecycles. Data science courses built on the top of this framework should thus be executed with user-based, tool-based, and application-based approaches. This framework will help our students think about data science problems from the big picture perspective and foster appropriate problem-solving skills in conjunction with broad perspectives of data science lifecycles. We hope the DSEF discussed in this article will help fellow iSchools in their design of new data science curricula.",2017.0,"I. Song, Yongjun Zhu"
19bb52bec8b5ced3175f4c3ef1b8fb7027cc5ff1,https://www.semanticscholar.org/paper/19bb52bec8b5ced3175f4c3ef1b8fb7027cc5ff1,Applications of Python to evaluate environmental data science problems,"There is a significant convergence of interests in the research community efforts to advance the development and application of software resources (capable of handling the relevant mathematical algorithms to provide scalable information) for solving data science problems. Anaconda is one of the many open source platforms that facilitate the use of open source programming languages (R, Python) for large‐scale data processing, predictive analytics, and scientific computing. The environmental research community may choose to adapt the use of either of the R or the Python programming languages for analyzing the data science problems on the Anaconda platform. This study demonstrated the applications of using Scikit‐learn (a Python machine learning library package) on Anaconda platform for analyzing the in‐bus carbon dioxide concentrations by (i) importing the data into Spyder (Python 3.6) in Anaconda, (ii) performing an exploratory data analysis, (iii) performing dimensionality reduction through RandomForestRegressor feature selection, (iv) developing statistical regression models, and (v) generating regression decision tree models with DecisionTreeRegressor feature. The readers may adopt the methods (inclusive of the Python coding) discussed in this article to successfully address their own data science problems. © 2017 American Institute of Chemical Engineers Environ Prog, 36: 1580–1586, 2017",2017.0,"Akhil Kadiyala, Ashok Kumar"
96f5a9360ccfd1c5c4210dc62948baac234c372d,https://www.semanticscholar.org/paper/96f5a9360ccfd1c5c4210dc62948baac234c372d,Predicting data science sociotechnical execution challenges by categorizing data science projects,"The challenge in executing a data science project is more than just identifying the best algorithm and tool set to use. Additional sociotechnical challenges include items such as how to define the project goals and how to ensure the project is effectively managed. This paper reports on a set of case studies where researchers were embedded within data science teams and where the researcher observations and analysis was focused on the attributes that can help describe data science projects and the challenges faced by the teams executing these projects, as opposed to the algorithms and technologies that were used to perform the analytics. Based on our case studies, we identified 14 characteristics that can help describe a data science project. We then used these characteristics to create a model that defines two key dimensions of the project. Finally, by clustering the projects within these two dimensions, we identified four types of data science projects, and based on the type of project, we identified some of the sociotechnical challenges that project teams should expect to encounter when executing data science projects.",2017.0,"J. Saltz, Ivan Shamshurin, Colin Connors"
bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,https://www.semanticscholar.org/paper/bd1c1d5540f246090e740c0d5a0fa7f2c64059d1,Data Science and its Relationship to Big Data and Data-Driven Decision Making,"Companies have realized they need to hire data scientists, academic institutions are scrambling to put together data-science programs, and publications are touting data science as a hot-even ""sexy""-career choice. However, there is confusion about what exactly data science is, and this confusion could lead to disillusionment as the concept diffuses into meaningless buzz. In this article, we argue that there are good reasons why it has been hard to pin down exactly what is data science. One reason is that data science is intricately intertwined with other important concepts also of growing importance, such as big data and data-driven decision making. Another reason is the natural tendency to associate what a practitioner does with the definition of the practitioner's field; this can result in overlooking the fundamentals of the field. We believe that trying to define the boundaries of data science precisely is not of the utmost importance. We can debate the boundaries of the field in an academic setting, but in order for data science to serve business effectively, it is important (i) to understand its relationships to other important related concepts, and (ii) to begin to identify the fundamental principles underlying data science. Once we embrace (ii), we can much better understand and explain exactly what data science has to offer. Furthermore, only once we embrace (ii) should we be comfortable calling it data science. In this article, we present a perspective that addresses all these concepts. We close by offering, as examples, a partial list of fundamental principles underlying data science.",2013.0,"F. Provost, Tom Fawcett"
d83a99bfb6f81565a186e0eb86858864568c1327,https://www.semanticscholar.org/paper/d83a99bfb6f81565a186e0eb86858864568c1327,Data science,"While it may not be possible to build a data brain identical to a human, data science can still aspire to imaginative machine thinking.",2017.0,Longbing Cao
ae118a88ada51dfdb2296cbaa948eb4a467942b6,https://www.semanticscholar.org/paper/ae118a88ada51dfdb2296cbaa948eb4a467942b6,"Computer Age Statistical Inference: Algorithms, Evidence, and Data Science","The twenty-first century has seen a breathtaking expansion of statistical methodology, both in scope and in influence. 'Big data', 'data science', and 'machine learning' have become familiar terms in the news, as statistical methods are brought to bear upon the enormous data sets of modern science and commerce. How did we get here? And where are we going? This book takes us on an exhilarating journey through the revolution in data analysis following the introduction of electronic computation in the 1950s. Beginning with classical inferential theories - Bayesian, frequentist, Fisherian - individual chapters take up a series of influential topics: survival analysis, logistic regression, empirical Bayes, the jackknife and bootstrap, random forests, neural networks, Markov chain Monte Carlo, inference after model selection, and dozens more. The distinctly modern approach integrates methodology and algorithms with statistical inference. The book ends with speculation on the future direction of statistics and data science.",2016.0,"B. Efron, Trevor J. Hastie"
4a6d46962d3f58d278cfb46d3ddebbb30bf275f5,https://www.semanticscholar.org/paper/4a6d46962d3f58d278cfb46d3ddebbb30bf275f5,Geographic Data Science,"Data science methods and approaches address all stages of transition from data to knowledge and action. Visualization of this data is essential for human understanding of the subject under study, analytical reasoning about it, and generating new knowledge. Geographic data science deals with data that incorporates spatial and, often, temporal elements. The articles selected for this special issue represent a mix of theoretical approaches and novel applications of geographic data science.",2017.0,"G. Andrienko, N. Andrienko, R. Weibel"
62806c60226d54ba1a4455bb1d7d2f034ef7c29a,https://www.semanticscholar.org/paper/62806c60226d54ba1a4455bb1d7d2f034ef7c29a,"Introducing Data Science: Big Data, Machine Learning, and more, using Python tools","Summary Introducing Data Science teaches you how to accomplish the fundamental tasks that occupy data scientists. Using the Python language and common Python libraries, you'll experience firsthand the challenges of dealing with data at scale and gain a solid foundation in data science. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Many companies need developers with data science skills to work on projects ranging from social media marketing to machine learning. Discovering what you need to learn to begin a career as a data scientist can seem bewildering. This book is designed to help you get started. About the BookIntroducing Data Science Introducing Data Science explains vital data science concepts and teaches you how to accomplish the fundamental tasks that occupy data scientists. Youll explore data visualization, graph databases, the use of NoSQL, and the data science process. Youll use the Python language and common Python libraries as you experience firsthand the challenges of dealing with data at scale. Discover how Python allows you to gain insights from data sets so big that they need to be stored on multiple machines, or from data moving so quickly that no single machine can handle it. This book gives you hands-on experience with the most popular Python data science libraries, Scikit-learn and Stats Models. After reading this book, youll have the solid foundation you need to start a career in data science. Whats Inside Handling large data Introduction to machine learning Using Python to work with data Writing data science algorithms About the ReaderThis book assumes you're comfortable reading code in Python or a similar language, such as C, Ruby, or JavaScript. No prior experience with data science is required. About the Authors Davy Cielen, Arno D. B. Meysman, and Mohamed Ali are the founders and managing partners of Optimately and Maiton, where they focus on developing data science projects and solutions in various sectors.",2016.0,"D. Cielen, Arno D. B. Meysman, M. Ali"
7c614fe86cc11c0430dd12b44e018e16e5dcf742,https://www.semanticscholar.org/paper/7c614fe86cc11c0430dd12b44e018e16e5dcf742,A Guide to Teaching Data Science,"ABSTRACT Demand for data science education is surging and traditional courses offered by statistics departments are not meeting the needs of those seeking training. This has led to a number of opinion pieces advocating for an update to the Statistics curriculum. The unifying recommendation is that computing should play a more prominent role. We strongly agree with this recommendation, but advocate the main priority is to bring applications to the forefront as proposed by Nolan and Speed in 1999. We also argue that the individuals tasked with developing data science courses should not only have statistical training, but also have experience analyzing data with the main objective of solving real-world problems. Here, we share a set of general principles and offer a detailed guide derived from our successful experience developing and teaching a graduate-level, introductory data science course centered entirely on case studies. We argue for the importance of statistical thinking, as defined by Wild and Pfannkuch in 1999 and describe how our approach teaches students three key skills needed to succeed in data science, which we refer to as creating, connecting, and computing. This guide can also be used for statisticians wanting to gain more practical knowledge about data science before embarking on teaching an introductory course. Supplementary materials for this article are available online.",2016.0,"S. Hicks, R. Irizarry"
0a9b30386408595ff0b3155d4de4a56dad80a97b,https://www.semanticscholar.org/paper/0a9b30386408595ff0b3155d4de4a56dad80a97b,The ambiguity of data science team roles and the need for a data science workforce framework,"This paper first reviews the benefits of well-defined roles and then discusses the current lack of standardized roles within the data science community, perhaps due to the newness of the field. Specifically, the paper reports on five case studies exploring five different attempts to define a standard set of roles. These case studies explore the usage of roles from an industry perspective as well as from national standard big data committee efforts. The paper then leverages the results of these case studies to explore the use of data science roles within online job postings. While some roles appeared frequently, such as data scientist and data engineer, no role was consistently used across all five case studies. Hence, the paper concludes by noting the need to create a data science workforce framework that could be used by students, employers, and academic institutions. This framework would enable organizations to staff their data science teams more accurately with the desired skillsets.",2017.0,"J. Saltz, N. Grady"
6b705d7ef453d42d87a9099b31344adad2367f40,https://www.semanticscholar.org/paper/6b705d7ef453d42d87a9099b31344adad2367f40,EDISON Data Science Framework: A Foundation for Building Data Science Profession for Research and Industry,"Data Science is an emerging field of science, which requires a multi-disciplinary approach and should be built with a strong link to emerging Big Data and data driven technologies, and consequently needs re-thinking and re-design of both traditional educational models and existing courses. The education and training of Data Scientists currently lacks a commonly accepted, harmonized instructional model that reflects by design the whole lifecycle of data handling in modern, data driven research and the digital economy. This paper presents the EDISON Data Science Framework (EDSF) that is intended to create a foundation for the Data Science profession definition. The EDSF includes the following core components: Data Science Competence Framework (CF-DS), Data Science Body of Knowledge (DS-BoK), Data Science Model Curriculum (MC-DS), and Data Science Professional profiles (DSP profiles). The MC-DS is built based on CF-DS and DS-BoK, where Learning Outcomes are defined based on CF-DS competences and Learning Units are mapped to Knowledge Units in DS-BoK. In its own turn, Learning Units are defined based on the ACM Classification of Computer Science (CCS2012) and reflect typical courses naming used by universities in their current programmes. The paper provides example how the proposed EDSF can be used for designing effective Data Science curricula and reports the experience of implementing EDSF by the Champion Universities that cooperate with the EDISON project.",2016.0,"Y. Demchenko, A. Belloum, W. Los, T. Wiktorski, A. Manieri, H. Brocks, Jana Becker, D. Heutelbeck, M. Hemmje, Steve Brewer"
12f3b97d76e2e07c3bf2914606d26bbfbbe85bd1,https://www.semanticscholar.org/paper/12f3b97d76e2e07c3bf2914606d26bbfbbe85bd1,Role of materials data science and informatics in accelerated materials innovation,"The goal of the Materials Genome Initiative is to substantially reduce the time and cost of materials design and deployment. Achieving this goal requires taking advantage of the recent advances in data and information sciences. This critical need has impelled the emergence of a new discipline, called materials data science and informatics. This emerging new discipline not only has to address the core scientific/technological challenges related to datafication of materials science and engineering, but also, a number of equally important challenges around data-driven transformation of the current culture, practices, and workflows employed for materials innovation. A comprehensive effort that addresses both of these aspects in a synergistic manner is likely to succeed in realizing the vision of scaled-up materials innovation. Key toolsets needed for the successful adoption of materials data science and informatics in materials innovation are identified and discussed in this article. Prototypical examples of emerging novel toolsets and their functionality are described along with select case studies.",2016.0,"S. Kalidindi, David B. Brough, Shengyen Li, A. Çeçen, Aleksandr L. Blekh, Faical Y. Congo, C. Campbell"
dd340315c44a9c68391d8d2f600a0adc76b70c09,https://www.semanticscholar.org/paper/dd340315c44a9c68391d8d2f600a0adc76b70c09,Fides: Towards a Platform for Responsible Data Science,"Issues of responsible data analysis and use are coming to the forefront of the discourse in data science research and practice, with most significant efforts to date on the part of the data mining, machine learning, and security and privacy communities. In these fields, the research has been focused on analyzing the fairness, accountability and transparency (FAT) properties of specific algorithms and their outputs. Although these issues are most apparent in the social sciences where fairness is interpreted in terms of the distribution of resources across protected groups, management of bias in source data affects a variety of fields. Consider climate change studies that require representative data from geographically diverse regions, or supply chain analyses that require data that represents the diversity of products and customers. Any domain that involves sparse or sampled data has exposure to potential bias. In this vision paper, we argue that FAT properties must be considered as database system issues, further upstream in the data science lifecycle: bias in source data goes unnoticed, and bias may be introduced during pre-processing (fairness), spurious correlations lead to reproducibility problems (accountability), and assumptions made during pre-processing have invisible but significant effects on decisions (transparency). As machine learning methods continue to be applied broadly by non-experts, the potential for misuse increases. We see a need for a data sharing and collaborative analytics platform with features to encourage (and in some cases, enforce) best practices at all stages of the data science lifecycle. We describe features of such a platform, which we term Fides, in the context of urban analytics, outlining a systems research agenda in responsible data science.",2017.0,"Julia Stoyanovich, Bill Howe, S. Abiteboul, G. Miklau, Arnaud Sahuguet, G. Weikum"
c04aaf36c8587e40747212e316d9bf44186ef64a,https://www.semanticscholar.org/paper/c04aaf36c8587e40747212e316d9bf44186ef64a,Developing a Research Agenda for Human-Centered Data Science,"The study and analysis of large and complex data sets offer a wealth of insights in a variety of applications. Computational approaches provide researchers access to broad assemblages of data, but the insights extracted may lack the rich detail that qualitative approaches have brought to the understanding of sociotechnical phenomena. How do we preserve the richness associated with traditional qualitative methods while utilizing the power of large data sets? How do we uncover social nuances or consider ethics and values in data use? These and other questions are explored by human-centered data science, an emerging field at the intersection of human-computer interaction (HCI), computer-supported cooperative work (CSCW), human computation, and the statistical and computational techniques of data science. This workshop, the first of its kind at CSCW, seeks to bring together researchers interested in human-centered approaches to data science to collaborate, define a research agenda, and form a community.",2016.0,"Cecilia M. Aragon, C. J. Hutto, A. Echenique, Brittany Fiore-Gartland, Yun Huang, Jinyoun Kim, Gina Neff, Wanli Xing, J. Bayer"
071036abe55e7247d7e6ec28a4afc8ef2670f479,https://www.semanticscholar.org/paper/071036abe55e7247d7e6ec28a4afc8ef2670f479,A Comparison of Open Source Tools for Data Science,"The next decade of competitive advantage revolves around the ability to make predictions and discover patterns in data. Data science is at the center of this revolution. Data science has been termed the sexiest job of the 21st century. Data science combines data mining, machine learning, and statistical methodologies to extract knowledge and leverage predictions from data. Given the need for data science in organizations, many small or medium organizations are not adequately funded to acquire expensive data science tools. Open source tools may provide the solution to this issue. While studies comparing open source tools for data mining or business intelligence exist, an update on the current state of the art is necessary. This work explores and compares common open source data science tools. Implications include an overview of the state of the art and knowledge for practitioners and academics to select an open source data science tool that suits the requirements of specific data science projects.",2016.0,"H. Wimmer, L. Powell"
8ffe80d758a78810c7d5a33a088cd4529b8a6a4b,https://www.semanticscholar.org/paper/8ffe80d758a78810c7d5a33a088cd4529b8a6a4b,Data science: supporting decision-making,"Abstract Data science is a new academic trans-discipline that builds on 60 years of research about supporting decision-making in organisations. It is an important and potentially significant concept and practice. Contemplating the need for data scientists encourages academics and managers to examine issues of decision-maker rationality, data and data analysis needs, analytical tools, job skills and academic preparation. This article explores data science and the data professionals who will use new data streams and analytics to support decision-making. It also examines the dimensions that are changing in the data stream and the skills needed by data scientists to analyse the new data streams. Organisations need data scientists, but academics need to understand the new data science jobs to prepare more people to support decision-making.",2016.0,D. Power
ca9f74a1a7b69214c670202bb4f66eb16194f836,https://www.semanticscholar.org/paper/ca9f74a1a7b69214c670202bb4f66eb16194f836,Datathons: An Experience Report of Data Hackathons for Data Science Education,Large amounts of data are becoming increasingly available through open data repositories as well as companies and governments collecting data to improve decision making and efficiencies. Consequently there is a need to increase the data literacy of computer science students. Data science is a relatively new area within computer science and the curriculum is rapidly evolving along with the tools required to perform analytics which students need to learn how to effectively use. To address the needs of students learning key data science and analytics skills we propose augmenting existing data science curriculums with hackathon events that focus on data also known as datathons. In this paper we present our experience at hosting and running four datathons that involved students and members from the community coming together to solve challenging problems with data from not-for-profit social good organizations and publicly open data. Our reported experience from our datathons will help inform other academics and community groups who also wish to host datathons to help facilitate their students and members to learn key data science and analytics skills.,2016.0,"C. Anslow, J. Brosz, F. Maurer, M. Boyes"
3af056b2aed8724dcddea074eb68aff6dd11c926,https://www.semanticscholar.org/paper/3af056b2aed8724dcddea074eb68aff6dd11c926,Building the biomedical data science workforce,"This article describes efforts at the National Institutes of Health (NIH) from 2013 to 2016 to train a national workforce in biomedical data science. We provide an analysis of the Big Data to Knowledge (BD2K) training program strengths and weaknesses with an eye toward future directions aimed at any funder and potential funding recipient worldwide. The focus is on extramurally funded programs that have a national or international impact rather than the training of NIH staff, which was addressed by the NIH’s internal Data Science Workforce Development Center. From its inception, the major goal of BD2K was to narrow the gap between needed and existing biomedical data science skills. As biomedical research increasingly relies on computational, mathematical, and statistical thinking, supporting the training and education of the workforce of tomorrow requires new emphases on analytical skills. From 2013 to 2016, BD2K jump-started training in this area for all levels, from graduate students to senior researchers.",2017.0,"Michelle Dunn, P. Bourne"
8ea48934b6f6a0717efb4e5355be3b008fc5b1bd,https://www.semanticscholar.org/paper/8ea48934b6f6a0717efb4e5355be3b008fc5b1bd,Coding the biodigital child: the biopolitics and pedagogic strategies of educational data science,"Abstract Educational data science is an emerging transdisciplinary field formed from an amalgamation of data science and elements of biological, psychological and neuroscientific knowledge about learning, or learning science. This article conceptualises educational data science as a biopolitical strategy focused on the evaluation and management of the corporeal, emotional and embrained lives of children. Such strategies are enacted through the development of new kinds of digitally-mediated ‘biopedagogies’ of body optimisation, ‘psychopedagogies’ of emotional maximisation, and ‘neuropedagogies’ of brain empowerment. The data practices, scientific knowledges, digital devices and pedagogies that constitute educational data science produce new systems of knowledge about the child that are consequential to their formation as ‘biodigital’ subjects, whose assumed qualities and capacities are defined through expert practices of biosensing, emotion analytics, and neurocomputation, combined with associated scientific knowledges. The article develops the concept of transcoding to account for the processes involved in the formation of the biodigital child.",2016.0,Ben Williamson
dd153ebd44a07dde2259c22d43bb9cd18db44d2a,https://www.semanticscholar.org/paper/dd153ebd44a07dde2259c22d43bb9cd18db44d2a,Modelling and Simulation in Materials Science and Engineering Visualization and analysis of atomistic simulation data with OVITO – the Open Visualization Tool,"The Open Visualization Tool (OVITO) is a new 3D visualization software designed for post-processing atomistic data obtained from molecular dynamics or Monte Carlo simulations. Unique analysis, editing and animations functions are integrated into its easy-to-use graphical user interface. The software is written in object-oriented C++, controllable via Python scripts and easily extendable through a plug-in interface. It is distributed as open-source software and can be downloaded from the website http://ovito.sourceforge.net/. (Some figures in this article are in colour only in the electronic version)",2009.0,A. Stukowski
f152a4008f114ac19076ee6b98d431268f4aea9e,https://www.semanticscholar.org/paper/f152a4008f114ac19076ee6b98d431268f4aea9e,A Practical and Sustainable Model for Learning and Teaching Data Science,"This paper details our experiences with design and implementation of data science curriculum at University at Buffalo (UB). We discuss (i) briefly the history of project, (ii) a certificate program that we created, (iii) a data-intensive computing course that forms the core of the curriculum and (iv) some of the challenges we faced and how we addressed them. Major goal of the project was to improve the preparedness of our workforce for the emerging data-intensive computing area. We measured this through assessment of student learning on various concepts and topics related to data-intensive computing. We also discuss the best practices in building a data science program. We highlight the importance of external funding support and multi-disciplinary collaborations in the success of the project. The pedagogical resources created for the project are freely available to help educators and other learners navigate the path to learning data science. We expect this paper about our experience will provide a road map for educators who desire to introduce data science in their curriculum.",2016.0,B. Ramamurthy
38fadf7c21c32b183fa3dcf32da1044e8441b813,https://www.semanticscholar.org/paper/38fadf7c21c32b183fa3dcf32da1044e8441b813,The Data Science Handbook,"microbial community dynamics, Support Vector Machines, a robust prediction method with applications in bioinformatics, Bayesian Model Selection for Data with High Dimension, High dimensional statistical inference: theoretical development to data analytics, Big data challenges in genomics, Analysis of microarray gene expression data using information theory and stochastic algorithm, Hybrid Models, Markov Chain Monte Carlo Methods: Theory and Practice, and more. Provides the authority and expertise of leading contributors from an international board of authors Presents the latest release in the Handbook of Statistics series Updated release includes the latest information on",2017.0,Field Cady
9c1b9598f82f9ed7d75ef1a9e627496759aa2387,https://www.semanticscholar.org/paper/9c1b9598f82f9ed7d75ef1a9e627496759aa2387,"Data Science, Predictive Analytics, and Big Data: A Revolution that Will Transform Supply Chain Design and Management","We illuminate the myriad of opportunities for research where supply chain management intersects with data science, predictive analytics, and big data, collectively referred to as DPB. We show that these terms are not only becoming popular but are also relevant to supply chain research and education. Data science requires both domain knowledge and a broad set of quantitative skills, but there is a dearth of literature on the topic and many questions. We call for research on skills that are needed by SCM data scientists and discuss how such skills and domain knowledge affect the effectiveness of a SCM data scientist. Such knowledge is crucial to developing future supply chain leaders. We propose definitions of data science and predictive analytics as applied to supply chain management. We examine possible applications of DPB in practice and provide examples of research questions from these applications, as well as examples of research questions employing DPB that stem from management theories. Finally, we propose specific steps interested researchers can take to respond to our call for research on the intersection of supply chain management and DPB.",2013.0,"M. Waller, S. Fawcett"
97a3726b3f9395c8919c6271540d87d1c44e10ac,https://www.semanticscholar.org/paper/97a3726b3f9395c8919c6271540d87d1c44e10ac,Deep feature synthesis: Towards automating data science endeavors,"In this paper, we develop the Data Science Machine, which is able to derive predictive models from raw data automatically. To achieve this automation, we first propose and develop the Deep Feature Synthesis algorithm for automatically generating features for relational datasets. The algorithm follows relationships in the data to a base field, and then sequentially applies mathematical functions along that path to create the final feature. Second, we implement a generalizable machine learning pipeline and tune it using a novel Gaussian Copula process based approach. We entered the Data Science Machine in 3 data science competitions that featured 906 other data science teams. Our approach beats 615 teams in these data science competitions. In 2 of the 3 competitions we beat a majority of competitors, and in the third, we achieved 94% of the best competitor's score. In the best case, with an ongoing competition, we beat 85.6% of the teams and achieved 95.7% of the top submissions score.",2015.0,"James Max Kanter, K. Veeramachaneni"
b0150dd118ebedbc3ece68726e065f9afaaf3b18,https://www.semanticscholar.org/paper/b0150dd118ebedbc3ece68726e065f9afaaf3b18,Big data analytics and big data science: a survey,"Big data has attracted much attention from academia and industry. But the discussion of big data is disparate, fragmented and distributed among different outlets. This paper conducts a systematic and extensive review on 186 journal publications about big data from 2011 to 2015 in the Science Citation Index (SCI) and the Social Science Citation Index (SSCI) database aiming to provide scholars and practitioners with a comprehensive overview and big picture about research on big data. The selected papers are grouped into 20 research categories. The contents of the paper(s) in each research category are summarized. Research directions for each category are outlined as well. The results in this study indicate that the selected papers were mainly published between 2013 and 2015 and focus on technological issues regarding big data. Diverse new approaches, methods, frameworks and systems are proposed for data collection, storage, transport, processing and analysis in the selected papers. Possible directions for f...",2016.0,"Yong Chen, Hong Chen, Anjee Gorkhali, Yang Lu, Yiqian Ma, L. Li"
bc0fff42a78b116e9338593a9149406715d7a531,https://www.semanticscholar.org/paper/bc0fff42a78b116e9338593a9149406715d7a531,Data science,"Design, development, evaluation 3D user interfaces. Interaction techniques metaphors, immersive. Selection manipulation. Travel and navigation. Symbolic, menu, gestural, and multimodal interaction. Dialogue design. 3D software support. 3D interaction devices and displays. Virtual and augmented reality. Tangible user interfaces. Review of relevant 3D math.",2019.0,Jae Woo Lee
c740a6816155fd123081d2f78926a0d3819926e7,https://www.semanticscholar.org/paper/c740a6816155fd123081d2f78926a0d3819926e7,LibGuides: *Data Science: Data Science Resources,"Data science resources, from finding ebooks and blogs, to finding raw datasets and analysis. Learn about data science resources, analysis, communities and data management. Also learn about hte datasets openly available and dataset purchase program.",2018.0,Allegra Tennis
47134cbdfa7c44a55de5697a35b6652d0fcfee30,https://www.semanticscholar.org/paper/47134cbdfa7c44a55de5697a35b6652d0fcfee30,Data Science in Libraries,"EDITOR'S SUMMARY 
 
The new field of data science involves advanced knowledge in statistics and computer science, combined with copious amounts of data. A report from the Big Data and Research Initiative under the Obama Administration, The Federal Big Data Research and Development Strategic Plan, calls attention to the roles that librarians will play in the future of data science. However, there are skills and management gaps librarians face that inhibit their ability to move forward in data science. A number of educational programs are now offered to remedy this problem, such as the Data and Visualization Institute for Librarians from North Carolina State University, the volunteer-led Library Carpentry program, and most recently, the Data Sciences in Libraries Project, funded by the IMLS. This project aims to get librarians and library managers together to discuss the world of data science and create a roadmap for strategic planning.",2017.0,"Matt Burton, L. Lyon"
469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,https://www.semanticscholar.org/paper/469fb7c7178c8370a93fb27dadc9c5c839a9b8ec,Information Science Roles in the Emerging Field of Data Science,"There has long been discussion about the distinctions of library science, information science, and informatics, and how these areas differ and overlap with computer science. Today the term data science is emerging that generates excitement and questions about how it relates to and differs from these other areas of study. For our purposes here, I consider information science to be the general term that subsumes library science and informatics and focuses on distinctions and similarities among these disciplines that each informs data science. At the most general levels, information science deals with the genesis, flow, use, and preservation of information; computer science deals with algorithms and techniques for computational processes. Data science as a concept emerges from the applications of existing studies of measurement, representation, interpretation, and management to problems in Citation: Gary Marchionini (2016). Information Science Roles in the Emerging Field of Data Science. Received: Mar. 10, 2016 Accepted: Mar. 22, 2016",2016.0,G. Marchionini
3eb5f2d152ead21ce528f9781c66197010eea3c8,https://www.semanticscholar.org/paper/3eb5f2d152ead21ce528f9781c66197010eea3c8,A Case for Data Commons: Toward Data Science as a Service,"Data commons collocate data, storage, and computing infrastructure with core services and commonly used tools and applications for managing, analyzing, and sharing data to create an interoperable resource for the research community. An architecture for data commons is described, as well as some lessons learned from operating several large-scale data commons.",2016.0,"R. Grossman, Allison P. Heath, Mark Murphy, M. Patterson, Walt Wells"
fa15d626d8905d08953abe646a75a31417ad61fa,https://www.semanticscholar.org/paper/fa15d626d8905d08953abe646a75a31417ad61fa,"Data science on the ground: Hype, criticism, and everyday work","Modern organizations often employ data scientists to improve business processes using diverse sets of data. Researchers and practitioners have both touted the benefits and warned of the drawbacks associated with data science and big data approaches, but few studies investigate how data science is carried out “on the ground.” In this paper, we first review the hype and criticisms surrounding data science and big data approaches. We then present the findings of semistructured interviews with 18 data analysts from various industries and organizational roles. Using qualitative coding techniques, we evaluated these interviews in light of the hype and criticisms surrounding data science in the popular discourse. We found that although the data analysts we interviewed were sensitive to both the allure and the potential pitfalls of data science, their motivations and evaluations of their work were more nuanced. We conclude by reflecting on the relationship between data analysts' work and the discourses around data science and big data, suggesting how future research can better account for the everyday practices of this profession.",2016.0,"Daniel Carter, Dan Sholler"
b26c93eba9e1d99a5c99b07d2476714b386c4d54,https://www.semanticscholar.org/paper/b26c93eba9e1d99a5c99b07d2476714b386c4d54,Agile big data analytics: AnalyticsOps for data science,"Big data analytic (BDA) systems leverage data distribution and parallel processing across a cluster of resources. This introduces a number of new challenges specifically for analytics. The analytics portion of the complete lifecycle has typically followed a waterfall process — completing one step before beginning the next. While efforts have been made to map different types of analytics to an agile methodology, the steps are often described as breaking activities into smaller tasks while the overall process is still consistent with step-by-step waterfall. BDA changes a number of the activities in the analytics lifecycle, as well as their ordering. The goal of agile analytics — to reach a point of optimality between generating value from data and the time spent getting there. This paper discusses the implications of an agile process for BDA in cleansing, transformation, and analytics.",2017.0,"N. Grady, Jason A. Payne, Huntley Parker"
f6705e68c71bc0b51ddb8d1e4f986c894ba8f34f,https://www.semanticscholar.org/paper/f6705e68c71bc0b51ddb8d1e4f986c894ba8f34f,"Data Science, Predictive Analytics, and Big Data in Supply Chain Management: Current State and Future Potential","While data science, predictive analytics, and big data have been frequently used buzzwords, rigorous academic investigations into these areas are just emerging. In this forward thinking article, we discuss the results of a recent large-scale survey on these topics among supply chain management (SCM) professionals, complemented with our experiences in developing, implementing, and administering one of the first master's degree programs in predictive analytics. As such, we effectively provide an assessment of the current state of the field via a large-scale survey, and offer insight into its future potential via the discussion of how a research university is training next-generation data scientists. Specifically, we report on the current use of predictive analytics in SCM and the underlying motivations, as well as perceived benefits and barriers. In addition, we highlight skills desired for successful data scientists, and provide illustrations of how predictive analytics can be implemented in the curriculum. Relying on one of the largest data sets of predictive analytics users in SCM collected to date and our experiences with one of the first master's degree programs in predictive analytics, it is our intent to provide a timely assessment of the field, illustrate its future potential, and motivate additional research and pedagogical advancements in this domain.",2015.0,"T. Schoenherr, Cheri Speier-Pero"
e78d7fa72a5dbe5f3bc93f6e200826004f23530b,https://www.semanticscholar.org/paper/e78d7fa72a5dbe5f3bc93f6e200826004f23530b,Data Science: Nature and Pitfalls,Data science is creating exciting trends as well as significant controversy. A critical matter for the healthy development of data science in its early stages is to deeply understand the nature of data and data science and discuss the various pitfalls. These important issues motivate the discussions in this article.,2016.0,Longbing Cao
d343c9823bcacf31ea4aca105d0366f3f18a75e5,https://www.semanticscholar.org/paper/d343c9823bcacf31ea4aca105d0366f3f18a75e5,The Role of Data Science in Web Science,"Web science relies on an interdisciplinary approach that seeks to go beyond what any one subject can say about the World Wide Web. By incorporating numerous disciplinary perspectives and relying heavily on domain knowledge and expertise, data science has emerged as an important new area that integrates statistics with computational knowledge, data collection, cleaning and processing, analysis methods, and visualization to produce actionable insights from big data. As a discipline to use within Web science research, data science offers significant opportunities for uncovering trends in large Web-based datasets. A Web science observatory exemplifies this relationship by offering an online platform of tools for carrying out Web science research, allowing users to carry out data science techniques to produce insights into Web science issues such as community development, online behavior, and information propagation. The authors outline the similarities and differences of these two growing subject areas to demonstrate the important relationship developing between them.",2016.0,"Christopher Phethean, E. Simperl, T. Tiropanis, Ramine Tinati, W. Hall"
bff0d1d3a3251cb7bcbeb424ae0580c3085649f7,https://www.semanticscholar.org/paper/bff0d1d3a3251cb7bcbeb424ae0580c3085649f7,Integrating Systems Modelling and Data Science: The Joint Future of Simulation and 'Big Data' Science,"Although System Dynamics modelling is sometimes referred to as data-poor modelling, it often is -or could be-applied in a data-rich manner. However, more can be done in the era of 'big data'. Big data refers here to situations with much more available data than was until recently manageable. The field of data science makes bigger data manageable. This paper provides a perspective on the future of System Dynamics with a prominent place for bigger data and data science. It discusses different approaches for dealing with bigger data. It reviews methods, techniques and tools for dealing with bigger data in System Dynamics, and sheds light on the modelling phases for which data science is most useful. Finally, it provides several examples of current applications in which big data, data science, and System Dynamics modelling and simulation are being merged.",2016.0,E. Pruyt
b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,https://www.semanticscholar.org/paper/b70cfcc6bbb764728f8aa55aa173cc692eb77bdf,The Process of Analyzing Data is the Emergent Feature of Data Science,"In recent years the term “data science” gained considerable attention worldwide. In a A Very Short History Of Data Science by Press (2013), the first appearance of the term is ascribed to Peter Naur in 1974 (Concise Survey of Computer Methods). Regardless who used the term first and in what context it has been used, we think that data science is a good term to indicate that data are the focus of scientific research. This is in analogy to computer science, where the first department of computer science in the USA had been established in 1962 at Purdue University, at a time when the first electronic computers became available and it was still not clear enough what computers can do, one created therefore a new field where the computer was the focus of the study. In this paper, we want to address a couple of questions in order to demystify the meaning and the goals of data science in general.",2016.0,"F. Emmert-Streib, S. Moutari, M. Dehmer"
def43235dba7eb98659fb8879fa9d27695029df2,https://www.semanticscholar.org/paper/def43235dba7eb98659fb8879fa9d27695029df2,Recent Activities in Earth Data Science [Technical Committees],"Recent trends on big Earth-observing (EO) data lead to some questions that the Earth science community needs to address. Are we experiencing a paradigm shift in Earth science research now? How can we better utilize the explosion of technology maturation to create new forms of EO data processing? Can we summarize the existing methodologies and technologies scaling to big EO data as a new field named earth data science? Big data technologies are being widely practiced in Earth sciences and remote sensing communities to support EO data access, processing, and knowledge discovery. The data-intensive scientific discovery, named the fourth paradigm, leads to data science in the big data era [1]. According to the definition by the U.S. National Institute of Standards and Technology, the data science paradigm is the ""extraction of actionable knowledge directly from data through a process of discovery, hypothesis, and hypothesis testing"" [2]. Earth data science is the art and science of applying the data science paradigm to EO data.",2016.0,"P. Yue, R. Ramachandran, P. Baumann, S. Khalsa, M. Deng, Liangcun Jiang"
e1a1ad4025e2c7a82882c7389d937cbdfd10b799,https://www.semanticscholar.org/paper/e1a1ad4025e2c7a82882c7389d937cbdfd10b799,Towards Data Science,"Currently, a huge amount of data is being rapidly generated in cyberspace. Datanature (all data in cyberspace) is forming due to a data explosion. Exploring the patterns and rules in datanature is necessary but difficult. A new discipline called Data Science is coming. It provides a type of novel research method (a data-intensive method) for natural and social sciences and goes beyond computer science in researching data. This paper presents the challenges presented by data and discusses what differentiates data science from the established sciences, data technologies, and big data. Our goal is to encourage data related researchers to transfer their focus towards this new science.",2015.0,"Yangyong Zhu, Yun Xiong"
5a56bbd762e9dd70dd20afe8740a6d09ec85ffed,https://www.semanticscholar.org/paper/5a56bbd762e9dd70dd20afe8740a6d09ec85ffed,Data science from scratch,"This is a first-principles-based, practical introduction to the fundamentals of data science aimed at the mathematically-comfortable reader with some programming skills. The book covers: The important parts of Python to know The important parts of Math / Probability / Statistics to know The basics of data science How commonly-used data science techniques work (learning by implementing them) What is Map-Reduce and how to do it in Python Other applications such as NLP, Network Analysis, and more",2015.0,Joel Grus
52ff64f7f26b28447af255fedeb2216a70b48d66,https://www.semanticscholar.org/paper/52ff64f7f26b28447af255fedeb2216a70b48d66,Large Scale Distributed Data Science using Apache Spark,"Apache Spark is an open-source cluster computing framework for big data processing. It has emerged as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce's linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices.",2015.0,"J. Shanahan, Liang Dai"
4c05d4410c0023e14f2bb0cbcf7613468855430b,https://www.semanticscholar.org/paper/4c05d4410c0023e14f2bb0cbcf7613468855430b,A Data Science Course for Undergraduates: Thinking With Data,"Data science is an emerging interdisciplinary field that combines elements of mathematics, statistics, computer science, and knowledge in a particular application domain for the purpose of extracting meaningful information from the increasingly sophisticated array of data available in many settings. These data tend to be nontraditional, in the sense that they are often live, large, complex, and/or messy. A first course in statistics at the undergraduate level typically introduces students to a variety of techniques to analyze small, neat, and clean datasets. However, whether they pursue more formal training in statistics or not, many of these students will end up working with data that are considerably more complex, and will need facility with statistical computing techniques. More importantly, these students require a framework for thinking structurally about data. We describe an undergraduate course in a liberal arts environment that provides students with the tools necessary to apply data science. The course emphasizes modern, practical, and useful skills that cover the full data analysis spectrum, from asking an interesting question to acquiring, managing, manipulating, processing, querying, analyzing, and visualizing data, as well communicating findings in written, graphical, and oral forms. Supplementary materials for this article are available online. [Received June 2014. Revised July 2015.]",2015.0,B. Baumer
0442b04b4e8741900b65de0721f0c3e152e044ef,https://www.semanticscholar.org/paper/0442b04b4e8741900b65de0721f0c3e152e044ef,Materials Data Science: Current Status and Future Outlook,"The field of materials science and engineering is on the cusp of a digital data revolution. After reviewing the nature of data science and Big Data, we discuss the features of materials data that distinguish them from data in other fields. We introduce the concept of process-structure-property (PSP) linkages and illustrate how the determination of PSPs is one of the main objectives of materials data science. Then we review a selection of materials databases, as well as important aspects of materials data management, such as storage hardware, archiving strategies, and data access strategies. We introduce the emerging field of materials data analytics, which focuses on data-driven approaches to extract and curate materials knowledge from available data sets. The critical need for materials e-collaboration platforms is highlighted, and we conclude the article with a number of suggestions regarding the near-term future of the materials data science field.",2015.0,"S. Kalidindi, M. Graef"
3e209c705350761fe676ac330503e8662279fbf2,https://www.semanticscholar.org/paper/3e209c705350761fe676ac330503e8662279fbf2,Processes Meet Big Data: Connecting Data Science with Process Science,"As more and more companies are embracing Big data, it has become apparent that the ultimate challenge is to relate massive amounts of event data to processes that are highly dynamic. To unleash the value of event data, events need to be tightly connected to the control and management of operational processes. However, the primary focus of Big data technologies is currently on storage, processing, and rather simple analytical tasks. Big data initiatives rarely focus on the improvement of end-to-end processes. To address this mismatch, we advocate a better integration of data science, data technology and process science. Data science approaches tend to be process agonistic whereas process science approaches tend to be model-driven without considering the “evidence” hidden in the data. Process mining aims to bridge this gap. This editorial discusses the interplay between data science and process science and relates process mining to Big data technologies, service orientation, and cloud computing.",2015.0,"Wil M.P. van der Aalst, E. Damiani"
2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,https://www.semanticscholar.org/paper/2ce0b954b5180fdc0834c3e4f0d14b5a0e668d53,Mining the Quantified Self: Personal Knowledge Discovery as a Challenge for Data Science,"The last several years have seen an explosion of interest in wearable computing, personal tracking devices, and the so-called quantified self (QS) movement. Quantified self involves ordinary people recording and analyzing numerous aspects of their lives to understand and improve themselves. This is now a mainstream phenomenon, attracting a great deal of attention, participation, and funding. As more people are attracted to the movement, companies are offering various new platforms (hardware and software) that allow ever more aspects of daily life to be tracked. Nearly every aspect of the QS ecosystem is advancing rapidly, except for analytic capabilities, which remain surprisingly primitive. With increasing numbers of qualified self participants collecting ever greater amounts and types of data, many people literally have more data than they know what to do with. This article reviews the opportunities and challenges posed by the QS movement. Data science provides well-tested techniques for knowledge discovery. But making these useful for the QS domain poses unique challenges that derive from the characteristics of the data collected as well as the specific types of actionable insights that people want from the data. Using a small sample of QS time series data containing information about personal health we provide a formulation of the QS problem that connects data to the decisions of interest to the user.",2015.0,Tom Fawcett
659890e52fe234cde0e02a2305e213d3e8cb14b2,https://www.semanticscholar.org/paper/659890e52fe234cde0e02a2305e213d3e8cb14b2,Data science and cyberinfrastructure: critical enablers for accelerated development of hierarchical materials,"The slow pace of new/improved materials development and deployment has been identified as the main bottleneck in the innovation cycles of most emerging technologies. Much of the continuing discussion in the materials development community is therefore focused on the creation of novel materials innovation ecosystems designed to dramatically accelerate materials development efforts, while lowering the overall cost involved. In this paper, it is argued that the recent advances in data science can be leveraged suitably to address this challenge by effectively mediating between the seemingly disparate, inherently uncertain, multiscale and multimodal measurements and computations involved in the current materials’ development efforts. Proper utilisation of modern data science in the materials’ development efforts can lead to a new generation of data-driven decision support tools for guiding effort investment (for both measurements and computations) at various stages of the materials development. It should also be recognised that the success of such ecosystems is predicated on the creation and utilisation of integration platforms for promoting intimate, synchronous collaborations between cross-disciplinary and distributed team members (i.e. cyberinfrastructure). Indeed, data sciences and cyberinfrastructure form the two main pillars of the emerging new discipline broadly referred to as materials informatics (MI). This paper provides a summary of current capabilities in this emerging new field as they relate to the accelerated development of advanced hierarchical materials (the internal structure plays a dominant role in controlling overall properties/performance in these materials) and identifies specific directions of research that offer the most promising avenues.",2015.0,S. Kalidindi
e12b5363078a6d435bfba80e9d5cbab6b2cac897,https://www.semanticscholar.org/paper/e12b5363078a6d435bfba80e9d5cbab6b2cac897,Data Science and Digital Art History,"I present a number of core concepts from data science that are relevant to digital art history and the use of quantitative methods to study any cultural artifacts or processes in general. These concepts are objects, features, data, feature space, and dimension reduction. These concepts enable computational exploration of both large and small visual cultural data. We can analyze relations between works on a single artist, many artists, all digitized production from a whole historical period, holdings in museum collections, collection metadata, or writings about art. The same concepts allow us to study contemporary vernacular visual media using massive social media content. (In our lab, we analyzed works by van Gogh, Mondrian, and Rothko, 6000 paintings by French Impressionists, 20,000 photographs from MoMA photo­graphy collection, one million manga pages from manga books, one million artworks of contemporary non-professional artists, and over 13 million Instagram images from 16 global cities.) While data science techniques do not replace other art historical methods, they allow us to see familiar art historical material in new ways, and also to study contemporary digital visual culture.",2015.0,Lev Manovich
061c3291d817076dbb3e5a41c51f99800a390e94,https://www.semanticscholar.org/paper/061c3291d817076dbb3e5a41c51f99800a390e94,"Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data","Data Science and Big Data Analytics: Discovering, Analyzing, Visualizing and Presenting Data By EMC Education Services Data Science and Big Data Analytics is about harnessing the power of data for new insights. The book covers the breadth of activities and methods and tools that Data Scientists use. The content focuses on concepts, principles and practical applications that are applicable to any industry and technology environment, and the learning is supported and explained with examples that you can replicate using open-source software. This book will help you: Become a contributor on a data science team ●",2015.0,Emc Education Services
b9111489ec08b50bc573982ede11f5bc2d7a4e88,https://www.semanticscholar.org/paper/b9111489ec08b50bc573982ede11f5bc2d7a4e88,Sjplot - Data Visualization For Statistics In Social Science.,"New functions


 tab_model() as replacement for sjt.lm() , sjt.glm() , sjt.lmer() and sjt.glmer() . Furthermore, tab_model() is designed to work with the same model-objects as plot_model() .
 New colour scales for ggplot-objects: scale_fill_sjplot() and scale_color_sjplot() . These provide predifined colour palettes from this package.
 show_sjplot_pals() to show all predefined colour palettes provided by this package.
 sjplot_pal() to return colour values of a specific palette.


Deprecated

Following functions are now deprecated:


 sjp.lm() , sjp.glm() , sjp.lmer() , sjp.glmer() and sjp.int() . Please use plot_model() instead.
 sjt.frq() . Please use sjmisc::frq(out = ""v"") instead.


Removed / Defunct

Following functions are now defunct:


 sjt.grpmean() , sjt.mwu() and sjt.df() . The replacements are sjstats::grpmean() , sjstats::mwu() and tab_df() resp. tab_dfs() .


Changes to functions


 plot_model() and plot_models() get a prefix.labels -argument, to prefix automatically retrieved term labels with either the related variable name or label.
 plot_model() gets a show.zeroinf -argument to show or hide the zero-inflation-part of models in the plot.
 plot_model() gets a jitter -argument to add some random variation to data points for those plot types that accept show.data = TRUE .
 plot_model() gets a legend.title -argument to define the legend title for plots that display a legend.
 plot_model() now passes more arguments in ... down to ggeffects::plot() for marginal effects plots.
 plot_model() now plots the zero-inflated part of the model for brmsfit -objects.
 plot_model() now plots multivariate response models, i.e. models with multiple outcomes.
 Diagnostic plots in plot_model() ( type = ""diag"" ) can now also be used with brmsfit -objects.
 Axis limits of diagnostic plots in plot_model() ( type = ""diag"" ) for Stan-models ( brmsfit or stanreg resp. stanfit ) can now be set with the axis.lim -argument.
 The grid.breaks -argument for plot_model() and plot_models() now also takes a vector of values to directly define the grid breaks for the plot.
 Better default calculation for grid breaks in plot_model() and plot_models() when the grid.breaks -argument is of length one.
 The terms -argument for plot_model() now also allows the specification of a range of numeric values in square brackets for marginal effects plots, e.g. terms = ""age [30:50]"" or terms = ""age [pretty]"" .
 For coefficient-plots, the terms - and rm.terms -arguments for plot_model() now also allows specification of factor levels for categorical terms. Coefficients for the indicted factor levels are kept resp. removed (see ?plot_model for details).
 plot_model() now supports clmm -objects (package ordinal).
 plot_model(type = ""diag"") now also shows random-effects QQ-plots for glmmTMB -models, and also plots random-effects QQ-plots for all random effects (if model has more than one random effect term).


Bug fixes


 plot_model(type = ""re"") now supports standard errors and confidence intervals for glmmTMB -objects.
 Fixed typo for glmmTMB -tidier, which may have returned wrong data for zero-inflation part of model.
 Multiple random intercepts for multilevel models fitted with brms area now shown in each own facet per intercept.
 Remove unnecessary warning in sjp.likert() for uneven category count when neutral category is specified.
 plot_model(type = ""int"") could not automatically select mdrt.values properly for non-integer variables.
 sjp.grpfrq() now correctly uses the complete space in facets when facet.grid = TRUE .
 sjp.grpfrq(type = ""boxplot"") did not correctly label the x-axis when one category had no elements in a vector.
 Problems with German umlauts when printing HTML tables were fixed.",2018.0,D. Lüdecke
de84e808462b8240c75987364a6d518eff7d8813,https://www.semanticscholar.org/paper/de84e808462b8240c75987364a6d518eff7d8813,Statistics: a data science for the 21st century,"The rise of data science could be seen as a potental threat to the long‐term status of the statistics discipline. I first argue that, although there is a threat, there is also a much greater opportunity to re‐emphasize the universal relevance of statistical method to the interpretation of data, and I give a short historical outline of the increasingly important links between statistics and information technology. The core of the paper is a summary of several recent research projects, through which I hope to demonstrate that statistics makes an essential, but incomplete, contribution to the emerging field of ‘electronic health’ research. Finally, I offer personal thoughts on how statistics might best be organized in a research‐led university, on what we should teach our students and on some issues broadly related to data science where the Royal Statistical Society can take a lead.",2015.0,P. Diggle
8f6a4609531ca9ff35915c32dae5cd146fc57c40,https://www.semanticscholar.org/paper/8f6a4609531ca9ff35915c32dae5cd146fc57c40,HEALTH BANK - A Workbench for Data Science Applications in Healthcare,"The enormous amounts of data that are generated in the healthcare process and stored in electronic health record (EHR) systems are an underutilized resource that, with the use of data science applica- tions, can be exploited to improve healthcare. To foster the development and use of data science applications in healthcare, there is a fundamen- tal need for access to EHR data, which is typically not readily available to researchers and developers. A relatively rare exception is the large EHR database, the Stockholm EPR Corpus, comprising data from more than two million patients, that has been been made available to a lim- ited group of researchers at Stockholm University. Here, we describe a number of data science applications that have been developed using this database, demonstrating the potential reuse of EHR data to support healthcare and public health activities, as well as facilitate medical re- search. However, in order to realize the full potential of this resource, it needs to be made available to a larger community of researchers, as well as to industry actors. To that end, we envision the provision of an in- frastructure around this database called HEALTH BANK – the Swedish Health Record Research Bank. It will function both as a workbench for the development of data science applications and as a data explo- ration tool, allowing epidemiologists, pharmacologists and other medical researchers to generate and evaluate hypotheses. Aggregated data will be fed into a pipeline for open e-access, while non-aggregated data will be provided to researchers within an ethical permission framework. We believe that HEALTH BANK has the potential to promote a growing industry around the development of data science applications that will ultimately increase the efficiency and effectiveness of healthcare.",2015.0,"H. Dalianis, Aron Henriksson, Maria Kvist, S. Velupillai, Rebecka Weegar"
b885916e9af51010ca7ebafbc9270f0e5e207b38,https://www.semanticscholar.org/paper/b885916e9af51010ca7ebafbc9270f0e5e207b38,Nursing Knowledge: Big Data Science—Implications for Nurse Leaders,"The integration of Big Data from electronic health records and other information systems within and across health care enterprises provides an opportunity to develop actionable predictive models that can increase the confidence of nursing leaders' decisions to improve patient outcomes and safety and control costs. As health care shifts to the community, mobile health applications add to the Big Data available. There is an evolving national action plan that includes nursing data in Big Data science, spearheaded by the University of Minnesota School of Nursing. For the past 3 years, diverse stakeholders from practice, industry, education, research, and professional organizations have collaborated through the “Nursing Knowledge: Big Data Science” conferences to create and act on recommendations for inclusion of nursing data, integrated with patient-generated, interprofessional, and contextual data. It is critical for nursing leaders to understand the value of Big Data science and the ways to standardize data and workflow processes to take advantage of newer cutting edge analytics to support analytic methods to control costs and improve patient quality and safety.",2015.0,"B. Westra, T. Clancy, J. Sensmeier, J. Warren, Charlotte A. Weaver, C. Delaney"
0e341b2a181f71dea088dbba800e70262f91a79e,https://www.semanticscholar.org/paper/0e341b2a181f71dea088dbba800e70262f91a79e,"Color Science: Concepts and Methods, Quantitative Data and Formulae, 2nd Edition",Physical Data. The Eye. Colorimetry. Photometry. Visual Equivalence and Visual Matching. Uniform Color Scales. Visual Thresholds. Theories and Models of Color Vision. Appendix. References. Author and Subject Indexes.,2000.0,"Gunther Wyszecki, W. Stiles"
87a7e55b4c3116751edb4b0f74e0484eaf7a853d,https://www.semanticscholar.org/paper/87a7e55b4c3116751edb4b0f74e0484eaf7a853d,"Editorial - Big Data, Data Science, and Analytics: The Opportunity and Challenge for IS Research","We address key questions related to the explosion of interest in the emerging fields of big data, analytics, and data science. We discuss the novelty of the fields and whether the underlying questions are fundamentally different, the strengths that the information systems IS community brings to this discourse, interesting research questions for IS scholars, the role of predictive and explanatory modeling, and how research in this emerging area should be evaluated for contribution and significance.",2014.0,"Ritu Agarwal, V. Dhar"
520515cfffcd2f439469398d7c959f8baa9ccc8b,https://www.semanticscholar.org/paper/520515cfffcd2f439469398d7c959f8baa9ccc8b,Philosophy of Big Data: Expanding the Human-Data Relation with Big Data Science Services,"Big data is growing as an area of information technology, service, and science, and so too is the need for its intellectual understanding and interpretation from a theoretical, philosophical, and societal perspective. The Philosophy of Big Data is the branch of philosophy concerned with the foundations, methods, and implications of big data, the definitions, meaning, conceptualization, knowledge possibilities, truth standards, and practices in situations involving very-large data sets that are big in volume, velocity, variety, veracity, and variability. The Philosophy of Big Data is evolving into a discipline at two levels, one internal to the field as a generalized articulation of the concepts, theory, and systems that comprise the overall conduct of big data science. The other is external to the field, as a consideration of the impact of big data science more broadly on individuals, society, and the world. Methods, tools, and concepts are evaluated at both the level of industry practice theory and social impact. Three aspects are considered: what might constitute a Philosophy of Big Data, how the disciplines of the Philosophy of Information and the Philosophy of Big Data are developing, and an example of the Philosophy of Big Data in application in the data-intensive science field of Synthetic Biology. Overall a Philosophy of Big Data might helpful in conceptualizing and realizing big data science as a service practice, and also in transitioning to data-rich futures with human and data entities more productively co-existing in mutual growth and collaboration.",2015.0,M. Swan
04831fedd16110da4cbd0798d16e21fbbc34ad06,https://www.semanticscholar.org/paper/04831fedd16110da4cbd0798d16e21fbbc34ad06,A survey of open source data science tools,"Purpose – Data science is the study of the generalizable extraction of knowledge from data. It includes a variety of components and develops on methods and concepts from many domains, containing mathematics, probability models, machine learning, statistical learning, computer programming, data engineering, pattern recognition and learning, visualization and data warehousing aiming to extract value from data. The purpose of this paper is to provide an overview of open source (OS) data science tools, proposing a classification scheme that can be used to study OS data science software. Design/methodology/approach – The proposed classification scheme is based on general characteristics, project activity, operational characteristics and data mining characteristics. The authors then use the proposed scheme to examine 70 identified Open Source Software. From this the authors provide insight about the current status of OS data science tools and reveal the state-of-the-art tools. Findings – The features of 70 OS t...",2015.0,"Panagiotis Barlas, Ivor Lanning, C. Heavey"
23a57b1e2beb4235d2020ed57f484c947e3d0816,https://www.semanticscholar.org/paper/23a57b1e2beb4235d2020ed57f484c947e3d0816,The Quantified Self: Fundamental Disruption in Big Data Science and Biological Discovery,"A key contemporary trend emerging in big data science is the quantified self (QS)-individuals engaged in the self-tracking of any kind of biological, physical, behavioral, or environmental information as n=1 individuals or in groups. There are opportunities for big data scientists to develop new models to support QS data collection, integration, and analysis, and also to lead in defining open-access database resources and privacy standards for how personal data is used. Next-generation QS applications could include tools for rendering QS data meaningful in behavior change, establishing baselines and variability in objective metrics, applying new kinds of pattern recognition techniques, and aggregating multiple self-tracking data streams from wearable electronics, biosensors, mobile phones, genomic data, and cloud-based services. The long-term vision of QS activity is that of a systemic monitoring approach where an individual's continuous personal information climate provides real-time performance optimization suggestions. There are some potential limitations related to QS activity-barriers to widespread adoption and a critique regarding scientific soundness-but these may be overcome. One interesting aspect of QS activity is that it is fundamentally a quantitative and qualitative phenomenon since it includes both the collection of objective metrics data and the subjective experience of the impact of these data. Some of this dynamic is being explored as the quantified self is becoming the qualified self in two new ways: by applying QS methods to the tracking of qualitative phenomena such as mood, and by understanding that QS data collection is just the first step in creating qualitative feedback loops for behavior change. In the long-term future, the quantified self may become additionally transformed into the extended exoself as data quantification and self-tracking enable the development of new sense capabilities that are not possible with ordinary senses. The individual body becomes a more knowable, calculable, and administrable object through QS activity, and individuals have an increasingly intimate relationship with data as it mediates the experience of reality.",2013.0,M. Swan
57c82a005ae353f4683938b15a52e1b0561f6e43,https://www.semanticscholar.org/paper/57c82a005ae353f4683938b15a52e1b0561f6e43,"R for Data Science: Import, Tidy, Transform, Visualize, and Model Data","Learn how to use R to turn raw data into insight, knowledge, and understanding. This book introduces you to R, RStudio, and the tidyverse, a collection of R packages designed to work together to make data science fast, fluent, and fun. Suitable for readers with no previous programming experience, R for Data Science is designed to get you doing data science as quickly as possible. Authors Hadley Wickham and Garrett Grolemund guide you through the steps of importing, wrangling, exploring, and modeling your data and communicating the results. Youll get a complete, big-picture understanding of the data science cycle, along with basic tools you need to manage the details. Each section of the book is paired with exercises to help you practice what youve learned along the way. Youll learn how to: Wrangletransform your datasets into a form convenient for analysisProgramlearn powerful R tools for solving data problems with greater clarity and easeExploreexamine your data, generate hypotheses, and quickly test themModelprovide a low-dimensional summary that captures true ""signals"" in your datasetCommunicatelearn R Markdown for integrating prose, code, and results",2014.0,"H. Wickham, Garrett Grolemund"
6f989651c4f592613e92c9e37a8c4ac205998cfe,https://www.semanticscholar.org/paper/6f989651c4f592613e92c9e37a8c4ac205998cfe,Data Science in Statistics Curricula: Preparing Students to “Think with Data”,"A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science. [Received November 2014. Revised July 2015.]",2014.0,"Johanna Hardin, R. Hoerl, N. Horton, Deborah Nolan"
9ba08d45d60130c7e5880f63a980b185a86e177c,https://www.semanticscholar.org/paper/9ba08d45d60130c7e5880f63a980b185a86e177c,A Big Data Guide to Understanding Climate Change: The Case for Theory-Guided Data Science,"Global climate change and its impact on human life has become one of our era's greatest challenges. Despite the urgency, data science has had little impact on furthering our understanding of our planet in spite of the abundance of climate data. This is a stark contrast from other fields such as advertising or electronic commerce where big data has been a great success story. This discrepancy stems from the complex nature of climate data as well as the scientific questions climate science brings forth. This article introduces a data science audience to the challenges and opportunities to mine large climate datasets, with an emphasis on the nuanced difference between mining climate data and traditional big data approaches. We focus on data, methods, and application challenges that must be addressed in order for big data to fulfill their promise with regard to climate science applications. More importantly, we highlight research showing that solely relying on traditional big data techniques results in dubious findings, and we instead propose a theory-guided data science paradigm that uses scientific theory to constrain both the big data techniques as well as the results-interpretation process to extract accurate insight from large climate data.",2014.0,"James H. Faghmous, Vipin Kumar"
0636653b82e152ba99b1d921b0aa2798aa845d1e,https://www.semanticscholar.org/paper/0636653b82e152ba99b1d921b0aa2798aa845d1e,"Scopus as a curated, high-quality bibliometric data source for academic research in quantitative science studies","Abstract Scopus is among the largest curated abstract and citation databases, with a wide global and regional coverage of scientific journals, conference proceedings, and books, while ensuring only the highest quality data are indexed through rigorous content selection and re-evaluation by an independent Content Selection and Advisory Board. Additionally, extensive quality assurance processes continuously monitor and improve all data elements in Scopus. Besides enriched metadata records of scientific articles, Scopus offers comprehensive author and institution profiles, obtained from advanced profiling algorithms and manual curation, ensuring high precision and recall. The trustworthiness of Scopus has led to its use as bibliometric data source for large-scale analyses in research assessments, research landscape studies, science policy evaluations, and university rankings. Scopus data have been offered for free for selected studies by the academic research community, such as through application programming interfaces, which have led to many publications employing Scopus data to investigate topics such as researcher mobility, network visualizations, and spatial bibliometrics. In June 2019, the International Center for the Study of Research was launched, with an advisory board consisting of bibliometricians, aiming to work with the scientometric research community and offering a virtual laboratory where researchers will be able to utilize Scopus data.",2020.0,"J. Baas, M. Schotten, A. Plume, Grégoire Côté, Reza Karimi"
92efba7c622f54b8cd7b0d70d7cc09063e17b4f3,https://www.semanticscholar.org/paper/92efba7c622f54b8cd7b0d70d7cc09063e17b4f3,An undergraduate degree in data science: curriculum and a decade of implementation experience,"We describe Data Science, a four-year undergraduate program in predictive analytics, machine learning, and data mining implemented at the College of Charleston, Charleston, South Carolina, USA. We present a ten-year status report detailing the program's origins, successes, and challenges. Our experience demonstrates that education and training for big data concepts are possible and practical at the undergraduate level. The development of this program parallels the growing demand for finding utility in data sets and streaming data. The curriculum is a seventy-seven credit-hour program that has been successfully implemented in a liberal arts and sciences institution by the faculties of computer science and mathematics.",2014.0,"Paul E. Anderson, J. Bowring, R. McCauley, George J. Pothering, Christopher W. Starr"
0040c830969302a8c88c0c083aee5051e405bfe5,https://www.semanticscholar.org/paper/0040c830969302a8c88c0c083aee5051e405bfe5,"Big Data, Big Problems: Emerging Issues in the Ethics of Data Science and Journalism","As big data techniques become widespread in journalism, both as the subject of reporting and as newsgathering tools, the ethics of data science must inform and be informed by media ethics. This article explores emerging problems in ethical research using big data techniques. It does so using the duty-based framework advanced by W.D. Ross, who has significantly influenced both research science and media ethics. A successful framework must provide stability and flexibility. Without stability, ethical precommitments will vanish as technology rapidly shifts costs. Without flexibility, traditional approaches will rapidly become obsolete in the face of technological change. The article concludes that Ross's duty-based approach both provides stability in the face of rapid technological change and flexibility to innovate to achieve the original purpose of basic ethical principles.",2014.0,"Joshua Fairfield, Hannah Shtein"
f707f6d7c3f874cb1a8aa961a50e50706731cd2d,https://www.semanticscholar.org/paper/f707f6d7c3f874cb1a8aa961a50e50706731cd2d,Mechanism design for data science,"The promise of data science is that if data from a system can be recorded and understood then this understanding can potentially be utilized to improve the system. Behavioral and economic data, however, is different from scientific data in that it is subjective to the system. Behavior changes when the system changes, and to predict behavior for any given system change or to optimize over system changes, the behavioral model that generates the data must be inferred from the data. The ease with which this inference can be performed generally also depends on the system. Trivially, a system that ignores behavior does not admit any inference of a behavior generating model that can be used to predict behavior in a system that is responsive to behavior. To realize the promise of data science in economic systems, a theory for the design of such systems must also incorporate the desired inference properties. Consider as an example the revenue-maximizing auctioneer. If the auctioneer has knowledge of the distribution of bidder values then she can run the first-price auction with a reserve price that is tuned to the distribution. Under some mild distributional assumptions, with the appropriate reserve price the first-price auction is revenue optimal [Myerson 1981]. Notice that the historical bid data for the first-price auction with a reserve price will in most cases not have bids for bidders whose values are below the reserve. Therefore, there is no data analysis that the auctioneer can perform that will enable properties of the distribution of bidder values below the reserve price to be inferred. It could be, nonetheless, that over time the population of potential bidders evolves and the optimal reserve price lowers. This change could go completely unnoticed in the auctioneer's data. The two main tools for optimizing revenue in an auction are reserve prices (as above) and ironing. Both of these tools cause pooling behavior (i.e., bidders with distinct values take the same action) and economic inference cannot thereafter differentiate these pooled bidders. In order to maintain the distributional knowledge necessary to be able to run a good auction in the long term, the auctioneer must sacrifice the short-term revenue by running a non-revenue-optimal auction.",2014.0,"Shuchi Chawla, Jason D. Hartline, Denis Nekipelov"
9d653160d048eecf1a8138407994bfc69952324b,https://www.semanticscholar.org/paper/9d653160d048eecf1a8138407994bfc69952324b,Practical Data Science with R,"Summary Practical Data Science with R lives up to its name. It explains basic principles without the theoretical mumbo-jumbo and jumps right to the real use cases you'll face as you collect, curate, and analyze the data crucial to the success of your business. You'll apply the R programming language and statistical analysis techniques to carefully explained examples based in marketing, business intelligence, and decision support. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Book Business analysts and developers are increasingly collecting, curating, analyzing, and reporting on crucial business data. The R language and its associated tools provide a straightforward way to tackle day-to-day data science tasks without a lot of academic theory or advanced mathematics. Practical Data Science with R shows you how to apply the R programming language and useful statistical techniques to everyday business situations. Using examples from marketing, business intelligence, and decision support, it shows you how to design experiments (such as A/B tests), build predictive models, and present results to audiences of all levels. This book is accessible to readers without a background in data science. Some familiarity with basic statistics, R, or another scripting language is assumed. What's Inside Data science for the business professional Statistical analysis using the R language Project lifecycle, from planning to delivery Numerous instantly familiar use cases Keys to effective data presentations About the Authors Nina Zumel and John Mount are cofounders of a San Francisco-based data science consulting firm. Both hold PhDs from Carnegie Mellon and blog on statistics, probability, and computer science at win-vector.com.",2014.0,"N. Zumel, J. Mount"
516a53c59a53b0a471cd8a277b229925e0582114,https://www.semanticscholar.org/paper/516a53c59a53b0a471cd8a277b229925e0582114,DataHub: Collaborative Data Science & Dataset Version Management at Scale,"Relational databases have limited support for data collaboration, where teams collaboratively curate and analyze large datasets. Inspired by software version control systems like git, we propose (a) a dataset version control system, giving users the ability to create, branch, merge, difference and search large, divergent collections of datasets, and (b) a platform, DATAHUB, that gives users the ability to perform collaborative data analysis building on this version control system. We outline the challenges in providing dataset version control at scale.",2014.0,"Anant P. Bhardwaj, Souvik Bhattacherjee, Amit Chavan, A. Deshpande, Aaron J. Elmore, S. Madden, Aditya G. Parameswaran"
010a8ed71c6a80c2c02c7f55e1718151f91ff35a,https://www.semanticscholar.org/paper/010a8ed71c6a80c2c02c7f55e1718151f91ff35a,Web of Science as a data source for research on scientific and scholarly activity,"Abstract Web of Science (WoS) is the world’s oldest, most widely used and authoritative database of research publications and citations. Based on the Science Citation Index, founded by Eugene Garfield in 1964, it has expanded its selective, balanced, and complete coverage of the world’s leading research to cover around 34,000 journals today. A wide range of use cases are supported by WoS from daily search and discovery by researchers worldwide through to the supply of analytical data sets and the provision of specialized access to raw data for bibliometric partners. A long- and well-established network of such partners enables the Institute for Scientific Information (ISI) to continue to work closely with bibliometric groups around the world to the benefit of both the community and the services that the company provides to researchers and analysts.",2020.0,"C. Birkle, D. Pendlebury, Joshua Schnell, Jonathan Adams"
5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,https://www.semanticscholar.org/paper/5a92ccd20e551c191ff19bdd8e75bf1b64faa54b,Dealing with Data: Science Librarians' Participation in Data Management at Association of Research Libraries Institutions,"As long as empirical research has existed, researchers have been doing “data management” in one form or another. However, funding agency mandates for doing formal data management are relatively recent, and academic libraries’ involvement has been concentrated mainly in the last few years. The National Science Foundation implemented a new mandate in January 2011, requiring researchers to include a data management plan with their proposals for funding. This has prompted many academic libraries to work more actively than before in data management, and science librarians in particular are uniquely poised to step into new roles to meet researchers’ data management needs. This study, a survey of science librarians at institutions affiliated with the Association of Research Libraries, investigates science librarians’ awareness of and involvement in institutional repositories, data repositories, and data management support services at their institutions. The study also explores the roles and responsibilities, both new and traditional, that science librarians have assumed related to data management, and the skills that science librarians believe are necessary to meet the demands of data management work. The results reveal themes of both uncertainty and optimism—uncertainty about the roles of librarians, libraries, and other campus entities; uncertainty about the skills that will be required; but also optimism about applying “traditional” librarian skills to this emerging field of academic librarianship.",2014.0,"K. Antell, J. B. Foote, Jaymie Turner, Brian C. Shults"
da3b9f75b4313d741d73699634cd956cf7769331,https://www.semanticscholar.org/paper/da3b9f75b4313d741d73699634cd956cf7769331,Doing Data Science,"Big data is a relative term describing a situation where the volume, velocity and variety of data exceed an organizations storage or compute capacity for accurate and timely decision making . Big data is not a single technology but a combination of old and new technologies that helps companies gain actionable insight. Therefore, big data is the capability to manage a huge volume of disparate data, at the right speed, and within the right time frame to allow real-time analysis and reaction. As we note earlier in this chapter, big data is typically broken down by three characteristics: Volume:How much data Velocity:How fast that data is processed Variety:The various types of data Although its convenient to simplify big data into the three Vs, it can be misleading and overly simplistic. For example, you may be managing a relatively small amount of very disparate, complex data or you may be processing a huge volume of very simple data. That simple data may be all structured or all unstructured. Even more important is the fourth V:veracity. How accurate is that data in predicting business value? Do the results of a big data analysis actually make sense? Determining relevant data is key to delivering value from massive amounts of data. However, big data is defined less by volume which is a constantly moving target than by its ever-increasing variety, velocity, variability and complexity .",2014.0,Sarah H. Kanaan
0a7dd279ee312c9ef9c6fe04cd6f4f5e974abae3,https://www.semanticscholar.org/paper/0a7dd279ee312c9ef9c6fe04cd6f4f5e974abae3,A Data Science Solution for Mining Interesting Patterns from Uncertain Big Data,"Nowadays, high volumes of valuable uncertain data can be easily collected or generated at high velocity in many real-life applications. Mining these uncertain Big data is computationally intensive due to the presence of existential probability values associated with items in every transaction in the uncertain data. Each existential probability value expresses the likelihood of that item to be present in a particular transaction in the Big data. In some situations, users may be interested in mining all frequent patterns from these uncertain Big data, in other situations, users may be interested in only a tiny portion of these mined patterns. To reduce the computation and to focus the mining for the latter situations, we propose a tree-based algorithm that (i) allows users to express the patterns to be mined according to their intention via the use of constraints and (ii) uses MapReduce to mine uncertain Big data for only those frequent patterns that satisfy user-specified constraints. Experimental results show the effectiveness of our algorithm in mining interesting patterns from uncertain Big data.",2014.0,"C. Leung, Fan Jiang"
8e981ddb4877615f7d5f944a8d64789d1388ee87,https://www.semanticscholar.org/paper/8e981ddb4877615f7d5f944a8d64789d1388ee87,LSST: From Science Drivers to Reference Design and Anticipated Data Products,"We describe here the most ambitious survey currently planned in the optical, the Large Synoptic Survey Telescope (LSST). The LSST design is driven by four main science themes: probing dark energy and dark matter, taking an inventory of the solar system, exploring the transient optical sky, and mapping the Milky Way. LSST will be a large, wide-field ground-based system designed to obtain repeated images covering the sky visible from Cerro Pachón in northern Chile. The telescope will have an 8.4 m (6.5 m effective) primary mirror, a 9.6 deg2 field of view, a 3.2-gigapixel camera, and six filters (ugrizy) covering the wavelength range 320–1050 nm. The project is in the construction phase and will begin regular survey operations by 2022. About 90% of the observing time will be devoted to a deep-wide-fast survey mode that will uniformly observe a 18,000 deg2 region about 800 times (summed over all six bands) during the anticipated 10 yr of operations and will yield a co-added map to r ∼ 27.5. These data will result in databases including about 32 trillion observations of 20 billion galaxies and a similar number of stars, and they will serve the majority of the primary science programs. The remaining 10% of the observing time will be allocated to special projects such as Very Deep and Very Fast time domain surveys, whose details are currently under discussion. We illustrate how the LSST science drivers led to these choices of system parameters, and we describe the expected data products and their characteristics.",2008.0,"vZeljko Ivezi'c, S. Kahn, J. Tyson, B. Abel, Emily Acosta, R. Allsman, D. Alonso, Y. AlSayyad, S. Anderson, J. Andrew, J. Angel, G. Angeli, R. Ansari, P. Antilogus, C. Araujo, R. Armstrong, K. Arndt, P. Astier, 'Eric Aubourg, Nicole Auza, T. Axelrod, D. Bard, J. Barr, A. Barrau, J. Bartlett, A. Bauer, Brian Jeffrey Bauman, S. Baumont, A. Becker, J. Becla, C. Beldica, S. Bellavia, F. Bianco, R. Biswas, G. Blanc, J. Blazek, R. Blandford, J. Bloom, J. Bogart, T. Bond, A. Borgland, K. Borne, J. Bosch, D. Boutigny, Craig A. Brackett, A. Bradshaw, W. Brandt, M. Brown, J. Bullock, P. Burchat, D. Burke, G. Cagnoli, D. Calabrese, S. Callahan, Alice Callen, Srinivasan Chandrasekharan, Glenaver Charles-Emerson, S. Chesley, E. Cheu, Hsin-Fang Chiang, J. Chiang, Carol Chirino, Der-Mei. Chow, D. Ciardi, C. Claver, J. Cohen-Tanugi, Joseph J. Cockrum, R. Coles, A. Connolly, K. Cook, A. Cooray, K. Covey, C. Cribbs, Wei Cui, R. Cutri, P. Daly, S. Daniel, F. Daruich, G. Daubard, G. Daues, W. Dawson, F. Delgado, A. Dellapenna, R. Peyster, Miguel de Val-Borro, S. Digel, P. Doherty, R. Dubois, G. Dubois-Felsmann, J. Ďurech, F. Economou, M. Eracleous, H. Ferguson, Enrique Figueroa, M. Fisher-Levine, W. Focke, M. Foss, J. Frank, M. Freemon, E. Gangler, E. Gawiser, J. Geary, P. Gee, M. Geha, C. Gessner, R. Gibson, D. Gilmore, T. Glanzman, W. Glick, T. Goldina, D. Goldstein, Iain Goodenow, M. Graham, W. Gressler, P. Gris, L. Guy, A. Guyonnet, G. Haller, Ronald C. Harris, P. Hascall, J. Haupt, Fabio Hernandez, S. Herrmann, E. Hileman, J. Hoblitt, J. Hodgson, C. Hogan, Dajun Huang, M. Huffer, P. Ingraham, W. Innes, S. Jacoby, B. Jain, Fabrice Jammes, J. Jee, T. Jenness, G. Jernigan, Darko Jevremovi'c, Kenneth Johns, A. S. Johnson, Margaret W. G. Johnson, R. L. Jones, C. Juramy-Gilles, Mario Juri'c, J. Kalirai, N. Kallivayalil, B. Kalmbach, J. Kantor, P. Karst, M. Kasliwal, H. Kelly, R. Kessler, Veronica Kinnison, D. Kirkby, L. Knox, I. Kotov, V. Krabbendam, K. Krughoff, P. Kub'anek, J. Kuczewski, S. Kulkarni, J. Ku, N. Kurita, C. Lage, R. Lambert, Travis Lange, J. Langton, L. Guillou, Deborah A. Levine, M. Liang, Kian-Tat Lim, C. Lintott, Kevin Long, Margaux Lopez, P. Lotz, R. Lupton, N. Lust, L. Macarthur, A. Mahabal, R. Mandelbaum, D. Marsh, P. Marshall, S. Marshall, M. May, R. McKercher, M. Mcqueen, J. Meyers, M. Migliore, Michelle Miller, D. Mills, C. Miraval, Joachim Moeyens, D. Monet, M. Moniez, S. Monkewitz, C. Montgomery, Fritz Mueller, G. Muller, Freddy Munoz Arancibia, D. Neill, Scott P. Newbry, Jean-Yves Nief, A. Nomerotski, M. Nordby, P. O'connor, J. Oliver, S. Olivier, K. Olsen, W. O'Mullane, Sandra Ortiz, S. Osier, R. Owen, R. Pain, Paul E. Palecek, J. Parejko, James B. Parsons, N. M. Pease, J. Peterson, J. Peterson, D. Petravick, M. Petrick, C. Petry, F. Pierfederici, Stephen Pietrowicz, Robin C. Pike, P. Pinto, R. Plante, S. Plate, P. Price, M. Prouza, V. Radeka, J. Rajagopal, A. Rasmussen, N. Regnault, K. Reil, D. Reiss, Michael A. Reuter, S. Ridgway, V. Riot, S. Ritz, S. Robinson, W. Roby, A. Roodman, W. Rosing, C. Roucelle, M. Rumore, S. Russo, A. Saha, B. Sassolas, T. Schalk, P. Schellart, R. Schindler, S. Schmidt, D. Schneider, M. Schneider, W. Schoening, G. Schumacher, M. Schwamb, J. Sebag, Brian Selvy, G. Sembroski, L. Seppala, Andrew W. Serio, Eduardo Serrano, R. Shaw, I. Shipsey, J. Sick, N. Silvestri, C. Slater, J. A. Smith, R. C. Smith, S. Sobhani, Christine Soldahl, L. Storrie-Lombardi, Edward Stover, M. Strauss, R. Street, C. Stubbs, I. Sullivan, D. Sweeney, J. Swinbank, A. Szalay, P. Takacs, S. Tether, J. Thaler, J. Thayer, Sandrine Thomas, V. Thukral, J. Tice, D. Trilling, M. Turri, R. Berg, D. Berk, K. Vetter, Françoise Virieux, T. Vucina, W. Wahl, L. Walkowicz, B. Walsh, C. Walter, Daniel L. Wang, Shin-Ywan Wang, M. Warner, O. Wiecha, B. Willman, S. Winters, D. Wittman, S. Wolff, W. M. Wood-Vasey, Xiuqin Wu, B. Xin, P. Yoachim, H. Zhan, for the Lsst Collaboration"
425744cb05e854071d06af0da2b8ef2d677f33d5,https://www.semanticscholar.org/paper/425744cb05e854071d06af0da2b8ef2d677f33d5,Harnessing the GPS Data Explosion for Interdisciplinary Science,"More GPS stations, faster data delivery, and better data processing provide an abundance of information for all kinds of Earth scientists.",2018.0,"G. Blewitt, W. Hammond, C. Kreemer"
e084f4021f30c483564dcccc29d1230ab213ce70,https://www.semanticscholar.org/paper/e084f4021f30c483564dcccc29d1230ab213ce70,"Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic","We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",2020.0,"M. Visser, Nees Jan van Eck, L. Waltman"
1163c2996dfd0a46639b094e34ad783e969a0692,https://www.semanticscholar.org/paper/1163c2996dfd0a46639b094e34ad783e969a0692,Data science and prediction,Big data promises automated actionable knowledge creation and predictive models for use by both humans and computers.,2012.0,V. Dhar
abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,https://www.semanticscholar.org/paper/abc0a9eb3ae901ece2f532f504c336fbb6ba81ca,"Data‐Driven Materials Science: Status, Challenges, and Perspectives","Data‐driven science is heralded as a new paradigm in materials science. In this field, data is the new resource, and knowledge is extracted from materials datasets that are too big or complex for traditional human reasoning—typically with the intent to discover new or improved materials or materials phenomena. Multiple factors, including the open science movement, national funding, and progress in information technology, have fueled its development. Such related tools as materials databases, machine learning, and high‐throughput methods are now established as parts of the materials research toolset. However, there are a variety of challenges that impede progress in data‐driven materials science: data veracity, integration of experimental and computational data, data longevity, standardization, and the gap between industrial interests and academic efforts. In this perspective article, the historical development and current state of data‐driven materials science, building from the early evolution of open science to the rapid expansion of materials data infrastructures are discussed. Key successes and challenges so far are also reviewed, providing a perspective on the future development of the field.",2019.0,"Lauri Himanen, A. Geurts, A. Foster, P. Rinke"
3402835f33e3e1342eb86b4d13907e3c9121c82b,https://www.semanticscholar.org/paper/3402835f33e3e1342eb86b4d13907e3c9121c82b,Data science for business,"Written by renowned data science experts Foster Provost and Tom Fawcett, Data Science for Business introduces the fundamental principles of data science, and walks you through the ""data-analytic thinking"" necessary for extracting useful knowledge and business value from the data you collect. This guide also helps you understand the many data-mining techniques in use today. Based on an MBA course Provost has taught at New York University over the past ten years, Data Science for Business provides examples of real-world business problems to illustrate these principles. You'll not only learn how to improve communication between business stakeholders and data scientists, but also how participate intelligently in your company's data science projects. You'll also discover how to think data-analytically, and fully appreciate how data science methods can support business decision-making. Understand how data science fits in your organization - and how you can use it for competitive advantage Treat data as a business asset that requires careful investment if you're to gain real value Approach business problems data-analytically, using the data-mining process to gather good data in the most appropriate way Learn general concepts for actually extracting knowledge from data Apply data science principles when interviewing data science job candidates",2013.0,"F. Provost, Tom Fawcett"
c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,https://www.semanticscholar.org/paper/c2fb0ded7b21a23cd0931558b52ddbc98fc4f934,Doing Data Science,"Now that people are aware that data can make the difference in an election or a business model, data science as an occupation is gaining ground. But how can you get started working in a wide-ranging, interdisciplinary field that's so clouded in hype? This insightful book, based on Columbia University's Introduction to Data Science class, tells you what you need to know. In many of these chapter-long lectures, data scientists from companies such as Google, Microsoft, and eBay share new algorithms, methods, and models by presenting case studies and the code they use. If you're familiar with linear algebra, probability, and statistics, and have programming experience, this book is an ideal introduction to data science. Topics include: Statistical inference, exploratory data analysis, and the data science process Algorithms Spam filters, Naive Bayes, and data wrangling Logistic regression Financial modeling Recommendation engines and causality Data visualization Social networks and data journalism Data engineering, MapReduce, Pregel, and Hadoop Doing Data Science is collaboration between course instructor Rachel Schutt, Senior VP of Data Science at News Corp, and data science consultant Cathy O'Neil, a senior data scientist at Johnson Research Labs, who attended and blogged about the course.",2013.0,"R. Schutt, Cathy O'Neil"
283a00005b90bbf9bdc44d8fba89084a4e61bff7,https://www.semanticscholar.org/paper/283a00005b90bbf9bdc44d8fba89084a4e61bff7,Extending the Global Mass Change Data Record: GRACE Follow‐On Instrument and Science Data Performance,"Since June, 2018, the Gravity Recovery and Climate Experiment Follow‐On (GRACE‐FO) is extending the 15‐year monthly mass change record of the GRACE mission, which ended in June 2017. The GRACE‐FO instrument and flight system performance has improved over GRACE. Better attitude solutions and enhanced pointing performance result in reduced fuel consumption and gravity range rate post‐fit residuals. One accelerometer requires additional calibrations due to unexpected measurement noise. The GRACE‐FO gravity and mass change fields from June 2018 through December 2019 continue the GRACE record at an equivalent precision and spatiotemporal sampling. During this period, GRACE‐FO observed large interannual terrestrial water variations associated with excess rainfall (Central US, Middle East), drought (Europe, Australia), and ice melt (Greenland). These observations are consistent with independent mass change estimates, providing high confidence that no intermission biases exist from GRACE to GRACE‐FO, despite the 11‐month gap. GRACE‐FO has also successfully demonstrated satellite‐to‐satellite laser ranging interferometry.",2020.0,"F. Landerer, F. Flechtner, H. Save, F. Webb, T. Bandikova, W. Bertiger, S. Bettadpur, S. Byun, C. Dahle, H. Dobslaw, E. Fahnestock, N. Harvey, Z. Kang, G. Kruizinga, B. Loomis, C. McCullough, M. Murböck, P. Nagel, Meegyeong Paik, N. Pie, S. Poole, D. Strekalov, M. Tamisiea, Furun Wang, M. Watkins, H. Wen, D. Wiese, D. Yuan"
00a4bdc5158945a0b9463a29da4810838e474875,https://www.semanticscholar.org/paper/00a4bdc5158945a0b9463a29da4810838e474875,Perspective: Materials informatics and big data: Realization of the “fourth paradigm” of science in materials science,"Our ability to collect “big data” has greatly surpassed our capability to analyze it, underscoring the emergence of the fourth paradigm of science, which is data-driven discovery. The need for data informatics is also emphasized by the Materials Genome Initiative (MGI), further boosting the emerging field of materials informatics. In this article, we look at how data-driven techniques are playing a big role in deciphering processing-structure-property-performance relationships in materials, with illustrative examples of both forward models (property prediction) and inverse models (materials discovery). Such analytics can significantly reduce time-to-insight and accelerate cost-effective materials discovery, which is the goal of MGI.",2016.0,"Ankit Agrawal, A. Choudhary"
379e9576dea9690cf88d9132287edbefb7626232,https://www.semanticscholar.org/paper/379e9576dea9690cf88d9132287edbefb7626232,Data Smart: Using Data Science to Transform Information into Insight,"Data Science gets thrown around in the press like it's magic. Major retailers are predicting everything from when their customers are pregnant to when they want a new pair of Chuck Taylors. It's a brave new world where seemingly meaningless data can be transformed into valuable insight to drive smart business decisions.But how does one exactly do data science? Do you have to hire one of these priests of the dark arts, the ""data scientist,"" to extract this gold from your data? Nope.Data science is little more than using straight-forward steps to process raw data into actionable insight. And inData Smart, author and data scientist John Foreman will show you how that's done within the familiar environment of a spreadsheet.",2013.0,Jack Foreman
ee013b1477e8f81cb5c66a9a93a342281f740042,https://www.semanticscholar.org/paper/ee013b1477e8f81cb5c66a9a93a342281f740042,Assessing data quality in citizen science (preprint),"Ecological and environmental citizen science projects have enormous potential to advance science, influence policy, and guide resource management by producing datasets that are otherwise infeasible to generate. This potential can only be realized, though, if the datasets are of high quality. While scientists are often skeptical of the ability of unpaid volunteers to produce accurate datasets, a growing body of publications clearly shows that diverse types of citizen science projects can produce data with accuracy equal to or surpassing that of professionals. Successful projects rely on a suite of methods to boost data accuracy and account for bias, including iterative project development, volunteer training and testing, expert validation, replication across volunteers, and statistical modeling of systematic error. Each citizen science dataset should therefore be judged individually, according to project design and application, rather than assumed to be substandard simply because volunteers generated it.",2016.0,"M. Kosmala, A. Wiggins, A. Swanson, Brooke D. Simmons"
b1691f718aedc8e6b31105c50773761c753676f4,https://www.semanticscholar.org/paper/b1691f718aedc8e6b31105c50773761c753676f4,“Invisible Sportswomen”: The Sex Data Gap in Sport and Exercise Science Research,"This study aimed to conduct an updated exploration of the ratio of male and female participants in sport and exercise science research. Publications involving humans were examined from The European Journal of Sports Science, Medicine & Science in Sport & Exercise, The Journal of Sport Science & Medicine, The Journal of Physiology, The American Journal of Sports Medicine, and The British Journal of Sports Medicine, 2014–2020. The total number of participants, the number of male and female participants, the title, and the topic, were recorded for each publication. Data were expressed in frequencies and percentages. Chi-square analyses were used to assess the differences in frequencies in each of the journals. About 5,261 publications and 12,511,386 participants were included in the analyses. Sixty-three percentage of publications included both males and females, 31% included males only, and 6% included females only (p < .0001). When analyzing participants included in all journals, a total of 8,253,236 (66%) were male and 4,254,445 (34%) were female (p < .0001). Females remain significantly underrepresented within sport and exercise science research. Therefore, at present most conclusions made from sport and exercise science research might only be applicable to one sex. As such, researchers and practitioners should be aware of the ongoing sex data gap within the current literature, and future research should address this.",2021.0,"E. Cowley, A. Olenick, Kelly L. McNulty, E. Ross"
e60d9464935582cda41becd7c1455c09392a2a93,https://www.semanticscholar.org/paper/e60d9464935582cda41becd7c1455c09392a2a93,The Science of Visual Data Communication: What Works,"Effectively designed data visualizations allow viewers to use their powerful visual systems to understand patterns in data across science, education, health, and public policy. But ineffectively designed visualizations can cause confusion, misunderstanding, or even distrust—especially among viewers with low graphical literacy. We review research-backed guidelines for creating effective and intuitive visualizations oriented toward communicating data to students, coworkers, and the general public. We describe how the visual system can quickly extract broad statistics from a display, whereas poorly designed displays can lead to misperceptions and illusions. Extracting global statistics is fast, but comparing between subsets of values is slow. Effective graphics avoid taxing working memory, guide attention, and respect familiar conventions. Data visualizations can play a critical role in teaching and communication, provided that designers tailor those visualizations to their audience.",2021.0,"S. Franconeri, Lace M. K. Padilla, P. Shah, Jeffrey M. Zacks, J. Hullman"
81b3a5d57cdf751b31699af2bed1ca3d410d8509,https://www.semanticscholar.org/paper/81b3a5d57cdf751b31699af2bed1ca3d410d8509,DATA SCIENCE,". Over the recent decades, the nature of multi-core processors caused changing the serial programming model to parallel mode. There are several programming languages for the parallel multi-core processors and processors with different architectures that these languages have faced programmers to challenges to achieve higher performance. In addition, different scheduling methods in the programming languages for the multi-core processors have the significant impact on the efficiency of the programming languages. Therefore, this article addresses the investigation of the conventional scheduling techniques in the programming languages of multi-core processors which allows the researcher to choose more suitable programing languages by comparing efficiency than application. Several languages such as Cilk++، OpenMP، TBB and PThread were studied",,"Mina Hosseini-Rad, M. Abdolrazzagh-Nezhad, Seyyed-Mohammad Javadi-Moghaddam"
40b2324cde863db7670178f0151fae400a9a2b93,https://www.semanticscholar.org/paper/40b2324cde863db7670178f0151fae400a9a2b93,Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation,"We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that “multiple imputation” is a superior approach to the problem of missing data scattered through one’s explanatory and dependent variables than the methods currently used in applied data analysis. The discrepancy occurs because the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and have demanded considerable expertise. We adapt an algorithm and use it to implement a general-purpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, illustrate how to use the new procedure, and evaluate this alternative through simulated data as well as actual empirical examples. Finally, we offer easy-to-use software that implements all methods discussed.",2001.0,"Gary King, James Honaker, Anne Joseph O'Connell, Kenneth F. Scheve"
6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,https://www.semanticscholar.org/paper/6009556bdf3aa3a111a6ddc2c9200a59af1e13e2,"Active learning increases student performance in science, engineering, and mathematics","Significance The President’s Council of Advisors on Science and Technology has called for a 33% increase in the number of science, technology, engineering, and mathematics (STEM) bachelor’s degrees completed per year and recommended adoption of empirically validated teaching practices as critical to achieving that goal. The studies analyzed here document that active learning leads to increases in examination performance that would raise average grades by a half a letter, and that failure rates under traditional lecturing increase by 55% over the rates observed under active learning. The analysis supports theory claiming that calls to increase the number of students receiving STEM degrees could be answered, at least in part, by abandoning traditional lecturing in favor of active learning. To test the hypothesis that lecturing maximizes learning and course performance, we metaanalyzed 225 studies that reported data on examination scores or failure rates when comparing student performance in undergraduate science, technology, engineering, and mathematics (STEM) courses under traditional lecturing versus active learning. The effect sizes indicate that on average, student performance on examinations and concept inventories increased by 0.47 SDs under active learning (n = 158 studies), and that the odds ratio for failing was 1.95 under traditional lecturing (n = 67 studies). These results indicate that average examination scores improved by about 6% in active learning sections, and that students in classes with traditional lecturing were 1.5 times more likely to fail than were students in classes with active learning. Heterogeneity analyses indicated that both results hold across the STEM disciplines, that active learning increases scores on concept inventories more than on course examinations, and that active learning appears effective across all class sizes—although the greatest effects are in small (n ≤ 50) classes. Trim and fill analyses and fail-safe n calculations suggest that the results are not due to publication bias. The results also appear robust to variation in the methodological rigor of the included studies, based on the quality of controls over student quality and instructor identity. This is the largest and most comprehensive metaanalysis of undergraduate STEM education published to date. The results raise questions about the continued use of traditional lecturing as a control in research studies, and support active learning as the preferred, empirically validated teaching practice in regular classrooms.",2014.0,"S. Freeman, Sarah L. Eddy, Miles McDonough, Michelle K. Smith, Nnadozie Okoroafor, Hannah Jordt, M. Wenderoth"
40de1c316a7f4de8e547a717c905013642378996,https://www.semanticscholar.org/paper/40de1c316a7f4de8e547a717c905013642378996,The Critical Importance of Citizen Science Data,"Citizen science is an important vehicle for democratizing science and promoting the goal of universal and equitable access to scientific data and information. Data generated by citizen science groups have become an increasingly important source for scientists, applied users and those pursuing the 2030 Agenda for Sustainable Development. Citizen science data are used extensively in studies of biodiversity and pollution; crowdsourced data are being used by UN operational agencies for humanitarian activities; and citizen scientists are providing data relevant to monitoring the sustainable development goals (SDGs). This article provides an International Science Council (ISC) perspective on citizen science data generating activities in support of the 2030 Agenda and on needed improvements to the citizen science community's data stewardship practices for the benefit of science and society by presenting results of research undertaken by an ISC-sponsored Task Group.",2021.0,"A. de Sherbinin, A. Bowser, Tyng-Ruey Chuang, C. Cooper, F. Danielsen, Rorie Edmunds, P. Elias, E. Faustman, C. Hultquist, R. Mondardini, I. Popescu, Adenike Shonowo, K. Sivakumar"
7281fb2c44f4d73c86ffefd1fde7e4f8a1f5e75c,https://www.semanticscholar.org/paper/7281fb2c44f4d73c86ffefd1fde7e4f8a1f5e75c,Engagement in science through citizen science: Moving beyond data collection,"""To date, most studies of citizen science engagement focus on quantifiable measures related to the contribution of data or other output measures. Few studies have attempted to qualitatively characterize citizen science engagement across multiple projects and from the perspective of the participants. Building on pertinent literature and sociocultural learning theories, this study operationalizes engagement in citizen science through an analysis of interviews of 72 participants from six different environmentally based projects. We document engagement in citizen science through an examination of cognitive, affective, social, behavioral, and motivational dimensions. We assert that engagement in citizen science is enhanced by acknowledging these multiple dimensions and creating opportunities for volunteers to find personal relevance in their work with scientists. A Dimensions of Engagement framework is presented that can facilitate the innovation of new questions and methodologies for studying engagement in citizen science and other forms of informal science education.""",2019.0,"T. Phillips, H. Ballard, B. Lewenstein, R. Bonney"
e257edf34abd9a191fea1023a423abb497cca70f,https://www.semanticscholar.org/paper/e257edf34abd9a191fea1023a423abb497cca70f,The data science education dilemma,"The need for people fluent in working with data is growing rapidly and enormously, but U.S. K–12 education does not provide meaningful learning experiences designed to develop understanding of data science concepts or a fluency with data science skills. Data science is inherently inter- disciplinary, so it makes sense to integrate it with existing content areas, but difficulties abound. Consideration of the work involved in doing data science and the habits of mind that lie behind it leads to a way of thinking about integrating data science with mathematics and science. Examples drawn from current activity development in the Data Games project shed some light on what technology-based, data-driven might be like. The project’s ongoing research on learners’ conceptions of organizing data and the relevance to data science education is explained.",2012.0,W. Finzer
ca5e7580993170b1fe621bc16383ad2dfa6803b5,https://www.semanticscholar.org/paper/ca5e7580993170b1fe621bc16383ad2dfa6803b5,Utilization of text mining as a big data analysis tool for food science and nutrition.,"Big data analysis has found applications in many industries due to its ability to turn huge amounts of data into insights for informed business and operational decisions. Advanced data mining techniques have been applied in many sectors of supply chains in the food industry. However, the previous work has mainly focused on the analysis of instrument-generated data such as those from hyperspectral imaging, spectroscopy, and biometric receptors. The importance of digital text data in the food and nutrition has only recently gained attention due to advancements in big data analytics. The purpose of this review is to provide an overview of the data sources, computational methods, and applications of text data in the food industry. Text mining techniques such as word-level analysis (e.g., frequency analysis), word association analysis (e.g., network analysis), and advanced techniques (e.g., text classification, text clustering, topic modeling, information retrieval, and sentiment analysis) will be discussed. Applications of text data analysis will be illustrated with respect to food safety and food fraud surveillance, dietary pattern characterization, consumer-opinion mining, new-product development, food knowledge discovery, food supply-chain management, and online food services. The goal is to provide insights for intelligent decision-making to improve food production, food safety, and human nutrition.",2020.0,"Dandan Tao, Pengkun Yang, H. Feng"
e27acaf97f5b2eae4257bb5d8278fbe0e6405c39,https://www.semanticscholar.org/paper/e27acaf97f5b2eae4257bb5d8278fbe0e6405c39,Creating the CIPRES Science Gateway for inference of large phylogenetic trees,"Understanding the evolutionary history of living organisms is a central problem in biology. Until recently the ability to infer evolutionary relationships was limited by the amount of DNA sequence data available, but new DNA sequencing technologies have largely removed this limitation. As a result, DNA sequence data are readily available or obtainable for a wide spectrum of organisms, thus creating an unprecedented opportunity to explore evolutionary relationships broadly and deeply across the Tree of Life. Unfortunately, the algorithms used to infer evolutionary relationships are NP-hard, so the dramatic increase in available DNA sequence data has created a commensurate increase in the need for access to powerful computational resources. Local laptop or desktop machines are no longer viable for analysis of the larger data sets available today, and progress in the field relies upon access to large, scalable high-performance computing resources. This paper describes development of the CIPRES Science Gateway, a web portal designed to provide researchers with transparent access to the fastest available community codes for inference of phylogenetic relationships, and implementation of these codes on scalable computational resources. Meeting the needs of the community has included developing infrastructure to provide access, working with the community to improve existing community codes, developing infrastructure to insure the portal is scalable to the entire systematics community, and adopting strategies that make the project sustainable by the community. The CIPRES Science Gateway has allowed more than 1800 unique users to run jobs that required 2.5 million Service Units since its release in December 2009. (A Service Unit is a CPU-hour at unit priority).",2010.0,"Mark A. Miller, W. Pfeiffer, Terri Schwartz"
fff51615943e08d05080682009c9c656321ef0b2,https://www.semanticscholar.org/paper/fff51615943e08d05080682009c9c656321ef0b2,NOMAD: The FAIR concept for big data-driven materials science,"Data are a crucial raw material of this century. The amount of data that have been created in materials science thus far and that continues to be created every day is immense. Without a proper infrastructure that allows for collecting and sharing data, the envisioned success of big data-driven materials science will be hampered. For the field of computational materials science, the NOMAD (Novel Materials Discovery) Center of Excellence (CoE) has changed the scientific culture toward comprehensive and findable, accessible, interoperable, and reusable (FAIR) data, opening new avenues for mining materials science big data. Novel data-analytics concepts and tools turn data into knowledge and help in the prediction of new materials and in the identification of new properties of already known materials.",2018.0,"C. Draxl, M. Scheffler"
0db731c99879bb74c3850c53923d1df2c510f8c3,https://www.semanticscholar.org/paper/0db731c99879bb74c3850c53923d1df2c510f8c3,"AIRS/AMSU/HSB on the Aqua mission: design, science objectives, data products, and processing systems","The Atmospheric Infrared Sounder (AIRS), the Advanced Microwave Sounding Unit (AMSU), and the Humidity Sounder for Brazil (HSB) form an integrated cross-track scanning temperature and humidity sounding system on the Aqua satellite of the Earth Observing System (EOS). AIRS is an infrared spectrometer/radiometer that covers the 3.7-15.4-/spl mu/m spectral range with 2378 spectral channels. AMSU is a 15-channel microwave radiometer operating between 23 and 89 GHz. HSB is a four-channel microwave radiometer that makes measurements between 150 and 190 GHz. In addition to supporting the National Aeronautics and Space Administration's interest in process study and climate research, AIRS is the first hyperspectral infrared radiometer designed to support the operational requirements for medium-range weather forecasting of the National Ocean and Atmospheric Administration's National Centers for Environmental Prediction (NCEP) and other numerical weather forecasting centers. AIRS, together with the AMSU and HSB microwave radiometers, will achieve global retrieval accuracy of better than 1 K in the lower troposphere under clear and partly cloudy conditions. This paper presents an overview of the science objectives, AIRS/AMSU/HSB data products, retrieval algorithms, and the ground-data processing concepts. The EOS Aqua was launched on May 4, 2002 from Vandenberg AFB, CA, into a 705-km-high, sun-synchronous orbit. Based on the excellent radiometric and spectral performance demonstrated by AIRS during prelaunch testing, which has by now been verified during on-orbit testing, we expect the assimilation of AIRS data into the numerical weather forecast to result in significant forecast range and reliability improvements.",2003.0,"H. Aumann, M. Chahine, C. Gautier, M. Goldberg, E. Kalnay, L. McMillin, H. Revercomb, P. Rosenkranz, W. Smith, D. Staelin, L. Strow, J. Susskind"
3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf,https://www.semanticscholar.org/paper/3fe3924a5315fbb5b5cd0edf98533b8c61a3bbdf,Machine intelligence and the data-driven future of marine science,"
 Oceans constitute over 70% of the earth's surface, and the marine environment and ecosystems are central to many global challenges. Not only are the oceans an important source of food and other resources, but they also play a important roles in the earth's climate and provide crucial ecosystem services. To monitor the environment and ensure sustainable exploitation of marine resources, extensive data collection and analysis efforts form the backbone of management programmes on global, regional, or national levels. Technological advances in sensor technology, autonomous platforms, and information and communications technology now allow marine scientists to collect data in larger volumes than ever before. But our capacity for data analysis has not progressed comparably, and the growing discrepancy is becoming a major bottleneck for effective use of the available data, as well as an obstacle to scaling up data collection further. Recent years have seen rapid advances in the fields of artificial intelligence and machine learning, and in particular, so-called deep learning systems are now able to solve complex tasks that previously required human expertise. This technology is directly applicable to many important data analysis problems and it will provide tools that are needed to solve many complex challenges in marine science and resource management. Here we give a brief review of recent developments in deep learning, and highlight the many opportunities and challenges for effective adoption of this technology across the marine sciences.",2020.0,"K. Malde, N. Handegard, L. Eikvil, Arnt-Børre Salberg"
4e2f43dab69d690dc86422949e410ebf37f522d4,https://www.semanticscholar.org/paper/4e2f43dab69d690dc86422949e410ebf37f522d4,Bayesian data analysis.,"Bayesian methods have garnered huge interest in cognitive science as an approach to models of cognition and perception. On the other hand, Bayesian methods for data analysis have not yet made much headway in cognitive science against the institutionalized inertia of 20th century null hypothesis significance testing (NHST). Ironically, specific Bayesian models of cognition and perception may not long endure the ravages of empirical verification, but generic Bayesian methods for data analysis will eventually dominate. It is time that Bayesian data analysis became the norm for empirical methods in cognitive science. This article reviews a fatal flaw of NHST and introduces the reader to some benefits of Bayesian data analysis. The article presents illustrative examples of multiple comparisons in Bayesian analysis of variance and Bayesian approaches to statistical power. Copyright © 2010 John Wiley & Sons, Ltd. For further resources related to this article, please visit the WIREs website.",2010.0,J. Kruschke
0e00f0dbfc381661826f8ddbafe73e33bcfe040f,https://www.semanticscholar.org/paper/0e00f0dbfc381661826f8ddbafe73e33bcfe040f,Using Semistructured Surveys to Improve Citizen Science Data for Monitoring Biodiversity,"Abstract Biodiversity is being lost at an unprecedented rate, and monitoring is crucial for understanding the causal drivers and assessing solutions. Most biodiversity monitoring data are collected by volunteers through citizen science projects, and often crucial information is lacking to account for the inevitable biases that observers introduce during data collection. We contend that citizen science projects intended to support biodiversity monitoring must gather information about the observation process as well as species occurrence. We illustrate this using eBird, a global citizen science project that collects information on bird occurrences as well as vital contextual information on the observation process while maintaining broad participation. Our fundamental argument is that regardless of what species are being monitored, when citizen science projects collect a small set of basic information about how participants make their observations, the scientific value of the data collected will be dramatically improved.",2019.0,"S. Kelling, A. Johnston, A. Bonn, D. Fink, V. Ruiz‐Gutierrez, R. Bonney, Miguel Fernández, W. Hochachka, R. Julliard, Roland Kraemer, R. Guralnick"
a64c0c19cedee0a81286022945a60423e5c88610,https://www.semanticscholar.org/paper/a64c0c19cedee0a81286022945a60423e5c88610,Data Analytics for Environmental Science and Engineering Research.,"The advent of new data acquisition and handling techniques has opened the door to alternative and more comprehensive approaches to environmental monitoring that will improve our capacity to understand and manage environmental systems. Researchers have recently begun using machine learning (ML) techniques to analyze complex environmental systems and their associated data. Herein, we provide an overview of data analytics frameworks suitable for various Environmental Science and Engineering (ESE) research applications. We present current applications of ML algorithms within the ESE domain using three representative case studies: (1) Metagenomic data analysis for characterizing and tracking antimicrobial resistance in the environment; (2) Nontarget analysis for environmental pollutant profiling; and (3) Detection of anomalies in continuous data generated by engineered water systems. We conclude by proposing a path to advance incorporation of data analytics approaches in ESE research and application.",2021.0,"Suraj Gupta, D. Aga, A. Pruden, Liqing Zhang, P. Vikesland"
f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,https://www.semanticscholar.org/paper/f4c01d8780c86abdcfdd52c60843a2499fd5c1b6,Using Smartphones to Collect Behavioral Data in Psychological Science,"Smartphones now offer the promise of collecting behavioral data unobtrusively, in situ, as it unfolds in the course of daily life. Data can be collected from the onboard sensors and other phone logs embedded in today’s off-the-shelf smartphone devices. These data permit fine-grained, continuous collection of people’s social interactions (e.g., speaking rates in conversation, size of social groups, calls, and text messages), daily activities (e.g., physical activity and sleep), and mobility patterns (e.g., frequency and duration of time spent at various locations). In this article, we have drawn on the lessons from the first wave of smartphone-sensing research to highlight areas of opportunity for psychological research, present practical considerations for designing smartphone studies, and discuss the ongoing methodological and ethical challenges associated with research in this domain. It is our hope that these practical guidelines will facilitate the use of smartphones as a behavioral observation tool in psychological science.",2016.0,"Gabriella M. Harari, N. Lane, Rui Wang, Benjamin S Crosier, Andrew T. Campbell, S. Gosling"
adc22a722f2ad1d972de507779041e340f20a6a2,https://www.semanticscholar.org/paper/adc22a722f2ad1d972de507779041e340f20a6a2,Kadi4Mat: A Research Data Infrastructure for Materials Science,"The concepts and current developments of a research data infrastructure for materials science are presented, extending and combining the features of an electronic lab notebook and a repository. The objective of this infrastructure is to incorporate the possibility of structured data storage and data exchange with documented and reproducible data analysis and visualization, which finally leads to the publication of the data. This way, researchers can be supported throughout the entire research process. The software is being developed as a web-based and desktop-based system, offering both a graphical user interface and a programmatic interface. The focus of the development is on the integration of technologies and systems based on both established as well as new concepts. Due to the heterogeneous nature of materials science data, the current features are kept mostly generic, and the structuring of the data is largely left to the users. As a result, an extension of the research data infrastructure to other disciplines is possible in the future. The source code of the project is publicly available under a permissive Apache 2.0 license.",2021.0,"Nico Brandt, Lars Griem, C. Herrmann, Ephraim Schoof, G. Tosato, Yinghan Zhao, Philipp Zschumme, M. Selzer"
8a9f26a4cee210e51c96f4016737605e31d490ee,https://www.semanticscholar.org/paper/8a9f26a4cee210e51c96f4016737605e31d490ee,A data ecosystem to support machine learning in materials science,"Facilitating the application of machine learning (ML) to materials science problems requires enhancing the data ecosystem to enable discovery and collection of data from many sources, automated dissemination of new data across the ecosystem, and the connecting of data with materials-specific ML models. Here, we present two projects, the Materials Data Facility (MDF) and the Data and Learning Hub for Science (DLHub), that address these needs. We use examples to show how MDF and DLHub capabilities can be leveraged to link data with ML models and how users can access those capabilities through web and programmatic interfaces.",2019.0,"B. Blaiszik, Logan T. Ward, Marcus Schwarting, Jonathon Gaff, Ryan Chard, D. Pike, K. Chard, Ian T Foster"
22737046fbbe822deaaffddddb8f16be076d3f95,https://www.semanticscholar.org/paper/22737046fbbe822deaaffddddb8f16be076d3f95,"Open Science, Open Data, and Open Scholarship: European Policies to Make Science Fit for the Twenty-First Century","Open science will make science more efficient, reliable, and responsive to societal challenges. The European Commission has sought to advance open science policy from its inception in a holistic and integrated way, covering all aspects of the research cycle from scientific discovery and review to sharing knowledge, publishing, and outreach. We present the steps taken with a forward-looking perspective on the challenges laying ahead, in particular the necessary change of the rewards and incentives system for researchers (for which various actors are co-responsible and which goes beyond the mandate of the European Commission). Finally, we discuss the role of artificial intelligence (AI) within an open science perspective.",2019.0,"J. Burgelman, Corina Pascu, K. Szkuta, R. V. Schomberg, A. Karalopoulos, Konstantinos Repanas, M. Schouppe"
13c56c63385e84bca0e045133afe2c0a5d25d2d4,https://www.semanticscholar.org/paper/13c56c63385e84bca0e045133afe2c0a5d25d2d4,Improving big citizen science data: Moving beyond haphazard sampling,"Citizen science is mainstream: millions of people contribute data to a growing array of citizen science projects annually, forming massive datasets that will drive research for years to come. Many citizen science projects implement a “leaderboard” framework, ranking the contributions based on number of records or species, encouraging further participation. But is every data point equally “valuable?” Citizen scientists collect data with distinct spatial and temporal biases, leading to unfortunate gaps and redundancies, which create statistical and informational problems for downstream analyses. Up to this point, the haphazard structure of the data has been seen as an unfortunate but unchangeable aspect of citizen science data. However, we argue here that this issue can actually be addressed: we provide a very simple, tractable framework that could be adapted by broadscale citizen science projects to allow citizen scientists to optimize the marginal value of their efforts, increasing the overall collective knowledge.",2019.0,"C. Callaghan, J. Rowley, W. Cornwell, A. Poore, R. Major"
bd8a307efcffbf57d2e5c3c23577de44d883d865,https://www.semanticscholar.org/paper/bd8a307efcffbf57d2e5c3c23577de44d883d865,MedRec: Using Blockchain for Medical Data Access and Permission Management,"Years of heavy regulation and bureaucratic inefficiency have slowed innovation for electronic medical records (EMRs). We now face a critical need for such innovation, as personalization and data science prompt patients to engage in the details of their healthcare and restore agency over their medical data. In this paper, we propose MedRec: a novel, decentralized record management system to handle EMRs, using blockchain technology. Our system gives patients a comprehensive, immutable log and easy access to their medical information across providers and treatment sites. Leveraging unique blockchain properties, MedRec manages authentication, confidentiality, accountability and data sharing- crucial considerations when handling sensitive information. A modular design integrates with providers' existing, local data storage solutions, facilitating interoperability and making our system convenient and adaptable. We incentivize medical stakeholders (researchers, public health authorities, etc.) to participate in the network as blockchain “miners”. This provides them with access to aggregate, anonymized data as mining rewards, in return for sustaining and securing the network via Proof of Work. MedRec thus enables the emergence of data economics, supplying big data to empower researchers while engaging patients and providers in the choice to release metadata. The purpose of this short paper is to expose, prior to field tests, a working prototype through which we analyze and discuss our approach.",2016.0,"Asaph Azaria, A. Ekblaw, Thiago Vieira, A. Lippman"
d11eb2610f099db2490434d04543afcdd49ac532,https://www.semanticscholar.org/paper/d11eb2610f099db2490434d04543afcdd49ac532,Data Feminism,"Feminism, at its very core, aims to dismantle systems of oppression; however, the identification of which systems are oppressive and what kinds of beings are harmed by them has been the subject of debate in feminist circles for more than a century. Across the many waves of feminist movements and throughout the halls of humanities and social sciences departments around the world, feminist thought and feminist practices are heavily contested and often come into conflict. From difference and discord arise efforts to make both feminism and the world a more inclusive and just place. Yet there remain strong tensions over how to define feminism, how to realize feminists’ demands, how to apply feminist theory to a wide variety of subject matter, and how to bridge the gaps between theory and practice to build a better world for all. In an ambitious attempt to resolve some of those tensions in the field of data science, Catherine D’Ignazio and Lauren F. Klein’s 2021 book, Data Feminism, poses seven principles and strategies that are worthy of examination for those of us who might not hold the title of “data scientist,” but work with data nonetheless. As geographers and geography departments engage in efforts to improve our approaches to justice and equity, the principles of data feminism can be effective tools to guide our discussions for how to integrate feminism into our research practices and pedagogy, as well for how we apply geography in the public sphere.",2022.0,"C. D’Ignazio, Lauren F. Klein"
9445423239efb633f5c15791a7abe352199ce678,https://www.semanticscholar.org/paper/9445423239efb633f5c15791a7abe352199ce678,General Data Protection Regulation,"Presentacio sobre l'Oficina de Proteccio de Dades Personals de la UAB i la politica Open Science. Va formar part de la conferencia ""Les politiques d'Open Data / Open Acces: Implicacions a la recerca"" orientada a investigadors i gestors de projectes europeus que va tenir lloc el 20 de setembre de 2018 a la Universitat Autonoma de Barcelona",2018.0,"Agustí Verde Parera, Xavier Costa"
51995dc568874ea34911833355234b1f696dacfc,https://www.semanticscholar.org/paper/51995dc568874ea34911833355234b1f696dacfc,Science Mapping: A Systematic Review of the Literature,"Abstract Purpose We present a systematic review of the literature concerning major aspects of science mapping to serve two primary purposes: First, to demonstrate the use of a science mapping approach to perform the review so that researchers may apply the procedure to the review of a scientific domain of their own interest, and second, to identify major areas of research activities concerning science mapping, intellectual milestones in the development of key specialties, evolutionary stages of major specialties involved, and the dynamics of transitions from one specialty to another. Design/methodology/approach We first introduce a theoretical framework of the evolution of a scientific specialty. Then we demonstrate a generic search strategy that can be used to construct a representative dataset of bibliographic records of a domain of research. Next, progressively synthesized co-citation networks are constructed and visualized to aid visual analytic studies of the domain’s structural and dynamic patterns and trends. Finally, trajectories of citations made by particular types of authors and articles are presented to illustrate the predictive potential of the analytic approach. Findings The evolution of the science mapping research involves the development of a number of interrelated specialties. Four major specialties are discussed in detail in terms of four evolutionary stages: conceptualization, tool construction, application, and codification. Underlying connections between major specialties are also explored. The predictive analysis demonstrates citations trajectories of potentially transformative contributions. Research limitations The systematic review is primarily guided by citation patterns in the dataset retrieved from the literature. The scope of the data is limited by the source of the retrieval, i.e. the Web of Science, and the composite query used. An iterative query refinement is possible if one would like to improve the data quality, although the current approach serves our purpose adequately. More in-depth analyses of each specialty would be more revealing by incorporating additional methods such as citation context analysis and studies of other aspects of scholarly publications. Practical implications The underlying analytic process of science mapping serves many practical needs, notably bibliometric mapping, knowledge domain visualization, and visualization of scientific literature. In order to master such a complex process of science mapping, researchers often need to develop a diverse set of skills and knowledge that may span multiple disciplines. The approach demonstrated in this article provides a generic method for conducting a systematic review. Originality/value Incorporating the evolutionary stages of a specialty into the visual analytic study of a research domain is innovative. It provides a systematic methodology for researchers to achieve a good understanding of how scientific fields evolve, to recognize potentially insightful patterns from visually encoded signs, and to synthesize various information so as to capture the state of the art of the domain.",2017.0,Chaomei Chen
2809d4876e34b8c64fc1783fe6a0a278770505b0,https://www.semanticscholar.org/paper/2809d4876e34b8c64fc1783fe6a0a278770505b0,A survey of data provenance in e-science,"Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.",2005.0,"Yogesh L. Simmhan, Beth Plale, Dennis Gannon"
2660fbc3b666145a87f05de10066fc2a3e7467dd,https://www.semanticscholar.org/paper/2660fbc3b666145a87f05de10066fc2a3e7467dd,The Science Of Real Time Data Capture Self Reports In Health Research,"The National Cancer Institute (NCI) has designated the topic of real-time data capture as an important and innovative research area. As such, the NCI sponsored a national meeting of distinguished research scientists to discuss the state of the science in this emerging and burgeoning field. This book reflects the findings of the conference and discusses the state of the science of real-time data capture and its application to health and cancer research. It provides a conceptual framework for minute-by-minute data captureecological momentary assessments (EMA)and discusses health-related topics where these assessements have been applied. In addition, future directions in real-time data capture assessment, interventions, methodology, and technology are discussed.",2016.0,S. Schulze
3aa1b70fdc97ae96091c5fb39cd911015ac5253e,https://www.semanticscholar.org/paper/3aa1b70fdc97ae96091c5fb39cd911015ac5253e,Novel methods improve prediction of species' distributions from occurrence data,"Prediction of species' distributions is central to diverse applications in ecology, evolution and conservation science. There is increasing electronic access to vast sets of occurrence records in museums and herbaria, yet little effective guidance on how best to use this information in the context of numerous approaches for modelling distributions. To meet this need, we compared 16 modelling methods over 226 species from 6 regions of the world, creating the most comprehensive set of model comparisons to date. We used presence-only data to fit models, and independent presence-absence data to evaluate the predictions. Along with well-established modelling methods such as generalised additive models and GARP and BIOCLIM, we explored methods that either have been developed recently or have rarely been applied to modelling species' distributions. These include machine-learning methods and community models, both of which have features that may make them particularly well suited to noisy or sparse information, as is typical of species' occurrence data. Presence-only data were effective for modelling species' distributions for many species and regions. The novel methods consistently outperformed more established methods. The results of our analysis are promising for the use of data from museums and herbaria, especially as methods suited to the noise inherent in such data improve.",2006.0,
439ede62248e5f6202982afead02b33d3feffae7,https://www.semanticscholar.org/paper/439ede62248e5f6202982afead02b33d3feffae7,TCGAbiolinks: an R/Bioconductor package for integrative analysis of TCGA data,"The Cancer Genome Atlas (TCGA) research network has made public a large collection of clinical and molecular phenotypes of more than 10 000 tumor patients across 33 different tumor types. Using this cohort, TCGA has published over 20 marker papers detailing the genomic and epigenomic alterations associated with these tumor types. Although many important discoveries have been made by TCGA's research network, opportunities still exist to implement novel methods, thereby elucidating new biological pathways and diagnostic markers. However, mining the TCGA data presents several bioinformatics challenges, such as data retrieval and integration with clinical data and other molecular data types (e.g. RNA and DNA methylation). We developed an R/Bioconductor package called TCGAbiolinks to address these challenges and offer bioinformatics solutions by using a guided workflow to allow users to query, download and perform integrative analyses of TCGA data. We combined methods from computer science and statistics into the pipeline and incorporated methodologies developed in previous TCGA marker studies and in our own group. Using four different TCGA tumor types (Kidney, Brain, Breast and Colon) as examples, we provide case studies to illustrate examples of reproducibility, integrative analysis and utilization of different Bioconductor packages to advance and accelerate novel discoveries.",2015.0,"A. Colaprico, T. Silva, Catharina Olsen, L. Garofano, C. Cava, Davide Garolini, Thais S Sabedot, T. Malta, S. Pagnotta, I. Castiglioni, M. Ceccarelli, Gianluca Bontempi, H. Noushmehr"
16f4135a229c79e60fa25259100c8cdcedfab8cc,https://www.semanticscholar.org/paper/16f4135a229c79e60fa25259100c8cdcedfab8cc,Patent citation data in social science research: Overview and best practices,"The last 2 decades have witnessed a dramatic increase in the use of patent citation data in social science research. Facilitated by digitization of the patent data and increasing computing power, a community of practice has grown up that has developed methods for using these data to: measure attributes of innovations such as impact and originality; to trace flows of knowledge across individuals, institutions and regions; and to map innovation networks. The objective of this article is threefold. First, it takes stock of these main uses. Second, it discusses 4 pitfalls associated with patent citation data, related to office, time and technology, examiner, and strategic effects. Third, it highlights gaps in our understanding and offers directions for future research.",2016.0,"A. Jaffe, Gaétan de Rassenfosse"
4449bd1f5bb8f86fec9ca7ded29ac8bf322c0114,https://www.semanticscholar.org/paper/4449bd1f5bb8f86fec9ca7ded29ac8bf322c0114,"AVONET: morphological, ecological and geographical data for all birds.","Functional traits offer a rich quantitative framework for developing and testing theories in evolutionary biology, ecology and ecosystem science. However, the potential of functional traits to drive theoretical advances and refine models of global change can only be fully realised when species-level information is complete. Here we present the AVONET dataset containing comprehensive functional trait data for all birds, including six ecological variables, 11 continuous morphological traits, and information on range size and location. Raw morphological measurements are presented from 90,020 individuals of 11,009 extant bird species sampled from 181 countries. These data are also summarised as species averages in three taxonomic formats, allowing integration with a global phylogeny, geographical range maps, IUCN Red List data and the eBird citizen science database. The AVONET dataset provides the most detailed picture of continuous trait variation for any major radiation of organisms, offering a global template for testing hypotheses and exploring the evolutionary origins, structure and functioning of biodiversity.",2022.0,"J. Tobias, C. Sheard, A. Pigot, A. Devenish, Jingyi Yang, F. Sayol, M. H. Neate‐Clegg, Nico Alioravainen, Thomas L. Weeks, Robert A. Barber, Patrick A. Walkden, H. E. MacGregor, Samuel E. I. Jones, Claire Vincent, A. G. Phillips, N. Marples, Flavia A. Montaño‐Centellas, V. Leandro-Silva, S. Claramunt, Bianca Darski, B. Freeman, Tom P. Bregman, Christopher R. Cooney, Emma C. Hughes, Elliot J. R. Capp, Zoë K. Varley, Nicholas R. Friedman, H. Korntheuer, Andrea Corrales-Vargas, C. Trisos, B. Weeks, D. M. Hanz, T. Töpfer, Gustavo A. Bravo, V. Remeš, Larissa Nowak, L. Carneiro, A. Moncada R., Beata Matysioková, Daniel T. Baldassarre, A. Martínez-Salinas, Jared D. Wolfe, P. Chapman, Benjamin G. Daly, Marjorie C. Sorensen, Alexander Neu, M. A. Ford, Rebekah J. Mayhew, Luis Fabio Silveira, D. Kelly, Nathaniel N. D. Annorbah, Henry S. Pollock, A. Grabowska-Zhang, Jay P. McEntee, Juan Carlos T. Gonzalez, Camila G. Meneses, M. Muñoz, Luke L. Powell, G. A. Jamie, T. Matthews, O. Johnson, G. R. Brito, Kristof Zyskowski, R. Crates, Michael G. Harvey, Maura Jurado Zevallos, P. Hosner, Tom Bradfer‐Lawrence, James M. Maley, F. G. Stiles, Hevana S. Lima, Kaiya L. Provost, Moses Chibesa, Mmatjie Mashao, J. Howard, Edson Mlamba, M. A. Chua, Bicheng Li, M. I. Gómez, Natalia C. García, M. Päckert, J. Fuchs, Jarome R. Ali, E. Derryberry, Monica L. Carlson, Rolly C. Urriza, Kristin E. Brzeski, D. Prawiradilaga, M. Rayner, E. Miller, R. C. Bowie, R. Lafontaine, R. Scofield, Yingqiang Lou, Lankani Somarathna, D. Lepage, Marshall Illif, E. Neuschulz, M. Templin, D. M. Dehling, Jacob C. Cooper, O. Pauwels, Kangkuso Analuddin, J. Fjeldså, N. Seddon, P. Sweet, F. DeClerck, L. Naka, J. Brawn, A. Aleixo, K. Böhning‐Gaese, C. Rahbek, Susanne A. Fritz, G. Thomas, M. Schleuning"
233e702fa7ccfd55061680e3af9bd2f7efe5e08f,https://www.semanticscholar.org/paper/233e702fa7ccfd55061680e3af9bd2f7efe5e08f,Science of Science,"The whys and wherefores of SciSci The science of science (SciSci) is based on a transdisciplinary approach that uses large data sets to study the mechanisms underlying the doing of science—from the choice of a research problem to career trajectories and progress within a field. In a Review, Fortunato et al. explain that the underlying rationale is that with a deeper understanding of the precursors of impactful science, it will be possible to develop systems and policies that improve each scientist's ability to succeed and enhance the prospects of science as a whole. Science, this issue p. eaao0185 BACKGROUND The increasing availability of digital data on scholarly inputs and outputs—from research funding, productivity, and collaboration to paper citations and scientist mobility—offers unprecedented opportunities to explore the structure and evolution of science. The science of science (SciSci) offers a quantitative understanding of the interactions among scientific agents across diverse geographic and temporal scales: It provides insights into the conditions underlying creativity and the genesis of scientific discovery, with the ultimate goal of developing tools and policies that have the potential to accelerate science. In the past decade, SciSci has benefited from an influx of natural, computational, and social scientists who together have developed big data–based capabilities for empirical analysis and generative modeling that capture the unfolding of science, its institutions, and its workforce. The value proposition of SciSci is that with a deeper understanding of the factors that drive successful science, we can more effectively address environmental, societal, and technological problems. ADVANCES Science can be described as a complex, self-organizing, and evolving network of scholars, projects, papers, and ideas. This representation has unveiled patterns characterizing the emergence of new scientific fields through the study of collaboration networks and the path of impactful discoveries through the study of citation networks. Microscopic models have traced the dynamics of citation accumulation, allowing us to predict the future impact of individual papers. SciSci has revealed choices and trade-offs that scientists face as they advance both their own careers and the scientific horizon. For example, measurements indicate that scholars are risk-averse, preferring to study topics related to their current expertise, which constrains the potential of future discoveries. Those willing to break this pattern engage in riskier careers but become more likely to make major breakthroughs. Overall, the highest-impact science is grounded in conventional combinations of prior work but features unusual combinations. Last, as the locus of research is shifting into teams, SciSci is increasingly focused on the impact of team research, finding that small teams tend to disrupt science and technology with new ideas drawing on older and less prevalent ones. In contrast, large teams tend to develop recent, popular ideas, obtaining high, but often short-lived, impact. OUTLOOK SciSci offers a deep quantitative understanding of the relational structure between scientists, institutions, and ideas because it facilitates the identification of fundamental mechanisms responsible for scientific discovery. These interdisciplinary data-driven efforts complement contributions from related fields such as scientometrics and the economics and sociology of science. Although SciSci seeks long-standing universal laws and mechanisms that apply across various fields of science, a fundamental challenge going forward is accounting for undeniable differences in culture, habits, and preferences between different fields and countries. This variation makes some cross-domain insights difficult to appreciate and associated science policies difficult to implement. The differences among the questions, data, and skills specific to each discipline suggest that further insights can be gained from domain-specific SciSci studies, which model and identify opportunities adapted to the needs of individual research fields. The complexity of science. Science can be seen as an expanding and evolving network of ideas, scholars, and papers. SciSci searches for universal and domain-specific laws underlying the structure and dynamics of science. ILLUSTRATION: NICOLE SAMAY Identifying fundamental drivers of science and developing predictive models to capture its evolution are instrumental for the design of policies that can improve the scientific enterprise—for example, through enhanced career paths for scientists, better performance evaluation for organizations hosting research, discovery of novel effective funding vehicles, and even identification of promising regions along the scientific frontier. The science of science uses large-scale data on the production of science to search for universal and domain-specific patterns. Here, we review recent developments in this transdisciplinary field.",2018.0,"S. Fortunato, Carl T. Bergstrom, K. Börner, James A. Evans, D. Helbing, Stasa Milojevic, A. Petersen, F. Radicchi, R. Sinatra, Brian Uzzi, Alessandro Vespignani, L. Waltman, Dashun Wang, A. Barabási"
4436ca7e9f91b7ad9ad6a09dbe12f48d9f6c3e7f,https://www.semanticscholar.org/paper/4436ca7e9f91b7ad9ad6a09dbe12f48d9f6c3e7f,Data-driven predictions in the science of science,"The desire to predict discoveries—to have some idea, in advance, of what will be discovered, by whom, when, and where—pervades nearly all aspects of modern science, from individual scientists to publishers, from funding agencies to hiring committees. In this Essay, we survey the emerging and interdisciplinary field of the “science of science” and what it teaches us about the predictability of scientific discovery. We then discuss future opportunities for improving predictions derived from the science of science and its potential impact, positive and negative, on the scientific community.",2017.0,"A. Clauset, D. Larremore, R. Sinatra"
02a9428b5b28d85ea330033fb990dc10cd15cc4e,https://www.semanticscholar.org/paper/02a9428b5b28d85ea330033fb990dc10cd15cc4e,Occupancy models for citizen‐science data,"Large‐scale citizen‐science projects, such as atlases of species distribution, are an important source of data for macroecological research, for understanding the effects of climate change and other drivers on biodiversity, and for more applied conservation tasks, such as early‐warning systems for biodiversity loss. However, citizen‐science data are challenging to analyse because the observation process has to be taken into account. Typically, the observation process leads to heterogeneous and non‐random sampling, false absences, false detections, and spatial correlations in the data. Increasingly, occupancy models are being used to analyse atlas data. We advocate a dual approach to strengthen inference from citizen science data for the questions the programme is intended to address: (a) the survey design should be chosen with a particular set of questions and associated analysis strategy in mind and (b) the statistical methods should be tailored not only to those questions but also to the specific characteristics of the data. We review the consequences of particular survey design choices that typically need to be made in atlas‐style citizen‐science projects. These include spatial resolution of the sampling units, allocation of effort in space, and collection of information about the observation process. On the analysis side, we review extensions of the basic occupancy models that are frequently necessary with atlas data, including methods for dealing with heterogeneity, non‐independent detections, false detections, and violation of the closure assumption. New technologies, such as cell‐phone apps and fixed remote detection devices, are revolutionizing citizen‐science projects. There is an opportunity to maximize the usefulness of the resulting datasets if the protocols are rooted in robust statistical designs and data analysis issues are being considered. Our review provides guidelines for designing new projects and an overview of the current methods that can be used to analyse data from such projects.",2019.0,"R. Altwegg, J. Nichols"
2ff6d7e05b1f74e0b17dbf97a59ac0d75ef65efc,https://www.semanticscholar.org/paper/2ff6d7e05b1f74e0b17dbf97a59ac0d75ef65efc,FAIR Data and Services in Biodiversity Science and Geoscience,"We examine the intersection of the FAIR principles (Findable, Accessible, Interoperable and Reusable), the challenges and opportunities presented by the aggregation of widely distributed and heterogeneous data about biological and geological specimens, and the use of the Digital Object Architecture (DOA) data model and components as an approach to solving those challenges that offers adherence to the FAIR principles as an integral characteristic. This approach will be prototyped in the Distributed System of Scientific Collections (DiSSCo) project, the pan-European Research Infrastructure which aims to unify over 110 natural science collections across 21 countries. We take each of the FAIR principles, discuss them as requirements in the creation of a seamless virtual collection of bio/geo specimen data, and map those requirements to Digital Object components and facilities such as persistent identification, extended data typing, and the use of an additional level of abstraction to normalize existing heterogeneous data structures. The FAIR principles inform and motivate the work and the DO Architecture provides the technical vision to create the seamless virtual collection vitally needed to address scientific questions of societal importance.",2020.0,"L. Lannom, D. Koureas, A. Hardisty"
c36991759325bedd19f69264f72d1cbf59a6158c,https://www.semanticscholar.org/paper/c36991759325bedd19f69264f72d1cbf59a6158c,Data Mining: Concepts and Techniques,"The increasing volume of data in modern business and science calls for more complex and sophisticated tools. Although advances in data mining technology have made extensive data collection much easier, it's still always evolving and there is a constant need for new techniques and tools that can help us transform this data into useful information and knowledge. Since the previous edition's publication, great advances have been made in the field of data mining. Not only does the third of edition of Data Mining: Concepts and Techniques continue the tradition of equipping you with an understanding and application of the theory and practice of discovering patterns hidden in large data sets, it also focuses on new, important topics in the field: data warehouses and data cube technology, mining stream, mining social networks, and mining spatial, multimedia and other complex data. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. This is the resource you need if you want to apply today's most powerful data mining techniques to meet real business challenges. * Presents dozens of algorithms and implementation examples, all in pseudo-code and suitable for use in real-world, large-scale data mining projects. * Addresses advanced topics such as mining object-relational databases, spatial databases, multimedia databases, time-series databases, text databases, the World Wide Web, and applications in several fields. *Provides a comprehensive, practical look at the concepts and techniques you need to get the most out of real business data",2000.0,"Jiawei Han, M. Kamber"
d9a668e849fe780081b9d133d84c72039283f30b,https://www.semanticscholar.org/paper/d9a668e849fe780081b9d133d84c72039283f30b,From Open Data to Open Science,"The open science movement continues to gain momentum, attention, and discussion. However, there are a number of different interpretations, viewpoints, and perspectives as to what the term “open science” means. In this study, we define open science as a collaborative culture enabled by technology that empowers the open sharing of data, information, and knowledge within the scientific community and the wider public to accelerate scientific research and understanding. As science has become increasingly data driven, data programs now play a critical role in enabling and accelerating open science.",2020.0,"R. Ramachandran, K. Bugbee, K. Murphy"
4a6e74d4bf4fd0106891e5518692a77c7aa8811d,https://www.semanticscholar.org/paper/4a6e74d4bf4fd0106891e5518692a77c7aa8811d,Outlier Detection in High Dimensional Data,"Artificial intelligence (AI) is the science that allows
computers to replicate human intelligence in areas such as
decision-making, text processing, visual perception. Artificial
Intelligence is the broader field that contains several subfields
such as machine learning, robotics, and computer vision.
Machine Learning is a branch of Artificial Intelligence that
allows a machine to learn and improve at a task over time. Deep
Learning is a subset of machine learning that makes use of deep
artificial neural networks for training. The paper proposed on
outlier detection for multivariate high dimensional data for
Autoencoder unsupervised model.",2021.0,"C. Aggarwal, Philip S. Yu"
391a5f286f814d852dddcab1b2b68e5c1af6c79e,https://www.semanticscholar.org/paper/391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",2016.0,"Xindong Wu, Xingquan Zhu, Gong-Qing Wu, Wei Ding"
6a697a4b3bdbbfb7681d9f9a518fc0be73744037,https://www.semanticscholar.org/paper/6a697a4b3bdbbfb7681d9f9a518fc0be73744037,Big data of materials science: critical role of the descriptor.,"Statistical learning of materials properties or functions so far starts with a largely silent, nonchallenged step: the choice of the set of descriptive parameters (termed descriptor). However, when the scientific connection between the descriptor and the actuating mechanisms is unclear, the causality of the learned descriptor-property relation is uncertain. Thus, a trustful prediction of new promising materials, identification of anomalies, and scientific advancement are doubtful. We analyze this issue and define requirements for a suitable descriptor. For a classic example, the energy difference of zinc blende or wurtzite and rocksalt semiconductors, we demonstrate how a meaningful descriptor can be found systematically.",2014.0,"L. Ghiringhelli, J. Vybíral, S. Levchenko, C. Draxl, M. Scheffler"
7bd598f6a7c6eb4265fe5a9ca64504d1d639684a,https://www.semanticscholar.org/paper/7bd598f6a7c6eb4265fe5a9ca64504d1d639684a,Educational data mining and learning analytics: An updated survey,"This survey is an updated and improved version of the previous one published in 2013 in this journal with the title “data mining in education”. It reviews in a comprehensible and very general way how Educational Data Mining and Learning Analytics have been applied over educational data. In the last decade, this research area has evolved enormously and a wide range of related terms are now used in the bibliography such as Academic Analytics, Institutional Analytics, Teaching Analytics, Data‐Driven Education, Data‐Driven Decision‐Making in Education, Big Data in Education, and Educational Data Science. This paper provides the current state of the art by reviewing the main publications, the key milestones, the knowledge discovery cycle, the main educational environments, the specific tools, the free available datasets, the most used methods, the main objectives, and the future trends in this research area.",2020.0,"C. Romero, S. Ventura"
33aeb033401ec748633bdd5b806db4f58288ee69,https://www.semanticscholar.org/paper/33aeb033401ec748633bdd5b806db4f58288ee69,The Accuracy of Citizen Science Data: A Quantitative Review,"Author(s): Aceves-Bueno, Erendira; Adeleye, Adeyemi S; Feraud, Marina; Huang, Yuxiong; Tao, Mengya; Yang, Yi; Anderson, Sarah E",2017.0,"Eréndira Aceves-Bueno, A. Adeleye, Marina Feraud, Yuxiong Huang, Mengya Tao, Yi Yang, Sarah E. Anderson"
969f983d000ac68ca77548b5bba2e8d1b89086c4,https://www.semanticscholar.org/paper/969f983d000ac68ca77548b5bba2e8d1b89086c4,Materials science with large-scale data and informatics: Unlocking new opportunities,"Universal access to abundant scientific data, and the software to analyze the data at scale, could fundamentally transform the field of materials science. Today, the materials community faces serious challenges to bringing about this data-accelerated research paradigm, including diversity of research areas within materials, lack of data standards, and missing incentives for sharing, among others. Nonetheless, the landscape is rapidly changing in ways that should benefit the entire materials research enterprise. We provide an overview of the current state of the materials data and informatics landscape, highlighting a few selected efforts that make more data freely available and useful to materials researchers.",2016.0,"Joanne Hill, G. Mulholland, K. Persson, R. Seshadri, C. Wolverton, B. Meredig"
e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,https://www.semanticscholar.org/paper/e0634f2945b43d4c13a0aa2ff31f2c1c5fe597b9,The Role of Anomalous Data in Knowledge Acquisition: A Theoretical Framework and Implications for Science Instruction,"Understanding how science students respond to anomalous data is essential to understanding knowledge acquisition in science classrooms. This article presents a detailed analysis of the ways in which scientists and science students respond to such data. We postulate that there are seven distinct forms of response to anomalous data, only one of which is to accept the data and change theories. The other six responses involve discounting the data in various ways in order to protect the preinstructional theory. We analyze the factors that influence which of these seven forms of response a scientist or student will choose, giving special attention to the factors that make theory change more likely. Finally, we discuss the implications of our framework for science instruction.",1993.0,"C. Chinn, W. Brewer"
02eaaf87f9cae34cca398fed146079e6eeb1f868,https://www.semanticscholar.org/paper/02eaaf87f9cae34cca398fed146079e6eeb1f868,"Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data","The success of the large neural language models on many NLP tasks is exciting. However, we find that these successes sometimes lead to hype in which these models are being described as “understanding” language or capturing “meaning”. In this position paper, we argue that a system trained only on form has a priori no way to learn meaning. In keeping with the ACL 2020 theme of “Taking Stock of Where We’ve Been and Where We’re Going”, we argue that a clear understanding of the distinction between form and meaning will help guide the field towards better science around natural language understanding.",2020.0,"Emily M. Bender, Alexander Koller"
87f7c170aecf8f3465b26a11b9a384fef934337b,https://www.semanticscholar.org/paper/87f7c170aecf8f3465b26a11b9a384fef934337b,Measurement and Data Analysis for Engineering and Science,"Fundamentals of Experimentation Introduction Experiments Chapter Overview Experimental Approach Role of Experiments The Experiment Classification of Experiments Plan for Successful Experimentation Hypothesis Testing* Design of Experiments* Factorial Design* Problems Bibliography Fundamental Electronics Chapter Overview Concepts and Definitions Circuit Elements RLC Combinations Elementary DC Circuit Analysis Elementary AC Circuit Analysis Equivalent Circuits* Meters* Impedance Matching and Loading Error* Electrical Noise* Problems Bibliography Measurement Systems: Sensors and Transducers Chapter Overview Measurement System Overview Sensor Domains Sensor Characteristics Physical Principles of Sensors Electric Piezoelectric Fluid Mechanic Optic Photoelastic Thermoelectric Electrochemical Sensor Scaling* Problems Bibliography Measurement Systems: Other Components Chapter Overview Signal Conditioning, Processing, and Recording Amplifiers Filters Analog-to-Digital Converters Smart Measurement Systems Other Example Measurement Systems Problems Bibliography Measurement Systems: Calibration and Response Chapter Overview Static Response Characterization by Calibration Dynamic Response Characterization Zero-Order System Dynamic Response First-Order System Dynamic Response Second-Order System Dynamic Response Measurement System Dynamic Response Problems Bibliography Measurement Systems: Design-Stage Uncertainty Chapter Overview Design-Stage Uncertainty Analysis Design-Stage Uncertainty Estimate of a Measurand Design-Stage Uncertainty Estimate of a Result Problems Bibliography Signal Characteristics Chapter Overview Signal Classification Signal Variables Signal Statistical Parameters Problems Bibliography The Fourier Transform Chapter Overview Fourier Series of a Periodic Signal Complex Numbers and Waves Exponential Fourier Series Spectral Representations Continuous Fourier Transform Continuous Fourier Transform Properties* Discrete Fourier Transform Fast Fourier Transform Problems Bibliography Digital Signal Analysis Chapter Overview Digital Sampling Digital Sampling Errors Windowing* Determining a Sample Period Problems Bibliography Probability Chapter Overview Relation to Measurements Basic Probability Concepts Sample versus Population Plotting Statistical Information Probability Density Function Various Probability Density Functions Central Moments Probability Distribution Function Problems Bibliography Statistics Chapter Overview Normal Distribution Normalized Variables Student's t Distribution Rejection of Data Standard Deviation of the Means Chi-Square Distribution Pooling Samples* Problems Bibliography Uncertainty Analysis Chapter Overview Modeling and Experimental Uncertainties Probabilistic Basis of Uncertainty Identifying Sources of Error Systematic and Random Errors Quantifying Systematic and Random Errors Measurement Uncertainty Analysis Uncertainty Analysis of a Multiple-Measurement Result Uncertainty Analyses for Other Measurement Situations Uncertainty Analysis Summary Finite-Difference Uncertainties* Uncertainty Based upon Interval Statistics* Problems Bibliography Regression and Correlation Chapter Overview Least-Squares Approach Least-Squares Regression Analysis Linear Analysis Higher-Order Analysis* Multi-Variable Linear Analysis* Determining the Appropriate Fit Regression Confidence Intervals Regression Parameters Linear Correlation Analysis Signal Correlations in Time* Problems Bibliography Units and Significant Figures Chapter Overview English and Metric Systems Systems of Units SI Standards Technical English and SI Conversion Factors Prefixes Significant Figures Problems Bibliography Technical Communication Chapter Overview Guidelines for Writing Technical Memo Technical Report Oral Technical Presentation Problems Bibliography A Glossary B Symbols C Review Problem Answers Index",2017.0,P. Dunn
d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f,https://www.semanticscholar.org/paper/d2a595c5efb4b26245c4353d5d85cbe6c7ecac0f,Machine learning for data-driven discovery in solid Earth geoscience,"Automating geoscience analysis Solid Earth geoscience is a field that has very large set of observations, which are ideal for analysis with machine-learning methods. Bergen et al. review how these methods can be applied to solid Earth datasets. Adopting machine-learning techniques is important for extracting information and for understanding the increasing amount of complex data collected in the geosciences. Science, this issue p. eaau0323 BACKGROUND The solid Earth, oceans, and atmosphere together form a complex interacting geosystem. Processes relevant to understanding Earth’s geosystem behavior range in spatial scale from the atomic to the planetary, and in temporal scale from milliseconds to billions of years. Physical, chemical, and biological processes interact and have substantial influence on this complex geosystem, and humans interact with it in ways that are increasingly consequential to the future of both the natural world and civilization as the finiteness of Earth becomes increasingly apparent and limits on available energy, mineral resources, and fresh water increasingly affect the human condition. Earth is subject to a variety of geohazards that are poorly understood, yet increasingly impactful as our exposure grows through increasing urbanization, particularly in hazard-prone areas. We have a fundamental need to develop the best possible predictive understanding of how the geosystem works, and that understanding must be informed by both the present and the deep past. This understanding will come through the analysis of increasingly large geo-datasets and from computationally intensive simulations, often connected through inverse problems. Geoscientists are faced with the challenge of extracting as much useful information as possible and gaining new insights from these data, simulations, and the interplay between the two. Techniques from the rapidly evolving field of machine learning (ML) will play a key role in this effort. ADVANCES The confluence of ultrafast computers with large memory, rapid progress in ML algorithms, and the ready availability of large datasets place geoscience at the threshold of dramatic progress. We anticipate that this progress will come from the application of ML across three categories of research effort: (i) automation to perform a complex prediction task that cannot easily be described by a set of explicit commands; (ii) modeling and inverse problems to create a representation that approximates numerical simulations or captures relationships; and (iii) discovery to reveal new and often unanticipated patterns, structures, or relationships. Examples of automation include geologic mapping using remote-sensing data, characterizing the topology of fracture systems to model subsurface transport, and classifying volcanic ash particles to infer eruptive mechanism. Examples of modeling include approximating the viscoelastic response for complex rheology, determining wave speed models directly from tomographic data, and classifying diverse seismic events. Examples of discovery include predicting laboratory slip events using observations of acoustic emissions, detecting weak earthquake signals using similarity search, and determining the connectivity of subsurface reservoirs using groundwater tracer observations. OUTLOOK The use of ML in solid Earth geosciences is growing rapidly, but is still in its early stages and making uneven progress. Much remains to be done with existing datasets from long-standing data sources, which in many cases are largely unexplored. Newer, unconventional data sources such as light detection and ranging (LiDAR), fiber-optic sensing, and crowd-sourced measurements may demand new approaches through both the volume and the character of information that they present. Practical steps could accelerate and broaden the use of ML in the geosciences. Wider adoption of open-science principles such as open source code, open data, and open access will better position the solid Earth community to take advantage of rapid developments in ML and artificial intelligence. Benchmark datasets and challenge problems have played an important role in driving progress in artificial intelligence research by enabling rigorous performance comparison and could play a similar role in the geosciences. Testing on high-quality datasets produces better models, and benchmark datasets make these data widely available to the research community. They also help recruit expertise from allied disciplines. Close collaboration between geoscientists and ML researchers will aid in making quick progress in ML geoscience applications. Extracting maximum value from geoscientific data will require new approaches for combining data-driven methods, physical modeling, and algorithms capable of learning with limited, weak, or biased labels. Funding opportunities that target the intersection of these disciplines, as well as a greater component of data science and ML education in the geosciences, could help bring this effort to fruition. Digital geology. Digital representation of the geology of the conterminous United States. [Geology of the Conterminous United States at 1:2,500,000 scale; a digital representation of the 1974 P. B. King and H. M. Beikman map by P. G. Schruben, R. E. Arndt, W. J. Bawiec] The list of author affiliations is available in the full article online. Understanding the behavior of Earth through the diverse fields of the solid Earth geosciences is an increasingly important task. It is made challenging by the complex, interacting, and multiscale processes needed to understand Earth’s behavior and by the inaccessibility of nearly all of Earth’s subsurface to direct observation. Substantial increases in data availability and in the increasingly realistic character of computer simulations hold promise for accelerating progress, but developing a deeper understanding based on these capabilities is itself challenging. Machine learning will play a key role in this effort. We review the state of the field and make recommendations for how progress might be broadened and accelerated.",2019.0,"K. Bergen, P. Johnson, Maarten V. de Hoop, G. Beroza"
06a81f63fc4ccfcf02934647a7c17454b91853b0,https://www.semanticscholar.org/paper/06a81f63fc4ccfcf02934647a7c17454b91853b0,Machine Learning - The Art and Science of Algorithms that Make Sense of Data,"As one of the most comprehensive machine learning texts around, this book does justice to the field's incredible richness, but without losing sight of the unifying principles. Peter Flach's clear, example-based approach begins by discussing how a spam filter works, which gives an immediate introduction to machine learning in action, with a minimum of technical fuss. Flach provides case studies of increasing complexity and variety with well-chosen examples and illustrations throughout. He covers a wide range of logical, geometric and statistical models and state-of-the-art topics such as matrix factorisation and ROC analysis. Particular attention is paid to the central role played by features. The use of established terminology is balanced with the introduction of new and useful concepts, and summaries of relevant background material are provided with pointers for revision if necessary. These features ensure Machine Learning will set a new standard as an introductory textbook.",2012.0,P. Flach
a461233e56079fc5af6e48d75f38be8c9ff87c1e,https://www.semanticscholar.org/paper/a461233e56079fc5af6e48d75f38be8c9ff87c1e,Machine Learning: New Ideas and Tools in Environmental Science and Engineering.,"The rapid increase in both the quantity and complexity of data that are being generated daily in the field of environmental science and engineering (ESE) demands accompanied advancement in data analytics. Advanced data analysis approaches, such as machine learning (ML), have become indispensable tools for revealing hidden patterns or deducing correlations for which conventional analytical methods face limitations or challenges. However, ML concepts and practices have not been widely utilized by researchers in ESE. This feature explores the potential of ML to revolutionize data analysis and modeling in the ESE field, and covers the essential knowledge needed for such applications. First, we use five examples to illustrate how ML addresses complex ESE problems. We then summarize four major types of applications of ML in ESE: making predictions; extracting feature importance; detecting anomalies; and discovering new materials or chemicals. Next, we introduce the essential knowledge required and current shortcomings in ML applications in ESE, with a focus on three important but often overlooked components when applying ML: correct model development, proper model interpretation, and sound applicability analysis. Finally, we discuss challenges and future opportunities in the application of ML tools in ESE to highlight the potential of ML in this field.",2021.0,"Shifa Zhong, Kai Zhang, M. Bagheri, J. Burken, A. Gu, Baikun Li, Xingmao Ma, B. Marrone, Z. Ren, Joshua Schrier, W. Shi, Haoyue Tan, Tianbao Wang, Xu Wang, Bryan M. Wong, Xusheng Xiao, X. Yu, Jun‐Jie Zhu, Huichun Zhang"
b7d5dda24d0c540929cd58b0226eac8a85e9769b,https://www.semanticscholar.org/paper/b7d5dda24d0c540929cd58b0226eac8a85e9769b,Consistent Covariance Matrix Estimation with Spatially Dependent Panel Data,"Many panel data sets encountered in macroeconomics, international economics, regional science, and finance are characterized by cross-sectional or spatial dependence. Standard techniques that fail to account for this dependence will result in inconsistently estimated standard errors. In this paper we present conditions under which a simple extension of common nonparametric covariance matrix estimation techniques yields standard error estimates that are robust to very general forms of spatial and temporal dependence as the time dimension becomes large. We illustrate the relevance of this approach using Monte Carlo simulations and a number of empirical examples.",1998.0,"J. Driscoll, Aart C. Kraay"
943fcdb089194efd02d60b8649f3872389de9797,https://www.semanticscholar.org/paper/943fcdb089194efd02d60b8649f3872389de9797,Data-Driven Machine Learning in Environmental Pollution: Gains and Problems.,"The complexity and dynamics of the environment make it extremely difficult to directly predict and trace the temporal and spatial changes in pollution. In the past decade, the unprecedented accumulation of data, the development of high-performance computing power, and the rise of diverse machine learning (ML) methods provide new opportunities for environmental pollution research. The ML methodology has been used in satellite data processing to obtain ground-level concentrations of atmospheric pollutants, pollution source apportionment, and spatial distribution modeling of water pollutants. However, unlike the active practices of ML in chemical toxicity prediction, advanced algorithms such as deep neural networks in environmental process studies of pollutants are still deficient. In addition, over 40% of the environmental applications of ML go to air pollution, and its application range and acceptance in other aspects of environmental science remain to be increased. The use of ML methods to revolutionize environmental science and its problem-solving scenarios has its own challenges. Several issues should be taken into consideration, such as the tradeoff between model performance and interpretability, prerequisites of the machine learning model, model selection, and data sharing.",2022.0,"Xian Liu, Dawei Lu, A. Zhang, Qian S. Liu, G. Jiang"
852c3c29c319ba1ae2c6efac3471a3f5c5b4a232,https://www.semanticscholar.org/paper/852c3c29c319ba1ae2c6efac3471a3f5c5b4a232,Comparing science communication theory with practice: An assessment and critique using Australian data,"Scholars have variously described different models of science communication over the past 20 years. However, there has been little assessment of theorised models against science communication practice. This article compares 515 science engagement activities recorded in a 2012 Australian audit against the theorised characteristics of the three dominant models of deficit, dialogue and participation. Most engagement activities had objectives that reflected a mix of deficit and dialogue activities. Despite increases in scientific controversies like climate change, there appears to be a paucity of participatory activities in Australia. Those that do exist are mostly about people being involved with science through activities like citizen science. These participatory activities appear to coexist with and perhaps even depend on deficit activities. Science communication scholars could develop their models by examining the full range of objectives for engagement found in practice and by recognising that any engagement will likely include a mix of approaches.",2019.0,J. Metcalfe
fb3140c9766a5bc92400ac8ce9d48a4272bba69e,https://www.semanticscholar.org/paper/fb3140c9766a5bc92400ac8ce9d48a4272bba69e,A New Kind of Science,"nationwide data set of losses from 1975 to 1998 was compiled to assess the trends. Temporal patterns of deaths and injuries, monetary damages, and—in some cases—the number of events are systematically examined by year in chapter 5, and the authors undertake a systematic spatial assessment of the statewide totals in chapter 6. Explanations for some of the patterns are offered, particularly for the most significant disasters and for the states with most events or the greatest losses. Further refinement and evaluation of patterns of economic losses and death are undertaken by normalizing losses by population, land area, and gross domestic product (GDP). The authors advance the discussion from simple descriptions of loss patterns to explanations of the patterns of disaster-loss burden, and some surprises emerge from the arithmetic. For instance, North Dakota, Iowa, andMississippi not only suffered the greatest monetary losses per capita during the period, but also suffered the greatest losses of property and crops compared to their state GDP!For afinal analysis, the authors created an overall hazard score (averaged proportion of the states’ contributions to the national totals of events, deaths, and damages) and used it to rank the states. Using this ranking, states were assigned to categories of ‘‘proneness,’’ from highest (Florida, Texas, andCalifornia) to lowest (Rhode Island, Delaware, Alaska and other small or lightly populated states). The conclusion we are to draw is that the amount of loss a state has experienced indicates its disaster proneness. Finally, ‘‘Charting a Course for theNext Two Decades’’ by Cutter describes what is needed to produce the models and data appropriate for mitigation and planning assessments. In order for an effective assessment of events and losses to occur, progress is required in several areas: development of vulnerability science, the creation of a national hazard events and losses database, and the establishment of a national loss inventory and events clearinghouse. To do so, Cutter argues, we need to rethink thewaywe monitor, assess, andmanage our vulnerabilities. She briefly describes the shifts needed in data gathering and provision, sustainability and distributive justice, strategic planning, research funding, and societal awareness of issues that influence the prospects for disaster. While American Hazardscapes is intended to provide a broadunderstanding of the geography of loss due to hazards in the United States, it suffers from its openly acknowledged limitations. Though criticizing the quality of currently available data, the authors use those data to indicate the prospects for future disasters. The elimination of extreme events is no longer believed tobe the key loss reduction. Instead,we must identify and avoid places too dynamic for permanent occupation and adjust to the inevitable events in ways that limit prospects for loss. Mitigation must address the vulnerabilities that cause greater exposure and profound upset of our social systems and create more complex catastrophes. The data employed in this assessment describe (however imperfectly) the losses suffered over two and a half decades. The largest disasters overwhelm the patterns of loss in their analysis. The authors imply, based on proneness rankings, that those who lost the most are the most prone to loss. But in reality, losses are byproducts of the interplay of two dynamic geographies: the pattern of extreme events and the pattern of human use of the landscape. The former is often poorly understood, may not behave consistently, andmay operate on greater than twenty-five-year cycles. The latter may change so rapidly that it surpasses our capacity to measure it and map it, and postdisaster land use and human perception may be radically changed. These geographies were outside the scope of this book, however, and given new homeland security efforts and reorganization of the Federal Emergency Management Agency, the past is an even poorer indicator of the future.",2003.0,Raymond Kurzweil
efa5558bddd68abe4adc81adbbef6f739e648392,https://www.semanticscholar.org/paper/efa5558bddd68abe4adc81adbbef6f739e648392,Big Data: Astronomical or Genomical?,"Genomics is a Big Data science and is going to get much bigger, very soon, but it is not known whether the needs of genomics will exceed other Big Data domains. Projecting to the year 2025, we compared genomics with three other major generators of Big Data: astronomy, YouTube, and Twitter. Our estimates show that genomics is a “four-headed beast”—it is either on par with or the most demanding of the domains analyzed here in terms of data acquisition, storage, distribution, and analysis. We discuss aspects of new technologies that will need to be developed to rise up and meet the computational challenges that genomics poses for the near future. Now is the time for concerted, community-wide planning for the “genomical” challenges of the next decade.",2015.0,"Z. Stephens, Sk Lee, F. Faghri, R. Campbell, ChengXiang Zhai, Miles Efron, R. Iyer, M. Schatz, S. Sinha, G. Robinson"
daaf02de10f338d98ed6f58c13987b63b275825a,https://www.semanticscholar.org/paper/daaf02de10f338d98ed6f58c13987b63b275825a,Gaia Early Data Release 3,"Context. Since July 2014, the Gaia mission has been engaged in a high-spatial-resolution, time-resolved, precise, accurate astrometric, and photometric survey of the entire sky.
Aims. We present the Gaia Science Alerts project, which has been in operation since 1 June 2016. We describe the system which has been developed to enable the discovery and publication of transient photometric events as seen by Gaia.
Methods. We outline the data handling, timings, and performances, and we describe the transient detection algorithms and filtering procedures needed to manage the high false alarm rate. We identify two classes of events: (1) sources which are new to Gaia and (2) Gaia sources which have undergone a significant brightening or fading. Validation of the Gaia transit astrometry and photometry was performed, followed by testing of the source environment to minimise contamination from Solar System objects, bright stars, and fainter near-neighbours.
Results. We show that the Gaia Science Alerts project suffers from very low contamination, that is there are very few false-positives. We find that the external completeness for supernovae, CE = 0.46, is dominated by the Gaia scanning law and the requirement of detections from both fields-of-view. Where we have two or more scans the internal completeness is CI = 0.79 at 3 arcsec or larger from the centres of galaxies, but it drops closer in, especially within 1 arcsec.
Conclusions. The per-transit photometry for Gaia transients is precise to 1% at G = 13, and 3% at G = 19. The per-transit astrometry is accurate to 55 mas when compared to Gaia DR2. The Gaia Science Alerts project is one of the most homogeneous and productive transient surveys in operation, and it is the only survey which covers the whole sky at high spatial resolution (subarcsecond), including the Galactic plane and bulge.",2021.0,"S. Hodgkin, D. Harrison, E. Breedt, T. Wevers, G. Rixon, A. Delgado, A. Yoldas, Z. Kostrzewa-Rutkowska, L. Wyrzykowski, M. Leeuwen, N. Blagorodnova, H. Campbell, D. Eappachen, M. Fraser, N. Ihanec, Sergey E. Koposov, K. Kruszy'nska, G. Marton, K. Rybicki, A. Brown, P. Burgess, G. Busso, S. Cowell, F. Angeli, C. Diener, D. Evans, G. Gilmore, G. Holland, P. Jonker, F. V. Leeuwen, F. Mignard, P. Osborne, J. Portell, T. Prusti, P. J. Richards, M. Riello, G. Seabroke, N. Walton, P. 'Abrah'am, G. Altavilla, S. Baker, U. Bastian, P. O'Brien, J. Bruijne, T. Butterley, J. Carrasco, J. Castañeda, J. Clark, G. Clementini, C. Copperwheat, M. Cropper, G. Damljanovic, M. Davidson, C. Davis, M. Dennefeld, V. Dhillon, C. Dolding, M. Dominik, P. Esquej, L. Eyer, C. Fabricius, M. Fridman, D. Froebrich, N. Garralda, A. Gomboc, J. J. Gonz'alez-Vidal, R. Guerra, N. Hambly, L. Hardy, B. Holl, A. Hourihane, J. Japelj, D. A. Kann, C. Kiss, C. Knigge, U. Kolb, S. Komossa, 'A. K'osp'al, G. Kov'acs, M. Kun, G. Leto, F. Lewis, S. Littlefair, A. Mahabal, C. Mundell, Z. Nagy, D. Padeletti, L. Palaversa, A. Pigulski, M. Pretorius, W. V. Reeven, V. Ribeiro, M. Roelens, N. Rowell, N. Schartel, A. Scholz, A. Schwope, B. SipHocz, S. Smartt, M. Smith, I. Serraller, D. Steeghs, M. Sullivan, L. Szabados, E. Szegedi-Elek, P. Tisserand, L. Tomasella, S. Velzen, P. Whitelock, R. Wilson, D. Young"
08a2ef1648fa5ea539ebe1718da577dc79124a21,https://www.semanticscholar.org/paper/08a2ef1648fa5ea539ebe1718da577dc79124a21,Prospects and challenges for social media data in conservation science,"Social media data have been extensively used in numerous fields of science, but examples of their use in conservation science are still very limited. In this paper, we propose a framework on how social media data could be useful for conservation science and practice. We present the commonly used social media platforms and discuss how their content could be providing new data and information for conservation science. Based on this, we discuss how future work in conservation science and practice would benefit from social media data.",2015.0,"E. D. Minin, H. Tenkanen, T. Toivonen"
b48a917258f4e7e2b78a41289d005513db1de8c9,https://www.semanticscholar.org/paper/b48a917258f4e7e2b78a41289d005513db1de8c9,Earth Observation Open Science: Enhancing Reproducible Science Using Data Cubes,"Earth Observation Data Cubes (EODC) have emerged as a promising solution to efficiently and effectively handle Big Earth Observation (EO) Data generated by satellites and made freely and openly available from different data repositories. The aim of this Special Issue, “Earth Observation Data Cube”, in Data, is to present the latest advances in EODC development and implementation, including innovative approaches for the exploitation of satellite EO data using multi-dimensional (e.g., spatial, temporal, spectral) approaches. This Special Issue contains 14 articles covering a wide range of topics such as Synthetic Aperture Radar (SAR), Analysis Ready Data (ARD), interoperability, thematic applications (e.g., land cover, snow cover mapping), capacity development, semantics, processing techniques, as well as national implementations and best practices. These papers made significant contributions to the advancement of a more Open and Reproducible Earth Observation Science, reducing the gap between users’ expectations for decision-ready products and current Big Data analytical capabilities, and ultimately unlocking the information power of EO data by transforming them into actionable knowledge.",2019.0,"G. Giuliani, G. Câmara, B. Killough, S. Minchin"
24931dc3ddedfc2db5405af236e3ca84944d66d7,https://www.semanticscholar.org/paper/24931dc3ddedfc2db5405af236e3ca84944d66d7,Big Data and Social Science: A Practical Guide to Methods and Tools,"Both Traditional Students and Working Professionals Acquire the Skills to Analyze Social Problems. Big Data and Social Science: A Practical Guide to Methods and Tools shows how to apply data science to real-world problems in both research and the practice. The book provides practical guidance on combining methods and tools from computer science, statistics, and social science. This concrete approach is illustrated throughout using an important national problem, the quantitative study of innovation. The text draws on the expertise of prominent leaders in statistics, the social sciences, data science, and computer science to teach students how to use modern social science research principles as well as the best analytical and computational tools. It uses a real-world challenge to introduce how these tools are used to identify and capture appropriate data, apply data science models and tools to that data, and recognize and respond to data errors and limitations. For more information, including sample chapters and news, please visit the author's website.",2016.0,"Ian M. Foster, R. Ghani, Ron S. Jarmin, F. Kreuter, J. Lane"
b9921fb4d1448058642897797e77bdaf8f444404,https://www.semanticscholar.org/paper/b9921fb4d1448058642897797e77bdaf8f444404,Text as Data: The Promise and Pitfalls of Automatic Content Analysis Methods for Political Texts,"Politics and political conflict often occur in the written and spoken word. Scholars have long recognized this, but the massive costs of analyzing even moderately sized collections of texts have hindered their use in political science research. Here lies the promise of automated text analysis: it substantially reduces the costs of analyzing large collections of text. We provide a guide to this exciting new area of research and show how, in many instances, the methods have already obtained part of their promise. But there are pitfalls to using automated methods—they are no substitute for careful thought and close reading and require extensive and problem-specific validation. We survey a wide range of new methods, provide guidance on how to validate the output of the models, and clarify misconceptions and errors in the literature. To conclude, we argue that for automated text methods to become a standard tool for political scientists, methodologists must contribute new methods and new methods of validation.",2013.0,"Justin Grimmer, Brandon M Stewart"
6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,https://www.semanticscholar.org/paper/6802bbeea45ea9c44b8e9f69ee1d775f5af0717f,Ethical Issues Relating to Scientific Discovery in Exercise Science.,"This work aims to present concepts related to ethical issues in conducting and reporting scientific research in a clear and straightforward manner. Considerations around research design including authorship, sound research practices, non-discrimination in subject recruitment, objectivity, respect for intellectual property, and financial interests are detailed. Further, concepts relating to the conducting of research including the competency of the researcher, conflicts of interest, accurately representing data, and ethical practices in human and animal research are presented. Attention pertaining to the dissemination of research including plagiarism, duplicate submission, redundant publication, and figure manipulation is offered. Other considerations including responsible mentoring, respect for colleagues, and social responsibility are set forth. The International Journal of Exercise Science will now require a statement in all subsequent published manuscripts that the authors have complied with each of the ethics statements contained in this work.",2019.0,"J. Navalta, W. Stone"
7f29044de1a0e5a6d3ec1d33fb6ad482f3d10dd4,https://www.semanticscholar.org/paper/7f29044de1a0e5a6d3ec1d33fb6ad482f3d10dd4,"Analytics : An Overview from Data ‐ Driven Smart Computing , Decision ‐ Making and Applications Perspective","The digital world has a wealth of data, such as internet of things (IoT) data, business data, health data, mobile data, urban data, security data, and many more, in the current age of the Fourth Industrial Revolution (Industry 4.0 or 4IR). Extracting knowledge or useful insights from these data can be used for smart decision-making in various applications domains. In the area of data science, advanced analytics methods including machine learning modeling can provide actionable insights or deeper knowledge about data, which makes the computing process automatic and smart. In this paper, we present a comprehensive view on “Data Science” including various types of advanced analytics methods that can be applied to enhance the intelligence and capabilities of an application through smart decision-making in different scenarios. We also discuss and summarize ten potential real-world application domains including business, healthcare, cybersecurity, urban and rural data science, and so on by taking into account data-driven smart computing and decision making. Based on this, we finally highlight the challenges and potential research directions within the scope of our study. Overall, this paper aims to serve as a reference point on data science and advanced analytics to the researchers and decision-makers as well as application developers, particularly from the data-driven solution point of view for real-world problems.",2021.0,Sarker
cf9ecfbbd0095687c4cfbbbfa0546914e651b109,https://www.semanticscholar.org/paper/cf9ecfbbd0095687c4cfbbbfa0546914e651b109,"Calibration of the Computer Science and Applications, Inc. accelerometer.","PURPOSE
We established accelerometer count ranges for the Computer Science and Applications, Inc. (CSA) activity monitor corresponding to commonly employed MET categories.


METHODS
Data were obtained from 50 adults (25 males, 25 females) during treadmill exercise at three different speeds (4.8, 6.4, and 9.7 km x h(-1)).


RESULTS
Activity counts and steady-state oxygen consumption were highly correlated (r = 0.88), and count ranges corresponding to light, moderate, hard, and very hard intensity levels were < or = 1951, 1952-5724, 5725-9498, > or = 9499 cnts x min(-1), respectively. A model to predict energy expenditure from activity counts and body mass was developed using data from a random sample of 35 subjects (r2 = 0.82, SEE = 1.40 kcal x min(-1)). Cross validation with data from the remaining 15 subjects revealed no significant differences between actual and predicted energy expenditure at any treadmill speed (SEE = 0.50-1.40 kcal x min(-1)).


CONCLUSIONS
These data provide a template on which patterns of activity can be classified into intensity levels using the CSA accelerometer.",1998.0,"P. Freedson, E. Melanson, J. Sirard"
ad3d83248eae66580d4deada76e72e3be9a9b44c,https://www.semanticscholar.org/paper/ad3d83248eae66580d4deada76e72e3be9a9b44c,Named data networking,"Named Data Networking (NDN) is one of five projects funded by the U.S. National Science Foundation under its Future Internet Architecture Program. NDN has its roots in an earlier project, Content-Centric Networking (CCN), which Van Jacobson first publicly presented in 2006. The NDN project investigates Jacobson's proposed evolution from today's host-centric network architecture (IP) to a data-centric network architecture (NDN). This conceptually simple shift has far-reaching implications for how we design, develop, deploy, and use networks and applications. We describe the motivation and vision of this new architecture, and its basic components and operations. We also provide a snapshot of its current design, development status, and research challenges. More information about the project, including prototype implementations, publications, and annual reports, is available on named-data.net.",2014.0,"Lixia Zhang, Alexander Afanasyev, J. Burke, Van Jacobson, K. Claffy, Patrick Crowley, Christos Papadopoulos, Lan Wang, Beichuan Zhang"
952241d28abed7d221fc059845043a6463a522bc,https://www.semanticscholar.org/paper/952241d28abed7d221fc059845043a6463a522bc,Qualitative Descriptive Methods in Health Science Research,"Objective: The purpose of this methodology paper is to describe an approach to qualitative design known as qualitative descriptive that is well suited to junior health sciences researchers because it can be used with a variety of theoretical approaches, sampling techniques, and data collection strategies. Background: It is often difficult for junior qualitative researchers to pull together the tools and resources they need to embark on a high-quality qualitative research study and to manage the volumes of data they collect during qualitative studies. This paper seeks to pull together much needed resources and provide an overview of methods. Methods: A step-by-step guide to planning a qualitative descriptive study and analyzing the data is provided, utilizing exemplars from the authors’ research. Results: This paper presents steps to conducting a qualitative descriptive study under the following headings: describing the qualitative descriptive approach, designing a qualitative descriptive study, steps to data analysis, and ensuring rigor of findings. Conclusions: The qualitative descriptive approach results in a summary in everyday, factual language that facilitates understanding of a selected phenomenon across disciplines of health science researchers.",2016.0,"Karen Colorafi, B. Evans"
0b510ee69a507407008661aacb2345f73c70f8cb,https://www.semanticscholar.org/paper/0b510ee69a507407008661aacb2345f73c70f8cb,Strategies Employed by Citizen Science Programs to Increase the Credibility of Their Data,"The success of citizen science in producing important and unique data is attracting interest from scientists and resource managers. Nonetheless, questions remain about the credibility of citizen science data. Citizen science programs desire to meet the same standards of credibility as academic science, but they usually work within a different context, for example, training and managing significant numbers of volunteers with limited resources. We surveyed the credibility-building strategies of 30 citizen science programs that monitor environmental aspects of the California coast. We identified a total of twelve strategies: Three that are applied during training and planning; four that are applied during data collection; and five that are applied during data analysis and program evaluation. Variation in the application of these strategies by program is related to factors such as the number of participants, the focus on group or individual work, and the time commitment required of volunteers. The structure of each program and available resources require program designers to navigate tradeoffs in the choices of their credibility strategies. Our results illustrate those tradeoffs and provide a framework for the necessary discussions between citizen science programs and potential users of their data—including scientists and decision makers—about shared expectations for credibility and practical approaches for meeting those expectations. This article has been corrected here: http://dx.doi.org/10.5334/cstp.91",2016.0,"A. Freitag, R. Meyer, Liz Whiteman"
b36a93134fdbcb74de18d222d17fec177d32d363,https://www.semanticscholar.org/paper/b36a93134fdbcb74de18d222d17fec177d32d363,A Review on Data Preprocessing Techniques Toward Efficient and Reliable Knowledge Discovery From Building Operational Data,"The rapid development in data science and the increasing availability of building operational data have provided great opportunities for developing data-driven solutions for intelligent building energy management. Data preprocessing serves as the foundation for valid data analyses. It is an indispensable step in building operational data analysis considering the intrinsic complexity of building operations and deficiencies in data quality. Data preprocessing refers to a set of techniques for enhancing the quality of the raw data, such as outlier removal and missing value imputation. This article serves as a comprehensive review of data preprocessing techniques for analysing massive building operational data. A wide variety of data preprocessing techniques are summarised in terms of their applications in missing value imputation, outlier detection, data reduction, data scaling, data transformation, and data partitioning. In addition, three state-of-the-art data science techniques are proposed to tackle practical data challenges in the building field, i.e., data augmentation, transfer learning, and semi-supervised learning. In-depth discussions have been presented to describe the pros and cons of existing preprocessing methods, possible directions for future research and potential applications in smart building energy management. The research outcomes are helpful for the development of data-driven research in the building field.",2021.0,"C. Fan, Meiling Chen, Xinghua Wang, Jiayuan Wang, Bufu Huang"
500b73ecdf8ff5590718edb03367e3836a368485,https://www.semanticscholar.org/paper/500b73ecdf8ff5590718edb03367e3836a368485,Secondary Data Analysis: A Method of which the Time Has Come,"Technological advances have led to vast amounts of data that has been collected, compiled, and archived, and that is now easily accessible for research. As a result, utilizing existing data for research is becoming more prevalent, and therefore secondary data analysis. While secondary analysis is flexible and can be utilized in several ways, it is also an empirical exercise and a systematic method with procedural and evaluative steps, just as in collecting and evaluating primary data. This paper asserts that secondary data analysis is a viable method to utilize in the process of inquiry when a systematic procedure is followed and presents an illustrative research application utilizing secondary data analysis in library and information science research.",2017.0,Melissa P. Johnston
cebed64039064dfe950587b919ddc01dee7d871f,https://www.semanticscholar.org/paper/cebed64039064dfe950587b919ddc01dee7d871f,From Little Science to Big Science,"In Little Science, Big Science (1963), Derek J. de Solla Price undertook a sociology of science that dealt with the growth and changing shape of scientific publishing and the social organization of science. The focus of Price’s work was on the long-term, gradual shift from “little science,” with the solo scientist, small laboratory, and minimal research funds, to “big science,” with collaborative research teams, large-scale research hardware, extensive funding, and corporate-political suitors of scientists. We extend Price’s focus on scientific publications by moving beyond his analysis of practices in physics and chemistry to examine a social science; namely, sociology. Specifically, we analyze 3,000 articles in four long-standing sociology journals over the fifty-year period from 1960-2010 to determine the gender of authors, the prestige of authors’ departments, length of articles, number of references, sources of data for studies, and patterns of funding for research. We find that sociology is not immune from the shift from “little science” to “big science.”",2017.0,"R. Perrucci, C. Perrucci, M. Subramaniam"
cff7b1b98da6de583bf2d5ffd496c2e6d70a794c,https://www.semanticscholar.org/paper/cff7b1b98da6de583bf2d5ffd496c2e6d70a794c,From DFT to machine learning: recent approaches to materials science–a review,"Recent advances in experimental and computational methods are increasing the quantity and complexity of generated data. This massive amount of raw data needs to be stored and interpreted in order to advance the materials science field. Identifying correlations and patterns from large amounts of complex data is being performed by machine learning algorithms for decades. Recently, the materials science community started to invest in these methodologies to extract knowledge and insights from the accumulated data. This review follows a logical sequence starting from density functional theory as the representative instance of electronic structure methods, to the subsequent high-throughput approach, used to generate large amounts of data. Ultimately, data-driven strategies which include data mining, screening, and machine learning techniques, employ the data generated. We show how these approaches to modern computational materials science are being used to uncover complexities and design novel materials with enhanced properties. Finally, we point to the present research problems, challenges, and potential future perspectives of this new exciting field.",2019.0,"G. R. Schleder, A. C. Padilha, C. M. Acosta, M. Costa, A. Fazzio"
5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,https://www.semanticscholar.org/paper/5b5332e79aefa3b913d42a434b8ddb09b31b5b2e,Voronoi diagrams—a survey of a fundamental geometric data structure,"Computational geometry is concerned with the design and analysis of algorithms for geometrical problems. In addition, other more practically oriented, areas of computer science— such as computer graphics, computer-aided design, robotics, pattern recognition, and operations research—give rise to problems that inherently are geometrical. This is one reason computational geometry has attracted enormous research interest in the past decade and is a well-established area today. (For standard sources, we refer to the survey article by Lee and Preparata [19841 and to the textbooks by Preparata and Shames [1985] and Edelsbrunner [1987bl.) Readers familiar with the literature of computational geometry will have noticed, especially in the last few years, an increasing interest in a geometrical construct called the Voronoi diagram. This trend can also be observed in combinatorial geometry and in a considerable number of articles in natural science journals that address the Voronoi diagram under different names specific to the respective area. Given some number of points in the plane, their Voronoi diagram divides the plane according to the nearest-neighbor",1991.0,F. Aurenhammer
06ba782753bad19254db5d28ad4155556f286ee0,https://www.semanticscholar.org/paper/06ba782753bad19254db5d28ad4155556f286ee0,Data Management and Analysis Methods,"This chapter is about methods for managing and analyzing qualitative data. By qualitative data the authors mean text: newspapers, movies, sitcoms, e-mail traffic, folktales, life histories. They also mean narratives--narratives about getting divorced, about being sick, about surviving hand-to-hand combat, about selling sex, about trying to quit smoking. In fact, most of the archaeologically recoverable information about human thought and human behavior is text, the good stuff of social science.",2000.0,"G. Ryan, H. Bernard"
cf83811d697dc3419a52c9853807afb410eb3943,https://www.semanticscholar.org/paper/cf83811d697dc3419a52c9853807afb410eb3943,Tree-Based Models for Political Science Data,"Political scientists often find themselves analyzing data sets with a large number of observations, a large number of variables, or both. Yet, traditional statistical techniques fail to take full advantage of the opportunities inherent in “big data,” as they are too rigid to recover nonlinearities and do not facilitate the easy exploration of interactions in high-dimensional data sets. In this article, we introduce a family of tree-based nonparametric techniques that may, in some circumstances, be more appropriate than traditional methods for confronting these data challenges. In particular, tree models are very effective for detecting nonlinearities and interactions, even in data sets with many (potentially irrelevant) covariates. We introduce the basic logic of tree-based models, provide an overview of the most prominent methods in the literature, and conduct three analyses that illustrate how the methods can be implemented while highlighting both their advantages and limitations. Replication Materials: The data, code, and any additional materials required to replicate all analyses in this article are available on the American Journal of Political Science Dataverse within the Harvard Dataverse Network at: https://doi.org/10.7910/DVN/8ZJBLI. Social science scholars often work with data sets containing a large number of observations, many potential covariates, or (increasingly) both. Indeed, political scientists now regularly analyze data with levels of complexity unimaginable just two decades ago. Widely used surveys, for instance, interview tens of thousands of respondents about hundreds of topics. Scholars of institutions can quickly assemble data sets with thousands of observations using resources like the Comparative Agendas Project. Moreover, new measurement methods, such as text analysis, have combined with data sources, such as Twitter, to generate databases of almost unmanageable sizes. It is clear that political science, like all areas of the social sciences, will increasingly have access to a deluge of data so vast that it will dwarf everything that has come before. What statistical methods are needed in this datasaturated world? Surely, there is no one correct answer. Yet, just as surely, traditional statistical models are not always equipped to take full advantage of new data sources. Traditional models—largely variants of linear regressions—are ideal for evaluating theories that imply specific functional forms relating outcomes to predictors. In particular, they excel in their ability to leverage assumptions about the data-generating process, or DGP (additivity, linearity in the parameters, homoskedasticity, Jacob M. Montgomery is Associate Professor, Department of Political Science, Washington University in St. Louis, Campus Box 1063, One Brookings Drive, St. Louis, MO 63130 (jacob.montgomery@wustl.edu). Santiago Olivella is Assistant Professor, Department of Political Science, University of North Carolina at Chapel Hill, Hamilton Hall 361, CB 3265, Chapel Hill, NC 27599 (olivella@unc.edu). etc.) to make valid inferences despite inherent data limitations. Although appropriate when testing theories that conform with these assumptions, standard models are often insufficiently flexible to capture nuances in the data—such as complex nonlinear functional forms and deep interactions—when no clear a priori expectations exist. In this article, we introduce a family of tree-based nonparametric techniques from the machine learning literature. We argue that, under specific circumstances, regression and classification tree models are an appropriate standard choice for analyzing high-dimensional data sets. In particular, past research has shown tree-based methods to be very useful for making accurate predictions when the underlying DGP includes nonlinearities, discontinuities, and interactions among many covariates. Further, tree models require few assumptions. Rather than imposing a presumed structure on the DGP, tree-based methods allow the data to “speak for themselves.” Thus, our goal in this article is to introduce political scientists to this promising family of methods, which are well suited for today’s data analysis demands. In the next sections, we discuss the promise and perils of high-dimensional, “large”-N data sets and introduce the basic logic of tree models. We then provide an overview of the most prominent methods in the literature. American Journal of Political Science, Vol. 62, No. 3, July 2018, Pp. 729–744 C ©2018, Midwest Political Science Association DOI: 10.1111/ajps.12361",2018.0,"J. Montgomery, S. Olivella"
bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,https://www.semanticscholar.org/paper/bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",1979.0,"Gordon D. Booth, George E. P. Box, William G. Hunter, J. S. Hunter"
c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,https://www.semanticscholar.org/paper/c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information’s (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. INTRODUCTION Academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis, along with publications assessment and expert opinions, for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. In general, citation counts or rankings are considered partial indicators of research impact/quality, often used to support and/or question other indicators such as peer judgment (Borgman & Furner, 2002; Cronin, 1984; Holden, Rosenberg, & Barker, 2005; Moed, 2005; van Raan, 1996, 2005; Wallin, 2005). To whom all correspondence should be addressed. Many scholars have argued for and against the use of citations for assessing research impact or quality. Proponents have reported the validity and reliability of citation counts in research assessments as well as the positive correlation between these counts and peer reviews/assessments of publication venues (Aksnes & Taxt, 2004; Glänzel, 1996; Kostoff, 1996; Martin, 1996; Narin, 1976; So, 1998; van Raan, 2000). Critics, on the other hand, claim that citation counting has serious problems or limitations that affect its validity (MacRoberts & MacRoberts, 1996; Seglen, 1998). Important limitations reported in the literature focus on, among other things, the problems associated with the data sources used, especially the Institute for Scientific Information (ISI, currently Thomson Scientific) citation databases: Arts & Humanities Citation Index, Science Citation Index, and Social Sciences Citation Index—the standard and most widely used tools for generating citation data for research and other assessment purposes. These tools are now currently part of what is known as Web of Science (WoS), the portal used to search the three ISI citation databases. In this paper, we use ISI citation databases and WoS interchangeably. Critics of ISI citation databases note that they: (1) cover mainly North American, Western European, and English-language titles; (2) are limited to citations from 8,700 journals; (3) do not count citations from books and most conference proceedings; (4) provide different coverage between research fields; and (5) have citing errors, such as homonyms, synonyms, and inconsistency in the use of initials and in the spelling of non-English names (many of these errors, however, come from the primary documents themselves rather than being the result of faulty ISI indexing). Studies that have addressed problems of, and/or suggested alternative or complementary sources to, ISI citation databases are very few and can be divided into two main groups: Studies that examined the effect of certain limitations in ISI database, most often comparing its coverage with that of other citation sources; and Studies that suggested or explored different or additional sources and methods for identifying citations. Ulrich’s Periodicals Directory lists approximately 22,500 active academic/scholarly, refereed journals. Of these, approximately 7,500 are published in the United States, 4,150 in the United Kingdom, 1,600 in the Netherlands, 1,370 in Germany, 660 in Australia, 540 in Japan, and 500 in Canada, 450 in China, 440 in India, and 420 in France.",2007.0,"Lokman I. Meho, Kiduk Yang"
a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,https://www.semanticscholar.org/paper/a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",2015.0,"M. Head, L. Holman, R. Lanfear, A. Kahn, M. Jennions"
bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,https://www.semanticscholar.org/paper/bfcf1ed94050a4c60d459cd02456dfd9f08fdb4c,"Statistics for experimenters : an introduction to design, data analysis, and model building","Science and Statistics. COMPARING TWO TREATMENTS. Use of External Reference Distribution to Compare Two Means. Random Sampling and the Declaration of Independence. Randomization and Blocking with Paired Comparisons. Significance Tests and Confidence Intervals for Means, Variances, Proportions and Frequences. COMPARING MORE THAN TWO TREATMENTS. Experiments to Compare k Treatment Means. Randomized Block and Two--Way Factorial Designs. Designs with More Than One Blocking Variable. MEASURING THE EFFECTS OF VARIABLES. Empirical Modeling. Factorial Designs at Two Levels. More Applications of Factorial Designs. Fractional Factorial Designs at Two Levels. More Applications of Fractional Factorial Designs. BUILDING MODELS AND USING THEM. Simple Modeling with Least Squares (Regression Analysis). Response Surface Methods. Mechanistic Model Building. Study of Variation. Modeling Dependence: Times Series. Appendix Tables. Index.",1979.0,"Gordon D. Booth, George E. P. Box, William G. Hunter, J. S. Hunter"
c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,https://www.semanticscholar.org/paper/c8bad3f510224e5cb010ca422149bf6ebcaa1d7f,Impact of data sources on citation counts and rankings of LIS faculty: Web of science versus scopus and google scholar,"The Institute for Scientific Information’s (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours. INTRODUCTION Academic institutions, federal agencies, publishers, editors, authors, and librarians increasingly rely on citation analysis, along with publications assessment and expert opinions, for making hiring, promotion, tenure, funding, and/or reviewer and journal evaluation and selection decisions. In general, citation counts or rankings are considered partial indicators of research impact/quality, often used to support and/or question other indicators such as peer judgment (Borgman & Furner, 2002; Cronin, 1984; Holden, Rosenberg, & Barker, 2005; Moed, 2005; van Raan, 1996, 2005; Wallin, 2005). To whom all correspondence should be addressed. Many scholars have argued for and against the use of citations for assessing research impact or quality. Proponents have reported the validity and reliability of citation counts in research assessments as well as the positive correlation between these counts and peer reviews/assessments of publication venues (Aksnes & Taxt, 2004; Glänzel, 1996; Kostoff, 1996; Martin, 1996; Narin, 1976; So, 1998; van Raan, 2000). Critics, on the other hand, claim that citation counting has serious problems or limitations that affect its validity (MacRoberts & MacRoberts, 1996; Seglen, 1998). Important limitations reported in the literature focus on, among other things, the problems associated with the data sources used, especially the Institute for Scientific Information (ISI, currently Thomson Scientific) citation databases: Arts & Humanities Citation Index, Science Citation Index, and Social Sciences Citation Index—the standard and most widely used tools for generating citation data for research and other assessment purposes. These tools are now currently part of what is known as Web of Science (WoS), the portal used to search the three ISI citation databases. In this paper, we use ISI citation databases and WoS interchangeably. Critics of ISI citation databases note that they: (1) cover mainly North American, Western European, and English-language titles; (2) are limited to citations from 8,700 journals; (3) do not count citations from books and most conference proceedings; (4) provide different coverage between research fields; and (5) have citing errors, such as homonyms, synonyms, and inconsistency in the use of initials and in the spelling of non-English names (many of these errors, however, come from the primary documents themselves rather than being the result of faulty ISI indexing). Studies that have addressed problems of, and/or suggested alternative or complementary sources to, ISI citation databases are very few and can be divided into two main groups: Studies that examined the effect of certain limitations in ISI database, most often comparing its coverage with that of other citation sources; and Studies that suggested or explored different or additional sources and methods for identifying citations. Ulrich’s Periodicals Directory lists approximately 22,500 active academic/scholarly, refereed journals. Of these, approximately 7,500 are published in the United States, 4,150 in the United Kingdom, 1,600 in the Netherlands, 1,370 in Germany, 660 in Australia, 540 in Japan, and 500 in Canada, 450 in China, 440 in India, and 420 in France.",2007.0,"Lokman I. Meho, Kiduk Yang"
a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,https://www.semanticscholar.org/paper/a07a64ba110e0f9f7156f3bd1e376f0d2e1cddf1,The Extent and Consequences of P-Hacking in Science,"A focus on novel, confirmatory, and statistically significant results leads to substantial bias in the scientific literature. One type of bias, known as “p-hacking,” occurs when researchers collect or select data or statistical analyses until nonsignificant results become significant. Here, we use text-mining to demonstrate that p-hacking is widespread throughout science. We then illustrate how one can test for p-hacking when performing a meta-analysis and show that, while p-hacking is probably common, its effect seems to be weak relative to the real effect sizes being measured. This result suggests that p-hacking probably does not drastically alter scientific consensuses drawn from meta-analyses.",2015.0,"M. Head, L. Holman, R. Lanfear, A. Kahn, M. Jennions"
bf96377353bf9daa8dc0e98eee17335f54cbcc60,https://www.semanticscholar.org/paper/bf96377353bf9daa8dc0e98eee17335f54cbcc60,Data science as an academic discipline,"I recall being a proud young academic about 1970; I had just received a research grant to build and study a scientific database, and I had joined CODATA. I was looking forward to the future in this new exciting discipline when the head of my department, an internationally known professor, advised me that data was “a low level activity” not suitable for an academic. I recall my dismay. What can we do to ensure that this does not happen again and that data science is universally recognized as a worthwhile academic activity? Incidentally, I did not take that advice, or I would not be writing this essay, but moved into computer science. I will use my experience to draw comparisons between the problems computer science had to become academically recognized and those faced by data science.",2006.0,F. J. Smith
d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,https://www.semanticscholar.org/paper/d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,A Bibliometric Analysis and Visualization of Medical Big Data Research,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",2018.0,"Huchang Liao, Ming Tang, L. Luo, Chunyang Li, F. Chiclana, Xiao-Jun Zeng"
846883b7761cb5fe4468d42bf9d328b5d1030175,https://www.semanticscholar.org/paper/846883b7761cb5fe4468d42bf9d328b5d1030175,"The Zwicky Transient Facility: Data Processing, Products, and Archive","The Zwicky Transient Facility (ZTF) is a new robotic time-domain survey currently in progress using the Palomar 48-inch Schmidt Telescope. ZTF uses a 47 square degree field with a 600 megapixel camera to scan the entire northern visible sky at rates of ∼3760 square degrees/hour to median depths of g ∼ 20.8 and r ∼ 20.6 mag (AB, 5σ in 30 sec). We describe the Science Data System that is housed at IPAC, Caltech. This comprises the data-processing pipelines, alert production system, data archive, and user interfaces for accessing and analyzing the products. The real-time pipeline employs a novel image-differencing algorithm, optimized for the detection of point-source transient events. These events are vetted for reliability using a machine-learned classifier and combined with contextual information to generate data-rich alert packets. The packets become available for distribution typically within 13 minutes (95th percentile) of observation. Detected events are also linked to generate candidate moving-object tracks using a novel algorithm. Objects that move fast enough to streak in the individual exposures are also extracted and vetted. We present some preliminary results of the calibration performance delivered by the real-time pipeline. The reconstructed astrometric accuracy per science image with respect to Gaia DR1 is typically 45 to 85 milliarcsec. This is the RMS per-axis on the sky for sources extracted with photometric S/N ≥ 10 and hence corresponds to the typical astrometric uncertainty down to this limit. The derived photometric precision (repeatability) at bright unsaturated fluxes varies between 8 and 25 millimag. The high end of these ranges corresponds to an airmass approaching ∼2—the limit of the public survey. Photometric calibration accuracy with respect to Pan-STARRS1 is generally better than 2%. The products support a broad range of scientific applications: fast and young supernovae; rare flux transients; variable stars; eclipsing binaries; variability from active galactic nuclei; counterparts to gravitational wave sources; a more complete census of Type Ia supernovae; and solar-system objects.",2018.0,"F. Masci, R. Laher, B. Rusholme, D. Shupe, S. Groom, J. Surace, E. Jackson, S. Monkewitz, R. Beck, D. Flynn, S. Terek, W. Landry, Eugean Hacopians, V. Desai, J. Howell, T. Brooke, D. Imel, S. Wachter, Q. Ye, H. Lin, S. B. Cenko, V. Cunningham, U. Rebbapragada, B. Bue, A. A. Miller, A. Mahabal, E. Bellm, M. Patterson, Mario Juri'c, V. Golkhou, E. Ofek, R. Walters, M. Graham, M. Kasliwal, R. Dekany, T. Kupfer, K. Burdge, C. Cannella, T. Barlow, A. V. Sistine, M. Giomi, C. Fremling, N. Blagorodnova, D. Levitan, R. Riddle, Roger Smith, G. Helou, T. Prince, S. Kulkarni"
b8f75b848b6cef0f2b5a1a11b794332ca9bccb45,https://www.semanticscholar.org/paper/b8f75b848b6cef0f2b5a1a11b794332ca9bccb45,A review of machine learning applications in wildfire science and management,"Artificial intelligence has been applied in wildfire science and management since the 1990s, with early applications including neural networks and expert systems. Since then, the field has rapidly progressed congruently with the wide adoption of machine learning (ML) methods in the environmental sciences. Here, we present a scoping review of ML applications in wildfire science and management. Our overall objective is to improve awareness of ML methods among wildfire researchers and managers, as well as illustrate the diverse and challenging range of problems in wildfire science available to ML data scientists. To that end, we first present an overview of popular ML approaches used in wildfire science to date and then review the use of ML in wildfire science as broadly categorized into six problem domains, including (i) fuels characterization, fire detection, and mapping; (ii) fire weather and climate change; (iii) fire occurrence, susceptibility, and risk; (iv) fire behavior prediction; (v) fire effects; and (vi) fire management. Furthermore, we discuss the advantages and limitations of various ML approaches relating to data size, computational requirements, generalizability, and interpretability, as well as identify opportunities for future advances in the science and management of wildfires within a data science context. In total, to the end of 2019, we identified 300 relevant publications in which the most frequently used ML methods across problem domains included random forests, MaxEnt, artificial neural networks, decision trees, support vector machines, and genetic algorithms. As such, there exists opportunities to apply more current ML methods — including deep learning and agent-based learning — in the wildfire sciences, especially in instances involving very large multivariate datasets. We must recognize, however, that despite the ability of ML models to learn on their own, expertise in wildfire science is necessary to ensure realistic modelling of fire processes across multiple scales, while the complexity of some ML methods such as deep learning requires a dedicated and sophisticated knowledge of their application. Finally, we stress that the wildfire research and management communities play an active role in providing relevant, high-quality, and freely available wildfire data for use by practitioners of ML methods.",2020.0,"P. Jain, Sean C. P. Coogan, Sriram Ganapathi Subramanian, Mark Crowley, Steve Taylor, M. Flannigan"
54d9fc3ed4937ee546ed45aee7bef16b4ae3775d,https://www.semanticscholar.org/paper/54d9fc3ed4937ee546ed45aee7bef16b4ae3775d,Statistics for citizen science: extracting signals of change from noisy ecological data,"Policy‐makers increasingly demand robust measures of biodiversity change over short time periods. Long‐term monitoring schemes provide high‐quality data, often on an annual basis, but are taxonomically and geographically restricted. By contrast, opportunistic biological records are relatively unstructured but vast in quantity. Recently, these data have been applied to increasingly elaborate science and policy questions, using a range of methods. At present, we lack a firm understanding of which methods, if any, are capable of delivering unbiased trend estimates on policy‐relevant time‐scales. We identified a set of candidate methods that employ data filtering criteria and/or correction factors to deal with variation in recorder activity. We designed a computer simulation to compare the statistical properties of these methods under a suite of realistic data collection scenarios. We measured the Type I error rates of each method–scenario combination, as well as the power to detect genuine trends. We found that simple methods produce biased trend estimates, and/or had low power. Most methods are robust to variation in sampling effort, but biases in spatial coverage, sampling effort per visit, and detectability, as well as turnover in community composition, all induced some methods to fail. No method was wholly unaffected by all forms of variation in recorder activity, although some performed well enough to be useful. We warn against the use of simple methods. Sophisticated methods that model the data collection process offer the greatest potential to estimate timely trends, notably Frescalo and occupancy–detection models. The potential of these methods and the value of opportunistic data would be further enhanced by assessing the validity of model assumptions and by capturing small amounts of information about sampling intensity at the point of data collection.",2014.0,"N. Isaac, A. J. Strien, Tom A. August, Marnix P. Zeeuw, David Roy"
c3665722a7cc81caca8c90ac3c5b0572f7bba055,https://www.semanticscholar.org/paper/c3665722a7cc81caca8c90ac3c5b0572f7bba055,Can citizen science enhance public understanding of science?,"Over the past 20 years, thousands of citizen science projects engaging millions of participants in collecting and/or processing data have sprung up around the world. Here we review documented outcomes from four categories of citizen science projects which are defined by the nature of the activities in which their participants engage – Data Collection, Data Processing, Curriculum-based, and Community Science. We find strong evidence that scientific outcomes of citizen science are well documented, particularly for Data Collection and Data Processing projects. We find limited but growing evidence that citizen science projects achieve participant gains in knowledge about science knowledge and process, increase public awareness of the diversity of scientific research, and provide deeper meaning to participants’ hobbies. We also find some evidence that citizen science can contribute positively to social well-being by influencing the questions that are being addressed and by giving people a voice in local environmental decision making. While not all citizen science projects are intended to achieve a greater degree of public understanding of science, social change, or improved science -society relationships, those projects that do require effort and resources in four main categories: (1) project design, (2) outcomes measurement, (3) engagement of new audiences, and (4) new directions for research.",2016.0,"R. Bonney, T. Phillips, H. Ballard, Jody W. Enck"
48fc9c42522184c652742255fdf31f7b9ed7ebae,https://www.semanticscholar.org/paper/48fc9c42522184c652742255fdf31f7b9ed7ebae,Brief introduction of medical database and data mining technology in big data era,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",2020.0,"Jin Yang, Yuan-jie Li, Qingqing Liu, Li Li, Aozi Feng, Tianyi Wang, Shuai Zheng, Anding Xu, Jun Lyu"
915cd8e2b39eb02723553913d592b2237d4d9960,https://www.semanticscholar.org/paper/915cd8e2b39eb02723553913d592b2237d4d9960,Data science: An action plan for expanding the technical areas of the field of statistics,"An action plan to expand the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.",2001.0,W. Cleveland
835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,https://www.semanticscholar.org/paper/835a5484292f32a3c02f507cbd8fb1f5d9f4aacf,The natural selection of bad science,"Poor research design and data analysis encourage false-positive findings. Such poor methods persist despite perennial calls for improvement, suggesting that they result from something more than just misunderstanding. The persistence of poor methods results partly from incentives that favour them, leading to the natural selection of bad science. This dynamic requires no conscious strategizing—no deliberate cheating nor loafing—by scientists, only that publication is a principal factor for career advancement. Some normative methods of analysis have almost certainly been selected to further publication instead of discovery. In order to improve the culture of science, a shift must be made away from correcting misunderstandings and towards rewarding understanding. We support this argument with empirical evidence and computational modelling. We first present a 60-year meta-analysis of statistical power in the behavioural sciences and show that power has not improved despite repeated demonstrations of the necessity of increasing power. To demonstrate the logical consequences of structural incentives, we then present a dynamic model of scientific communities in which competing laboratories investigate novel or previously published hypotheses using culturally transmitted research methods. As in the real world, successful labs produce more ‘progeny,’ such that their methods are more often copied and their students are more likely to start labs of their own. Selection for high output leads to poorer methods and increasingly high false discovery rates. We additionally show that replication slows but does not stop the process of methodological deterioration. Improving the quality of research requires change at the institutional level.",2016.0,"P. Smaldino, R. Mcelreath"
d62126bfe0e1b299c9383bb30ee099c77aee5222,https://www.semanticscholar.org/paper/d62126bfe0e1b299c9383bb30ee099c77aee5222,"Interpreting Qualitative Data: Methods for Analysing Talk, Text and Interaction","This a much expanded and updated version of David Silvermans best-selling introductory textbook for the beginning qualitative researcher. 
 
Features of the New Edition: 
• Takes account of the flood of qualitative work since the 1990s 
• All chapters have been substantially rewritten with the aim of greater clarity 
• A new chapter on Visual Images and a considerably expanded treatment of discourse analysis are provided 
• The number of student exercises has been considerably increased and are now present at the end of every chapter 
• An even greater degree of student accessibility: Key Points and Recommended Readings appear at the end of each chapter and technical terms are highlighted and appear in a Glossary 
• A more inter-disciplinary social science text which takes account of the growing interest in qualitative research outside sociology and anthropology from psychology to geography, information systems, health promotion, management and many other disciplines 
• Expanded coverage 50% longer than the First Edition 
This book has a more recent edition (2006)",1994.0,D. Silverman
6cccadad7e25f9c431c0e50d30c8eb6eddecc835,https://www.semanticscholar.org/paper/6cccadad7e25f9c431c0e50d30c8eb6eddecc835,Data Cleaners for Pristine Datasets: Visibility and Invisibility of Data Processors in Social Science,"This article investigates the work of processors who curate and “clean” the data sets that researchers submit to data archives for archiving and further dissemination. Based on ethnographic fieldwork conducted at the data processing unit of a major US social science data archive, I investigate how these data processors work, under which status, and how they contribute to data sharing. This article presents two main results. First, it contributes to the study of invisible technicians in science by showing that the same procedures can make technical work invisible outside and visible inside the archive, to allow peer review and quality control. Second, this article contributes to the social study of scientific data sharing, by showing that the organization of data processing directly stems from the conception that the archive promotes of a valid data set—that is, a data set that must look “pristine” at the end of its processing. After critically interrogating this notion of pristineness, I show how it perpetuates a misleading conception of data as “raw” instead of acknowledging the important contribution of data processors to data sharing and social science.",2018.0,J. Plantin
116927fbe4c9732fd1e392035a100c33b14e9d59,https://www.semanticscholar.org/paper/116927fbe4c9732fd1e392035a100c33b14e9d59,Big Data and cloud computing: innovation opportunities and challenges,"ABSTRACT Big Data has emerged in the past few years as a new paradigm providing abundant data and opportunities to improve and/or enable research and decision-support applications with unprecedented value for digital earth applications including business, sciences and engineering. At the same time, Big Data presents challenges for digital earth to store, transport, process, mine and serve the data. Cloud computing provides fundamental support to address the challenges with shared computing resources including computing, storage, networking and analytical software; the application of these resources has fostered impressive Big Data advancements. This paper surveys the two frontiers – Big Data and cloud computing – and reviews the advantages and consequences of utilizing cloud computing to tackling Big Data in the digital earth and relevant science domains. From the aspects of a general introduction, sources, challenges, technology status and research opportunities, the following observations are offered: (i) cloud computing and Big Data enable science discoveries and application developments; (ii) cloud computing provides major solutions for Big Data; (iii) Big Data, spatiotemporal thinking and various application domains drive the advancement of cloud computing and relevant technologies with new requirements; (iv) intrinsic spatiotemporal principles of Big Data and geospatial sciences provide the source for finding technical and theoretical solutions to optimize cloud computing and processing Big Data; (v) open availability of Big Data and processing capability pose social challenges of geospatial significance and (vi) a weave of innovations is transforming Big Data into geospatial research, engineering and business values. This review introduces future innovations and a research agenda for cloud computing supporting the transformation of the volume, velocity, variety and veracity into values of Big Data for local to global digital earth science and applications.",2017.0,"C. Yang, Qunying Huang, Zhenlong Li, Kai Liu, F. Hu"
277083fdacf033996e3249013da815655f2fbf5b,https://www.semanticscholar.org/paper/277083fdacf033996e3249013da815655f2fbf5b,Data-Driven Strategies for Accelerated Materials Design,"Conspectus The ongoing revolution of the natural sciences by the advent of machine learning and artificial intelligence sparked significant interest in the material science community in recent years. The intrinsically high dimensionality of the space of realizable materials makes traditional approaches ineffective for large-scale explorations. Modern data science and machine learning tools developed for increasingly complicated problems are an attractive alternative. An imminent climate catastrophe calls for a clean energy transformation by overhauling current technologies within only several years of possible action available. Tackling this crisis requires the development of new materials at an unprecedented pace and scale. For example, organic photovoltaics have the potential to replace existing silicon-based materials to a large extent and open up new fields of application. In recent years, organic light-emitting diodes have emerged as state-of-the-art technology for digital screens and portable devices and are enabling new applications with flexible displays. Reticular frameworks allow the atom-precise synthesis of nanomaterials and promise to revolutionize the field by the potential to realize multifunctional nanoparticles with applications from gas storage, gas separation, and electrochemical energy storage to nanomedicine. In the recent decade, significant advances in all these fields have been facilitated by the comprehensive application of simulation and machine learning for property prediction, property optimization, and chemical space exploration enabled by considerable advances in computing power and algorithmic efficiency. In this Account, we review the most recent contributions of our group in this thriving field of machine learning for material science. We start with a summary of the most important material classes our group has been involved in, focusing on small molecules as organic electronic materials and crystalline materials. Specifically, we highlight the data-driven approaches we employed to speed up discovery and derive material design strategies. Subsequently, our focus lies on the data-driven methodologies our group has developed and employed, elaborating on high-throughput virtual screening, inverse molecular design, Bayesian optimization, and supervised learning. We discuss the general ideas, their working principles, and their use cases with examples of successful implementations in data-driven material discovery and design efforts. Furthermore, we elaborate on potential pitfalls and remaining challenges of these methods. Finally, we provide a brief outlook for the field as we foresee increasing adaptation and implementation of large scale data-driven approaches in material discovery and design campaigns.",2021.0,"R. Pollice, Gabriel dos Passos Gomes, Matteo Aldeghi, Riley J. Hickman, Mario Krenn, C. Lavigne, M. Lindner-D’Addario, AkshatKumar Nigam, C. Ser, Zhenpeng Yao, Alán Aspuru-Guzik"
7ac8f533a18f584387dd412a0a27feb9af1c5c93,https://www.semanticscholar.org/paper/7ac8f533a18f584387dd412a0a27feb9af1c5c93,A Systematic Review on Imbalanced Data Challenges in Machine Learning,"In machine learning, the data imbalance imposes challenges to perform data analytics in almost all areas of real-world research. The raw primary data often suffers from the skewed perspective of data distribution of one class over the other as in the case of computer vision, information security, marketing, and medical science. The goal of this article is to present a comparative analysis of the approaches from the reference of data pre-processing, algorithmic and hybrid paradigms for contemporary imbalance data analysis techniques, and their comparative study in lieu of different data distribution and their application areas.",2019.0,"H. kaur, H. Pannu, A. Malhi"
2aca01aa1d0a6083986dcd4614b1d0733028dcc2,https://www.semanticscholar.org/paper/2aca01aa1d0a6083986dcd4614b1d0733028dcc2,"Multimodal Data Fusion: An Overview of Methods, Challenges, and Prospects","In various disciplines, information about the same phenomenon can be acquired from different types of detectors, at different conditions, in multiple experiments or subjects, among others. We use the term “modality” for each such acquisition framework. Due to the rich characteristics of natural phenomena, it is rare that a single modality provides complete knowledge of the phenomenon of interest. The increasing availability of several modalities reporting on the same system introduces new degrees of freedom, which raise questions beyond those related to exploiting each modality separately. As we argue, many of these questions, or “challenges,” are common to multiple domains. This paper deals with two key issues: “why we need data fusion” and “how we perform it.” The first issue is motivated by numerous examples in science and technology, followed by a mathematical framework that showcases some of the benefits that data fusion provides. In order to address the second issue, “diversity” is introduced as a key concept, and a number of data-driven solutions based on matrix and tensor decompositions are discussed, emphasizing how they account for diversity across the data sets. The aim of this paper is to provide the reader, regardless of his or her community of origin, with a taste of the vastness of the field, the prospects, and the opportunities that it holds.",2015.0,"D. Lahat, T. Adalı, C. Jutten"
da619a6c524f5ab800b44c8728db3cef3d3b25d9,https://www.semanticscholar.org/paper/da619a6c524f5ab800b44c8728db3cef3d3b25d9,"Big Data, new epistemologies and paradigm shifts","This article examines how the availability of Big Data, coupled with new data analytics, challenges established epistemologies across the sciences, social sciences and humanities, and assesses the extent to which they are engendering paradigm shifts across multiple disciplines. In particular, it critically explores new forms of empiricism that declare ‘the end of theory’, the creation of data-driven rather than knowledge-driven science, and the development of digital humanities and computational social sciences that propose radically different ways to make sense of culture, history, economy and society. It is argued that: (1) Big Data and new data analytics are disruptive innovations which are reconfiguring in many instances how research is conducted; and (2) there is an urgent need for wider critical reflection within the academy on the epistemological implications of the unfolding data revolution, a task that has barely begun to be tackled despite the rapid changes in research practices presently taking place. After critically reviewing emerging epistemological positions, it is contended that a potentially fruitful approach would be the development of a situated, reflexive and contextually nuanced epistemology.",2014.0,Rob Kitchin
0bc97adfb3c77f27397d19395af2fdff9f04aaa0,https://www.semanticscholar.org/paper/0bc97adfb3c77f27397d19395af2fdff9f04aaa0,The TESS science processing operations center,"The Transiting Exoplanet Survey Satellite (TESS) will conduct a search for Earth's closest cousins starting in early 2018 and is expected to discover ∼1,000 small planets with Rp < 4 R⊕ and measure the masses of at least 50 of these small worlds. The Science Processing Operations Center (SPOC) is being developed at NASA Ames Research Center based on the Kepler science pipeline and will generate calibrated pixels and light curves on the NASA Advanced Supercomputing Division's Pleiades supercomputer. The SPOC will also search for periodic transit events and generate validation products for the transit-like features in the light curves. All TESS SPOC data products will be archived to the Mikulski Archive for Space Telescopes (MAST).",2016.0,"J. Jenkins, J. Twicken, S. McCauliff, Jennifer Campbell, D. Sanderfer, D. Lung, M. Mansouri-Samani, F. Girouard, P. Tenenbaum, T. Klaus, Jeffrey C. Smith, D. Caldwell, A. D. Chacon, C. Henze, Cory Heiges, D. Latham, E. Morgan, D. Swade, S. Rinehart, R. Vanderspek"
ff1068a7e2acaa41fae2a8e1b180264434f06ce8,https://www.semanticscholar.org/paper/ff1068a7e2acaa41fae2a8e1b180264434f06ce8,Liberating field science samples and data,"Promote reproducibility by moving beyond “available upon request” Transparency and reproducibility enhance the integrity of research results for scientific and public uses and empower novel research applications. Access to data, samples, methods, and reagents used to conduct research and analysis, as well as to the code used to analyze and process data and samples, is a fundamental requirement for transparency and reproducibility. The field sciences (e.g., geology, ecology, and archaeology), where each study is temporally (and often spatially) unique, provide exemplars for the importance of preserving data and samples for further analysis. Yet field sciences, if they even address such access, commonly do so by simply noting “data and samples available upon request.” They lag behind some laboratory sciences in making data and samples available to the broader research community. It is time for this to change. We discuss cultural, financial, and technical barriers to change and ways in which funders, publishers, scientific societies, and others are responding.",2016.0,"M. McNutt, K. Lehnert, B. Hanson, Brian A. Nosek, A. Ellison, J. King"
c1e49d830e67269d4d2053a5f124ea773c79b740,https://www.semanticscholar.org/paper/c1e49d830e67269d4d2053a5f124ea773c79b740,Computational social science: Obstacles and opportunities,"Data sharing, research ethics, and incentives must improve The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.",2020.0,"D. Lazer, A. Pentland, D. Watts, Sinan Aral, S. Athey, N. Contractor, Deen Freelon, Sandra González-Bailón, Gary King, H. Margetts, Alondra Nelson, Matthew J. Salganik, M. Strohmaier, A. Vespignani, Claudia Wagner"
86b05bc7e953e683fa839ad01d6100a8f99558df,https://www.semanticscholar.org/paper/86b05bc7e953e683fa839ad01d6100a8f99558df,Concrete mathematics - a foundation for computer science,"From the Publisher: 
This book introduces the mathematics that supports advanced computer programming and the analysis of algorithms. The primary aim of its well-known authors is to provide a solid and relevant base of mathematical skills - the skills needed to solve complex problems, to evaluate horrendous sums, and to discover subtle patterns in data. It is an indispensable text and reference not only for computer scientists - the authors themselves rely heavily on it! - but for serious users of mathematics in virtually every discipline. 
 
Concrete Mathematics is a blending of CONtinuous and disCRETE mathematics. ""More concretely,"" the authors explain, ""it is the controlled manipulation of mathematical formulas, using a collection of techniques for solving problems."" The subject matter is primarily an expansion of the Mathematical Preliminaries section in Knuth's classic Art of Computer Programming, but the style of presentation is more leisurely, and individual topics are covered more deeply. Several new topics have been added, and the most significant ideas have been traced to their historical roots. The book includes more than 500 exercises, divided into six categories. Complete answers are provided for all exercises, except research problems, making the book particularly valuable for self-study. 
 
Major topics include: 
 
Sums 
Recurrences 
Integer functions 
Elementary number theory 
Binomial coefficients 
Generating functions 
Discrete probability 
Asymptotic methods 
 
 
This second edition includes important new material about mechanical summation. In response to the widespread use ofthe first edition as a reference book, the bibliography and index have also been expanded, and additional nontrivial improvements can be found on almost every page. Readers will appreciate the informal style of Concrete Mathematics. Particularly enjoyable are the marginal graffiti contributed by students who have taken courses based on this material. The authors want to convey not only the importance of the techniques presented, but some of the fun in learning and using them.",1991.0,"R. Graham, D. Knuth, Oren Patashnik"
f4156a05a47fdeda30638e10954d3674cc056ab6,https://www.semanticscholar.org/paper/f4156a05a47fdeda30638e10954d3674cc056ab6,Discovering Knowledge in Data: An Introduction to Data Mining,"This book is the first volume of a three-volume series on data mining, which introduces the reader to this rapidly growing field. Data mining, which has gained noticeable popularity in the past decade, is essentially an interdisciplinary field bringing together techniques from machine learning, pattern recognition, statistics, databases, and visualization (Cabena et al., 1998) to address the issue of exploring large and complicated databases to identify “interesting” relationships, e.g., high order interactions, or very non-linear relationships that ordinarily would not be detected by standard statistical analyses (Borok, 1997; Szolvits, 1995). This area has been approached by computer scientists and statisticians from slightly different perspectives. The author of the book is a statistician, but has tried to include a computer science theme throughout the book, in which I think he has been successful. As he mentions in the preface, the book is intended to be used either by analysts, managers, and decision makers in industry or as a textbook for an introductory course in data mining for graduate or advanced undergraduate students (in computer science or statistics). Chapter 1 is a short introductory chapter, in which in addition to a brief description of the Cross-Industry Standard Process for Data Mining (CRISP-DM), several real-world case studies are covered to motivate the topics of subsequent chapters. These case studies are also used to describe the first phase of the CRISPDM process, namely business understanding. Chapters 2 and 3 examine the next two phases of the CRISP-DM process, i.e., data understanding and data preparation. Chapter 2 is on data preprocessing, which is divided into the two major tasks of data cleaning and data transformation. In data cleaning, general methods for handling missing data, identifying misclassified records in the data, and also a graphical method for detecting outliers are described. In the data transformation section, min-max normalization and z-score standardization methods are discussed. A numerical method for detecting outliers based on z-score standardization is also covered. Exploratory data analysis is the topic of Chapter 3, which focuses on data understanding. The chapter begins with making a contrast between hypothesis testing and exploratory data analysis, and is followed by the basic ideas of dealing with correlated variables in the data set. Most of this chapter is dedicated to exploring the variables in a real data set, in which by using several diagrams a number of intuitive approaches for obtaining a high level understanding of the data are proposed.",2005.0,N. Chawla
ac8db14cbc7ad0119d0130e88f98ccb3ec61780f,https://www.semanticscholar.org/paper/ac8db14cbc7ad0119d0130e88f98ccb3ec61780f,"Big Data, Digital Media, and Computational Social Science",forecasts and misrepresent,2015.0,"Dhavan V. Shah, J. Cappella, W. R. Neuman"
09ee0ba924ffd21fc7e14ad3147284133cf2f576,https://www.semanticscholar.org/paper/09ee0ba924ffd21fc7e14ad3147284133cf2f576,"Color Science, Concepts and Methods. Quantitative Data and Formulas","G. Wyszecki and W. S. Stiles London: John Wiley. 1967. Pp. xiv + 628. Price £11. This remarkable and unusual book is by two outstanding authorities on the science of colour: Dr. Stiles, for many years a senior member of the Light Division at the National Physical Laboratory, and Dr. Wyszecki, currently in charge of the Radiation Optics Section of the Canadian National Research Council. The authors' aim has been to provide a comprehensive source book of data required by the practical and theoretical worker in the field of colour and they have achieved this aim so successfully that their book is likely to become the standard work on the subject and to remain so for a good many years.",1967.0,W. D. Wright
8e600778160ff986b5460bc2584066148e55e5d4,https://www.semanticscholar.org/paper/8e600778160ff986b5460bc2584066148e55e5d4,Protein structure determination using metagenome sequence data,"Filling in the protein fold picture Fewer than a third of the 14,849 known protein families have at least one member with an experimentally determined structure. This leaves more than 5000 protein families with no structural information. Protein modeling using residue-residue contacts inferred from evolutionary data has been successful in modeling unknown structures, but it requires large numbers of aligned sequences. Ovchinnikov et al. augmented such sequence alignments with metagenome sequence data (see the Perspective by Söding). They determined the number of sequences required to allow modeling, developed criteria for model quality, and, where possible, improved modeling by matching predicted contacts to known structures. Their method predicted quality structural models for 614 protein families, of which about 140 represent newly discovered protein folds. Science, this issue p. 294; see also p. 248 Combining metagenome data with protein structure prediction generates models for 614 families with unknown structures. Despite decades of work by structural biologists, there are still ~5200 protein families with unknown structure outside the range of comparative modeling. We show that Rosetta structure prediction guided by residue-residue contacts inferred from evolutionary information can accurately model proteins that belong to large families and that metagenome sequence data more than triple the number of protein families with sufficient sequences for accurate modeling. We then integrate metagenome data, contact-based structure matching, and Rosetta structure calculations to generate models for 614 protein families with currently unknown structures; 206 are membrane proteins and 137 have folds not represented in the Protein Data Bank. This approach provides the representative models for large protein families originally envisioned as the goal of the Protein Structure Initiative at a fraction of the cost.",2017.0,"S. Ovchinnikov, Hahnbeom Park, N. Varghese, Po-Ssu Huang, Georgios A. Pavlopoulos, David E. Kim, Hetunandan Kamisetty, N. Kyrpides, David Baker"
938a6209fe95dd4e5f801a14b6b650dc7b2f6108,https://www.semanticscholar.org/paper/938a6209fe95dd4e5f801a14b6b650dc7b2f6108,Could Big Data be the end of theory in science?,"Afew years ago, Chris Anderson, former editor in chief of Wired magazine, published a provocative and thought‐provoking article: “The end of theory: the data deluge makes the scientific method obsolete” (http://archive.wired.com/science/discoveries/magazine/16-07/pb_theory/). As the title indicates, Anderson asserted that in the era of petabyte information and supercomputing, the traditional, hypothesis‐driven scientific method would become obsolete. No more theories or hypotheses, no more discussions whether the experimental results refute or support the original hypotheses. In this new era, what counts are sophisticated algorithms and statistical tools to sift through a massive amount of data to find information that could be turned into knowledge.

> … [an] imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button…

Anderson's essay started an intense discussion about the relative merits of data‐driven research versus hypothesis‐driven research that has much relevance for many areas of research, including bioinformatics, systems biology, epidemiology and ecology. Yet, his imagined future in which the long‐established way of doing scientific research is replaced by computers that divulge knowledge from data at the press of a button deserves some inquiry from an epistemological point of view. Is data‐driven research a genuine mode of knowledge production, or is it above all a tool to identify potentially useful information? Given the amount of scientific data available, is it now possible to dismiss the role of theoretical assumptions and hypotheses? Should this new mode of gathering information supersede the old way of doing research?

The scientific method encompasses an ongoing process of formulate a hypothesis‐test with an experiment–analyze the results‐reformulate the hypothesis. Such a way of proceeding has been in use for centuries and is basically accepted in our Western society as the most reliable way to produce robust knowledge.

However, Anderson is not the …",2015.0,F. Mazzocchi
ecef432e7f6c9f431d5b34706a8de1fdebec46f9,https://www.semanticscholar.org/paper/ecef432e7f6c9f431d5b34706a8de1fdebec46f9,From Big Data to Precision Medicine,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",2019.0,"Tim Hulsen, S. Jamuar, A. Moody, J. Karnes, O. Varga, Stine Hedensted, R. Spreafico, D. Hafler, E. McKinney"
28aecd08b2488c5300abf399feeb83a1f9c19890,https://www.semanticscholar.org/paper/28aecd08b2488c5300abf399feeb83a1f9c19890,Open Government Data,"Story Slides Slide 1 W3C eGovernment Community: Data Science Slide 2 Agenda Slide 3 The Changing Landscape of Federal Information Technology Slide 4 Cloud: SOA, Semantic, & Data Science: September 10-11th Slide 5 Opportunities for Data Science Slide 6 Discussion 1 Slide 7 Discussion 2 Slide 8 Discussion 3 Slide 9 Discussion 4 Slide 10 Discussion 5 Spotfire Dashboard Research Notes Joshua Tauberer’s Blog Open Government Data 1 Big Data Meets Open Government Figure 1 The New Federal Register 2.0 Figure 2 This animated visualization of live wind speeds and directions Figure 3 Data is like refrigerator poetry Figure 4 http://www.GovTrack.us Figure 5 John Oliver parodies Schoolhouse Rock’s “I’m Just a Bill” References 1 2 3 4 5 6 7",2019.0,D. Agotai
46d71d947231f86e1f9d4581e61212385debbe14,https://www.semanticscholar.org/paper/46d71d947231f86e1f9d4581e61212385debbe14,OpenML: networked science in machine learning,"Many sciences have made significant breakthroughs by adopting online tools that help organize, structure and mine information that is too detailed to be printed in journals. In this paper, we introduce OpenML, a place for machine learning researchers to share and organize data in fine detail, so that they can work more effectively, be more visible, and collaborate with others to tackle harder problems. We discuss how OpenML relates to other examples of networked science and what benefits it brings for machine learning research, individual scientists, as well as students and practitioners.",2014.0,"J. Vanschoren, J. N. Rijn, B. Bischl, Luís Torgo"
25e0d93ca47d86510d6a0f9cda9ae3594f3d05b2,https://www.semanticscholar.org/paper/25e0d93ca47d86510d6a0f9cda9ae3594f3d05b2,"Color Science: Concepts and Methods, Quantitative Data and Formulas","Eventually, you will agreed discover a further experience and achievement by spending more cash. still when? attain you acknowledge that you require to acquire those every needs in imitation of having significantly cash? Why don't you try to acquire something basic in the beginning? That's something that will lead you to comprehend even more in the region of the globe, experience, some places, later than history, amusement, and a lot more?",1968.0,"J. Cohen, G. Wyszecki, W. Stiles"
8807a8327e27298fd601fc65e6a9ccfae1cca195,https://www.semanticscholar.org/paper/8807a8327e27298fd601fc65e6a9ccfae1cca195,What Is Citizen Science? – A Scientometric Meta-Analysis,"Context The concept of citizen science (CS) is currently referred to by many actors inside and outside science and research. Several descriptions of this purportedly new approach of science are often heard in connection with large datasets and the possibilities of mobilizing crowds outside science to assists with observations and classifications. However, other accounts refer to CS as a way of democratizing science, aiding concerned communities in creating data to influence policy and as a way of promoting political decision processes involving environment and health. Objective In this study we analyse two datasets (N = 1935, N = 633) retrieved from the Web of Science (WoS) with the aim of giving a scientometric description of what the concept of CS entails. We account for its development over time, and what strands of research that has adopted CS and give an assessment of what scientific output has been achieved in CS-related projects. To attain this, scientometric methods have been combined with qualitative approaches to render more precise search terms. Results Results indicate that there are three main focal points of CS. The largest is composed of research on biology, conservation and ecology, and utilizes CS mainly as a methodology of collecting and classifying data. A second strand of research has emerged through geographic information research, where citizens participate in the collection of geographic data. Thirdly, there is a line of research relating to the social sciences and epidemiology, which studies and facilitates public participation in relation to environmental issues and health. In terms of scientific output, the largest body of articles are to be found in biology and conservation research. In absolute numbers, the amount of publications generated by CS is low (N = 1935), but over the past decade a new and very productive line of CS based on digital platforms has emerged for the collection and classification of data.",2016.0,"Christopher Kullenberg, Dick Kasperowski"
a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,https://www.semanticscholar.org/paper/a4b603ca6aaaa18968e08ac1b0ee093db8a99a6b,Topology and data,"An important feature of modern science and engineering is that data of various kinds is being produced at an unprecedented rate. This is so in part because of new experimental methods, and in part because of the increase in the availability of high powered computing technology. It is also clear that the nature of the data we are obtaining is significantly different. For example, it is now often the case that we are given data in the form of very long vectors, where all but a few of the coordinates turn out to be irrelevant to the questions of interest, and further that we don’t necessarily know which coordinates are the interesting ones. A related fact is that the data is often very high-dimensional, which severely restricts our ability to visualize it. The data obtained is also often much noisier than in the past and has more missing information (missing data). This is particularly so in the case of biological data, particularly high throughput data from microarray or other sources. Our ability to analyze this data, both in terms of quantity and the nature of the data, is clearly not keeping pace with the data being produced. In this paper, we will discuss how geometry and topology can be applied to make useful contributions to the analysis of various kinds of data. Geometry and topology are very natural tools to apply in this direction, since geometry can be regarded as the study of distance functions, and what one often works with are distance functions on large finite sets of data. The mathematical formalism which has been developed for incorporating geometric and topological techniques deals with point clouds, i.e. finite sets of points equipped with a distance function. It then adapts tools from the various branches of geometry to the study of point clouds. The point clouds are intended to be thought of as finite samples taken from a geometric object, perhaps with noise. Here are some of the key points which come up when applying these geometric methods to data analysis. • Qualitative information is needed: One important goal of data analysis is to allow the user to obtain knowledge about the data, i.e. to understand how it is organized on a large scale. For example, if we imagine that we are looking at a data set constructed somehow from diabetes patients, it would be important to develop the understanding that there are two types of the disease, namely the juvenile and adult onset forms. Once that is established, one of course wants to develop quantitative methods for distinguishing them, but the first insight about the distinct forms of the disease is key.",2009.0,G. Carlsson
b41fd82432999628e34d07e64ccda783273c15c0,https://www.semanticscholar.org/paper/b41fd82432999628e34d07e64ccda783273c15c0,Data integration enables global biodiversity synthesis,"Significance As anthropogenic impacts to Earth systems accelerate, biodiversity knowledge integration is urgently required to support responses to underpin a sustainable future. Consolidating information from disparate sources (e.g., community science programs, museums) and data types (e.g., environmental, biological) can connect the biological sciences across taxonomic, disciplinary, geographical, and socioeconomic boundaries. In an analysis of the research uses of the world’s largest cross-taxon biodiversity data network, we report the emerging roles of open-access data aggregation in the development of increasingly diverse, global research. These results indicate a new biodiversity science landscape centered on big data integration, informing ongoing initiatives and the strategic prioritization of biodiversity data aggregation across diverse knowledge domains, including environmental sciences and policy, evolutionary biology, conservation, and human health. The accessibility of global biodiversity information has surged in the past two decades, notably through widespread funding initiatives for museum specimen digitization and emergence of large-scale public participation in community science. Effective use of these data requires the integration of disconnected datasets, but the scientific impacts of consolidated biodiversity data networks have not yet been quantified. To determine whether data integration enables novel research, we carried out a quantitative text analysis and bibliographic synthesis of >4,000 studies published from 2003 to 2019 that use data mediated by the world’s largest biodiversity data network, the Global Biodiversity Information Facility (GBIF). Data available through GBIF increased 12-fold since 2007, a trend matched by global data use with roughly two publications using GBIF-mediated data per day in 2019. Data-use patterns were diverse by authorship, geographic extent, taxonomic group, and dataset type. Despite facilitating global authorship, legacies of colonial science remain. Studies involving species distribution modeling were most prevalent (31% of literature surveyed) but recently shifted in focus from theory to application. Topic prevalence was stable across the 17-y period for some research areas (e.g., macroecology), yet other topics proportionately declined (e.g., taxonomy) or increased (e.g., species interactions, disease). Although centered on biological subfields, GBIF-enabled research extends surprisingly across all major scientific disciplines. Biodiversity data mobilization through global data aggregation has enabled basic and applied research use at temporal, spatial, and taxonomic scales otherwise not possible, launching biodiversity sciences into a new era.",2021.0,"J. M. Heberling, Joseph T. Miller, Daniel Noesgaard, Scott B. Weingart, D. Schigel"
db8335198bd47c8865d0b3408b97e547abfd9ba2,https://www.semanticscholar.org/paper/db8335198bd47c8865d0b3408b97e547abfd9ba2,The Fourth Paradigm: Data-Intensive Scientific Discovery,"This presentation will set out the eScience agenda by explaining the current scientific data deluge and the case for a “Fourth Paradigm” for scientific exploration. Examples of data intensive science will be used to illustrate the explosion of data and the associated new challenges for data capture, curation, analysis, and sharing. The role of cloud computing, collaboration services, and research repositories will be discussed.",2009.0,Tony (Anthony) John Grenville Hey
64ad643e8084486ca7d3312ed491a814d3fe440c,https://www.semanticscholar.org/paper/64ad643e8084486ca7d3312ed491a814d3fe440c,The Synthetic Data Vault,"The goal of this paper is to build a system that automatically creates synthetic data to enable data science endeavors. To achieve this, we present the Synthetic Data Vault (SDV), a system that builds generative models of relational databases. We are able to sample from the model and create synthetic data, hence the name SDV. When implementing the SDV, we also developed an algorithm that computes statistics at the intersection of related database tables. We then used a state-of-the-art multivariate modeling approach to model this data. The SDV iterates through all possible relations, ultimately creating a model for the entire database. Once this model is computed, the same relational information allows the SDV to synthesize data by sampling from any part of the database. After building the SDV, we used it to generate synthetic data for five different publicly available datasets. We then published these datasets, and asked data scientists to develop predictive models for them as part of a crowdsourced experiment. By analyzing the outcomes, we show that synthetic data can successfully replace original data for data science. Our analysis indicates that there is no significant difference in the work produced by data scientists who used synthetic data as opposed to real data. We conclude that the SDV is a viable solution for synthetic data generation.",2016.0,"Neha Patki, Roy Wedge, K. Veeramachaneni"
f2b66923db74a16169d040a51ada555d5b6f8851,https://www.semanticscholar.org/paper/f2b66923db74a16169d040a51ada555d5b6f8851,Data Mining and Analysis: Fundamental Concepts and Algorithms,"The fundamental algorithms in data mining and analysis form the basis for the emerging field of data science, which includes automated methods to analyze patterns and models for all kinds of data, with applications ranging from scientific discovery to business intelligence and analytics. This textbook for senior undergraduate and graduate data mining courses provides a broad yet in-depth overview of data mining, integrating related concepts from machine learning and statistics. The main parts of the book include exploratory data analysis, pattern mining, clustering, and classification. The book lays the basic foundations of these tasks, and also covers cutting-edge topics such as kernel methods, high-dimensional data analysis, and complex graphs and networks. With its comprehensive coverage, algorithmic perspective, and wealth of examples, this book offers solid guidance in data mining for students, researchers, and practitioners alike. Key features: Covers both core methods and cutting-edge research Algorithmic approach with open-source implementations Minimal prerequisites: all key mathematical concepts are presented, as is the intuition behind the formulas Short, self-contained chapters with class-tested examples and exercises allow for flexibility in designing a course and for easy reference Supplementary website with lecture slides, videos, project ideas, and more",2014.0,Mohammed J. Zaki
0578dfb2a28b77abde19b32de777e0365df3020e,https://www.semanticscholar.org/paper/0578dfb2a28b77abde19b32de777e0365df3020e,Data-driven materials research enabled by natural language processing and information extraction,"Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",2020.0,"E. Olivetti, J. Cole, Edward Kim, O. Kononova, G. Ceder, T. Y. Han, A. Hiszpanski"
6a46d93284c71b5e3374a40311cf53214946b257,https://www.semanticscholar.org/paper/6a46d93284c71b5e3374a40311cf53214946b257,Topic Modeling in Management Research: Rendering New Theory from Textual Data,"Increasingly, management researchers are using topic modeling, a new method borrowed from computer science, to reveal phenomenon-based constructs and grounded conceptual relationships in textual da...",2019.0,"Timothy R. Hannigan, Richard F.J. Haans, Keyvan Vakili, Hovig Tchalian, Vern L. Glaser, M. Wang, Sarah Kaplan, P. Jennings"
fe5bb5d8d6b7ac251d87bc16e75ea5889cc92425,https://www.semanticscholar.org/paper/fe5bb5d8d6b7ac251d87bc16e75ea5889cc92425,Explaining Fixed Effects: Random Effects Modeling of Time-Series Cross-Sectional and Panel Data*,"This article challenges Fixed Effects (FE) modeling as the ‘default’ for time-series-cross-sectional and panel data. Understanding different within and between effects is crucial when choosing modeling strategies. The downside of Random Effects (RE) modeling—correlated lower-level covariates and higher-level residuals—is omitted-variable bias, solvable with Mundlak's (1978a) formulation. Consequently, RE can provide everything that FE promises and more, as confirmed by Monte-Carlo simulations, which additionally show problems with Plümper and Troeger's FE Vector Decomposition method when data are unbalanced. As well as incorporating time-invariant variables, RE models are readily extendable, with random coefficients, cross-level interactions and complex variance functions. We argue not simply for technical solutions to endogeneity, but for the substantive importance of context/heterogeneity, modeled using RE. The implications extend beyond political science to all multilevel datasets. However, omitted variables could still bias estimated higher-level variable effects; as with any model, care is required in interpretation.",2014.0,"Andrew Bell, K. Jones"
fbd9ddc0a3862512ce7a0ba2bb9cb159da0a9d2f,https://www.semanticscholar.org/paper/fbd9ddc0a3862512ce7a0ba2bb9cb159da0a9d2f,Editorial - Marketing Science and Big Data,"This article was downloaded by: [128.97.27.20] On: 25 May 2016, At: 09:44 Publisher: Institute for Operations Research and the Management Sciences (INFORMS) INFORMS is located in Maryland, USA Marketing Science Publication details, including instructions for authors and subscription information: http://pubsonline.informs.org Editorial—Marketing Science and Big Data Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser To cite this article: Pradeep Chintagunta, Dominique M. Hanssens, John R. Hauser (2016) Editorial—Marketing Science and Big Data. Marketing Science 35(3):341-342. http://dx.doi.org/10.1287/mksc.2016.0996 Full terms and conditions of use: http://pubsonline.informs.org/page/terms-and-conditions This article may be used only for the purposes of research, teaching, and/or private study. Commercial use or systematic downloading (by robots or other automatic processes) is prohibited without explicit Publisher approval, unless otherwise noted. For more information, contact permissions@informs.org. The Publisher does not warrant or guarantee the article’s accuracy, completeness, merchantability, fitness for a particular purpose, or non-infringement. Descriptions of, or references to, products or publications, or inclusion of an advertisement in this article, neither constitutes nor implies a guarantee, endorsement, or support of claims made of that product, publication, or service. Copyright © 2016, INFORMS Please scroll down for article—it is on subsequent pages INFORMS is the largest professional society in the world for professionals in the fields of operations research, management science, and analytics. For more information on INFORMS, its publications, membership, or meetings visit http://www.informs.org",2016.0,"Pradeep Chintagunta, D. Hanssens, J. Hauser"
db019eec15d8080086bbc7dc8f5832e431202e0e,https://www.semanticscholar.org/paper/db019eec15d8080086bbc7dc8f5832e431202e0e,Jupyter: Thinking and Storytelling With Code and Data,"Project Jupyter is an open-source project for interactive computing widely used in data science, machine learning, and scientific computing. We argue that even though Jupyter helps users perform complex, technical work, Jupyter itself solves problems that are fundamentally human in nature. Namely, Jupyter helps humans to think and tell stories with code and data. We illustrate this by describing three dimensions of Jupyter: 1) interactive computing; 2) computational narratives; and 3) the idea that Jupyter is more than software. We illustrate the impact of these dimensions on a community of practice in earth and climate science.",2021.0,"B. Granger, Fernando Pérez"
2e888654c68524163fbf7a54396488249e73a702,https://www.semanticscholar.org/paper/2e888654c68524163fbf7a54396488249e73a702,Citizen Science: A Developing Tool for Expanding Science Knowledge and Scientific Literacy,"Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.",2009.0,"R. Bonney, C. Cooper, J. Dickinson, S. Kelling, T. Phillips, K. Rosenberg, Jennifer Shirk"
917943472ec4a00443d78bb696ed4d8f8d8c7f0a,https://www.semanticscholar.org/paper/917943472ec4a00443d78bb696ed4d8f8d8c7f0a,Understanding the Science Experiences of Successful Women of Color: Science Identity as an Analytic Lens.,"In this study, we develop a model of science identity to make sense of the science experiences of 15 successful women of color over the course of their undergraduate and graduate studies in science and into science-related careers. In our view, science identity accounts both for how women make meaning of science experiences and how society structures possible meanings. Primary data included ethnographic interviews during students' undergraduate careers, follow-up interviews 6 years later, and ongoing member-checking. Our results highlight the importance of recognition by others for women in the three science identity trajectories: research scientist; altruistic scientist; and disrupted scientist. The women with research scientist identities were passionate about science and recognized themselves and were recognized by science faculty as science people. The women with altruistic scientist identities regarded science as a vehicle for altruism and created innovative meanings of ''science,'' ''recognition by others,'' and ''woman of color in science.'' The women with disrupted scientist identities sought, but did not often receive, recognition by meaningful scientific others. Although they were ultimately successful, their trajectories were more difficult because, in part, their bids for recognition were disrupted by the interaction with gendered, ethnic, and racial factors. This study clarifies theoretical conceptions of science identity, promotes a rethinking of recruitment and retention efforts, and illuminates various ways women of color experience, make meaning of, and negotiate the culture of science. 2007 Wiley Periodicals, Inc. J Res Sci Teach 44: 1187-1218, 2007.",2007.0,"Heidi Carlone, Angela C. Johnson"
993c9eb9bba80e2d8993e8c99acca1825cd0302f,https://www.semanticscholar.org/paper/993c9eb9bba80e2d8993e8c99acca1825cd0302f,Next Steps for Citizen Science,"Strategic investments and coordination are needed for citizen science to reach its full potential. Around the globe, thousands of research projects are engaging millions of individuals—many of whom are not trained as scientists—in collecting, categorizing, transcribing, or analyzing scientific data. These projects, known as citizen science, cover a breadth of topics from microbiomes to native bees to water quality to galaxies. Most projects obtain or manage scientific information at scales or resolutions unattainable by individual researchers or research teams, whether enrolling thousands of individuals collecting data across several continents, enlisting small armies of participants in categorizing vast quantities of online data, or organizing small groups of volunteers to tackle local problems.",2014.0,"R. Bonney, Jennifer Shirk, T. Phillips, A. Wiggins, H. Ballard, A. Miller‐Rushing, J. Parrish"
e281464d9a558cc1d25084687efb75683e65d4f0,https://www.semanticscholar.org/paper/e281464d9a558cc1d25084687efb75683e65d4f0,Growth rates of modern science: A bibliometric analysis based on the number of publications and cited references,"Many studies (in information science) have looked at the growth of science. In this study, we reexamine the question of the growth of science. To do this we (a) use current data up to publication year 2012 and (b) analyze the data across all disciplines and also separately for the natural sciences and for the medical and health sciences. Furthermore, the data were analyzed with an advanced statistical technique—segmented regression analysis—which can identify specific segments with similar growth rates in the history of science. The study is based on two different sets of bibliometric data: (a) the number of publications held as source items in the Web of Science (WoS, Thomson Reuters) per publication year and (b) the number of cited references in the publications of the source items per cited reference year. We looked at the rate at which science has grown since the mid‐1600s. In our analysis of cited references we identified three essential growth phases in the development of science, which each led to growth rates tripling in comparison with the previous phase: from less than 1% up to the middle of the 18th century, to 2 to 3% up to the period between the two world wars, and 8 to 9% to 2010.",2014.0,"L. Bornmann, Rüdiger Mutz"
85cd1c3c6346d8fe3b245cc41e2757631301bc27,https://www.semanticscholar.org/paper/85cd1c3c6346d8fe3b245cc41e2757631301bc27,The lure of rationality: Why does the deficit model persist in science communication?,"Science communication has been historically predicated on the knowledge deficit model. Yet, empirical research has shown that public communication of science is more complex than what the knowledge deficit model suggests. In this essay, we pose four lines of reasoning and present empirical data for why we believe the deficit model still persists in public communication of science. First, we posit that scientists’ training results in the belief that public audiences can and do process information in a rational manner. Second, the persistence of this model may be a product of current institutional structures. Many graduate education programs in science, technology, engineering, and math (STEM) fields generally lack formal training in public communication. We offer empirical evidence that demonstrates that scientists who have less positive attitudes toward the social sciences are more likely to adhere to the knowledge deficit model of science communication. Third, we present empirical evidence of how scientists conceptualize “the public” and link this to attitudes toward the deficit model. We find that perceiving a knowledge deficit in the public is closely tied to scientists’ perceptions of the individuals who comprise the public. Finally, we argue that the knowledge deficit model is perpetuated because it can easily influence public policy for science issues. We propose some ways to uproot the deficit model and move toward more effective science communication efforts, which include training scientists in communication methods grounded in social science research and using approaches that engage community members around scientific issues.",2016.0,"Molly Simis, Haley C. Madden, M. Cacciatore, Sara K. Yeo"
4d12b00963aa6e0d9b9b84a62f0543de608fccb5,https://www.semanticscholar.org/paper/4d12b00963aa6e0d9b9b84a62f0543de608fccb5,"If We Share Data, Will Anyone Use Them? Data Sharing and Reuse in the Long Tail of Science and Technology","Research on practices to share and reuse data will inform the design of infrastructure to support data collection, management, and discovery in the long tail of science and technology. These are research domains in which data tend to be local in character, minimally structured, and minimally documented. We report on a ten-year study of the Center for Embedded Network Sensing (CENS), a National Science Foundation Science and Technology Center. We found that CENS researchers are willing to share their data, but few are asked to do so, and in only a few domain areas do their funders or journals require them to deposit data. Few repositories exist to accept data in CENS research areas.. Data sharing tends to occur only through interpersonal exchanges. CENS researchers obtain data from repositories, and occasionally from registries and individuals, to provide context, calibration, or other forms of background for their studies. Neither CENS researchers nor those who request access to CENS data appear to use external data for primary research questions or for replication of studies. CENS researchers are willing to share data if they receive credit and retain first rights to publish their results. Practices of releasing, sharing, and reusing of data in CENS reaffirm the gift culture of scholarship, in which goods are bartered between trusted colleagues rather than treated as commodities.",2013.0,"J. Wallis, E. Rolando, C. Borgman"
53834f0ee8df731cf0e629cd594dce0afaaa3d97,https://www.semanticscholar.org/paper/53834f0ee8df731cf0e629cd594dce0afaaa3d97,The inevitable application of big data to health care.,"THE AMOUNT OF DATA BEING DIGITALLY COLLECTED AND stored is vast and expanding rapidly. As a result, the science of data management and analysis is also advancing to enable organizations to convert this vast resource into information and knowledge that helps them achieve their objectives. Computer scientists have invented the term big data to describe this evolving technology. Big data has been successfully used in astronomy (eg, the Sloan Digital Sky Survey of telescopic information), retail sales (eg, Walmart’s expansive number of transactions), search engines (eg, Google’s customization of individual searches based on previous web data), and politics (eg, a campaign’s focus of political advertisements on people most likely to support their candidate based on web searches). In this Viewpoint, we discuss the application of big data to health care, using an economic framework to highlight the opportunities it will offer and the roadblocks to implementation. We suggest that leveraging the collection of patient and practitioner data could be an important way to improve quality and efficiency of health care delivery. Widespread uptake of electronic health records (EHRs) has generated massive data sets. A survey by the American Hospital Association showed that adoption of EHRs has doubled from 2009 to 2011, partly a result of funding provided by the Health Information Technology for Economic and Clinical Health Act of 2009. Most EHRs now contain quantitative data (eg, laboratory values), qualitative data (eg, text-based documents and demographics), and transactional data (eg, a record of medication delivery). However, much of this rich data set is currently perceived as a byproduct of health care delivery, rather than a central asset to improve its efficiency. The transition of data from refuse to riches has been key in the big data revolution of other industries. Advances in analytic techniques in the computer sciences, especially in machine learning, have been a major catalyst for dealing with these large information sets. These analytic techniques are in contrast to traditional statistical methods (derived from the social and physical sciences), which are largely not useful for analysis of unstructured data such as text-based documents that do not fit into relational tables. One estimate suggests that 80% of business-related data exist in an unstructured format. The same could probably be said for health care data, a large proportion of which is text-based. In contrast to most consumer service industries, medicine adopted a practice of generating evidence from experimental (randomized trials) and quasi-experimental studies to inform patients and clinicians. The evidence-based movement is founded on the belief that scientific inquiry is superior to expert opinion and testimonials. In this way, medicine was ahead of many other industries in terms of recognizing the value of data and information guiding rational decision making. However, health care has lagged in uptake of newer techniques to leverage the rich information contained in EHRs. There are 4 ways big data may advance the economic mission of health care delivery by improving quality and efficiency. First, big data may greatly expand the capacity to generate new knowledge. The cost of answering many clinical questions prospectively, and even retrospectively, by collecting structured data is prohibitive. Analyzing the unstructured data contained within EHRs using computational techniques (eg, natural language processing to extract medical concepts from free-text documents) permits finer data acquisition in an automated fashion. For instance, automated identification within EHRs using natural language processing was superior in detecting postoperative complications compared with patient safety indicators based on discharge coding. Big data offers the potential to create an observational evidence base for clinical questions that would otherwise not be possible and may be especially helpful with issues of generalizability. The latter issue limits the application of conclusions derived from randomized trials performed on a narrow spectrum of participants to patients who exhibit very different characteristics. Second, big data may help with knowledge dissemination. Most physicians struggle to stay current with the latest evidence guiding clinical practice. The digitization of medical literature has greatly improved access; however, the sheer",2013.0,"T. Murdoch, A. Detsky"
929607741b2a12656ff8d3360ca96fe76a6557a4,https://www.semanticscholar.org/paper/929607741b2a12656ff8d3360ca96fe76a6557a4,Next Generation Science Standards,"Science and Engineering Practices that connect to garden-based education (all 8): • Asking questions (for science) and defining problems (for engineering) • Developing and using models • Planning and carrying out investigations • Analyzing and interpreting data • Using mathematics and computational thinking • Constructing explanations (for science) and designing solutions (for engineering) • Engaging in argument from evidence • Obtaining, evaluating, and communicating information",2013.0,P. Adams
1a46465ab69ec13d3c84d66166e979989afa596d,https://www.semanticscholar.org/paper/1a46465ab69ec13d3c84d66166e979989afa596d,Comment on “Estimating the reproducibility of psychological science”,"A paper from the Open Science Collaboration (Research Articles, 28 August 2015, aac4716) attempting to replicate 100 published studies suggests that the reproducibility of psychological science is surprisingly low. We show that this article contains three statistical errors and provides no support for such a conclusion. Indeed, the data are consistent with the opposite conclusion, namely, that the reproducibility of psychological science is quite high.",2016.0,"D. Gilbert, Gary King, Stephen Pettigrew, Timothy D. Wilson"
f567f5a4a57509c2288f510d6703212ce8499527,https://www.semanticscholar.org/paper/f567f5a4a57509c2288f510d6703212ce8499527,The Ames Stereo Pipeline: NASA's Open Source Software for Deriving and Processing Terrain Data,"The NASA Ames Stereo Pipeline is a suite of free and open source automated geodesy and stereogrammetry tools designed for processing stereo images captured from satellites (around Earth and other planets), robotic rovers, aerial cameras, and historical images, with and without accurate camera pose information. It produces cartographic products, including digital terrain models, ortho‐projected images, 3‐D models, and bundle‐adjusted networks of cameras. Ames Stereo Pipeline's data products are suitable for science analysis, mission planning, and public outreach.",2018.0,"R. Beyer, O. Alexandrov, S. McMichael"
0131258a516da6f9d86795fc6ed4968206dba005,https://www.semanticscholar.org/paper/0131258a516da6f9d86795fc6ed4968206dba005,A Criteria-based Assessment of the Coverage of Scopus and Web of Science,"Abstract Purpose The purpose of this study is to assess the coverage of the scientific literature in Scopus and Web of Science from the perspective of research evaluation. Design/methodology/approach The academic communities of Norway have agreed on certain criteria for what should be included as original research publications in research evaluation and funding contexts. These criteria have been applied since 2004 in a comprehensive bibliographic database called the Norwegian Science Index (NSI). The relative coverages of Scopus and Web of Science are compared with regard to publication type, field of research and language. Findings Our results show that Scopus covers 72 percent of the total Norwegian scientific and scholarly publication output in 2015 and 2016, while the corresponding figure for Web of Science Core Collection is 69 percent. The coverages are most comprehensive in medicine and health (89 and 87 percent) and in the natural sciences and technology (85 and 84 percent). The social sciences (48 percent in Scopus and 40 percent in Web of Science Core Collection) and particularly the humanities (27 and 23 percent) are much less covered in the two international data sources. Research limitation Comparing with data from only one country is a limitation of the study, but the criteria used to define a country’s scientific output as well as the identification of patterns of field-dependent partial representations in Scopus and Web of Science should be recognizable and useful also for other countries. Originality/value The novelty of this study is the criteria-based approach to studying coverage problems in the two data sources.",2019.0,"D. Aksnes, G. Sivertsen"
760d38a08bff329ff67719935c18fa1631e3ded8,https://www.semanticscholar.org/paper/760d38a08bff329ff67719935c18fa1631e3ded8,The View from Above: Applications of Satellite Data in Economics,"The past decade or so has seen a dramatic change in the way that economists can learn by watching our planet from above. A revolution has taken place in remote sensing and allied fields such as computer science, engineering, and geography. Petabytes of satellite imagery have become publicly accessible at increasing resolution, many algorithms for extracting meaningful social science information from these images are now routine, and modern cloud-based processing power allows these algorithms to be run at global scale. This paper seeks to introduce economists to the science of remotely sensed data, and to give a flavor of how this new source of data has been used by economists so far and what might be done in the future.",2016.0,"D. Donaldson, A. Storeygard"
40f19bdaa4e869ab9784880fec5e9e229a2a61ab,https://www.semanticscholar.org/paper/40f19bdaa4e869ab9784880fec5e9e229a2a61ab,The Pan-STARRS1 Database and Data Products,"This paper describes the organization of the database and the catalog data products from the Pan-STARRS1 3π Steradian Survey. The catalog data products are available in the form of an SQL-based relational database from MAST, the Mikulski Archive for Space Telescopes at STScI. The database is described in detail, including the construction of the database, the provenance of the data, the schema, and how the database tables are related. Examples of queries for a range of science goals are included.",2016.0,"H. Flewelling, E. Magnier, K. Chambers, J. Heasley, C. Holmberg, M. Huber, W. Sweeney, C. Waters, A. Calamida, S. Casertano, X. Chen, D. Farrow, G. Hasinger, R. Henderson, K. Long, N. Metcalfe, G. Narayan, M. Nieto-Santisteban, P. Norberg, A. Rest, R. Saglia, A. Szalay, A. Thakar, J. Tonry, J. Valenti, S. Werner, R. White, L. Denneau, P. Draper, K. Hodapp, R. Jedicke, N. Kaiser, R. Kudritzki, P. Price, R. Wainscoat, Ps Builders, S. Chastel, B. McLean, M. Postman, B. Shiao"
c8bc2d5edb9307b5c420adc4eee3cf641a781b14,https://www.semanticscholar.org/paper/c8bc2d5edb9307b5c420adc4eee3cf641a781b14,Online analysis enhances use of NASA Earth science data,"Giovanni, the Goddard Earth Sciences Data and Information Services Center (GES DISC) Interactive Online Visualization and Analysis Infrastructure, has provided researchers with advanced capabilities to perform data exploration and analysis with observational data from NASA Earth observation satellites. In the past 5–10 years, examining geophysical events and processes with remote-sensing data required a multistep process of data discovery, data acquisition, data management, and ultimately data analysis. Giovanni accelerates this process by enabling basic visualization and analysis directly on the World Wide Web. In the last two years, Giovanni has added new data acquisition functions and expanded analysis options to increase its usefulness to the Earth science research community.",2007.0,"J. Acker, G. Leptoukh"
720400bf69c1af50795d7ec1b58e95c682d217aa,https://www.semanticscholar.org/paper/720400bf69c1af50795d7ec1b58e95c682d217aa,Best Practices in Data Analysis and Sharing in Neuroimaging using MRI,"Neuroimaging enables rich noninvasive measurements of human brain activity, but translating such data into neuroscientific insights and clinical applications requires complex analyses and collaboration among a diverse array of researchers. The open science movement is reshaping scientific culture and addressing the challenges of transparency and reproducibility of research. To advance open science in neuroimaging the Organization for Human Brain Mapping created the Committee on Best Practice in Data Analysis and Sharing (COBIDAS), charged with creating a report that collects best practice recommendations from experts and the entire brain imaging community. The purpose of this work is to elaborate the principles of open and reproducible research for neuroimaging using Magnetic Resonance Imaging (MRI), and then distill these principles to specific research practices. Many elements of a study are so varied that practice cannot be prescribed, but for these areas we detail the information that must be reported to fully understand and potentially replicate a study. For other elements of a study, like statistical modelling where specific poor practices can be identified, and the emerging areas of data sharing and reproducibility, we detail both good practice and reporting standards. For each of seven areas of a study we provide tabular listing of over 100 items to help plan, execute, report and share research in the most transparent fashion. Whether for individual scientists, or for editors and reviewers, we hope these guidelines serve as a benchmark, to raise the standards of practice and reporting in neuroimaging using MRI.",2016.0,"Thomas E. Nichols, Samir Das, S. Eickhoff, Alan C. Evans, T. Glatard, Michael Hanke, N. Kriegeskorte, M. Milham, R. Poldrack, J. Poline, Erika Proal, B. Thirion, D. V. Van Essen, Tonya White, B. Yeo"
b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,https://www.semanticscholar.org/paper/b55fda1f58af7fd9ecde8f1dc193ddd6ab6e9d26,Handbook of theoretical computer science - Part A: Algorithms and complexity; Part B: Formal models and semantics,"""Of all the books I have covered in the Forum to date, this set is the most unique and possibly the most useful to the SIGACT community, in support both of teaching and research.... The books can be used by anyone wanting simply to gain an understanding of one of these areas, or by someone desiring to be in research in a topic, or by instructors wishing to find timely information on a subject they are teaching outside their major areas of expertise."" -- Rocky Ross, ""SIGACT News"" ""This is a reference which has a place in every computer science library."" -- Raymond Lauzzana, ""Languages of Design"" The Handbook of Theoretical Computer Science provides professionals and students with a comprehensive overview of the main results and developments in this rapidly evolving field. Volume A covers models of computation, complexity theory, data structures, and efficient computation in many recognized subdisciplines of theoretical computer science. Volume B takes up the theory of automata and rewriting systems, the foundations of modern programming languages, and logics for program specification and verification, and presents several studies on the theoretic modeling of advanced information processing. The two volumes contain thirty-seven chapters, with extensive chapter references and individual tables of contents for each chapter. There are 5,387 entry subject indexes that include notational symbols, and a list of contributors and affiliations in each volume.",1990.0,J. Leeuwen
6d962e9f04c653f732da82073a3446f75a371055,https://www.semanticscholar.org/paper/6d962e9f04c653f732da82073a3446f75a371055,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",1996.0,"U. Fayyad, G. Piatetsky-Shapiro, Padhraic Smyth"
41692ed07f393c1c3e335db99c7e3c5a0d265a78,https://www.semanticscholar.org/paper/41692ed07f393c1c3e335db99c7e3c5a0d265a78,Citation indexes for science; a new dimension in documentation through association of ideas.,"‘The uncritical citation of disputed data by a writer, whether it be deliberate or not, is a serious matter. Of course, knowingly propagandizing unsubstantiated claims is particularly abhorrent, but just as many naive students may be swayed by unfounded assertions presented by a writer who is unaware of the criticisms. Buried in scholarly journals, critical notes are increasingly likely to be overlooked with the passage of time, while the studies to which they pertain, having been reported more widely, are apt to be rediscovered.’ 1",2006.0,E. Garfield
43d75d3a22db904d052d4c435e2d1f22be3887e0,https://www.semanticscholar.org/paper/43d75d3a22db904d052d4c435e2d1f22be3887e0,Outlier Detection for Temporal Data: A Survey,"In the statistics community, outlier detection for time series data has been studied for decades. Recently, with advances in hardware and software technology, there has been a large body of work on temporal outlier detection from a computational perspective within the computer science community. In particular, advances in hardware technology have enabled the availability of various forms of temporal data collection mechanisms, and advances in software technology have enabled a variety of data management mechanisms. This has fueled the growth of different kinds of data sets such as data streams, spatio-temporal data, distributed streams, temporal networks, and time series data, generated by a multitude of applications. There arises a need for an organized and detailed study of the work done in the area of outlier detection with respect to such temporal datasets. In this survey, we provide a comprehensive and structured overview of a large set of interesting outlier definitions for various forms of temporal data, novel techniques, and application scenarios in which specific definitions and techniques have been widely used.",2014.0,"Manish Gupta, Jing Gao, C. Aggarwal, Jiawei Han"
9a7dfcd3c35ebfbce9e359a1a97d6892b83a37ec,https://www.semanticscholar.org/paper/9a7dfcd3c35ebfbce9e359a1a97d6892b83a37ec,Citizen Science as an Ecological Research Tool: Challenges and Benefits,"Citizen science, the involvement of volunteers in research, has increased the scale of ecological field studies with continent-wide, centralized monitoring efforts and, more rarely, tapping of volunteers to conduct large, coordinated, field experiments. The unique benefit for the field of ecology lies in understanding processes occurring at broad geographic scales and on private lands, which are impossible to sample extensively with traditional field research models. Citizen science produces large, longitudinal data sets, whose potential for error and bias is poorly understood. Because it does not usually aim to uncover mechanisms underlying ecological patterns, citizen science is best viewed as complementary to more localized, hypothesis-driven research. In the process of addressing the impacts of current, global “experiments” altering habitat and climate, large-scale citizen science has led to new, quantitative approaches to emerging questions about the distribution and abundance of organisms across spa...",2010.0,"J. Dickinson, B. Zuckerberg, David N. Bonter"
18a940ff6dce8bc140658da52d686291ca965979,https://www.semanticscholar.org/paper/18a940ff6dce8bc140658da52d686291ca965979,The Analysis of Social Science Data with Missing Values,"Methods for handling missing data in social science data sets are reviewed. Limitations of common practical approaches, including complete-case analysis, available-case analysis and imputation, are illustrated on a simple missing-data problem with one complete and one incomplete variable. Two more principled approaches, namely maximum likelihood under a model for the data and missing-data mechanism and multiple imputation, are applied to the bivariate problem. General properties of these methods are outlined, and applications to more complex missing-data problems are discussed. The EM algorithm, a convenient method for computing maximum likelihood estimates in missing-data problems, is described and applied to two common models, the multivariate normal model for continuous data and the multinomial model for discrete data. Multiple imputation under explicit or implicit models is recommended as a method that retains the advantages of imputation and overcomes its limitations.",1989.0,"R. Little, D. Rubin"
c8b3f78bdead3596c4e7cb3aaad07a79cfa86ce4,https://www.semanticscholar.org/paper/c8b3f78bdead3596c4e7cb3aaad07a79cfa86ce4,Calling Bullshit: The Art of Skepticism in a Data-Driven World,"This week on the Science podcast, evolutionary biologist Carl Bergstrom explains how to identify data-driven misinformation and disinformation.",2020.0,
d90f276316589f503690d541392989031f9d046b,https://www.semanticscholar.org/paper/d90f276316589f503690d541392989031f9d046b,Online Citizen Science: A Systematic Review of Effects on Learning and Scientific Literacy,"Participation in online citizen science is increasingly popular, yet studies that examine the impact on participants’ learning are limited. The aims of this paper are to identify the learning impact on volunteers who participate in online citizen science projects and to explore the methods used to study the impact. The ten empirical studies, examined in this systematic review, report learning impacts on citizens’ attitudes towards science, on their understanding of the nature of science, on topic-specific knowledge, on science knowledge, and on generic knowledge. These impacts were measured using self-reports, content analysis of contributed data and of forum posts, accuracy checks of contributed data, science and project-specific quizzes, and instruments for measuring scientific attitudes and beliefs. The findings highlight that certain technological affordances in online citizen science projects can cultivate citizens’ knowledge and skills, and they point to unexplored areas, including the lack of experimental and long-term studies, and studies in formal education settings.",2020.0,"M. Aristeidou, C. Herodotou"
299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,https://www.semanticscholar.org/paper/299bab6b327e34c3e4f97cc8d0f9c64c9741fa99,Where are human subjects in Big Data research? The emerging ethics divide,"There are growing discontinuities between the research practices of data science and established tools of research ethics regulation. Some of the core commitments of existing research ethics regulations, such as the distinction between research and practice, cannot be cleanly exported from biomedical research to data science research. Such discontinuities have led some data science practitioners and researchers to move toward rejecting ethics regulations outright. These shifts occur at the same time as a proposal for major revisions to the Common Rule—the primary regulation governing human-subjects research in the USA—is under consideration for the first time in decades. We contextualize these revisions in long-running complaints about regulation of social science research and argue data science should be understood as continuous with social sciences in this regard. The proposed regulations are more flexible and scalable to the methods of non-biomedical research, yet problematically largely exclude data science methods from human-subjects regulation, particularly uses of public datasets. The ethical frameworks for Big Data research are highly contested and in flux, and the potential harms of data science research are unpredictable. We examine several contentious cases of research harms in data science, including the 2014 Facebook emotional contagion study and the 2016 use of geographical data techniques to identify the pseudonymous artist Banksy. To address disputes about application of human-subjects research ethics in data science, critical data studies should offer a historically nuanced theory of “data subjectivity” responsive to the epistemic methods, harms and benefits of data science and commerce.",2016.0,"Jacob Metcalf, K. Crawford"
5ae073986408c9931bf6887fafb85e253866f7cc,https://www.semanticscholar.org/paper/5ae073986408c9931bf6887fafb85e253866f7cc,Fuzzy-Set Social Science,"In this innovative approach to the practice of social science, Charles Ragin explores the use of fuzzy sets to bridge the divide between quantitative and qualitative methods. Paradoxically, the fuzzy set is a powerful tool because it replaces an unwieldy, ""fuzzy"" instrument—the variable, which establishes only the positions of cases relative to each other, with a precise one—degree of membership in a well-defined set. Ragin argues that fuzzy sets allow a far richer dialogue between ideas and evidence in social research than previously possible. They let quantitative researchers abandon ""homogenizing assumptions"" about cases and causes, they extend diversity-oriented research strategies, and they provide a powerful connection between theory and data analysis. Most important, fuzzy sets can be carefully tailored to fit evolving theoretical concepts, sharpening quantitative tools with in-depth knowledge gained through qualitative, case-oriented inquiry. This book will revolutionize research methods not only in sociology, political science, and anthropology but in any field of inquiry dealing with complex patterns of causation.",2001.0,Charles C. Ragin
b7118fca8e7cd69d76090a5c145e89f303249eb8,https://www.semanticscholar.org/paper/b7118fca8e7cd69d76090a5c145e89f303249eb8,The current state of citizen science as a tool for ecological research and public engagement,"Approaches to citizen science – an indispensable means of combining ecological research with environmental education and natural history observation – range from community-based monitoring to the use of the internet to “crowd-source” various scientific tasks, from data collection to discovery. With new tools and mechanisms for engaging learners, citizen science pushes the envelope of what ecologists can achieve, both in expanding the potential for spatial ecology research and in supplementing existing, but localized, research programs. The primary impacts of citizen science are seen in biological studies of global climate change, including analyses of phenology, landscape ecology, and macro-ecology, as well as in sub-disciplines focused on species (rare and invasive), disease, populations, communities, and ecosystems. Citizen science and the resulting ecological data can be viewed as a public good that is generated through increasingly collaborative tools and resources, while supporting public participation in science and Earth stewardship.",2012.0,"J. Dickinson, Jennifer Shirk, David N. Bonter, R. Bonney, Rhiannon Crain, J. Martin, T. Phillips, K. Purcell"
62e0c6cf57bc345026d56fd654e80beaf9315c92,https://www.semanticscholar.org/paper/62e0c6cf57bc345026d56fd654e80beaf9315c92,JENDL-4.0: A New Library for Nuclear Science and Engineering,"The fourth version of the Japanese Evaluated Nuclear Data Library has been produced in cooperation with the Japanese Nuclear Data Committee. In the new library, much emphasis is placed on the improvements of fission product and minor actinoid data. Two nuclear model codes were developed in order to evaluate the cross sections of fission products and minor actinoids. Coupled-channel optical model parameters, which can be applied to wide mass and energy regions, were obtained for nuclear model calculations. Thermal cross sections of actinoids were carefully examined by considering experimental data or by the systematics of neighboring nuclei. Most of the fission cross sections were derived from experimental data. A simultaneous evaluation was performed for the fission cross sections of important uranium and plutonium isotopes above 10 keV. New evaluations were performed for the thirty fissionproduct nuclides that had not been contained in the previous library JENDL-3.3. The data for light elements and structural materials were partly reevaluated. Moreover, covariances were estimated mainly for actinoids. The new library was released as JENDL-4.0, and the data can be retrieved from the Web site of the JAEA Nuclear Data Center.",2011.0,"K. Shibata, O. Iwamoto, T. Nakagawa, N. Iwamoto, A. Ichihara, S. Kunieda, S. Chiba, K. Furutaka, N. Otuka, T. Ohsawa, T. Murata, H. Matsunobu, A. Zukeran, S. Kamada, J. Katakura"
e5f9c255b1155c2eea7f75482b82d4aeef1dfa90,https://www.semanticscholar.org/paper/e5f9c255b1155c2eea7f75482b82d4aeef1dfa90,Predicting Materials Properties with Little Data Using Shotgun Transfer Learning,"There is a growing demand for the use of machine learning (ML) to derive fast-to-evaluate surrogate models of materials properties. In recent years, a broad array of materials property databases have emerged as part of a digital transformation of materials science. However, recent technological advances in ML are not fully exploited because of the insufficient volume and diversity of materials data. An ML framework called “transfer learning” has considerable potential to overcome the problem of limited amounts of materials data. Transfer learning relies on the concept that various property types, such as physical, chemical, electronic, thermodynamic, and mechanical properties, are physically interrelated. For a given target property to be predicted from a limited supply of training data, models of related proxy properties are pretrained using sufficient data; these models capture common features relevant to the target task. Repurposing of such machine-acquired features on the target task yields outstanding prediction performance even with exceedingly small data sets, as if highly experienced human experts can make rational inferences even for considerably less experienced tasks. In this study, to facilitate widespread use of transfer learning, we develop a pretrained model library called XenonPy.MDL. In this first release, the library comprises more than 140 000 pretrained models for various properties of small molecules, polymers, and inorganic crystalline materials. Along with these pretrained models, we describe some outstanding successes of transfer learning in different scenarios such as building models with only dozens of materials data, increasing the ability of extrapolative prediction through a strategic model transfer, and so on. Remarkably, transfer learning has autonomously identified rather nontrivial transferability across different properties transcending the different disciplines of materials science; for example, our analysis has revealed underlying bridges between small molecules and polymers and between organic and inorganic chemistry.",2019.0,"H. Yamada, Chang Liu, Stephen Wu, Y. Koyama, S. Ju, J. Shiomi, Junko Morikawa, Ryo Yoshida"
c50dca78e97e335d362d6b991ae0e1448914e9a3,https://www.semanticscholar.org/paper/c50dca78e97e335d362d6b991ae0e1448914e9a3,Reducing the Dimensionality of Data with Neural,"http://www.sciencemag.org/cgi/content/full/313/5786/504 version of this article at: including high-resolution figures, can be found in the online Updated information and services, http://www.sciencemag.org/cgi/content/full/313/5786/504/DC1 can be found at: Supporting Online Material found at: can be related to this article A list of selected additional articles on the Science Web sites http://www.sciencemag.org/cgi/content/full/313/5786/504#related-content http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles , 6 of which can be accessed for free: cites 8 articles This article 15 article(s) on the ISI Web of Science. cited by This article has been http://www.sciencemag.org/cgi/content/full/313/5786/504#otherarticles 4 articles hosted by HighWire Press; see: cited by This article has been http://www.sciencemag.org/about/permissions.dtl in whole or in part can be found at: this article permission to reproduce of this article or about obtaining reprints Information about obtaining",2008.0,Geoffrey E. Hinton
9386590554c429e80402c082e9d6a2398bcc36b3,https://www.semanticscholar.org/paper/9386590554c429e80402c082e9d6a2398bcc36b3,Data streams: algorithms and applications,"Data stream algorithms as an active research agenda emerged only over the past few years, even though the concept of making few passes over the data for performing computations has been around since the early days of Automata Theory. The data stream agenda now pervades many branches of Computer Science including databases, networking, knowledge discovery and data mining, and hardware systems. Industry is in synch too, with Data Stream Management Systems (DSMSs) and special hardware to deal with data speeds. Even beyond Computer Science, data stream concerns are emerging in physics, atmospheric science and statistics. Data Streams: Algorithms and Applications focuses on the algorithmic foundations of data streaming. In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Data Streams: Algorithms and Applications surveys the emerging area of algorithms for processing data streams and associated applications. An extensive bibliography with over 200 entries points the reader to further resources for exploration.",2005.0,S. Muthukrishnan
954f2a7b1c6f28c4a845ccda5761eb09da032a64,https://www.semanticscholar.org/paper/954f2a7b1c6f28c4a845ccda5761eb09da032a64,Data sharing,"The Science family of journals is committed to sharing data relevant to public health emergencies, and therefore we are signatories to, and wholeheartedly endorse, the following statement by funders and journals.*",2016.0,M. McNutt
69732dcf45024f28e5c43de68d1208f6e737eada,https://www.semanticscholar.org/paper/69732dcf45024f28e5c43de68d1208f6e737eada,The BIG Data Center: from deposition to integration to translation,"Biological data are generated at unprecedentedly exponential rates, posing considerable challenges in big data deposition, integration and translation. The BIG Data Center, established at Beijing Institute of Genomics (BIG), Chinese Academy of Sciences, provides a suite of database resources, including (i) Genome Sequence Archive, a data repository specialized for archiving raw sequence reads, (ii) Gene Expression Nebulas, a data portal of gene expression profiles based entirely on RNA-Seq data, (iii) Genome Variation Map, a comprehensive collection of genome variations for featured species, (iv) Genome Warehouse, a centralized resource housing genome-scale data with particular focus on economically important animals and plants, (v) Methylation Bank, an integrated database of whole-genome single-base resolution methylomes and (vi) Science Wikis, a central access point for biological wikis developed for community annotations. The BIG Data Center is dedicated to constructing and maintaining biological databases through big data integration and value-added curation, conducting basic research to translate big data into big knowledge and providing freely open access to a variety of data resources in support of worldwide research activities in both academia and industry. All of these resources are publicly available and can be found at http://bigd.big.ac.cn.",2016.0,"Wenming Zhao, Jingfa Xiao"
3954e2d220d9a7b7a46f9561cafb6251524d8ee5,https://www.semanticscholar.org/paper/3954e2d220d9a7b7a46f9561cafb6251524d8ee5,Mars Reconnaissance Orbiter's High Resolution Imaging Science Experiment (HiRISE),"[1] The HiRISE camera features a 0.5 m diameter primary mirror, 12 m effective focal length, and a focal plane system that can acquire images containing up to 28 Gb (gigabits) of data in as little as 6 seconds. HiRISE will provide detailed images (0.25 to 1.3 m/pixel) covering ∼1% of the Martian surface during the 2-year Primary Science Phase (PSP) beginning November 2006. Most images will include color data covering 20% of the potential field of view. A top priority is to acquire ∼1000 stereo pairs and apply precision geometric corrections to enable topographic measurements to better than 25 cm vertical precision. We expect to return more than 12 Tb of HiRISE data during the 2-year PSP, and use pixel binning, conversion from 14 to 8 bit values, and a lossless compression system to increase coverage. HiRISE images are acquired via 14 CCD detectors, each with 2 output channels, and with multiple choices for pixel binning and number of Time Delay and Integration lines. HiRISE will support Mars exploration by locating and characterizing past, present, and future landing sites, unsuccessful landing sites, and past and potentially future rover traverses. We will investigate cratering, volcanism, tectonism, hydrology, sedimentary processes, stratigraphy, aeolian processes, mass wasting, landscape evolution, seasonal processes, climate change, spectrophotometry, glacial and periglacial processes, polar geology, and regolith properties. An Internet Web site (HiWeb) will enable anyone in the world to suggest HiRISE targets on Mars and to easily locate, view, and download HiRISE data products.",2007.0,"A. S. McEwen, N. Thomas, Hirise Team"
34ad09cda075101dc4ce3c04006ff804aca3ebf8,https://www.semanticscholar.org/paper/34ad09cda075101dc4ce3c04006ff804aca3ebf8,"Big data: Issues, challenges, tools and Good practices","Big data is defined as large amount of data which requires new technologies and architectures so that it becomes possible to extract value from it by capturing and analysis process. Due to such large size of data it becomes very difficult to perform effective analysis using the existing traditional techniques. Big data due to its various properties like volume, velocity, variety, variability, value and complexity put forward many challenges. Since Big data is a recent upcoming technology in the market which can bring huge benefits to the business organizations, it becomes necessary that various challenges and issues associated in bringing and adapting to this technology are brought into light. This paper introduces the Big data technology along with its importance in the modern world and existing projects which are effective and important in changing the concept of science into big science and society too. The various challenges and issues in adapting and accepting Big data technology, its tools (Hadoop) are also discussed in detail along with the problems Hadoop is facing. The paper concludes with the Good Big data practices to be followed.",2013.0,"Avita Katal, M. Wazid, R. Goudar"
951eab2b27c673e0ff1a20800f576d4792f60d5f,https://www.semanticscholar.org/paper/951eab2b27c673e0ff1a20800f576d4792f60d5f,Crisis informatics—New data for extraordinary times,"Focus on behaviors, not on fetishizing social media tools Crisis informatics is a multidisciplinary field combining computing and social science knowledge of disasters; its central tenet is that people use personal information and communication technology to respond to disaster in creative ways to cope with uncertainty. We study and develop computational support for collection and sociobehavioral analysis of online participation (i.e., tweets and Facebook posts) to address challenges in disaster warning, response, and recovery. Because such data are rarely tidy, we offer lessons—learned the hard way, as we have made every mistake described below—with respect to the opportunities and limitations of social media research on crisis events.",2016.0,"L. Palen, K. Anderson"
06d2a3fde80c5644f14f743b29a57f6b02e850d9,https://www.semanticscholar.org/paper/06d2a3fde80c5644f14f743b29a57f6b02e850d9,The iPlant Collaborative: Cyberinfrastructure for Enabling Data to Discovery for the Life Sciences,"The iPlant Collaborative provides life science research communities access to comprehensive, scalable, and cohesive computational infrastructure for data management; identity management; collaboration tools; and cloud, high-performance, high-throughput computing. iPlant provides training, learning material, and best practice resources to help all researchers make the best use of their data, expand their computational skill set, and effectively manage their data and computation when working as distributed teams. iPlant’s platform permits researchers to easily deposit and share their data and deploy new computational tools and analysis workflows, allowing the broader community to easily use and reuse those data and computational analyses.",2016.0,"Nirav C. Merchant, Eric Lyons, S. Goff, M. Vaughn, D. Ware, D. Micklos, P. Antin"
687e00a5fec7d747d18866f60b7a21973e80b04f,https://www.semanticscholar.org/paper/687e00a5fec7d747d18866f60b7a21973e80b04f,The ethics of smart cities and urban science,"Software-enabled technologies and urban big data have become essential to the functioning of cities. Consequently, urban operational governance and city services are becoming highly responsive to a form of data-driven urbanism that is the key mode of production for smart cities. At the heart of data-driven urbanism is a computational understanding of city systems that reduces urban life to logic and calculative rules and procedures, which is underpinned by an instrumental rationality and realist epistemology. This rationality and epistemology are informed by and sustains urban science and urban informatics, which seek to make cities more knowable and controllable. This paper examines the forms, practices and ethics of smart cities and urban science, paying particular attention to: instrumental rationality and realist epistemology; privacy, datafication, dataveillance and geosurveillance; and data uses, such as social sorting and anticipatory governance. It argues that smart city initiatives and urban science need to be re-cast in three ways: a re-orientation in how cities are conceived; a reconfiguring of the underlying epistemology to openly recognize the contingent and relational nature of urban systems, processes and science; and the adoption of ethical principles designed to realize benefits of smart cities and urban science while reducing pernicious effects. This article is part of the themed issue ‘The ethical impact of data science’.",2016.0,Rob Kitchin
f7d7f1eb559d8e2f410289fca37bb6cec7a3a907,https://www.semanticscholar.org/paper/f7d7f1eb559d8e2f410289fca37bb6cec7a3a907,Data politics,"The commentary raises political questions about the ways in which data has been constituted as an object vested with certain powers, influence, and rationalities. We place the emergence and transformation of professional practices such as ‘data science’, ‘data journalism’, ‘data brokerage’, ‘data mining’, ‘data storage’, and ‘data analysis’ as part of the reconfiguration of a series of fields of power and knowledge in the public and private accumulation of data. Data politics asks questions about the ways in which data has become such an object of power and explores how to critically intervene in its deployment as an object of knowledge. It is concerned with the conditions of possibility of data that involve things (infrastructures of servers, devices, and cables), language (code, programming, and algorithms), and people (scientists, entrepreneurs, engineers, information technologists, designers) that together create new worlds. We define ‘data politics’ as both the articulation of political questions about these worlds and the ways in which they provoke subjects to govern themselves and others by making rights claims. We contend that without understanding these conditions of possibility – of worlds, subjects and rights – it would be difficult to intervene in or shape data politics if by that it is meant the transformation of data subjects into data citizens.",2017.0,"E. Ruppert, E. Isin, D. Bigo"
e7f5ab8f486487dcefdf9b989d0eff2f0beff48c,https://www.semanticscholar.org/paper/e7f5ab8f486487dcefdf9b989d0eff2f0beff48c,A Qualitative Framework for Collecting and Analyzing Data in Focus Group Research,"Despite the abundance of published material on conducting focus groups, scant specific information exists on how to analyze focus group data in social science research. Thus, the authors provide a new qualitative framework for collecting and analyzing focus group data. First, they identify types of data that can be collected during focus groups. Second, they identify the qualitative data analysis techniques best suited for analyzing these data. Third, they introduce what they term as a micro-interlocutor analysis, wherein meticulous information about which participant responds to each question, the order in which each participant responds, response characteristics, the nonverbal communication used, and the like is collected, analyzed, and interpreted. They conceptualize how conversation analysis offers great potential for analyzing focus group data. They believe that their framework goes far beyond analyzing only the verbal communication of focus group participants, thereby increasing the rigor of focus group analyses in social science research.",2009.0,"A. Onwuegbuzie, W. Dickinson, N. Leech, A. G. Zoran"
59b2796c176636a3222d7b129c6209fa6e979aa7,https://www.semanticscholar.org/paper/59b2796c176636a3222d7b129c6209fa6e979aa7,Data infrastructure literacy,"A recent report from the UN makes the case for “global data literacy” in order to realise the opportunities afforded by the “data revolution”. Here and in many other contexts, data literacy is characterised in terms of a combination of numerical, statistical and technical capacities. In this article, we argue for an expansion of the concept to include not just competencies in reading and working with datasets but also the ability to account for, intervene around and participate in the wider socio-technical infrastructures through which data is created, stored and analysed – which we call “data infrastructure literacy”. We illustrate this notion with examples of “inventive data practice” from previous and ongoing research on open data, online platforms, data journalism and data activism. Drawing on these perspectives, we argue that data literacy initiatives might cultivate sensibilities not only for data science but also for data sociology, data politics as well as wider public engagement with digital data infrastructures. The proposed notion of data infrastructure literacy is intended to make space for collective inquiry, experimentation, imagination and intervention around data in educational programmes and beyond, including how data infrastructures can be challenged, contested, reshaped and repurposed to align with interests and publics other than those originally intended.",2018.0,"J. Gray, C. Gerlitz, Liliana Bounegru"
e981f16fde9185373634b53d94baa1f9185ff890,https://www.semanticscholar.org/paper/e981f16fde9185373634b53d94baa1f9185ff890,A correlated topic model of Science,"Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139--177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990--1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.",2007.0,"D. Blei, J. Lafferty"
ab2b6f43b0a99dc513f46e7f1684f55ce12de5d9,https://www.semanticscholar.org/paper/ab2b6f43b0a99dc513f46e7f1684f55ce12de5d9,The end of theory: The data deluge makes the scientific method obsolete,"Illustration: Marian Bantjes The Petabyte Age: Sensors everywhere. Infinite storage. Clouds of processors. Our ability to capture, warehouse, and understand massive amounts of data is changing science, medicine, business, and technology. As our collection of facts and figures grows, so will the opportunity to find answers to fundamental questions. Because in the era of big data, more isn't just more. More is different. The End of Theory: Essay: The Data Deluge Makes the Scientific Method Obsolete",2008.0,C. Anderson
a418d8fd1cc0abb34cf131d81723bc5da8817c93,https://www.semanticscholar.org/paper/a418d8fd1cc0abb34cf131d81723bc5da8817c93,Politicization of Science in the Public Sphere,"This study explores time trends in public trust in science in the United States from 1974 to 2010. More precisely, I test Mooney’s (2005) claim that conservatives in the United States have become increasingly distrustful of science. Using data from the 1974 to 2010 General Social Survey, I examine group differences in trust in science and group-specific change in these attitudes over time. Results show that group differences in trust in science are largely stable over the period, except for respondents identifying as conservative. Conservatives began the period with the highest trust in science, relative to liberals and moderates, and ended the period with the lowest. The patterns for science are also unique when compared to public trust in other secular institutions. Results show enduring differences in trust in science by social class, ethnicity, gender, church attendance, and region. I explore the implications of these findings, specifically, the potential for political divisions to emerge over the cultural authority of science and the social role of experts in the formation of public policy.",2012.0,G. Gauchat
8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,https://www.semanticscholar.org/paper/8ee4eda834e95124aca1e5ff05a1b8ce7d1487ec,Why Are Big Data Matrices Approximately Low Rank?,"Matrices of (approximate) low rank are pervasive in data science, appearing in movie preferences, text documents, survey data, medical records, and genomics. While there is a vast literature on how...",2017.0,"Madeleine Udell, Alex Townsend"
29196eb8c80a6fd6a159373f14ff323f081a8b7a,https://www.semanticscholar.org/paper/29196eb8c80a6fd6a159373f14ff323f081a8b7a,Physical and Virtual Laboratories in Science and Engineering Education,"The world needs young people who are skillful in and enthusiastic about science and who view science as their future career field. Ensuring that we will have such young people requires initiatives that engage students in interesting and motivating science experiences. Today, students can investigate scientific phenomena using the tools, data collection techniques, models, and theories of science in physical laboratories that support interactions with the material world or in virtual laboratories that take advantage of simulations. Here, we review a selection of the literature to contrast the value of physical and virtual investigations and to offer recommendations for combining the two to strengthen science learning.",2013.0,"Ton de Jong, M. Linn, Z. Zacharia"
f6ce14f91b4641942947882062682125369847f7,https://www.semanticscholar.org/paper/f6ce14f91b4641942947882062682125369847f7,The V–Dem Measurement Model: Latent Variable Analysis for Cross-National and Cross-Temporal Expert-Coded Data,"This material is based upon work supported by the National Science Foundation (SES-1423944, PI: Daniel Pemstein), Riksbankens Jubileumsfond (Grant M13-0559:1, PI: Staffan I. Lindberg), the Swedish Research Council (2013.0166, PI: Staffan I. Lindberg and Jan Teorell), the Knut and Alice Wallenberg Foundation (PI: Staffan I. Lindberg), and the University of Gothenburg (E 2013/43); as well as internal grants from the Vice-Chancellor’s office, the Dean of the College of Social Sciences, and the Department of Political Science at University of Gothenburg. Marquardt acknowledges research support from the Russian Academic Excellence Project ‘5-100.’ We performed simulations and other computational tasks using resources provided by the Notre Dame Center for Research Computing (CRC) through the High Performance Computing section and the Swedish National Infrastructure for Computing (SNIC) at the National Supercomputer Centre in Sweden (SNIC 2016/1-382, SNIC 2017/1-406 and 2017/1-68). We specifically acknowledge the assistance of In-Saeng Suh at CRC and Johan Raber and Peter Mu nger at SNIC in facilitating our use of their respective systems.",2015.0,"Daniel Pemstein, Kyle L. Marquardt, Eitan Tzelgov, Yi-ting Wang, Farhad Miri"
052fcf10e96f282c0fb50f778150afeaf92bb65d,https://www.semanticscholar.org/paper/052fcf10e96f282c0fb50f778150afeaf92bb65d,Inquiry-based science instruction—what is it and does it matter? Results from a research synthesis years 1984 to 2002,"The goal of the Inquiry Synthesis Project was to synthesize findings from research conducted between 1984 and 2002 to address the research question, What is the impact of inquiry science instruction on K-12 student outcomes? The timeframe of 1984 to 2002 was selected to continue a line of synthesis work last completed in 1983 by Bredderman (Bredderman (1983) Review of Educational Research 53: 499-518) and Shymansky, Kyle, and Alport (Shymansky et al. (1983) Journal of Research in Science Teaching 20: 387-404), and to accommodate a practicable cut- off date given the research project timeline, which ran from 2001 to 2006. The research question for the project was addressed by developing a conceptual framework that clarifies and specifies what is meant by ''inquiry-based science instruction,'' and by using a mixed-methodology approach to analyze both numerical and text data describing the impact of instruction on K-12 student science conceptual learning. Various findings across 138 analyzed studies indicate a clear, positive trend favoring inquiry-based instructional practices, particularly instruction that emphasizes student active thinking and drawing conclusions from data. Teaching strategies that actively engage students in the learning process through scientific investigations are more likely to increase conceptual understanding than are strategies that rely on more passive techniques, which are often necessary in the current standardized-assessment laden educational environment.",2010.0,"D. Minner, Abigail Jurist Levy, J. Century"
825725943fe1774b1b490d69094ba6269cc9c6b2,https://www.semanticscholar.org/paper/825725943fe1774b1b490d69094ba6269cc9c6b2,SciMAT: A new science mapping analysis software tool,"This article presents a new open-source software tool, SciMAT, which performs science mapping analysis within a longitudinal framework. It provides different modules that help the analyst to carry out all the steps of the science mapping workflow. In addition, SciMAT presents three key features that are remarkable in respect to other science mapping software tools: (a) a powerful preprocessing module to clean the raw bibliographical data, (b) the use of bibliometric measures to study the impact of each studied element, and (c) a wizard to configure the analysis. © 2012 Wiley Periodicals, Inc.",2012.0,"M. Cobo, A. G. López-Herrera, E. Herrera-Viedma, F. Herrera"
e0b16bb5d747d61a0f689a73354a5736a909378d,https://www.semanticscholar.org/paper/e0b16bb5d747d61a0f689a73354a5736a909378d,Spatio-Temporal Data Mining,"Large volumes of spatio-temporal data are increasingly collected and studied in diverse domains, including climate science, social sciences, neuroscience, epidemiology, transportation, mobile health, and Earth sciences. Spatio-temporal data differ from relational data for which computational approaches are developed in the data-mining community for multiple decades in that both spatial and temporal attributes are available in addition to the actual measurements/attributes. The presence of these attributes introduces additional challenges that needs to be dealt with. Approaches for mining spatio-temporal data have been studied for over a decade in the data-mining community. In this article, we present a broad survey of this relatively young field of spatio-temporal data mining. We discuss different types of spatio-temporal data and the relevant data-mining questions that arise in the context of analyzing each of these datasets. Based on the nature of the data-mining problem studied, we classify literature on spatio-temporal data mining into six major categories: clustering, predictive learning, change detection, frequent pattern mining, anomaly detection, and relationship mining. We discuss the various forms of spatio-temporal data-mining problems in each of these categories.",2017.0,"G. Atluri, A. Karpatne, Vipin Kumar"
851a4c4e9d9bf8f023bc4cd29e023e4c43957b7d,https://www.semanticscholar.org/paper/851a4c4e9d9bf8f023bc4cd29e023e4c43957b7d,The Art and Science of Data-Driven Journalism,"Journalists have been using data in their stories for as long as the profession has existed. A revolution in computing in the 20th century created opportunities for data integration into investigations, as journalists began to bring technology into their work. In the 21st century, a revolution in connectivity is leading the media toward new horizons. The Internet, cloud computing, agile development, mobile devices, and open source software have transformed the practice of journalism, leading to the emergence of a new term: data journalism. Although journalists have been using data in their stories for as long as they have been engaged in reporting, data journalism is more than traditional journalism with more data. Decades after early pioneers successfully applied computer-assisted reporting and social science to investigative journalism, journalists are creating news apps and interactive features that help people understand data, explore it, and act upon the insights derived from it. New business models are emerging in which data is a raw material for profit, impact, and insight, co-created with an audience that was formerly reduced to passive consumption. Journalists around the world are grappling with the excitement and the challenge of telling compelling stories by harnessing the vast quantity of data that our increasingly networked lives, devices, businesses, and governments produce every day. While the potential of data journalism is immense, the pitfalls and challenges to its adoption throughout the media are similarly significant, from digital literacy to competition for scarce resources in newsrooms. Global threats to press freedom, digital security, and limited access to data create difficult working conditions for journalists in many countries. A combination of peer-to-peer learning, mentorship, online training, open data initiatives, and new programs at journalism schools rising to the challenge, however, offer reasons to be optimistic about more journalists learning to treat data as a source.",2014.0,A. Howard
c984f429334fb6d20041cf3959084c19ccdc27b8,https://www.semanticscholar.org/paper/c984f429334fb6d20041cf3959084c19ccdc27b8,"A Framework for Articulating and Measuring Individual Learning Outcomes
 from Participation in Citizen Science","Since first being introduced in the mid 1990s, the term “citizen science”—the intentional engagement of the public in scientific research—has seen phenomenal growth as measured by the number of projects developed, people involved, and articles published. In addition to contributing to scientific knowledge, many citizen science projects attempt to achieve learning outcomes among their participants, however, little guidance is available for practitioners regarding the types of learning that can be supported through citizen science or the measuring of learning outcomes. This study provides empirical data to understand how intended learning outcomes first described by the informal science education field have been employed and measured within the citizen science field. We also present a framework for describing learning outcomes that should help citizen science practitioners, researchers, and evaluators in designing projects and in studying and evaluating their impacts. This is a first step in building evaluation capacity across the field of citizen science.",2018.0,"T. Phillips, Norman Porticella, M. Constas, R. Bonney"
6f0ec43983a6e3aedd6e297fa94871116b401a07,https://www.semanticscholar.org/paper/6f0ec43983a6e3aedd6e297fa94871116b401a07,BIG DATA ANALYTICS AND PRECISION ANIMAL AGRICULTURE SYMPOSIUM: Machine learning and data mining advance predictive big data analysis in precision animal agriculture,"Abstract Precision animal agriculture is poised to rise to prominence in the livestock enterprise in the domains of management, production, welfare, sustainability, health surveillance, and environmental footprint. Considerable progress has been made in the use of tools to routinely monitor and collect information from animals and farms in a less laborious manner than before. These efforts have enabled the animal sciences to embark on information technology-driven discoveries to improve animal agriculture. However, the growing amount and complexity of data generated by fully automated, high-throughput data recording or phenotyping platforms, including digital images, sensor and sound data, unmanned systems, and information obtained from real-time noninvasive computer vision, pose challenges to the successful implementation of precision animal agriculture. The emerging fields of machine learning and data mining are expected to be instrumental in helping meet the daunting challenges facing global agriculture. Yet, their impact and potential in “big data” analysis have not been adequately appreciated in the animal science community, where this recognition has remained only fragmentary. To address such knowledge gaps, this article outlines a framework for machine learning and data mining and offers a glimpse into how they can be applied to solve pressing problems in animal sciences.",2018.0,"G. Morota, R. Ventura, F. F. Silva, M. Koyama, C. Samodha, Fernando"
8cd71d704f9d3eeb5eb697e412ba54b680f00636,https://www.semanticscholar.org/paper/8cd71d704f9d3eeb5eb697e412ba54b680f00636,Big Data and Clinicians: A Review on the State of the Science,"Background In the past few decades, medically related data collection saw a huge increase, referred to as big data. These huge datasets bring challenges in storage, processing, and analysis. In clinical medicine, big data is expected to play an important role in identifying causality of patient symptoms, in predicting hazards of disease incidence or reoccurrence, and in improving primary-care quality. Objective The objective of this review was to provide an overview of the features of clinical big data, describe a few commonly employed computational algorithms, statistical methods, and software toolkits for data manipulation and analysis, and discuss the challenges and limitations in this realm. Methods We conducted a literature review to identify studies on big data in medicine, especially clinical medicine. We used different combinations of keywords to search PubMed, Science Direct, Web of Knowledge, and Google Scholar for literature of interest from the past 10 years. Results This paper reviewed studies that analyzed clinical big data and discussed issues related to storage and analysis of this type of data. Conclusions Big data is becoming a common feature of biological and clinical studies. Researchers who use clinical big data face multiple challenges, and the data itself has limitations. It is imperative that methodologies for data analysis keep pace with our ability to collect and store data.",2014.0,"Weiqi Wang, E. Krishnan"
a3324c0dcb1efaf5d88003b3fe22a3351b4c16da,https://www.semanticscholar.org/paper/a3324c0dcb1efaf5d88003b3fe22a3351b4c16da,"""Big Data"" : big gaps of knowledge in the field of internet science","Research on so-called ‘Big Data’ has received a considerable momentum and is expected to grow in the future. One very interesting stream of research on Big Data analyzes online networks. Many online networks are known to have some typical macro-characteristics, such as ‘small world’ properties. Much less is known about underlying micro-processes leading to these properties. The models used by Big Data researchers usually are inspired by mathematical ease of exposition. We propose to follow in addition a different strategy that leads to knowledge about micro-processes that match with actual online behavior. This knowledge can then be used for the selection of mathematically-tractable models of online network formation and evolution. Insight from social and behavioral research is needed for pursuing this strategy of knowledge generation about micro-processes. Accordingly, our proposal points to a unique role that social scientists could play in Big Data research.",2012.0,"Chris J. Snijders, U. Matzat, Ulf-Dietrich Reips"
edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,https://www.semanticscholar.org/paper/edf27bb5272ea6fe244deb3bbc8da0429bfe3ac5,The reusable holdout: Preserving validity in adaptive data analysis,"Testing hypotheses privately Large data sets offer a vast scope for testing already-formulated ideas and exploring new ones. Unfortunately, researchers who attempt to do both on the same data set run the risk of making false discoveries, even when testing and exploration are carried out on distinct subsets of data. Based on ideas drawn from differential privacy, Dwork et al. now provide a theoretical solution. Ideas are tested against aggregate information, whereas individual data set components remain confidential. Preserving that privacy also preserves statistical inference validity. Science, this issue p. 636 A statistical approach allows large data sets to be reanalyzed to test new hypotheses. Misapplication of statistical data analysis is a common cause of spurious discoveries in scientific research. Existing approaches to ensuring the validity of inferences drawn from data assume a fixed procedure to be performed, selected before the data are examined. In common practice, however, data analysis is an intrinsically adaptive process, with new analyses generated on the basis of data exploration, as well as the results of previous analyses on the same data. We demonstrate a new approach for addressing the challenges of adaptivity based on insights from privacy-preserving data analysis. As an application, we show how to safely reuse a holdout data set many times to validate the results of adaptively chosen analyses.",2015.0,"C. Dwork, V. Feldman, Moritz Hardt, T. Pitassi, Omer Reingold, Aaron Roth"
9355e60deaad86d1efea4b7767dd77103d647f37,https://www.semanticscholar.org/paper/9355e60deaad86d1efea4b7767dd77103d647f37,Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,"It is our great pleasure to welcome you to the 2016 ACM Conference on Knowledge Discovery and Data Mining -- KDD'16. We hope that the content and the professional network at KDD'16 will help you succeed professionally by enabling you to: identify technology trends early; make new/creative contributions; increase your productivity by using newer/better tools, processes or ways of organizing teams; identify new job opportunities; and hire new team members. 
 
We are living in an exciting time for our profession. On the one hand, we are witnessing the industrialization of data science, and the emergence of the industrial assembly line processes characterized by the division of labor, integrated processes/pipelines of work, standards, automation, and repeatability. Data science practitioners are organizing themselves in more sophisticated ways, embedding themselves in larger teams in many industry verticals, improving their productivity substantially, and achieving a much larger scale of social impact. On the other hand we are also witnessing astonishing progress from research in algorithms and systems -- for example the field of deep neural networks has revolutionized speech recognition, NLP, computer vision, image recognition, etc. By facilitating interaction between practitioners at large companies & startups on the one hand, and the algorithm development researchers including leading academics on the other, KDD'16 fosters technological and entrepreneurial innovation in the area of data science. 
 
This year's conference continues its tradition of being the premier forum for presentation of results in the field of data mining, both in the form of cutting edge research, and in the form of insights from the development and deployment of real world applications. Further, the conference continues with its tradition of a strong tutorial and workshop program on leading edge issues of data mining. The mission of this conference has broadened in recent years even as we placed a significant amount of focus on both the research and applied aspects of data mining. As an example of this broadened focus, this year we have introduced a strong hands-on tutorial program nduring the conference in which participants will learn how to use practical tools for data mining. KDD'16 also gives researchers and practitioners a unique opportunity to form professional networks, and to share their perspectives with others interested in the various aspects of data mining. For example, we have introduced office hours for budding entrepreneurs from our community to meet leading Venture Capitalists investing in this area. We hope that KDD 2016 conference will serve as a meeting ground for researchers, practitioners, funding agencies, and investors to help create new algorithms and commercial products. 
 
The call for papers attracted a significant number of submissions from countries all over the world. In particular, the research track attracted 784 submissions and the applied data science track attracted 331 submissions. Papers were accepted either as full papers or as posters. The overall acceptance rate either as full papers or posters was less than 20%. For full papers in the research track, the acceptance rate was lower than 10%. This is consistent with the fact that the KDD Conference is a premier conference in data mining and the acceptance rates historically tend to be low. It is noteworthy that the applied data science track received a larger number of submissions compared to previous years. We view this as an encouraging sign that research in data mining is increasingly becoming relevant to industrial applications. All papers were reviewed by at least three program committee members and then discussed by the PC members in a discussion moderated by a meta-reviewer. Borderline papers were thoroughly reviewed by the program chairs before final decisions were made.",2016.0,"Balaji Krishnapuram, Mohak Shah, Alex Smola, C. Aggarwal, Dou Shen, R. Rastogi"
832139bd87f51f0a173b5bd9255944748bc31a96,https://www.semanticscholar.org/paper/832139bd87f51f0a173b5bd9255944748bc31a96,Global multi-resolution terrain elevation data 2010 (GMTED2010),"For more information on the USGS—the Federal source for science about the Earth, its natural and living resources, natural hazards, and the environment, visit http://www.usgs.gov or call 1–888–ASK–USGS. For an overview of USGS information products, including maps, imagery, and publications, Any use of trade, product, or firm names is for descriptive purposes only and does not imply endorsement by the U.S. Government. Although this report is in the public domain, permission must be secured from the individual copyright owners to reproduce any copyrighted materials contained within this report. 10. Diagram showing the GMTED2010 layer extents (minimum and maximum latitude and longitude) are a result of the coordinate system inherited from the 1-arc-second SRTM",2011.0,"J. Danielson, D. Gesch"
2592aa0de9955a5b5bfc0039387dacb5874a1107,https://www.semanticscholar.org/paper/2592aa0de9955a5b5bfc0039387dacb5874a1107,Conscientious Classification: A Data Scientist's Guide to Discrimination-Aware Classification,"Recent research has helped to cultivate growing awareness that machine-learning systems fueled by big data can create or exacerbate troubling disparities in society. Much of this research comes from outside of the practicing data science community, leaving its members with little concrete guidance to proactively address these concerns. This article introduces issues of discrimination to the data science community on its own terms. In it, we tour the familiar data-mining process while providing a taxonomy of common practices that have the potential to produce unintended discrimination. We also survey how discrimination is commonly measured, and suggest how familiar development processes can be augmented to mitigate systems' discriminatory potential. We advocate that data scientists should be intentional about modeling and reducing discriminatory outcomes. Without doing so, their efforts will result in perpetuating any systemic discrimination that may exist, but under a misleading veil of data-driven objectivity.",2017.0,"Brian d'Alessandro, Cathy O'Neil, T. LaGatta"
538eb48ad02761ed8423b734cfc61582c000167b,https://www.semanticscholar.org/paper/538eb48ad02761ed8423b734cfc61582c000167b,"ICES: Data, Discovery, Better Health","ICES was founded in 1992 to study the health care system in Ontario and promote effective, efficient and equitable health care. Over 27 years later, the goal remains largely unchanged, though the institute has grown in size and impact. Originally known as the Institute for Clinical Evaluative Sciences, ICES was created as an independent not-for-profit research institute and given what was, at the time, unprecedented access to administrative health data records for the population of Ontario. ICES’ initial focus was to better understand the delivery of hospital services and translate its findings into better health care and policy. From modest beginnings with a handful of researchers located in a few hospital offices, ICES has grown to encompass a community of almost 500 scientists and staff across a network of seven physical sites in Ontario. The original focus on hospital-based services has expanded significantly and now includes research and analysis of community-based health services, health policy, Indigenous health, social determinants of health, and data science.",2020.0,"Michael J. Schull, Mahmoud Azimaee, Marcel Marra, Rosario G. Cartagena, Marian J Vermeulen, Minnie M Ho, Astrid Guttmann"
0d7a9a5233b1460941b51a50e032b3c5d3a711cc,https://www.semanticscholar.org/paper/0d7a9a5233b1460941b51a50e032b3c5d3a711cc,The Interview: Data Collection in Descriptive Phenomenological Human Scientific Research*,"Abstract In this article, interviewing from a descriptive, phenomenological, human scientific perspective is examined. Methodological issues are raised in relation to evaluative criteria as well as reflective matters that concern the phenomenological researcher. The data collection issues covered are 1) the selection of participants, 2) the number of participants in a study, 3) the interviewer and the questions, and 4) data collection procedures. Certain conclusions were drawn indicating that phenomenological research methods cannot be evaluated on the basis of an empiricist theory of science, but must be critiqued from within a phenomenological theory of science. Some reflective matters, experienced by the phenomenological researcher, are also elaborated upon.",2012.0,Magnus Englander
1fae0c586f66a9f08fdc76b8bf38ff27a7a08dc4,https://www.semanticscholar.org/paper/1fae0c586f66a9f08fdc76b8bf38ff27a7a08dc4,Enhancing the quality of argumentation in school science,"The research reported in this paper focussed on the design of learning environments that support the teaching and learning of argumentation in a scientific context. The research took place over two years between 1999 and 2001 in junior high schools in the greater London area. The research was conducted in two phases. In the first developmental phase, working with a group of 12 science teachers, the main emphasis was to develop sets of materials and strategies to support argumentation in the classroom and to assess teachers‘ development with teaching argumentation. Data were collected by videoing and audio recording the teachers attempts to implement these lessons at the beginning and end of the year. During this phase, analytical tools for evaluating the quality of argumentation were developed based on Toulmin‘s argument pattern. Analysis of the data shows that there was significant development in the majority of teachers use of argumentation across the year. Results indicate that the pattern of use of argumentation is teacher specific, as is the nature of the change. In the second phase of the project, teachers taught the experimental groups a minimum of nine lessons which involved socioscientific or scientific argumentation. In addition, these teachers taught similar lessons to a control group at the beginning and end of the year. Here the emphasis lay on assessing the progression in student capabilities with argumentation. Hence data were collected from several lessons of two groups of students engaging in argumentation. Using a framework for evaluating the nature of the discourse and its quality, the findings show that there was an improvement in the quality of students‘ argumentation. In addition, the research offers methodological developments for work in this field.",2004.0,"J. Osborne, S. Erduran, S. Simon"
40e1ac2c42a7c9c1aeb80d3c0dab6de55b40eddd,https://www.semanticscholar.org/paper/40e1ac2c42a7c9c1aeb80d3c0dab6de55b40eddd,The conundrum of sharing research data,"We must all accept that science is data and that data are science, and thus provide for, and justify the need for the support of, much-improved data curation. (Hanson, Sugden, & Alberts) 
 
Researchers are producing an unprecedented deluge of data by using new methods and instrumentation. Others may wish to mine these data for new discoveries and innovations. However, research data are not readily available as sharing is common in only a few fields such as astronomy and genomics. Data sharing practices in other fields vary widely. Moreover, research data take many forms, are handled in many ways, using many approaches, and often are difficult to interpret once removed from their initial context. Data sharing is thus a conundrum. Four rationales for sharing data are examined, drawing examples from the sciences, social sciences, and humanities: (1) to reproduce or to verify research, (2) to make results of publicly funded research available to the public, (3) to enable others to ask new questions of extant data, and (4) to advance the state of research and innovation. These rationales differ by the arguments for sharing, by beneficiaries, and by the motivations and incentives of the many stakeholders involved. The challenges are to understand which data might be shared, by whom, with whom, under what conditions, why, and to what effects. Answers will inform data policy and practice. © 2012 Wiley Periodicals, Inc.",2012.0,C. Borgman
eb286565b6a18e21b9daf5375c75d56513cd2853,https://www.semanticscholar.org/paper/eb286565b6a18e21b9daf5375c75d56513cd2853,Big Earth data: A new frontier in Earth and information sciences,"Abstract Big data is a revolutionary innovation that has allowed the development of many new methods in scientific research. This new way of thinking has encouraged the pursuit of new discoveries. Big data occupies the strategic high ground in the era of knowledge economies and also constitutes a new national and global strategic resource. “Big Earth data”, derived from, but not limited to, Earth observation has macro-level capabilities that enable rapid and accurate monitoring of the Earth, and is becoming a new frontier contributing to the advancement of Earth science and significant scientific discoveries. Within the context of the development of big data, this paper analyzes the characteristics of scientific big data and recognizes its great potential for development, particularly with regard to the role that big Earth data can play in promoting the development of Earth science. On this basis, the paper outlines the Big Earth Data Science Engineering Project (CASEarth) of the Chinese Academy of Sciences Strategic Priority Research Program. Big data is at the forefront of the integration of geoscience, information science, and space science and technology, and it is expected that big Earth data will provide new prospects for the development of Earth science.",2017.0,Huadong Guo
05859c8d47b16ce84c817c16d29ad6ec9d1d3a33,https://www.semanticscholar.org/paper/05859c8d47b16ce84c817c16d29ad6ec9d1d3a33,The Science DMZ: A network design pattern for data-intensive science,"The ever-increasing scale of scientific data has become a significant challenge for researchers that rely on networks to interact with remote computing systems and transfer results to collaborators worldwide. Despite the availability of high-capacity connections, scientists struggle with inadequate cyberinfrastructure that cripples data transfer performance, and impedes scientific progress. The Science DMZ paradigm comprises a proven set of network design patterns that collectively address these problems for scientists. We explain the Science DMZ model, including network architecture, system configuration, cybersecurity, and performance tools, that creates an optimized network environment for science. We describe use cases from universities, supercomputing centers and research laboratories, highlighting the effectiveness of the Science DMZ model in diverse operational settings. In all, the Science DMZ model is a solid platform that supports any science workflow, and flexibly accommodates emerging network technologies. As a result, the Science DMZ vastly improves collaboration, accelerating scientific discovery.",2013.0,"E. Dart, Lauren Rotman, B. Tierney, Mary Hester, J. Zurawski"
112f5b6f640efcc03edef5ec2ab87194b80dbd24,https://www.semanticscholar.org/paper/112f5b6f640efcc03edef5ec2ab87194b80dbd24,USING TRADITIONAL ECOLOGICAL KNOWLEDGE IN SCIENCE: METHODS AND APPLICATIONS,"Advocates of Traditional Ecological Knowledge (TEK) have promoted its use in scientific research, impact assessment, and ecological understanding. While several examples illustrate the utility of applying TEK in these contexts, wider application of TEK- derived information remains elusive. In part, this is due to continued inertia in favor of established scientific practices and the need to describe TEK in Western scientific terms. In part, it is also due to the difficulty of accessing TEK, which is rarely written down and must in most cases be documented as a project on its own prior to its incorporation into another scientific undertaking. This formidable practical obstacle is exacerbated by the need to use social science methods to gather biological data, so that TEK research and application becomes a multidisciplinary undertaking. By examining case studies involving bowhead whales, beluga whales, and herring, this paper describes some of the benefits of using TEK in scientific and management contexts. It also reviews some of the methods that are available to do so, including semi-directive interviews, questionnaires, facilitated workshops, and collaborative field projects.",2000.0,H. Huntington
fe446b7f9e475ca4627f7b1ab44631a28db78813,https://www.semanticscholar.org/paper/fe446b7f9e475ca4627f7b1ab44631a28db78813,Competing on Analytics: The New Science of Winning,"You have more information at hand about your business environment than ever before. But are you using it to ""out-think"" your rivals? If not, you may be missing out on a potent competitive tool. In ""Competing on Analytics: The New Science of Winning"" , Thomas H. Davenport and Jeanne G. Harris argue that the frontier for using data to make decisions has shifted dramatically. Certain high-performing enterprises are now building their competitive strategies around data-driven insights that in turn generate impressive business results. Their secret weapon: Analytics: sophisticated quantitative and statistical analysis and predictive modeling. Exemplars of analytics are using new tools to identify their most profitable customers and offer them the right price, to accelerate product innovation, to optimize supply chains, and to identify the true drivers of financial performance. A wealth of examples - from organizations as diverse as Amazon, Barclay's, Capital One, Harrah's, Procter & Gamble, Wachovia, and the Boston Red Sox - illuminate how to leverage the power of analytics.",2007.0,"T. Davenport, Jeanne Harris"
70ce1a955ed9fc08c427c03c15d9a04e5ce7a9bd,https://www.semanticscholar.org/paper/70ce1a955ed9fc08c427c03c15d9a04e5ce7a9bd,Causal inference from observational data.,"Randomized controlled trials have long been considered the 'gold standard' for causal inference in clinical research. In the absence of randomized experiments, identification of reliable intervention points to improve oral health is often perceived as a challenge. But other fields of science, such as social science, have always been challenged by ethical constraints to conducting randomized controlled trials. Methods have been established to make causal inference using observational data, and these methods are becoming increasingly relevant in clinical medicine, health policy and public health research. This study provides an overview of state-of-the-art methods specifically designed for causal inference in observational data, including difference-in-differences (DiD) analyses, instrumental variables (IV), regression discontinuity designs (RDD) and fixed-effects panel data analysis. The described methods may be particularly useful in dental research, not least because of the increasing availability of routinely collected administrative data and electronic health records ('big data').",2016.0,"S. Listl, Hendrik Jürges, R. Watt"
7657d17d796659427e067f1cac93e4038d01725f,https://www.semanticscholar.org/paper/7657d17d796659427e067f1cac93e4038d01725f,Big Scholarly Data: A Survey,"With the rapid growth of digital publishing, harvesting, managing, and analyzing scholarly information have become increasingly challenging. The term Big Scholarly Data is coined for the rapidly growing scholarly data, which contains information including millions of authors, papers, citations, figures, tables, as well as scholarly networks and digital libraries. Nowadays, various scholarly data can be easily accessed and powerful data analysis technologies are being developed, which enable us to look into science itself with a new perspective. In this paper, we examine the background and state of the art of big scholarly data. We first introduce the background of scholarly data management and relevant technologies. Second, we review data analysis methods, such as statistical analysis, social network analysis, and content analysis for dealing with big scholarly data. Finally, we look into representative research issues in this area, including scientific impact evaluation, academic recommendation, and expert finding. For each issue, the background, main challenges, and latest research are covered. These discussions aim to provide a comprehensive review of this emerging area. This survey paper concludes with a discussion of open issues and promising future directions.",2017.0,"Feng Xia, Wei Wang, T. M. Bekele, Huan Liu"
306d35707288caafb27e4c81d33eb83c0384f204,https://www.semanticscholar.org/paper/306d35707288caafb27e4c81d33eb83c0384f204,"Statistics : methods and applications : a comprehensive reference for science, industry, and data mining",Elementary concepts in statistics -- Basic statistics and tables -- ANOVA/MANOVA -- Association rules -- Boosting trees -- Canonical analysis -- CHAID analysis -- Classification and regression trees (CART) -- Classification trees -- Cluster analysis -- Correspondence analysis -- Data mining techniques -- Discriminant function analysis -- Distribution fitting -- Experimental design (Industrial DOE) -- Factor analysis and principal components -- General discrimination analysis (GDA) -- General linear models (GLM) -- General regression models (GRM) -- Generalized additive models (GAM) -- Generalized linear/nonlinear models (GLZ) -- Log linear analysis of frequency tables -- Machine learning -- Multivariate adaptive regression splines (MARSplines) -- Multidimensional scaling (MDS) -- Multiple linear regression -- Neural networks -- Nonlinear estimation -- Nonparametric statistics -- Partial least squares (PLS) -- Power analysis -- Process analysis -- Quality control charts -- Reliabilty/item analysis -- Structural equation modeling -- Survival/failure time analysis -- Text mining -- Time series/forecasting -- Variance components and mixed model ANOVA/ANCOVA.,2006.0,"Thomas Hill, P. Lewicki"
53b9b242f8cb2007e8e3dd9db5cd11b88fa6c4a7,https://www.semanticscholar.org/paper/53b9b242f8cb2007e8e3dd9db5cd11b88fa6c4a7,Nonparametric Statistics for the Behavioral Sciences,"diabetes statistics cdc ��glucagon megaroll.infoCorn oil, but not cocaine, is a more effective reinforcer Data Analysis of Students Marks with Descriptive StatisticsFriedman test WikipediaDownload Free any eBook PDF, Epub, Tuebl and MobiStatistics (STAT) < University of PennsylvaniaErik Sudderth Donald Bren School of Information and Bootstrapping (statistics) WikipediaRunze Li's Homepage Pennsylvania State UniversityCausal inference in statistics: An overviewFind a Doctor | Clinicians, Researchers & Nurses ETDAUndergraduate Course Descriptions Statistics DepartmentComputation of different effect sizes like d, f, r and Biography and Activities | Susan HolmesFaculty | Department of StatisticsNonparametric Method Definition InvestopediaStatistics Final Exam Flashcards | QuizletTest di Kruskal-Wallis WikipediaDepartment of Statistics and Data Science < Carnegie The use of statistics in social sciences | Emerald InsightBehavioral Genetics Psychology Oxford BibliographiesInterpreting statistics Introduction to statistics G*Power 3: a flexible statistical power analysis program Lifetime Data Analysis | Home SpringerWilcoxon Test Definition InvestopediaGraphPad Prism 9 Statistics Guide Interpreting results Log In BACBNonparametric Tests Boston UniversityTopic #1: Introduction to measurement and statisticsStatistics Assignment Help | Statistics Homework HelpStatistics (STAT) | Iowa State University CatalogEric J. Tchetgen Tchetgen – Department of Statistics and Journals American Statistical AssociationWhat is the rationale behind the magic number 30 in",2022.0,"Donald Bren, E. J. Tchetgen"
7e95c6f943b7c47af1b2ef1651b86022a001ce81,https://www.semanticscholar.org/paper/7e95c6f943b7c47af1b2ef1651b86022a001ce81,A Review of Microsoft Academic Services for Science of Science Studies,"Since the relaunch of Microsoft Academic Services (MAS) 4 years ago, scholarly communications have undergone dramatic changes: more ideas are being exchanged online, more authors are sharing their data, and more software tools used to make discoveries and reproduce the results are being distributed openly. The sheer amount of information available is overwhelming for individual humans to keep up and digest. In the meantime, artificial intelligence (AI) technologies have made great strides and the cost of computing has plummeted to the extent that it has become practical to employ intelligent agents to comprehensively collect and analyze scholarly communications. MAS is one such effort and this paper describes its recent progresses since the last disclosure. As there are plenty of independent studies affirming the effectiveness of MAS, this paper focuses on the use of three key AI technologies that underlies its prowess in capturing scholarly communications with adequate quality and broad coverage: (1) natural language understanding in extracting factoids from individual articles at the web scale, (2) knowledge assisted inference and reasoning in assembling the factoids into a knowledge graph, and (3) a reinforcement learning approach to assessing scholarly importance for entities participating in scholarly communications, called the saliency, that serves both as an analytic and a predictive metric in MAS. These elements enhance the capabilities of MAS in supporting the studies of science of science based on the GOTO principle, i.e., good and open data with transparent and objective methodologies. The current direction of development and how to access the regularly updated data and tools from MAS, including the knowledge graph, a REST API and a website, are also described.",2019.0,"Kuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Darrin Eide, Yuxiao Dong, Junjie Qian, Anshul Kanakia, Alvin Chen, Richard Rogahn"
b2239452680e97c503a90f62ccdc8137a893b1e9,https://www.semanticscholar.org/paper/b2239452680e97c503a90f62ccdc8137a893b1e9,Big Data and quality data for fake news and misinformation detection,"Fake news has become an important topic of research in a variety of disciplines including linguistics and computer science. In this paper, we explain how the problem is approached from the perspective of natural language processing, with the goal of building a system to automatically detect misinformation in news. The main challenge in this line of research is collecting quality data, i.e., instances of fake and real news articles on a balanced distribution of topics. We review available datasets and introduce the MisInfoText repository as a contribution of our lab to the community. We make available the full text of the news articles, together with veracity labels previously assigned based on manual assessment of the articles’ truth content. We also perform a topic modelling experiment to elaborate on the gaps and sources of imbalance in currently available datasets to guide future efforts. We appeal to the community to collect more data and to make it available for research purposes.",2019.0,"Fatemeh Torabi Asr, Maite Taboada"
4a6377c0d1512ee5c780320828699ed19e158323,https://www.semanticscholar.org/paper/4a6377c0d1512ee5c780320828699ed19e158323,Big data and the future of ecology,"The need for sound ecological science has escalated alongside the rise of the information age and “big data” across all sectors of society. Big data generally refer to massive volumes of data not readily handled by the usual data tools and practices and present unprecedented opportunities for advancing science and inform- ing resource management through data-intensive approaches. The era of big data need not be propelled only by “big science” – the term used to describe large-scale efforts that have had mixed success in the individual-driven culture of ecology. Collectively, ecologists already have big data to bolster the scientific effort – a large volume of distributed, high-value information – but many simply fail to contribute. We encourage ecologists to join the larger scientific community in global initiatives to address major scientific and societal problems by bringing their distributed data to the table and harnessing its collective power. The scientists who contribute such information will be at the forefront of socially relevant science – but will they be ecologists?",2013.0,"S. Hampton, C. Strasser, J. Tewksbury, W. Gram, A. Budden, A. Batcheller, C. Duke, J. Porter"
9f14c7b599d3cbd1646215291b5e843d38d42b48,https://www.semanticscholar.org/paper/9f14c7b599d3cbd1646215291b5e843d38d42b48,Cryptography and Data Security,"From the Preface (See Front Matter for full Preface) 
 
Electronic computers have evolved from exiguous experimental enterprises in the 1940s to prolific practical data processing systems in the 1980s. As we have come to rely on these systems to process and store data, we have also come to wonder about their ability to protect valuable data. 
 
Data security is the science and study of methods of protecting data in computer and communication systems from unauthorized disclosure and modification. The goal of this book is to introduce the mathematical principles of data security and to show how these principles apply to operating systems, database systems, and computer networks. The book is for students and professionals seeking an introduction to these principles. There are many references for those who would like to study specific topics further. 
 
Data security has evolved rapidly since 1975. We have seen exciting developments in cryptography: public-key encryption, digital signatures, the Data Encryption Standard (DES), key safeguarding schemes, and key distribution protocols. We have developed techniques for verifying that programs do not leak confidential data, or transmit classified data to users with lower security clearances. We have found new controls for protecting data in statistical databases--and new methods of attacking these databases. We have come to a better understanding of the theoretical and practical limitations to security.",1982.0,D. Denning
17cdb988ac355a292d7f358dd29893aaf229b859,https://www.semanticscholar.org/paper/17cdb988ac355a292d7f358dd29893aaf229b859,Reflexive Accounts and Accounts of Reflexivity in Qualitative Data Analysis,"While the importance of being reflexive is acknowledged within social science research, the difficulties, practicalities and methods of doing it are rarely addressed. Thus, the implications of current theoretical and philosophical discussions about reflexivity, epistemology and the construction of knowledge for empirical sociological research practice, specifically the analysis of qualitative data, remain under-developed. Drawing on our doctoral experiences, we reflect on the possibilities and limits of reflexivity during the interpretive stages of research. We explore how reflexivity can be operationalized and discuss reflexivity in terms of the personal, interpersonal, institutional, pragmatic, emotional, theoretical, epistemological and ontological influences on our research and data analysis processes. We argue that data analysis methods are not just neutral techniques. They reflect, and are imbued with, theoretical, epistemological and ontological assumptions - including conceptions of subjects and subjectivities, and understandings of how knowledge is constructed and produced. In suggesting how epistemological and ontological positionings can be translated into research practice, our article contributes to current debates aiming to bridge the gap between abstract epistemological discussions and the nitty-gritty of research practice.",2003.0,"N. Mauthner, A. Doucet"
8801ce73bea0c97f2d35f5e3bd4f4fdb49698461,https://www.semanticscholar.org/paper/8801ce73bea0c97f2d35f5e3bd4f4fdb49698461,Lessons from lady beetles: accuracy of monitoring data from US and UK citizen-science programs,"Citizen scientists have the potential to play a crucial role in the study of rapidly changing lady beetle (Coccinellidae) populations. We used data derived from three coccinellid-focused citizen-science programs to examine the costs and benefits of data collection from direct citizen-science (data used without verification) and verified citizen-science (observations verified by trained experts) programs. Data collated through direct citizen science overestimated species richness and diversity values in comparison to verified data, thereby influencing interpretation. The use of citizen scientists to collect data also influenced research costs; our analysis shows that verified citizen science was more cost effective than traditional science (in terms of data gathered per dollar). The ability to collect a greater number of samples through direct citizen science may compensate for reduced accuracy, depending on the type of data collected and the type(s) and extent of errors committed by volunteers.",2012.0,"Mary M. Gardiner, L. Allee, P. Brown, John E. Losey, Helen E. Roy, R. Smyth"
62f2a0d676a2260a400c7ba675338958c5e95706,https://www.semanticscholar.org/paper/62f2a0d676a2260a400c7ba675338958c5e95706,Modeling Multilevel Data Structures,"data are becoming quite common in political science and provide numerous opportunities for theory testing and development. Unfortunately this type of data typically generates a number of statistical problems, of which clustering is particularly impor? tant. To exploit the opportunities of? fered by multilevel data, and to solve the statistical problems inherent in them, special statistical techniques are required. In this article, we focus on a technique that has become popular in educational statistics and sociology?multilevel analysis. In multilevel analysis, researchers build models that capture the layered structure of multilevel data, and determine how layers interact and impact a dependent variable of interest. Our objective in this article is to introduce the logic and statistical theory behind multilevel models, to illustrate how such models can be applied fruitfully in political science, and to call atten? tion to some of the pitfalls in multilevel analysis.",2002.0,"M. Steenbergen, Bradford S. Jones"
06fd9101e742ca920c23f3797c19022c13f5a9b5,https://www.semanticscholar.org/paper/06fd9101e742ca920c23f3797c19022c13f5a9b5,"Science friction: Data, metadata, and collaboration","When scientists from two or more disciplines work together on related problems, they often face what we call ‘science friction’. As science becomes more data-driven, collaborative, and interdisciplinary, demand increases for interoperability among data, tools, and services. Metadata – usually viewed simply as ‘data about data’, describing objects such as books, journal articles, or datasets – serve key roles in interoperability. Yet we find that metadata may be a source of friction between scientific collaborators, impeding data sharing. We propose an alternative view of metadata, focusing on its role in an ephemeral process of scientific communication, rather than as an enduring outcome or product. We report examples of highly useful, yet ad hoc, incomplete, loosely structured, and mutable, descriptions of data found in our ethnographic studies of several large projects in the environmental sciences. Based on this evidence, we argue that while metadata products can be powerful resources, usually they must be supplemented with metadata processes. Metadata-as-process suggests the very large role of the ad hoc, the incomplete, and the unfinished in everyday scientific work.",2011.0,"P. N. Edwards, M. Mayernik, A. Batcheller, G. Bowker, C. Borgman"
579c2aad3834e525a90740913fb58a8c8e9ef218,https://www.semanticscholar.org/paper/579c2aad3834e525a90740913fb58a8c8e9ef218,Big Data Methods,"Advances in data science, such as data mining, data visualization, and machine learning, are extremely well-suited to address numerous questions in the organizational sciences given the explosion of available data. Despite these opportunities, few scholars in our field have discussed the specific ways in which the lens of our science should be brought to bear on the topic of big data and big data's reciprocal impact on our science. The purpose of this paper is to provide an overview of the big data phenomenon and its potential for impacting organizational science in both positive and negative ways. We identifying the biggest opportunities afforded by big data along with the biggest obstacles, and we discuss specifically how we think our methods will be most impacted by the data analytics movement. We also provide a list of resources to help interested readers incorporate big data methods into their existing research. Our hope is that we stimulate interest in big data, motivate future research using big data sources, and encourage the application of associated data science techniques more broadly in the organizational sciences.",2018.0,"Scott Tonidandel, E. King, J. Cortina"
f5a64acfd63e9b0ad3e951f638af62b76606c7ca,https://www.semanticscholar.org/paper/f5a64acfd63e9b0ad3e951f638af62b76606c7ca,Data Curation,"In their Top Trends of 2012, the Association of College and Research Libraries (ACRL) named data curation as one of the issues to watch in academic libraries in the near future (ACRL, 2012, p. 312). Data curation can be summarized as “the active and ongoing management of data through its life cycle of interest and usefulness to scholarship, science, and education” (“Data Curation,” 2012, n.p.). Central to the mission of academic public services librarians, participation in data curation allows for easy discovery and retrieval of data over time. While data curation is often associated with scholarly research and institutional repositories, tasks related to data maintenance are not solely relegated to scholarly communications librarians. Public services librarians are also involved in data curation through liaison work, statistics collection, and more. The reviews in this column feature tools that can assist librarians with document management as well as resources that serve as toolkits or planning guides for librarians involved in data curation. The Public Services Quarterly Internet Resources Column is designed to be a clearinghouse for free, online websites; each column will focus on themes relevant to current issues in academic libraries and feature resources designed to make the lives of public services librarians easier. Any comments about the column, including suggestions for themes or recommendations of web resources, can be directed to Melissa Mallon at melissa.mallon@wichita.edu.",2012.0,Melissa Mallon
f41f3ad8c286a8dc509b2fa42b5febe81bcec9d8,https://www.semanticscholar.org/paper/f41f3ad8c286a8dc509b2fa42b5febe81bcec9d8,The Open Knowledge Foundation: Open Data Means Better Science,"Open data leads to better science, but overcoming the barriers to widespread publication and availability of open scientific data requires a community effort. The Open Knowledge Foundation Open Data in Science Working Group describes their role in this movement.",2011.0,Jennifer C. Molloy
1a67f9a4624b4ea989e4ea9b14ea178a010017c0,https://www.semanticscholar.org/paper/1a67f9a4624b4ea989e4ea9b14ea178a010017c0,Editors’ Introduction to the Special Section on Replicability in Psychological Science,"Is there currently a crisis of confidence in psychological science reflecting an unprecedented level of doubt among practitioners about the reliability of research findings in the field? It would certainly appear that there is. These doubts emerged and grew as a series of unhappy events unfolded in 2011: the Diederik Stapel fraud case (see Stroebe, Postmes, & Spears, 2012, this issue), the publication in a major social psychology journal of an article purporting to show evidence of extrasensory perception (Bem, 2011) followed by widespread public mockery (see Galak, LeBoeuf, Nelson, & Simmons, in press; Wagenmakers, Wetzels, Borsboom, & van der Maas, 2011), reports by Wicherts and colleagues that psychologists are often unwilling or unable to share their published data for reanalysis (Wicherts, Bakker, & Molenaar, 2011; see also Wicherts, Borsboom, Kats, & Molenaar, 2006), and the publication of an important article in Psychological Science showing how easily researchers can, in the absence of any real effects, nonetheless obtain statistically significant differences through various questionable research practices (QRPs) such as exploring multiple dependent variables or covariates and only reporting these when they yield significant results (Simmons, Nelson, & Simonsohn, 2011). For those psychologists who expected that the embarrassments of 2011 would soon recede into memory, 2012 offered instead a quick plunge from bad to worse, with new indications of outright fraud in the field of social cognition (Simonsohn, 2012), an article in Psychological Science showing that many psychologists admit to engaging in at least some of the QRPs examined by Simmons and colleagues (John, Loewenstein, & Prelec, 2012), troubling new meta-analytic evidence suggesting that the QRPs described by Simmons and colleagues may even be leaving telltale signs visible in the distribution of p values in the psychological literature (Masicampo & Lalande, in press; Simonsohn, 2012), and an acrimonious dust-up in science magazines and blogs centered around the problems some investigators were having in replicating well-known results from the field of social cognition (Bower, 2012; Yong, 2012). Although the very public problems experienced by psychology over this 2-year period are embarrassing to those of us working in the field, some have found comfort in the fact that, over the same period, similar concerns have been arising across the scientific landscape (triggered by revelations that will be described shortly). Some of the suspected causes of unreplicability, such as publication bias (the tendency to publish only positive findings) have been discussed for years; in fact, the phrase file-drawer problem was first coined by a distinguished psychologist several decades ago (Rosenthal, 1979). However, many have speculated that these problems have been exacerbated in recent years as academia reaps the harvest of a hypercompetitive academic climate and an incentive scheme that provides rich rewards for overselling one’s work and few rewards at all for caution and circumspection (see Giner-Sorolla, 2012, this issue). Equally disturbing, investigators seem to be replicating each others’ work even less often than they did in the past, again presumably reflecting an incentive scheme gone askew (a point discussed in several articles in this issue, e.g., Makel, Plucker, & Hegarty, 2012). The frequency with which errors appear in the psychological literature is not presently known, but a number of facts suggest it might be disturbingly high. Ioannidis (2005) has shown through simple mathematical modeling that any scientific field that ignores replication can easily come to the miserable state wherein (as the title of his most famous article puts it) “most published research findings are false” (see also Ioannidis, 2012, this issue, and Pashler & Harris, 2012, this issue). Meanwhile, reports emerging from cancer research have made such grim scenarios seem more plausible: In 2012, several large pharmaceutical companies revealed that their efforts to replicate exciting preclinical findings from published academic studies in cancer biology were only rarely verifying the original results (Begley & Ellis, 2012; see also Osherovich, 2011; Prinz, Schlange, & Asadullah, 2011).",2012.0,"H. Pashler, E. Wagenmakers"
050d2be0b031878282e2bc626ffe31e064679188,https://www.semanticscholar.org/paper/050d2be0b031878282e2bc626ffe31e064679188,Big Social Data Analytics in Journalism and Mass Communication,"This article presents an empirical study that investigated and compared two “big data” text analysis methods: dictionary-based analysis, perhaps the most popular automated analysis approach in social science research, and unsupervised topic modeling (i.e., Latent Dirichlet Allocation [LDA] analysis), one of the most widely used algorithms in the field of computer science and engineering. By applying two “big data” methods to make sense of the same dataset—77 million tweets about the 2012 U.S. presidential election—the study provides a starting point for scholars to evaluate the efficacy and validity of different computer-assisted methods for conducting journalism and mass communication research, especially in the area of political communication.",2016.0,"Lei Guo, Chris J. Vargo, Zixuan Pan, Weicong Ding, P. Ishwar"
995ac6673fffac3d0f209077bab3ed77c8d69c88,https://www.semanticscholar.org/paper/995ac6673fffac3d0f209077bab3ed77c8d69c88,Crowd-sourced Text Analysis: Reproducible and Agile Production of Political Data,"Empirical social science often relies on data that are not observed in the field, but are transformed into quantitative variables by expert researchers who analyze and interpret qualitative raw sources. While generally considered the most valid way to produce data, this expert-driven process is inherently difficult to replicate or to assess on grounds of reliability. Using crowd-sourcing to distribute text for reading and interpretation by massive numbers of nonexperts, we generate results comparable to those using experts to read and interpret the same texts, but do so far more quickly and flexibly. Crucially, the data we collect can be reproduced and extended transparently, making crowd-sourced datasets intrinsically reproducible. This focuses researchers’ attention on the fundamental scientific objective of specifying reliable and replicable methods for collecting the data needed, rather than on the content of any particular dataset. We also show that our approach works straightforwardly with different types of political text, written in different languages. While findings reported here concern text analysis, they have far-reaching implications for expert-generated data in the social sciences.",2016.0,"K. Benoit, D. Conway, Benjamin E. Lauderdale, M. Laver, Slava Jankin"
ab55a0836880293c5e5e7b1ade89bb8f3106b11b,https://www.semanticscholar.org/paper/ab55a0836880293c5e5e7b1ade89bb8f3106b11b,Community-driven data analysis training for biology,"The primary problem with the explosion of biomedical datasets is not the data itself, not computational resources, and not the required storage space, but the general lack of trained and skilled researchers to manipulate and analyze these data. Eliminating this problem requires development of comprehensive educational resources. Here we present a community-driven framework that enables modern, interactive teaching of data analytics in life sciences and facilitates the development of training materials. The key feature of our system is that it is not a static but a continuously improved collection of tutorials. By coupling tutorials with a web-based analysis framework, biomedical researchers can learn by performing computation themselves through a web-browser without the need to install software or search for example datasets. Our ultimate goal is to expand the breadth of training materials to include fundamental statistical and data science topics and to precipitate a complete re-engineering of undergraduate and graduate curricula in life sciences.",2017.0,"Bérénice Batut, Saskia D. Hiltemann, Andrea Bagnacani, D. Baker, Vivek Bhardwaj, Clemens Blank, A. Bretaudeau, Loraine Brillet-Guéguen, Martin Čech, J. Chilton, Dave Clements, Olivia Doppelt-Azeroual, Anika Erxleben, M. Freeberg, Simon L. Gladman, Y. Hoogstrate, H. Hotz, Torsten Houwaart, P. Jagtap, D. Larivière, Gildas Le Corguillé, T. Manke, Fabien Mareuil, Fidel Ramírez, D. Ryan, F. C. Sigloch, N. Soranzo, Joachim Wolff, Pavankumar Videm, M. Wolfien, Aisanjiang Wubuli, Dilmurat Yusuf, R. Backofen, A. Nekrutenko, Bjürn Gröning"
5952a9f10ef65983042794369d376e23d2682d7e,https://www.semanticscholar.org/paper/5952a9f10ef65983042794369d376e23d2682d7e,Openness in Political Science: Data Access and Research Transparency,"In 2012, the American Political Science Association (APSA) Council adopted new policies guiding data access and research transparency in political science. The policies appear as a revision to APSA's Guide to Professional Ethics in Political Science. The revisions were the product of an extended and broad consultation with a variety of APSA committees and the association's membership.",2013.0,"A. Lupia, Colin Elman"
1f53b2c6428ca6b4484b201e14b56d41db5c5ce1,https://www.semanticscholar.org/paper/1f53b2c6428ca6b4484b201e14b56d41db5c5ce1,Prognostics and Health Management: A Review on Data Driven Approaches,"Prognostics and health management (PHM) is a framework that offers comprehensive yet individualized solutions for managing system health. In recent years, PHM has emerged as an essential approach for achieving competitive advantages in the global market by improving reliability, maintainability, safety, and affordability. Concepts and components in PHM have been developed separately in many areas such as mechanical engineering, electrical engineering, and statistical science, under varied names. In this paper, we provide a concise review of mainstream methods in major aspects of the PHM framework, including the updated research from both statistical science and engineering, with a focus on data-driven approaches. Real world examples have been provided to illustrate the implementation of PHM in practice.",2015.0,"K. Tsui, Nan Chen, Qiang Zhou, Yizhen Hai, Wenbin Wang"
f7ddf5129b392cc23411c2ca28ece3bc0ad682d6,https://www.semanticscholar.org/paper/f7ddf5129b392cc23411c2ca28ece3bc0ad682d6,A Survey Of Blockchain Security Issues And Challenges,"Proceedings of Sixth International Congress on Information and Communication TechnologyBlockchain for BusinessArtificial IntelligenceBlockchain TechnologyHandbook of Research on Blockchain TechnologyResearch Anthology on Blockchain Technology in Business, Healthcare, Education, and GovernmentBlockchain for Cybersecurity and PrivacyBlockchain Cybersecurity, Trust and PrivacyTransforming Businesses With Bitcoin Mining and Blockchain ApplicationsPrinciples of Security and TrustProceedings of the Tenth International Conference on Soft Computing and Pattern Recognition (SoCPaR 2018)Blockchain Technology and the Internet of ThingsEnabling Blockchain Technology for Secure Networking and CommunicationsCommunications and NetworkingBlockchain for Smart CitiesProceedings of International Conference on Computational Intelligence, Data Science and Cloud ComputingCross-Industry Use of Blockchain Technology and Opportunities for the FutureLarge-Scale Data Streaming, Processing, and Blockchain SecurityBlockchain for Distributed Systems SecurityAdvances in Data Science, Cyber Security and IT ApplicationsRole of Blockchain Technology in IoT ApplicationsConvergence of Internet of Things and Blockchain TechnologiesComputational Intelligence in Pattern RecognitionBlockchains for Network SecurityBlockchain and AI Technology in the Industrial Internet of ThingsHandbook of Research on Cyber Crime and Information PrivacyCross-Industry Use of Blockchain Technology and Opportunities for the FutureBitcoin and Blockchain Security6G Mobile Wireless NetworksWeb, Artificial Intelligence and Network ApplicationsCommercializing BlockchainBlockchain Security in Cloud ComputingPractical CryptographyBlockchain and Trustworthy SystemsHands-On Cybersecurity with Blockchain2019 International Conference on System Science and Engineering (ICSSE)Smart BlockchainBlockchain Applications in IoT SecurityBlockchain in the Industrial Internet of ThingsBlockchain Technology for Data Privacy Management",2022.0,
f92dfb398cea359297e2fac4f8ce7ac316158d9e,https://www.semanticscholar.org/paper/f92dfb398cea359297e2fac4f8ce7ac316158d9e,On the Reuse of Scientific Data,"While science policy promotes data sharing and open data, these are not ends in themselves. Arguments for data sharing are to reproduce research, to make public assets available to the public, to leverage investments in research, and to advance research and innovation. To achieve these expected benefits of data sharing, data must actually be reused by others. Data sharing practices, especially motivations and incentives, have received far more study than has data reuse, perhaps because of the array of contested concepts on which reuse rests and the disparate contexts in which it occurs. Here we explicate concepts of data, sharing, and open data as a means to examine data reuse. We explore distinctions between use and reuse of data. Lastly we propose six research questions on data reuse worthy of pursuit by the community: How can uses of data be distinguished from reuses? When is reproducibility an essential goal? When is data integration an essential goal? What are the tradeoffs between collecting new data and reusing existing data? How do motivations for data collection influence the ability to reuse data? How do standards and formats for data release influence reuse opportunities? We conclude by summarizing the implications of these questions for science policy and for investments in data reuse.",2017.0,"Irene V. Pasquetto, Bernadette M. Randles, C. Borgman"
abd6905fec5e1323dedb1311dbb7885e836c1877,https://www.semanticscholar.org/paper/abd6905fec5e1323dedb1311dbb7885e836c1877,Response to Comment on “Estimating the reproducibility of psychological science”,"Gilbert et al. conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.",2016.0,"Christopher J. Anderson, Š. Bahník, M. Barnett-Cowan, Frank Bosco, Jesse J. Chandler, Christopher R. Chartier, F. Cheung, Cody Daniel Christopherson, A. Cordes, E. Cremata, Nicolás Della Penna, Vivien Estel, A. Fedor, Stanka A. Fitneva, Michael C. Frank, J. Grange, Joshua K. Hartshorne, F. Hasselman, Felix Henninger, M. Hulst, K. Jonas, Calvin K. Lai, C. Levitan, Jeremy K. Miller, K. Moore, Johannes M. Meixner, M. Munafo, K. Neijenhuijs, G. Nilsonne, Brian A. Nosek, F. Plessow, J. Prenoveau, Ashley A. Ricker, Kathleen Schmidt, Jeffrey R. Spies, S. Stieger, Nina Strohminger, G. Sullivan, R. V. Aert, M. V. Assen, Wolf Vanpaemel, M. Vianello, M. Voracek, Kellylynn Zuni"
617df497461f76861ebba1b7ef926fb21a81e13a,https://www.semanticscholar.org/paper/617df497461f76861ebba1b7ef926fb21a81e13a,Science as a Map in Technological Search,"A large body of work argues that scientific research increases the rate of technological advance, and with it economic growth. The precise mechanism through which science accelerates the rate of invention, however, remains an open question. Conceptualizing invention as a combinatorial search process, this paper argues that science alters inventors' search processes, by leading them more directly to useful combinations, eliminating fruitless paths of research, and motivating them to continue even in the face of negative feedback. These mechanisms prove most useful when inventors attempt to combine highly coupled components; therefore, the value of scientific research to invention varies systematically across applications. Empirical analyses of patent data support this thesis. Copyright © 2004 John Wiley & Sons, Ltd.",2000.0,"L. Fleming, O. Sorenson"
8cb976b33c9f628afbfe16eb26b5b067001b24b5,https://www.semanticscholar.org/paper/8cb976b33c9f628afbfe16eb26b5b067001b24b5,The sequence read archive: explosive growth of sequencing data,"New generation sequencing platforms are producing data with significantly higher throughput and lower cost. A portion of this capacity is devoted to individual and community scientific projects. As these projects reach publication, raw sequencing datasets are submitted into the primary next-generation sequence data archive, the Sequence Read Archive (SRA). Archiving experimental data is the key to the progress of reproducible science. The SRA was established as a public repository for next-generation sequence data as a part of the International Nucleotide Sequence Database Collaboration (INSDC). INSDC is composed of the National Center for Biotechnology Information (NCBI), the European Bioinformatics Institute (EBI) and the DNA Data Bank of Japan (DDBJ). The SRA is accessible at www.ncbi.nlm.nih.gov/sra from NCBI, at www.ebi.ac.uk/ena from EBI and at trace.ddbj.nig.ac.jp from DDBJ. In this article, we present the content and structure of the SRA and report on updated metadata structures, submission file formats and supported sequencing platforms. We also briefly outline our various responses to the challenge of explosive data growth.",2011.0,"Y. Kodama, Martin Shumway, R. Leinonen"
f39f562f706e9f8771e6305d086fed159366b5a8,https://www.semanticscholar.org/paper/f39f562f706e9f8771e6305d086fed159366b5a8,Assessing citizen science data quality: an invasive species case study,"An increase in the number of citizen science programs has prompted an examination of their ability to provide data of sufficient quality. We tested the ability of volunteers relative to professionals in identifying invasive plant species, mapping their distributions, and estimating their abundance within plots. We generally found that volunteers perform almost as well as professionals in some areas, but that we should be cautious about data quality in both groups. We analyzed predictors of volunteer success (age, education, experience, science literacy, attitudes) in training‐related skills, but these proved to be poor predictors of performance and could not be used as effective eligibility criteria. However, volunteer success with species identification increased with their self‐identified comfort level. Based on our case study results, we offer lessons learned and their application to other programs and provide recommendations for future research in this area.",2011.0,"Alycia Crall, Greg Newman, T. Stohlgren, Kirstin A. Holfelder, J. Graham, D. Waller"
0c74b1d0e8b39b7900e04429a360b53d6cf8a599,https://www.semanticscholar.org/paper/0c74b1d0e8b39b7900e04429a360b53d6cf8a599,The Origins of C4 Grasslands: Integrating Evolutionary and Ecosystem Science,"Grassland Emergence The evolution of the C4 photosynthetic pathway from the ancestral C3 pathway in grasses led to the establishment of grasslands in warm climates during the Late Miocene (8 to 3 million years ago). This was a major event in plant evolutionary history, and their high rates of foliage production sustained high levels of herbivore consumption. The past decade has seen significant advances in understanding C4 grassland ecosystem ecology, and now a wealth of data on the geological history of these ecosystems has accumulated and the phylogeny of grasses is much better known. Edwards et al. (p. 587) review this multidisciplinary research area and attempt to synthesize emerging knowledge about the evolution of grass species within the context of plant and ecosystem ecology. The evolution of grasses using C4 photosynthesis and their sudden rise to ecological dominance 3 to 8 million years ago is among the most dramatic examples of biome assembly in the geological record. A growing body of work suggests that the patterns and drivers of C4 grassland expansion were considerably more complex than originally assumed. Previous research has benefited substantially from dialog between geologists and ecologists, but current research must now integrate fully with phylogenetics. A synthesis of grass evolutionary biology with grassland ecosystem science will further our knowledge of the evolution of traits that promote dominance in grassland systems and will provide a new context in which to evaluate the relative importance of C4 photosynthesis in transforming ecosystems across large regions of Earth.",2010.0,"E. Edwards, C. Osborne, C. Strömberg, Stephen A. Smith"
478cd69ab5de77a7dc8d2419e49b14a8cac82e73,https://www.semanticscholar.org/paper/478cd69ab5de77a7dc8d2419e49b14a8cac82e73,Six degrees of scientific data: reading patterns for extreme scale science IO,"Petascale science simulations generate 10s of TBs of application data per day, much of it devoted to their checkpoint/restart fault tolerance mechanisms. Previous work demonstrated the importance of carefully managing such output to prevent application slowdown due to IO blocking, resource contention negatively impacting simulation performance and to fully exploit the IO bandwidth available to the petascale machine. This paper takes a further step in understanding and managing extreme-scale IO. Specifically, its evaluations seek to understand how to efficiently read data for subsequent data analysis, visualization, checkpoint restart after a failure, and other read-intensive operations. In their entirety, these actions support the 'end-to-end' needs of scientists enabling the scientific processes being undertaken. Contributions include the following. First, working with application scientists, we define 'read' benchmarks that capture the common read patterns used by analysis codes. Second, these read patterns are used to evaluate different IO techniques at scale to understand the effects of alternative data sizes and organizations in relation to the performance seen by end users. Third, defining the novel notion of a 'data district' to characterize how data is organized for reads, we experimentally compare the read performance seen with the ADIOS middleware's log-based BP format to that seen by the logically contiguous NetCDF or HDF5 formats commonly used by analysis tools. Measurements assess the performance seen across patterns and with different data sizes, organizations, and read process counts. Outcomes demonstrate that high end-to-end IO performance requires data organizations that offer flexibility in data layout and placement on parallel storage targets, including in ways that can make tradeoffs in the performance of data writes vs. reads.",2011.0,"J. Lofstead, Milo Polte, Garth A. Gibson, S. Klasky, K. Schwan, Ron A. Oldfield, M. Wolf, Qing Liu"
c5dae4440044b015fd4ae8fd59aba43d7515c889,https://www.semanticscholar.org/paper/c5dae4440044b015fd4ae8fd59aba43d7515c889,Big Data Analytics for Earth Sciences: the EarthServer approach,"Big Data Analytics is an emerging field since massive storage and computing capabilities have been made available by advanced e-infrastructures. Earth and Environmental sciences are likely to benefit from Big Data Analytics techniques supporting the processing of the large number of Earth Observation datasets currently acquired and generated through observations and simulations. However, Earth Science data and applications present specificities in terms of relevance of the geospatial information, wide heterogeneity of data models and formats, and complexity of processing. Therefore, Big Earth Data Analytics requires specifically tailored techniques and tools. The EarthServer Big Earth Data Analytics engine offers a solution for coverage-type datasets, built around a high performance array database technology, and the adoption and enhancement of standards for service interaction (OGC WCS and WCPS). The EarthServer solution, led by the collection of requirements from scientific communities and international initiatives, provides a holistic approach that ranges from query languages and scalability up to mobile access and visualization. The result is demonstrated and validated through the development of lighthouse applications in the Marine, Geology, Atmospheric, Planetary and Cryospheric science domains.",2016.0,"P. Baumann, P. Mazzetti, J. Ungar, R. Barbera, D. Barboni, A. Beccati, L. Bigagli, E. Boldrini, R. Bruno, A. Calanducci, P. Campalani, Oliver Clements, A. Dumitru, M. Grant, Pasquale Herzig, G. Kakaletris, J. Laxton, Panagiota Koltsida, Kinga Lipskoch, Alireza Rezaei Mahdiraji, S. Mantovani, Vlad Merticariu, A. Messina, D. Misev, S. Natali, S. Nativi, J. Oosthoek, M. Pappalardo, J. Passmore, A. Rossi, F. Rundo, M. Sen, Vittorio Sorbera, Don Sullivan, M. Torrisi, L. Trovato, M. G. Veratelli, Sebastian Wagner"
6e78b1133713cb17aabbc3bf421a6e51bc538eca,https://www.semanticscholar.org/paper/6e78b1133713cb17aabbc3bf421a6e51bc538eca,Social Science in the Era of Big Data,"Digital technologies keep track of everything we do and say while we are online, and we spend online an increasing portion of our time. Databases hidden behind web services and applications are constantly fed with information of our movements and communication patterns, and a significant dimension of our lives, quantified to unprecedented levels, gets stored in those vast online repositories. This article considers some of the implications of this torrent of data for social science research, and for the types of questions we can ask of the world we inhabit. The goal of the article is twofold: to explain why, in spite of all the data, theory still matters to build credible stories of what the data reveal; and to show how this allows social scientists to revisit old questions at the intersection of new technologies and disciplinary approaches. The article also considers how Big Data research can transform policy making, with a focus on how it can help us improve communication and governance in policy-relevant domains.",2013.0,Sandra González-Bailón
0c0b55fbb3fa524f3863eaaeba8606d403f65d45,https://www.semanticscholar.org/paper/0c0b55fbb3fa524f3863eaaeba8606d403f65d45,Assessing the quality and trustworthiness of citizen science data,"The Internet, Web 2.0 and Social Networking technologies are enabling citizens to actively participate in ‘citizen science’ projects by contributing data to scientific programmes via the Web. However, the limited training, knowledge and expertise of contributors can lead to poor quality, misleading or even malicious data being submitted. Subsequently, the scientific community often perceive citizen science data as not worthy of being used in serious scientific research—which in turn, leads to poor retention rates for volunteers. In this paper, we describe a technological framework that combines data quality improvements and trust metrics to enhance the reliability of citizen science data. We describe how online social trust models can provide a simple and effective mechanism for measuring the trustworthiness of community‐generated data. We also describe filtering services that remove unreliable or untrusted data and enable scientists to confidently reuse citizen science data. The resulting software services are evaluated in the context of the CoralWatch project—a citizen science project that uses volunteers to collect comprehensive data on coral reef health. Copyright © 2012 John Wiley & Sons, Ltd.",2013.0,"J. Hunter, A. Alabri, C. Ingen"
fd6d8762bd76c58226653af12597fbcb893f183f,https://www.semanticscholar.org/paper/fd6d8762bd76c58226653af12597fbcb893f183f,The Data Deluge: An e-Science Perspective,"This paper previews the imminent flood of scientific data expected from the next generation of experiments, simulations, sensors and satellites. In order to be exploited by search engines and data mining software tools, such experimental data needs to be annotated with relevant metadata giving information as to provenance, content, conditions and so on. The case for automating the process of going from raw data to information to knowledge is briefly discussed. The paper argues the case for creating new types of digital libraries for scientific data with the same sort of management services as conventional digital libraries in addition to other data-specific services. Some likely implications of both the Open Archives Initiative and e-Science data for the future role for university libraries are briefly mentioned. A substantial subset of this e-Science data needs to archived and curated for long-term preservation. Some of the issues involved in the digital preservation of both scientific data and of the programs needed to interpret the data are reviewed. Finally, the implications of this wealth of e-Science data for the Grid middleware infrastructure are highlighted. * Postal address: EPSRC, Polaris House, North Star Avenue, Swindon SN2 1 ET, UK + On secondment from the Department of Electronics and Computer Science, University of Southampton, Southampton SO17 1BJ, UK",2003.0,"A. Hey, Anne E. Trefethen"
f1702cf02205c17fa0454fd07085dbbcb573c50b,https://www.semanticscholar.org/paper/f1702cf02205c17fa0454fd07085dbbcb573c50b,Data Sources,"There are many sources that relevant data for clinical data science can originate from. The brief overview in this chapter highlights the most frequent sources, but is definitely not exhaustive. The goal of this chapter is to provide an introduction to the most common data sources and to familiarize the reader with basic terminology in this context, in order to more easily understand discussions in next chapters and in literature in general.",2018.0,Pieter Kubben
611544418ca53cdad254df444addc7814abcfddc,https://www.semanticscholar.org/paper/611544418ca53cdad254df444addc7814abcfddc,An introduction to statistical learning with applications in R,"The fundamental mathematical tools needed to understand machine learning include linear algebra, analytic geometry, matrix decompositions, vector calculus, optimization, probability and statistics. These topics are traditionally taught in disparate courses, making it hard for data science or computer science students, or professionals, to efficiently learn the mathematics. This self-contained textbook bridges the gap between mathematical and machine learning texts, introducing the mathematical concepts with a minimum of prerequisites. It uses these concepts to derive four central machine learning methods: linear regression, principal component analysis, Gaussian mixture models and support vector machines. For students and others with a mathematical background, these derivations provide a starting point to machine learning texts. For those learning the mathematics for the first time, the methods help build intuition and practical experience with applying mathematical concepts. Every chapter includes worked examples and exercises to test understanding. Programming tutorials are offered on the book's web site. This textbook considers statistical learning applications when interest centers on the conditional distribution of a response variable, given a set of predictors, and in the absence of a credible model that can be specified before the data analysis begins. Consistent with modern data analytics, it emphasizes that a proper statistical learning data analysis depends in an integrated fashion on sound data collection, intelligent data management, appropriate statistical procedures, and an",2021.0,"Fariha Sohil, Muhammad Umair Sohali, J. Shabbir"
cfa1d4b37ff51e28f9d263a662147bee9c6062b3,https://www.semanticscholar.org/paper/cfa1d4b37ff51e28f9d263a662147bee9c6062b3,PISA 2006: Science Competencies for Tomorrow's World,"The US can draw on the most highly educated labor force among the principal industrialized nations, when measured in terms of the formal qualifications attained by 25-to-64-year-olds in the labor force. However, this advantage is largely a result of the “first-mover advantage” which the US gained after World War II by massively increasing enrolments. While the US had, well into the 1960s, the highest high school completion rates among OECD countries, in 2005 it ranked, with a high school completion rate of 76%, 21st among the 27 OECD countries with available data, followed only by Spain, New Zealand, Portugal, Turkey and Mexico. Similar trends are visible in college education, where the US slipped between 1995 and 2005 from the 2 nd to the 14 th rank, not because US college graduation rates declined, but because they rose so much faster in many OECD countries. Graduate output is particularly low in science, where the number of people with a college degree per 100,000 employed 25-to-34-year-olds was 1,100 compared with 1,295 on average across OECD countries and more than 2,000 in Australia, Finland, France and Korea (Education at a Glance, 2007).",2007.0,Andreas Schleicher
07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b,https://www.semanticscholar.org/paper/07912741c6c96e6ad5b2c2d6c6c3b2de5c8a271b,Advances and Open Problems in Federated Learning,"Federated learning (FL) is a machine learning setting where many clients (e.g. mobile devices or whole organizations) collaboratively train a model under the orchestration of a central server (e.g. service provider), while keeping the training data decentralized. FL embodies the principles of focused data collection and minimization, and can mitigate many of the systemic privacy risks and costs resulting from traditional, centralized machine learning and data science approaches. Motivated by the explosive growth in FL research, this paper discusses recent advances and presents an extensive collection of open problems and challenges.",2019.0,"P. Kairouz, H. B. McMahan, Brendan Avent, A. Bellet, M. Bennis, A. Bhagoji, Keith Bonawitz, Zachary B. Charles, Graham Cormode, Rachel Cummings, Rafael G. L. D'Oliveira, S. E. Rouayheb, David Evans, Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip B. Gibbons, M. Gruteser, Zaïd Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo, Ben Hutchinson, Justin Hsu, Martin Jaggi, T. Javidi, Gauri Joshi, M. Khodak, Jakub Konecný, A. Korolova, F. Koushanfar, Oluwasanmi Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, M. Mohri, R. Nock, A. Özgür, R. Pagh, Mariana Raykova, Hang Qi, Daniel Ramage, R. Raskar, D. Song, Weikang Song, Sebastian U. Stich, Ziteng Sun, A. Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang, Li Xiong, Zheng Xu, Qiang Yang, Felix X. Yu, Han Yu, Sen Zhao"
efc91095d187abafeab8785eb57bbdef66c82d1c,https://www.semanticscholar.org/paper/efc91095d187abafeab8785eb57bbdef66c82d1c,Data and Computer Communications,"Data and Computer Communications, 9e, is a two-time winner of the best Computer Science and Engineering textbook of the year award from the Textbook and Academic Authors Association. It is ideal for one/two-semester courses in Computer Networks, Data Communications, and Communications Networks in CS, CIS, and Electrical Engineering departments. With a focus on the most current technology and a convenient modular format, this best-selling text offers a clear and comprehensive survey of the entire data and computer communications field. Emphasizing both the fundamental principles as well as the critical role of performance in driving protocol and network design, it explores in detail all the critical technical areas in data communications, wide-area networking, local area networking, and protocol design.",1985.0,W. Stallings
5efc7309a02bc1954c9db5d29fb790c2ee97383b,https://www.semanticscholar.org/paper/5efc7309a02bc1954c9db5d29fb790c2ee97383b,Prospecting (in) the data sciences,"Data science is characterized by engaging heterogeneous data to tackle real world questions and problems. But data science has no data of its own and must seek it within real world domains. We call this search for data “prospecting” and argue that the dynamics of prospecting are pervasive in, even characteristic of, data science. Prospecting aims to render the data, knowledge, expertise, and practices of worldly domains available and tractable to data science method and epistemology. Prospecting precedes data synthesis, analysis, or visualization, and is constituted by the upstream work of discovering disordered or inaccessible data resources, thereafter to be ordered and rendered available for computation. Through this work, data science positions itself in the middle of all things—capable of engaging this, that, or any domain—and thus prospecting is a key driver of data science’s ongoing formation as a universal(izing) science.",2020.0,"S. Slota, Andrew Hoffman, David Ribes, G. Bowker"
d422df8bff4e677a3077635db116679d25142bfc,https://www.semanticscholar.org/paper/d422df8bff4e677a3077635db116679d25142bfc,"Machine learning: Trends, perspectives, and prospects","Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",2015.0,"Michael I. Jordan, T. Mitchell"
04638c67b715b9d85ae5a44afd3730b83330fb66,https://www.semanticscholar.org/paper/04638c67b715b9d85ae5a44afd3730b83330fb66,Economics in the age of big data,"Background Economic science has evolved over several decades toward greater emphasis on empirical work. The data revolution of the past decade is likely to have a further and profound effect on economic research. Increasingly, economists make use of newly available large-scale administrative data or private sector data that often are obtained through collaborations with private firms, giving rise to new opportunities and challenges. The rising use of non–publicly available data in economic research. Here we show the percentage of papers published in the American Economic Review (AER) that obtained an exemption from the AER’s data availability policy, as a share of all papers published by the AER that relied on any form of data (excluding simulations and laboratory experiments). Notes and comments, as well as AER Papers and Proceedings issues, are not included in the analysis. We obtained a record of exemptions directly from the AER administrative staff and coded each exemption manually to reflect public sector versus private data. Our check of nonexempt papers suggests that the AER records may possibly understate the percentage of papers that actually obtained exemptions. The asterisk indicates that data run from when the AER started collecting these data (December 2005 issue) to the September 2014 issue. To make full use of the data, we define year 2006 to cover October 2005 through September 2006, year 2007 to cover October 2006 through September 2007, and so on. Advances These new data are affecting economic research along several dimensions. Many fields have shifted from a reliance on relatively small-sample government surveys to administrative data with universal or near-universal population coverage. This shift is transformative, as it allows researchers to rigorously examine variation in wages, health, productivity, education, and other measures across different subpopulations; construct consistent long-run statistical indices; generate new quasi-experimental research designs; and track diverse outcomes from natural and controlled experiments. Perhaps even more notable is the expansion of private sector data on economic activity. These data, sometimes available from public sources but other times obtained through data-sharing agreements with private firms, can help to create more granular and real-time measurement of aggregate economic statistics. The data also offer researchers a look inside the “black box” of firms and markets by providing meaningful statistics on economic behavior such as search and information gathering, communication, decision-making, and microlevel transactions. Collaborations with data-oriented firms also create new opportunities to conduct and evaluate randomized experiments. Economic theory plays an important role in the analysis of large data sets with complex structure. It can be difficult to organize and study this type of data (or even to decide which variables to construct) without a simplifying conceptual framework, which is where economic models become useful. Better data also allow for sharper tests of existing models and tests of theories that had previously been difficult to assess. Outlook The advent of big data is already allowing for better measurement of economic effects and outcomes and is enabling novel research designs across a range of topics. Over time, these data are likely to affect the types of questions economists pose, by allowing for more focus on population variation and the analysis of a broader range of economic activities and interactions. We also expect economists to increasingly adopt the large-data statistical methods that have been developed in neighboring fields and that often may complement traditional econometric techniques. These data opportunities also raise some important challenges. Perhaps the primary one is developing methods for researchers to access and explore data in ways that respect privacy and confidentiality concerns. This is a major issue in working with both government administrative data and private sector firms. Other challenges include developing the appropriate data management and programming capabilities, as well as designing creative and scalable approaches to summarize, describe, and analyze large-scale and relatively unstructured data sets. These challenges notwithstanding, the next few decades are likely to be a very exciting time for economic research. The quality and quantity of data on economic activity are expanding rapidly. Empirical research increasingly relies on newly available large-scale administrative data or private sector data that often is obtained through collaboration with private firms. Here we highlight some challenges in accessing and using these new data. We also discuss how new data sets may change the statistical methods used by economists and the types of questions posed in empirical research.",2014.0,"L. Einav, Jonathan Levin"
a714269fa44287c2d7c4c75d847cb72a823b7427,https://www.semanticscholar.org/paper/a714269fa44287c2d7c4c75d847cb72a823b7427,Data journeys: Capturing the socio-material constitution of data objects and flows,"In this paper, we discuss the development and piloting of a new methodology for illuminating the socio-material constitution of data objects and flows as data move between different sites of practice. The data journeys approach contributes to the development of critical, qualitative methodologies that can address the geographic and temporal scale of emerging knowledge infrastructures, and capture the ‘life of data’ from their initial generation through to re-use in different contexts. We discuss the theoretical development of the data journeys methodology and the application of the approach on a project examining meteorological data on their journey from initial production through to being re-used in climate science and financial markets. We then discuss three key conceptual findings from this project about: (1) the socio-material constitution of digital data objects, (2) ‘friction’ in the movement of data through space and time and (3) the mutability of digital data as a material property that contributes to driving the movement of data between different sites of practice.",2016.0,"J. Bates, Yuwei Lin, Paula Goodale"
681f6e6f0f754c5f6a73a2eafb05c564ca4ae235,https://www.semanticscholar.org/paper/681f6e6f0f754c5f6a73a2eafb05c564ca4ae235,Eliciting Expert Knowledge in Conservation Science,"Abstract:  Expert knowledge is used widely in the science and practice of conservation because of the complexity of problems, relative lack of data, and the imminent nature of many conservation decisions. Expert knowledge is substantive information on a particular topic that is not widely known by others. An expert is someone who holds this knowledge and who is often deferred to in its interpretation. We refer to predictions by experts of what may happen in a particular context as expert judgments. In general, an expert‐elicitation approach consists of five steps: deciding how information will be used, determining what to elicit, designing the elicitation process, performing the elicitation, and translating the elicited information into quantitative statements that can be used in a model or directly to make decisions. This last step is known as encoding. Some of the considerations in eliciting expert knowledge include determining how to work with multiple experts and how to combine multiple judgments, minimizing bias in the elicited information, and verifying the accuracy of expert information. We highlight structured elicitation techniques that, if adopted, will improve the accuracy and information content of expert judgment and ensure uncertainty is captured accurately. We suggest four aspects of an expert elicitation exercise be examined to determine its comprehensiveness and effectiveness: study design and context, elicitation design, elicitation method, and elicitation output. Just as the reliability of empirical data depends on the rigor with which it was acquired so too does that of expert knowledge.",2012.0,"T. Martin, M. Burgman, F. Fidler, P. Kuhnert, S. Low-Choy, M. McBride, K. Mengersen"
bd52d8dee23617d3531702124fc87402a42a32a4,https://www.semanticscholar.org/paper/bd52d8dee23617d3531702124fc87402a42a32a4,Data literacy for researchers and data librarians,"This paper describes data literacy and emphasizes its importance. Data literacy is vital for researchers who need to become data literate science workers and also for (potential) data management professionals. Its important characteristic is a close connection and similarity to information literacy. To support this argument, a review of literature was undertaken on the importance of data, and the data-intensive paradigm of scientific research, researchers’ expected and real behaviour, the nature of research data management, the possible roles of the academic library, data quality and data citation, Besides describing the nature of data literacy and enumerating the related skills, the application of phenomenographic approaches to data literacy and its relationship to the digital humanities have been identified as subjects for further investigation.",2017.0,T. Koltay
4dfddbf5b6b0815414313039d29f41900f47d003,https://www.semanticscholar.org/paper/4dfddbf5b6b0815414313039d29f41900f47d003,"The Data Revolution. Big Data, Open Data, Data Infrastructures and Their Consequences","The last few years have witnessed an increasing production of data that have become open, accessible and available at low cost. Although many disciplines are already using ‘big data’ as instrument of analysis, social sciences have apparently missed the opportunity to exploit their potentialities fully. The purpose of this excellent book is to prove how these data do not exist independently from the ideas, techniques, technologies, people and context that produce, process, manage, analyze and store them. Moreover, the author explores the definition, characteristics and the technique to manage big data, but he also focuses his attention on the challenges of this way of thinking and on how big data are changing existing epistemology and science. Before the big data revolution, the scientific approach was based on computational science based on the simulation of complex phenomena. In the age of big data, however, an exploratory approach based on data-intensive, statistical exploration and data mining was used. In the age of big data, however, an exploratory approach based on data-intensive, statistical exploration and data mining is used. The author focuses his attention on how big data are changing the approaches and methodologies in four different fields, that is, governing people, managing organizations, leveraging value and producing capitals, creating better places in which to live. The aim of the book is threefold: to provide a detailed reflection on the nature of the data and their wider assemblages; to chart how these assemblages are shifting and mutating all along the development of new data infrastructures; and to reflect on the consequences that these new ways to assemble data may entail the making of sense and on the effects they produce in the world. The 11 chapters ideally can be divided into two main sections. The first section (chapters 1–6) deals with the big data characteristics and the techniques to manage them. The last section (chapters 7–11) consider how big data are changing the epistemology of science across all domains (arts and humanities, social and life sciences, engineering). The interest of these last chapters lies in the core idea of data being not self-meaningful, as their meaningfulness is proportionate to the information they can provide. This is particularly interesting as it fosters dense insights and ideas on further development in research. The book starts with the definition of big data and enhances the concept by which data do not exist independently from the ideas, instruments, practices, context and knowledge used to generate, process, analyze and draw conclusions from them. The book continues with an analysis of the data characteristics. Data vary by forms (qualitative and quantitative), structure (structured, semi-structured and unstructured), source (captured, derived, exhaust, transient), producer (primary, secondary, tertiary), and type (indexical, attribute, metadata). However, these different types of data share the same characteristic as they all form the basis of the knowledge pyramid where data precede information which, in turn, precedes knowledge. The latter precedes understanding and wisdom. In order to make sense of data, they are usually pooled into datasets and databases designed and organized to enable specific analysis. How they are structured has consequences on the queries and obtainable results. The author underlines the importance of the data assembly process as an issue that needs further attention and research. Chapter 4 explores big data characteristics, that is, volume, velocity, variety, exhaustivity, resolution/indexicality, relationality and flexibility/scalability. The author then examines the interest that the access to large data with those specific characteristics may have for society, governments and business organizations. Chapter 5 concerns the sources of big data. The starting point is that the production of big data has been facilitated by the confluence of five technological innovations: growing computational power, internet, pervasive and ubiquitous computing, indexical and machine readable identification, and massive distributed storage. The data production can be divided into three categories, that is, directed data (generated by traditional forms of surveillance), automated data (generated by automatic function of the device or systems) and volunteered data (traded or gifted by people to a system). Once again, with his critical approach, the author underlines the importance of developing empirical studies to examine in depth the various ways in which big data are being generated, Regional Studies, 2016",2016.0,B. Martini
ca8b155e95d306e9b8c2e60f00a48001cabaf8f5,https://www.semanticscholar.org/paper/ca8b155e95d306e9b8c2e60f00a48001cabaf8f5,The science of climate change,"P i p 2 Variations in local Antarctic atmospheric temperature, as derived from oxygen isotopc data, as weU as concentrations of atmospheric carbon dioxidc and methane From Vostok. Antarctica ice core records. The fan that cold dimates aren't mainrainedwithout a depletion of greenhouse gases, and that warm dimates aren't maintained without an excess of these greenhouse gases is evident. Notice aLo that the current level of atmospheric CO2 (370 ppm) is ~ 2 0 % larger than at anyrime during the last 400,000 yun. Similarly, current levels of aunosphcric mcthane CH4 (1750 ppb) are morc dw double the maximum value found in thc 400,000 yea^ m r d . Notice also thar the increase in CO2 from 280 ppm to 370 ppm over the last 150 years, primarily due to fossil fuel burning, is about thc same as the incmase from the depths of the last ice age (21,000 years ago) to 1750 (190 ppm to 280 ppm) (from Petit et al., 1999).",2010.0,B. Pittock
454a5afdeeee4b10b912ea9cca4d9dd13beb2aa4,https://www.semanticscholar.org/paper/454a5afdeeee4b10b912ea9cca4d9dd13beb2aa4,Data validation in citizen science: a case study from Project FeederWatch,"To become more widely accepted as a valuable research tool, citizen-science projects must find ways to ensure that data gathered by large numbers of people with varying levels of expertise are of consistently high quality. Here, we describe a data validation protocol developed for Project FeederWatch, a continent-wide bird monitoring program, that is designed to increase researchers' and participants' confidence in the data being collected.",2012.0,"David N. Bonter, C. Cooper"
d886362c98b22d2110d7c0d0da62511c5b315f12,https://www.semanticscholar.org/paper/d886362c98b22d2110d7c0d0da62511c5b315f12,Shedding Light on the Dark Data in the Long Tail of Science,"One of the primary outputs of the scientific enterprise is data, but many institutions such as libraries that are charged with preserving and disseminating scholarly output have largely ignored this form of documentation of scholarly activity. This paper focuses on a particularly troublesome class of data, termed dark data. “Dark data” is not carefully indexed and stored so it becomes nearly invisible to scientists and other potential users and therefore is more likely to remain underutilized and eventually lost. The article discusses how the concepts from long-tail economics can be used to understand potential solutions for better curation of this data. The paper describes why this data is critical to scientific progress, some of the properties of this data, as well as some social and technical barriers to proper management of this class of data. Many potentially useful institutional, social, and technical solutions are under development and are introduced in the last sections of the paper, but these solutions are largely unproven and require additional research and development.",2008.0,"M. Cragin, P. Heidorn"
454a5afdeeee4b10b912ea9cca4d9dd13beb2aa4,https://www.semanticscholar.org/paper/454a5afdeeee4b10b912ea9cca4d9dd13beb2aa4,Data validation in citizen science: a case study from Project FeederWatch,"To become more widely accepted as a valuable research tool, citizen-science projects must find ways to ensure that data gathered by large numbers of people with varying levels of expertise are of consistently high quality. Here, we describe a data validation protocol developed for Project FeederWatch, a continent-wide bird monitoring program, that is designed to increase researchers' and participants' confidence in the data being collected.",2012.0,"David N. Bonter, C. Cooper"
d886362c98b22d2110d7c0d0da62511c5b315f12,https://www.semanticscholar.org/paper/d886362c98b22d2110d7c0d0da62511c5b315f12,Shedding Light on the Dark Data in the Long Tail of Science,"One of the primary outputs of the scientific enterprise is data, but many institutions such as libraries that are charged with preserving and disseminating scholarly output have largely ignored this form of documentation of scholarly activity. This paper focuses on a particularly troublesome class of data, termed dark data. “Dark data” is not carefully indexed and stored so it becomes nearly invisible to scientists and other potential users and therefore is more likely to remain underutilized and eventually lost. The article discusses how the concepts from long-tail economics can be used to understand potential solutions for better curation of this data. The paper describes why this data is critical to scientific progress, some of the properties of this data, as well as some social and technical barriers to proper management of this class of data. Many potentially useful institutional, social, and technical solutions are under development and are introduced in the last sections of the paper, but these solutions are largely unproven and require additional research and development.",2008.0,"M. Cragin, P. Heidorn"
9a411034c3631e878008599d41b8214c36d95dfe,https://www.semanticscholar.org/paper/9a411034c3631e878008599d41b8214c36d95dfe,Is Big Data challenging criminology?,"The advent of ‘Big Data’ and machine learning algorithms is predicted to transform how we work and think. Specifically, it is said that the capacity of Big Data analytics to move from sampling to census, its ability to deal with messy data and the demonstrated utility of moving from causality to correlation have fundamentally changed the practice of social sciences. Some have even predicted the end of theory—where the question why is replaced by what—and an enduring challenge to disciplinary expertise. This article critically reviews the available literature against such claims and draws on the example of predictive policing to discuss the likely impact of Big Data analytics on criminological research and policy.",2016.0,"Janet Chan, Lyria Bennett Moses"
2c86cbb4ffb6bf1b6d0d468d6d4a00b81c338dc7,https://www.semanticscholar.org/paper/2c86cbb4ffb6bf1b6d0d468d6d4a00b81c338dc7,Nursing Needs Big Data and Big Data Needs Nursing.,"PURPOSE
Contemporary big data initiatives in health care will benefit from greater integration with nursing science and nursing practice; in turn, nursing science and nursing practice has much to gain from the data science initiatives. Big data arises secondary to scholarly inquiry (e.g., -omics) and everyday observations like cardiac flow sensors or Twitter feeds. Data science methods that are emerging ensure that these data be leveraged to improve patient care.


ORGANIZING CONSTRUCT
Big data encompasses data that exceed human comprehension, that exist at a volume unmanageable by standard computer systems, that arrive at a velocity not under the control of the investigator and possess a level of imprecision not found in traditional inquiry. Data science methods are emerging to manage and gain insights from big data.


METHODS
The primary methods included investigation of emerging federal big data initiatives, and exploration of exemplars from nursing informatics research to benchmark where nursing is already poised to participate in the big data revolution. We provide observations and reflections on experiences in the emerging big data initiatives.


CONCLUSIONS
Existing approaches to large data set analysis provide a necessary but not sufficient foundation for nursing to participate in the big data revolution. Nursing's Social Policy Statement guides a principled, ethical perspective on big data and data science. There are implications for basic and advanced practice clinical nurses in practice, for the nurse scientist who collaborates with data scientists, and for the nurse data scientist.


CLINICAL RELEVANCE
Big data and data science has the potential to provide greater richness in understanding patient phenomena and in tailoring interventional strategies that are personalized to the patient.",2015.0,"P. Brennan, S. Bakken"
e1946e597b27cd4526ee71800f6454b8dcb5d4d1,https://www.semanticscholar.org/paper/e1946e597b27cd4526ee71800f6454b8dcb5d4d1,Fuzzy mathematical models in engineering and management science,"Theoretical Concepts. Fuzzy Set Theory and System Modelling. Theory of Fuzzy Sets. Theory of Fuzzy Numbers. Linear Ordering of Fuzzy Numbers. Evaluation of Imprecision in Fuzzy Numbers. Triangular Approximation of Various Functions of Triangular Fuzzy Numbers. Deconvolution of the Fuzzy Equations A(+)B=C, and A( . )B=C in R. T-Norms and T-Conorms. Fuzzy Numbers in [0,1]. Fuzzy Numbers in [0,1] with Higher-Order Intervals of Confidence in [0,1]. Models in Engineering and Management Science. Modelling Issues in Engineering and Management Science. Fuzzy Zero-Base Budgeting (F.Z.B.B.). Fuzzy Delphi Method (F.D.M.) in Forecasting and Decision Making. Discounting Problem using Fuzzy Numbers. Smoothing (Filtering) of Fuzzy Data. Reliability Modelling and Evaluation with Fuzzy Data. Ordering of Fuzzy Quotients. Critical Path Method (C.P.M.) with Fuzzy Data. Investment Problem with Fuzzy Data. Transportation Optimization with Fuzzy Data: (Fuzzy Stepping Stone Method). A General View about the Fuzzification of Models in Engineering and Management Science. Appendices.",1988.0,"A. Kaufmann, M. Gupta"
af7bdb358b366260b657d0af92eb5e50e916c6b8,https://www.semanticscholar.org/paper/af7bdb358b366260b657d0af92eb5e50e916c6b8,Advancing Science Through Collaborative Data Sharing and Synthesis,"The demand for researchers to share their data has increased dramatically in recent years. There is a need to replicate and confirm scientific findings to bolster confidence in many research areas. Data sharing also serves the critical function of allowing synthesis of findings across trials. As innovative statistical methods have helped resolve barriers to synthesis analyses, data sharing and synthesis can help answer research questions that cannot be answered by individual trials alone. However, the sharing of data among researchers remains challenging and infrequent. This article aims to (a) increase support for data sharing and synthesis collaborations among researchers to advance scientific knowledge and (b) provide a model for establishing these collaborations using the example of the ongoing National Institute of Mental Health’s Collaborative Data Synthesis on Adolescent Depression Trials. This study brings together datasets from existing prevention and treatment trials in adolescent depression, as well as researchers and stakeholders, to answer questions about “for whom interventions work” and “by what pathways interventions have their effects.” This is critical to improving interventions, including increasing knowledge about intervention efficacy among minority populations, or what we call “scientific equity.” The collaborative model described is relevant to fields with research questions that can only be addressed by synthesizing individual-level data.",2013.0,"T. Perrino, G. Howe, Anne Sperling, W. Beardslee, I. Sandler, D. Shern, Hilda M. Pantin, Sheila Kaupert, Nicole Cano, Gracelyn Cruden, Frank Bandiera, C. H. Brown"
df9bf28634d6499d3f9cf876f02c0d0cb5c3322d,https://www.semanticscholar.org/paper/df9bf28634d6499d3f9cf876f02c0d0cb5c3322d,Data-Intensive Science: A New Paradigm for Biodiversity Studies,"The increasing availability of massive volumes of scientific data requires new synthetic analysis techniques to explore and identify interesting patterns that are otherwise not apparent. For biodiversity studies, a “data-driven” approach is necessary because of the complexity of ecological systems, particularly when viewed at large spatial and temporal scales. Data-intensive science organizes large volumes of data from multiple sources and fields and then analyzes them using techniques tailored to the discovery of complex patterns in high-dimensional data through visualizations, simulations, and various types of model building. Through interpreting and analyzing these models, truly novel and surprising patterns that are “born from the data” can be discovered. These patterns provide valuable insight for concrete hypotheses about the underlying ecological processes that created the observed data. Data-intensive science allows scientists to analyze bigger and more complex systems efficiently, and complements more traditional scientific processes of hypothesis generation and experimental testing to refine our understanding of the natural world.",2009.0,"S. Kelling, W. Hochachka, D. Fink, Mirek Riedewald, R. Caruana, G. Ballard, G. Hooker"
e5553a3754a20cfe3e8a7b50bee715348e201a50,https://www.semanticscholar.org/paper/e5553a3754a20cfe3e8a7b50bee715348e201a50,"Citizen science in hydrology and water resources: opportunities for knowledge generation, ecosystem service management, and sustainable development","The participation of the general public in the research design, data collection and interpretation process together with scientists is often referred to as citizen science. While citizen science itself has existed since the start of scientific practice, developments in sensing technology, data processing and visualisation, and communication of ideas and results, are creating a wide range of new opportunities for public participation in scientific research. This paper reviews the state of citizen science in a hydrological context and explores the potential of citizen science to complement more traditional ways of scientific data collection and knowledge generation for hydrological sciences and water resources management. Although hydrological data collection often involves advanced technology, the advent of robust, cheap and low-maintenance sensing equipment provides unprecedented opportunities for data collection in a citizen science context. These data have a significant potential to create new hydrological knowledge, especially in relation to the characterisation of process heterogeneity, remote regions, and human impacts on the water cycle. However, the nature and quality of data collected in citizen science experiments is potentially very different from those of traditional monitoring networks. This poses challenges in terms of their processing, interpretation, and use, especially with regard to assimilation of traditional knowledge, the quantification of uncertainties, and their role in decision support. It also requires care in designing citizen science projects such that the generated data complement optimally other available knowledge. Lastly, we reflect on the challenges and opportunities in the integration of hydrologically-oriented citizen science in water resources management, the role of scientific knowledge in the decision-making process, and the potential contestation to established community institutions posed by co-generation of new knowledge.",2014.0,"W. Buytaert, Z. Zulkafli, S. Grainger, L. Acosta, Tilashwork C. Alemie, J. Bastiaensen, B. Bièvre, J. Bhusal, Julian Clark, A. Dewulf, Marc Foggin, D. Hannah, Christian Hergarten, Aiganysh Isaeva, T. Karpouzoglou, B. Pandeya, D. Paudel, K. Sharma, T. Steenhuis, S. Tilahun, G. Hecken, M. Zhumanova"
1c4c52d27a07ee59f062304ad53e3b524bd2b4a2,https://www.semanticscholar.org/paper/1c4c52d27a07ee59f062304ad53e3b524bd2b4a2,Citizen Science: Can Volunteers Do Real Research?,ABSTRACT Collaborations between scientists and volunteers have the potential to broaden the scope of research and enhance the ability to collect scientific data. Interested members of the public may contribute valuable information as they learn about wildlife in their local communities.,2008.0,J. Cohn
15fffafb4bfcf8f9b210de01ac5208b0d916147e,https://www.semanticscholar.org/paper/15fffafb4bfcf8f9b210de01ac5208b0d916147e,Trending: The Promises and the Challenges of Big Social Data,"Today the term "" big data "" is often used in popular media, business, computer science and computer industry. For instance, in June 2008 Wired magazine opened its special section on "" The Petabyte Age "" by stating: "" Our ability to capture, warehouse, and understand massive amounts of data is changing science, medicine, business, and technology. As our collection of facts and figures grows, so will the opportunity to find answers to fundamental questions "" ("" The Petabyte Age ""). In February 2010, Economist started its special report "" Data, data everywhere "" with the phrase "" the industrial revolution of data "" (coined by computer scientist Joe Hellerstein) and then went to note that "" The effect is being felt everywhere, from business to science, from government to the arts "" ("" Data, data everywhere ""). Discussions in popular media usually do not define "" big data "" in qualitative terms. However, in computer industry the term has a more precise meaning: "" Big Data is a term applied to data sets whose size is beyond the ability of commonly used software tools to capture, manage, and process the data within a tolerable elapsed time. Big data sizes are a constantly moving target currently ranging from a few dozen terabytes to many petabytes of data in a single data set "" ("" Big data "").. Since its formation in 2008, NEH Office of Digital Humanities has been systematically creating grant opportunities to help humanists work with large data sets. The following statement from 2011 grant competition organized by NEH together with a number of other research agencies in USA, Canada, UK, and Netherlands provides an excellent description of what is at stake: "" The idea behind the Digging into Data Challenge is to address how ""big data"" changes the research landscape for the humanities and social sciences. Now that we have massive databases of materials used by scholars in the humanities and social sciences-ranging from digitized books, newspapers, and music to transactional data like web searches, sensor data or cell phone records-what new, computationally-based research methods might we apply? As the world becomes increasingly digital, new techniques will be needed to search, analyze, and understand these everyday materials. "" ("" Digging into Data Challenge ""). The projects funded by 2009 Digging Into Data Challenge and earlier NEH 2008 Humanities High Performance Computing grant program begin to map the …",2012.0,Lev Manovich
8a88e9710ad9b4cb8db6a3086ede9c531d994917,https://www.semanticscholar.org/paper/8a88e9710ad9b4cb8db6a3086ede9c531d994917,The Service Revolution and the Transformation of Marketing Science,"The nature of marketing science is changing in a systematic, predictable, and irrevocable way. As information technology enables ubiquitous customer communication and big customer data, the fundamental nature of the firm's connection to the customer changes: better, more personalized service can be offered, from which service relationships are deepened, and consequently, more profitable customers grow the influence of service within the goods sector and expand the service sector in the economy. Marketing is becoming more personalized, and marketing science techniques that exploit customer heterogeneity are becoming more important. Information technology improvements also guarantee the increasing importance and usage of computationally intensive data processing and “big data.” Most importantly, these trends have already lasted for more than a century, and they will become even more pronounced in the coming years as a result of the monotonic nature of technology improvement. These changes imply a transformation of marketing science in both the topics to be emphasized and the methods to be employed. Increasingly, and inevitably, all of marketing will come to resemble to a greater degree the formerly specialized area of service marketing, only with an increased emphasis on marketing analytics.",2014.0,"R. Rust, Ming-Hui Huang"
361df482f4c74cb85d0f2fa897a4491fbf53ab6f,https://www.semanticscholar.org/paper/361df482f4c74cb85d0f2fa897a4491fbf53ab6f,The Matthew effect in empirical data,"The Matthew effect describes the phenomenon that in societies, the rich tend to get richer and the potent even more powerful. It is closely related to the concept of preferential attachment in network science, where the more connected nodes are destined to acquire many more links in the future than the auxiliary nodes. Cumulative advantage and success-breads-success also both describe the fact that advantage tends to beget further advantage. The concept is behind the many power laws and scaling behaviour in empirical data, and it is at the heart of self-organization across social and natural sciences. Here, we review the methodology for measuring preferential attachment in empirical data, as well as the observations of the Matthew effect in patterns of scientific collaboration, socio-technical and biological networks, the propagation of citations, the emergence of scientific progress and impact, career longevity, the evolution of common English words and phrases, as well as in education and brain development. We also discuss whether the Matthew effect is due to chance or optimization, for example related to homophily in social systems or efficacy in technological systems, and we outline possible directions for future research.",2014.0,M. Perc
bbe72eaaef4f03ea9bf1028379ee81ee88416dfe,https://www.semanticscholar.org/paper/bbe72eaaef4f03ea9bf1028379ee81ee88416dfe,U.S. science policy. Agencies rally to tackle big data.,"A federal effort is under way to improve the nation9s ability to manage, understand, and act upon the 1.2 zettabytes (1021) of electronic data generated each year. Its goal is to increase fundamental understanding of the technologies needed to manipulate and mine massive amounts of information; apply that knowledge to other scientific fields; address national goals in health, energy, defense, and education; and train more researchers to work with those technologies. The impetus for the initiative, to be managed by the Office of Science and Technology Policy, comes from a December 2010 report by a presidential task force that concluded the nation was ""underinvesting"" in the field. Computer scientists welcome the spotlight that the White House is shining on big-data research.",2012.0,J. Mervis
ae7743e0252a3a81e4505fc3354b2009b1068236,https://www.semanticscholar.org/paper/ae7743e0252a3a81e4505fc3354b2009b1068236,Is Science for Us? Black Students’ and Parents’ Views of Science and Science Careers,"ABSTRACT There are widespread policy concerns to improve (widen and increase) science, technology, engineering, and mathematics participation, which remains stratified by ethnicity, gender, and social class. Despite being interested in and highly valuing science, Black students tend to express limited aspirations to careers in science and remain underrepresented in post‐16 science courses and careers, a pattern which is not solely explained by attainment. This paper draws on survey data from nationally representative student cohorts and longitudinal interview data collected over 4 years from 10 Black African/Caribbean students and their parents, who were tracked from age 10–14 (Y6–Y9), as part of a larger study on children's science and career aspirations. The paper uses an intersectional analysis of the qualitative data to examine why science careers are less “thinkable” for Black students. A case study is also presented of two young Black women who “bucked the trend” and aspired to science careers. The paper concludes with implications for science education policy and practice.",2015.0,"L. Archer, J. DeWitt, J. Osborne"
6309f94eebc515709cf7bbd455953efbfab4e5c2,https://www.semanticscholar.org/paper/6309f94eebc515709cf7bbd455953efbfab4e5c2,"Science Aspirations, Capital, and Family Habitus","Low participation rates in the study of science, technology, engineering, and mathematics (STEM) post-16 are a matter of international concern. Existing evidence suggests children’s science aspirations are largely formed within the critical 10 to 14 age period. This article reports on survey data from over 9,000 elementary school children in England (age 10/11) and qualitative data from 160 semi-structured interviews (92 children aged 10/11 and 78 parents), collected as part of an ongoing 5-year longitudinal study in the United Kingdom tracking children from 10 to 14. Drawing on the conceptual framework of Bourdieu, the article explores how the interplay of family habitus and capital can make science aspirations more “thinkable” for some (notably middle-class) children than others. It is argued that while family habitus is not deterministic (there is no straightforward alignment between family habitus, capital, and a child’s science aspirations), social inequalities in the distribution of capital and differentially classed family habitus combine to produce uneven (classed, racialized) patterns in children’s science aspirations and potential future participation.",2012.0,"L. Archer, J. DeWitt, J. Osborne, J. Dillon, B. Willis, B. Wong"
5eef24aac20bfed5a6c7702b13ae51a1f29c1b29,https://www.semanticscholar.org/paper/5eef24aac20bfed5a6c7702b13ae51a1f29c1b29,Clustering For Data Mining: A Data Recovery Approach (Chapman & Hall/Crc Computer Science),INTRODUCTION: HISTORICAL REMARKS WHAT IS CLUSTERING Exemplary Problems Bird's Eye View WHAT IS DATA Feature Characteristics Bivariate Analysis Feature Space and Data Scatter Preprocessing and Standardizing Mixed Data K-MEANS CLUSTERING Conventional K-Means Initialization of K-Means Intelligent K-Means Interpretation Aids Overall Assessment WARD HIERARCHICAL CLUSTERING Agglomeration: Ward Algorithm Divisive Clustering with Ward Criterion Conceptual Clustering Extensions of Ward Clustering Overall Assessment DATA RECOVERY MODELS Statistics Modeling as Data Recovery Data Recovery Model for K-Means Data Recovery Models for Ward Criterion Extensions to Other Data Types One-by-One Clustering Overall Assessment DIFFERENT CLUSTERING APPROACHES Extensions of K-Means Clustering Graph-Theoretic Approaches Conceptual Description of Clusters Overall Assessment GENERAL ISSUES Feature Selection and Extraction Data Pre-Processing and Standardization Similarity on Subsets and Partitions Dealing with Missing Data Validity and Reliability Overall Assessment CONCLUSION: Data Recovery Approach in Clustering BIBLIOGRAPHY Each chapter also contains a section of Base Words,2005.0,B. Mirkin
1eef1bbb11ca40e374216c918a7cf59e0d5ad299,https://www.semanticscholar.org/paper/1eef1bbb11ca40e374216c918a7cf59e0d5ad299,Landolt-Bornstein: Numerical Data and Functional Relationships in Science and Technology,This book provides an up-to-date review of nanometer-scale magnetism and focuses on the investigation of the basic properties of magnetic nanostructures. It describes a wide range of physical aspects together with theoretical and experimental methods. A broad overview of the latest developments in this emerging and fascinating field of nanostructured materials is given with emphasis on the practical understanding and operation of submicron devices based on nanostructured magnetic materials.,1985.0,A. Miller
ffce6df6b54206f01e7a8eb515fbf716fa984215,https://www.semanticscholar.org/paper/ffce6df6b54206f01e7a8eb515fbf716fa984215,Are Scientific Data Repositories Coping with Research Data Publishing?,"Research data publishing is intended as the release of research data to make it possible for practitioners to (re)use them according to “open science” dynamics. There are three main actors called to deal with research data publishing practices: researchers, publishers, and data repositories. This study analyses the solutions offered by generalist scientific data repositories, i.e., repositories supporting the deposition of any type of research data. These repositories cannot make any assumption on the application domain. They are actually called to face with the almost open ended typologies of data used in science. The current practices promoted by such repositories are analysed with respect to eight key aspects of data publishing, i.e., dataset formatting, documentation, licensing, publication costs, validation, availability, discovery and access, and citation. From this analysis it emerges that these repositories implement well consolidated practices and pragmatic solutions for literature repositories. These practices and solutions can not totally meet the needs of management and use of datasets resources, especially in a context where rapid technological changes continuously open new exploitation prospects.",2016.0,"M. Assante, Leonardo Candela, D. Castelli, A. Tani"
5724121504a3cc1ed648dd61bc980ffa7c4e1fb5,https://www.semanticscholar.org/paper/5724121504a3cc1ed648dd61bc980ffa7c4e1fb5,Addressing big data issues in Scientific Data Infrastructure,"Big Data are becoming a new technology focus both in science and in industry. This paper discusses the challenges that are imposed by Big Data on the modern and future Scientific Data Infrastructure (SDI). The paper discusses a nature and definition of Big Data that include such features as Volume, Velocity, Variety, Value and Veracity. The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model and suggests the major infrastructure components for Big Data.",2013.0,"Y. Demchenko, P. Grosso, C. D. Laat, Peter Membrey"
d358825a09c1da5804434d0ed147cb03e2a7e491,https://www.semanticscholar.org/paper/d358825a09c1da5804434d0ed147cb03e2a7e491,Mechanisms for Data Quality and Validation in Citizen Science,"Data quality is a primary concern for researchers employing a public participation in scientific research (PPSR) or ``citizen science'' approach. This mode of scientific collaboration relies on contributions from a large, often unknown population of volunteers with variable expertise. In a survey of PPSR projects, we found that most projects employ multiple mechanisms to ensure data quality and appropriate levels of validation. We created a framework of 18 mechanisms commonly employed by PPSR projects for ensuring data quality, based on direct experience of the authors and a review of the survey data, noting two categories of sources of error (protocols, participants) and three potential intervention points (before, during and after participation), which can be used to guide project design.",2011.0,"A. Wiggins, Greg Newman, R. Stevenson, Kevin Crowston"
def0cdb0398081295dec0a6d913105f8d5d94cac,https://www.semanticscholar.org/paper/def0cdb0398081295dec0a6d913105f8d5d94cac,Toward a manifesto for the ‘public understanding of big data’,"In this article, we sketch a ‘manifesto’ for the ‘public understanding of big data’. On the one hand, this entails such public understanding of science and public engagement with science and technology–tinged questions as follows: How, when and where are people exposed to, or do they engage with, big data? Who are regarded as big data’s trustworthy sources, or credible commentators and critics? What are the mechanisms by which big data systems are opened to public scrutiny? On the other hand, big data generate many challenges for public understanding of science and public engagement with science and technology: How do we address publics that are simultaneously the informant, the informed and the information of big data? What counts as understanding of, or engagement with, big data, when big data themselves are multiplying, fluid and recursive? As part of our manifesto, we propose a range of empirical, conceptual and methodological exhortations. We also provide Appendix 1 that outlines three novel methods for addressing some of the issues raised in the article.",2016.0,"M. Michael, Deborah Lupton"
f1933d47885d4854003ca1d134b399bfd5ac104e,https://www.semanticscholar.org/paper/f1933d47885d4854003ca1d134b399bfd5ac104e,Analyzing social science data,"PART ONE: HOW TO PREPARE DATA FOR ANALYSIS How to Code Data How to Code Questions with Multiple Answers Can the Respondent's Answers be Relied on? How to Check that the Right Thing is Being Measured TWO: HOW TO PREPARE VARIABLES FOR ANALYSIS How to Deal with Variables with Lots of Categories How to Identify and Change the Level of Measurement of Variables How to Deal with Questions that Fail to Identify Real Differences Between Cases How to Rearrange the Categories of a Variable What to do with Gaps in the Data What to do with People who 'Don't Know', 'Have no Opinion' or 'Can't Decide' How to Tell if the Distribution is Normal How to Tell if the Relationship is Linear How to Tell if Outlier Cases are a Problem What to do if the Required Variable is not Available How to Compare Apples with Oranges Comparing Scores on Different Variables PART THREE: HOW TO REDUCE THE AMOUNT OF DATA TO ANALYSE How to Work Out Which Variables to Use How to Combine Information from a Set of Variables into a Single Measure How to Build a Good Likert Scale How to Build a Scale Using Factor Analysis PART FOUR: HOW AND WHEN TO GENERALISE What Does it Mean to Generalize? How to Judge the Extent and Effect of Sample Bias How to Weight Samples to Adjust for Bias What are Tests of Significance? Should Tests of Significance be Used? What Factors Affect Significance Levels? Is the Sample Large Enough to Achieve Statistical Significance? Should Confidence Intervals be Used? PART FIVE: HOW TO ANALYSE A SINGLE VARIABLE How to Use Tables Effectively to Display the Distribution of a Single Variable How to Use Graphs for Single Variables Which Summary Statistics to Use to Describe a Single Variable Which Statistics to Use to Generalise about a Single Variable PART SIX: HOW TO ANALYSE TWO VARIABLES How and When to Use Crosstabulations Which Graph to Use How to Narrow down the Choice When Selecting Summary Statistics How to Interpret a Correlation Coefficient Which Correlation? How much Impact Does a Variable Have? How to Tell if Groups are Different Which Test of Significance? How are Confidence Intervals used in Bivariate Analysis? PART SEVEN: HOW TO CARRY OUT MULTIVARIATE ANALYSIS Understanding Bivariate Relationships The Logic of Elaboration Analysis Using Conditional Tables as a Method of Elaboration Analysis Using Conditional Correlations for Elaboration Analysis Using Partial Tables as a Method of Elaboration Analysis Using Partial Correlations for Elaboration Analysis What Type of Data are Needed for Multiple Regression? How to do a Multiple Regression How to Use Non-interval Variables in Multiple Regression What Does the Multiple Regression Output Mean? What Other Multivariate Methods are Availabe?",2002.0,D. Vaus
33c95f4d13958e65e676a9cd64bd4d8df0fdb44d,https://www.semanticscholar.org/paper/33c95f4d13958e65e676a9cd64bd4d8df0fdb44d,Sentiment Analysis and Opinion Mining,"Sentiment analysis and opinion mining is the field of study that analyzes people's opinions, sentiments, evaluations, attitudes, and emotions from written language. It is one of the most active research areas in natural language processing and is also widely studied in data mining, Web mining, and text mining. In fact, this research has spread outside of computer science to the management sciences and social sciences due to its importance to business and society as a whole. The growing importance of sentiment analysis coincides with the growth of social media such as reviews, forum discussions, blogs, micro-blogs, Twitter, and social networks. For the first time in human history, we now have a huge volume of opinionated data recorded in digital form for analysis. Sentiment analysis systems are being applied in almost every business and social domain because opinions are central to almost all human activities and are key influencers of our behaviors. Our beliefs and perceptions of reality, and the choices we make, are largely conditioned on how others see and evaluate the world. For this reason, when we need to make a decision we often seek out the opinions of others. This is true not only for individuals but also for organizations. This book is a comprehensive introductory and survey text. It covers all important topics and the latest developments in the field with over 400 references. It is suitable for students, researchers and practitioners who are interested in social media analysis in general and sentiment analysis in particular. Lecturers can readily use it in class for courses on natural language processing, social media analysis, text mining, and data mining. Lecture slides are also available online.",2012.0,"Lei Zhang, B. Liu"
49b56a85f4813dd4747b0bbb5584e7e6e7f54552,https://www.semanticscholar.org/paper/49b56a85f4813dd4747b0bbb5584e7e6e7f54552,Handbook of Zeolite Science and Technology,"The Handbook of Zeolite Science and Technology offers effective analyses ofsalient cases selected expressly for their relevance to current and prospective research. Presenting the principal theoretical and experimental underpinnings of zeolites, this international effort is at once complete and forward-looking, combining fundamental concepts with the most sophisticated data for each scientific subtopic and budding technology. Supplying over 750 figures, and 350 display equations, this impressive achievement in zeolite science observes synthesis through the lens of MFI (ZSM-5 and silicalite). Chapters progress from conceptual building blocks to complex research presentations.",2003.0,"S. Auerbach, K. Carrado, P. Dutta"
a50fac59cf792546d79f027ad5c20d52506ae262,https://www.semanticscholar.org/paper/a50fac59cf792546d79f027ad5c20d52506ae262,The nature of science and instructional practice: Making the unnatural natural,"The purpose of this study was to delineate the factors that mediate the translation of preservice teachers' conceptions of the nature of science (NOS) into instructional planning and classroom practice. Fourteen preservice secondary science teachers participated in the study. Prior to their student teaching, participants responded to an open-ended questionnaire designed to assess their conceptions of the NOS. Analysis of the questionnaires was postponed until after the completion of student teaching to avoid biasing the collection and/or analysis of other data sources. Throughout student teaching, participants' daily lesson plans, classroom videotapes, and portfolios, and supervisors' weekly clinical observation notes were collated. These data were searched for explicit references to the NOS. Following student teaching, participants were individually interviewed to validate their responses to the open-ended questionnaire and to identify the factors or constraints that mediate the translation of their conceptions of the NOS into their classroom teaching. Participants were found to possess adequate understandings of several important aspects of the NOS including the empirical and tentative nature of science, the distinction between observation and inference, and the role of subjectivity and creativity in science. Many claimed to have taught the NOS through science-based activities. However, data analyses revealed that explicit references to the NOS were rare in their planning and instruction. Participants articulated several factors for this lack of attention to the NOS. These included viewing the NOS as less significant than other instructional outcomes, preoccupation with classroom management and routine chores, discomfort with their own understandings of the NOS, the lack of resources and experience for teaching the NOS, cooperating teachers' imposed restraints, and the lack of planning time. In addition to these volunteered constraints, the data revealed others related to an intricate interaction between participants' perspectives on the NOS, pedagogy, and instructional outcomes. © 1998 John Wiley & Sons, Inc. Sci Ed82:417–436, 1998.",1998.0,"F. Abd‐El‐Khalick, R. Bell, Norman G. Lederman"
cf2dcb37a9cd154e9acdb1b9acc3b9b8d2c579fe,https://www.semanticscholar.org/paper/cf2dcb37a9cd154e9acdb1b9acc3b9b8d2c579fe,Challenges and Opportunities of Open Data in Ecology,"Ecology is a synthetic discipline benefiting from open access to data from the earth, life, and social sciences. Technological challenges exist, however, due to the dispersed and heterogeneous nature of these data. Standardization of methods and development of robust metadata can increase data access but are not sufficient. Reproducibility of analyses is also important, and executable workflows are addressing this issue by capturing data provenance. Sociological challenges, including inadequate rewards for sharing data, must also be resolved. The establishment of well-curated, federated data repositories will provide a means to preserve data while promoting attribution and acknowledgement of its use.",2011.0,"O. Reichman, Matthew B. Jones, M. Schildhauer"
effa683ec6111aaf4b4d29ab2fb26b832844d9e1,https://www.semanticscholar.org/paper/effa683ec6111aaf4b4d29ab2fb26b832844d9e1,Networks: An Introduction,"The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks.The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together for the first time the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.",2010.0,M. Newman
94e2b74c4ff14698c05fdfee398c10d2e6de3263,https://www.semanticscholar.org/paper/94e2b74c4ff14698c05fdfee398c10d2e6de3263,The Internet as a Data Source for Advancement in Social Sciences,"This paper advocates the use of Internet data for social sciences with a special focus on human resources issues. It discusses the potentials and challenges of Internet data for social sciences and presents a selection of the relevant literature to establish the wide spectrum of topics, which can be reached. Such data represent a large and increasing part of everyday life, which cannot be measured otherwise. They are timely, perhaps even daily following the factual process, they typically involve large numbers of observations, and they allow for flexible conceptual forms and experimental settings. Internet data can successfully be applied to a very wide range of human resource issues including forecasting (e.g. of unemployment, consumption goods, tourism, festival winners and the like), nowcasting (obtaining relevant information much earlier than through traditional data collection techniques), detecting health issues and well-being (e.g. flu, malaise and ill-being during economic crises), documenting the matching process in various parts of individual life (e.g. jobs, partnership, shopping), and measuring complex processes where traditional data have known deficits (e.g. international migration, collective bargaining agreements in developing countries). Major problems in data analysis are still unsolved and more research on data reliability is needed. Current research is highly original but also exploratory and premature. Our article reviews the current attempts in the literature to incorporate Internet data into the mainstream of scholarly empirical research and guides the reader through this Special Issue. We provide some insights and a brief overview of the current state of research.",2015.0,"N. Askitas, K. Zimmermann"
5f0099070ab2ed282b52c15d1ccc4eb0cf1ac162,https://www.semanticscholar.org/paper/5f0099070ab2ed282b52c15d1ccc4eb0cf1ac162,"Data sharing, small science and institutional repositories","Results are presented from the Data Curation Profiles project research, on who is willing to share what data with whom and when. Emerging from scientists’ discussions on sharing are several dimensions suggestive of the variation in both what it means ‘to share’ and how these processes are carried out. This research indicates that data curation services will need to accommodate a wide range of subdisciplinary data characteristics and sharing practices. As part of a larger set of strategies emerging across academic institutions, institutional repositories (IRs) will contribute to the stewardship and mobilization of scientific research data for e-Research and learning. There will be particular types of data that can be managed well in an IR context when characteristics and practices are well understood. Findings from this study elucidate scientists’ views on ‘sharable’ forms of data—the particular representation that they view as most valued for reuse by others within their own research areas—and the anticipated duration for such reuse. Reported sharing incidents that provide insights into barriers to sharing and related concerns on data misuse are included.",2010.0,"M. Cragin, C. Palmer, Jacob Carlson, M. Witt"
f816c7407ee34eab3d40d87e286ff8e1608d3d19,https://www.semanticscholar.org/paper/f816c7407ee34eab3d40d87e286ff8e1608d3d19,KONECT: the Koblenz network collection,"We present the Koblenz Network Collection (KONECT), a project to collect network datasets in the areas of web science, network science and related areas, as well as provide tools for their analysis. In the cited areas, a surprisingly large number of very heterogeneous data can be modeled as networks and consequently, a unified representation of networks can be used to gain insight into many kinds of problems. Due to the emergence of the World Wide Web in the last decades many such datasets are now openly available. The KONECT project thus has the goal of collecting many diverse network datasets from the Web, and providing a way for their systematic study. The main parts of KONECT are (1) a collection of over 160 network datasets, consisting of directed, undirected, unipartite, bipartite, weighted, unweighted, signed and temporal networks collected from the Web, (2) a Matlab toolbox for network analysis and (3) a website giving a compact overview the various computed statistics and plots. In this paper, we describe KONECT's taxonomy of networks datasets, give an overview of the datasets included, review the supported statistics and plots, and briefly discuss KONECT's role in the area of web science and network science.",2013.0,Jérôme Kunegis
1faa856958fd22125dff44c40d5fd7ba92e7cb3b,https://www.semanticscholar.org/paper/1faa856958fd22125dff44c40d5fd7ba92e7cb3b,Clustering by fast search and find of density peaks,"Discerning clusters of data points Cluster analysis is used in many disciplines to group objects according to a defined measure of distance. Numerous algorithms exist, some based on the analysis of the local density of data points, and others on predefined probability distributions. Rodriguez and Laio devised a method in which the cluster centers are recognized as local density maxima that are far away from any points of higher density. The algorithm depends only on the relative densities rather than their absolute values. The authors tested the method on a series of data sets, and its performance compared favorably to that of established techniques. Science, this issue p. 1492 Local density of points is ranked and analyzed to categorize data. Cluster analysis is aimed at classifying elements into categories on the basis of their similarity. Its applications range from astronomy to bioinformatics, bibliometrics, and pattern recognition. We propose an approach based on the idea that cluster centers are characterized by a higher density than their neighbors and by a relatively large distance from points with higher densities. This idea forms the basis of a clustering procedure in which the number of clusters arises intuitively, outliers are automatically spotted and excluded from the analysis, and clusters are recognized regardless of their shape and of the dimensionality of the space in which they are embedded. We demonstrate the power of the algorithm on several test cases.",2014.0,"Alex Rodriguez, A. Laio"
f0a3e1752e1146da927adc24ae07144ab2e744ec,https://www.semanticscholar.org/paper/f0a3e1752e1146da927adc24ae07144ab2e744ec,Nonlinear dimensionality reduction by locally linear embedding.,"Many areas of science depend on exploratory data analysis and visualization. The need to analyze large amounts of multivariate data raises the fundamental problem of dimensionality reduction: how to discover compact representations of high-dimensional data. Here, we introduce locally linear embedding (LLE), an unsupervised learning algorithm that computes low-dimensional, neighborhood-preserving embeddings of high-dimensional inputs. Unlike clustering methods for local dimensionality reduction, LLE maps its inputs into a single global coordinate system of lower dimensionality, and its optimizations do not involve local minima. By exploiting the local symmetries of linear reconstructions, LLE is able to learn the global structure of nonlinear manifolds, such as those generated by images of faces or documents of text.",2000.0,"S. Roweis, L. Saul"
e61cd349edf4f5e48246f7e89172996f6150d338,https://www.semanticscholar.org/paper/e61cd349edf4f5e48246f7e89172996f6150d338,The practice of social research,"Part I: AN INTRODUCTION TO INQUIRY. 1. Human Inquiry and Science. 2. Paradigms, Theory, and Social Research. 3. The Ethics and Politics of Social Research. Part II: THE STRUCTURING OF INQUIRY: QUANTITATIVE AND QUALITATIVE. 4. Research Design. 5. Conceptualization, Operationalization, and Measurement. 6. Indexes, Scales, and Typologies. 7. The Logic of Sampling. Part III: MODES OF OPERATION: QUANTITATIVE AND QUALITATIVE. 8. Experiments. 9. Survey Research. 10. Qualitative Field Research. 11. Unobtrusive Research. 12. Evaluation Research. Part IV: ANALYSIS OF DATA: QUANTITATIVE AND QUALITATIVE. 13. Qualitative Data Analysis. 14. Quantitative Data Analysis. 15. The Logic of Multivariate Analysis. 16. Statistical Analyses. 17. Reading and Writing Social Research. APPENDICES. A. Using the Library. B. GSS Household Enumeration Questionnaire. C. Random Numbers. D. Distribution of Chi Square. E. Normal Curve Areas. F. Estimated Sampling Error. Preface. Acknowledgments.",1969.0,E. Babbie
59bb17b27d7220e930ee6bebe1f94cf43ad42c2d,https://www.semanticscholar.org/paper/59bb17b27d7220e930ee6bebe1f94cf43ad42c2d,Model selection and multimodel inference : a practical information-theoretic approach,"The second edition of this book is unique in that it focuses on methods for making formal statistical inference from all the models in an a priori set (Multi-Model Inference). A philosophy is presented for model-based data analysis and a general strategy outlined for the analysis of empirical data. The book invites increased attention on a priori science hypotheses and modeling. Kullback-Leibler Information represents a fundamental quantity in science and is Hirotugu Akaike's basis for model selection. The maximized log-likelihood function can be bias-corrected as an estimator of expected, relative Kullback-Leibler information. This leads to Akaike's Information Criterion (AIC) and various extensions. These methods are relatively simple and easy to use in practice, but based on deep statistical theory. The information theoretic approaches provide a unified and rigorous theory, an extension of likelihood theory, an important application of information theory, and are objective and practical to employ across a very wide class of empirical problems. The book presents several new ways to incorporate model selection uncertainty into parameter estimates and estimates of precision. An array of challenging examples is given to illustrate various technical issues. This is an applied book written primarily for biologists and statisticians wanting to make inferences from multiple models and is suitable as a graduate text or as a reference for professional analysts.",2003.0,"K. Burnham, David R. Anderson"
d91557927a1571efc5a1599a9c0889d7f1bff7a2,https://www.semanticscholar.org/paper/d91557927a1571efc5a1599a9c0889d7f1bff7a2,Applied Logistic Regression: Hosmer/Applied Logistic Regression,"""A new edition of the definitive guide to logistic regression modeling for health science and other applicationsThis thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: A chapter on the analysis of correlated outcome data. A wealth of additional material for topics ranging from Bayesian methods to assessing model fit Rich data sets from real-world studies that demonstrate each method under discussion. Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines""--",2005.0,"D. Hosmer, S. Lemeshow, Rodney X. Sturdivant"
cae155b295abd5b0ab02fb26351720c40e969907,https://www.semanticscholar.org/paper/cae155b295abd5b0ab02fb26351720c40e969907,The Structure of Scientific Revolutions,"A good book may have the power to change the way we see the world, but a great book actually becomes part of our daily consciousness, pervading our thinking to the point that we take it for granted, and we forget how provocative and challenging its ideas once were-and still are. ""The Structure of Scientific Revolutions"" is that kind of book. When it was first published in 1962, it was a landmark event in the history and philosophy of science. And fifty years later, it still has many lessons to teach. With ""The Structure of Scientific Revolutions"", Kuhn challenged long-standing linear notions of scientific progress, arguing that transformative ideas don't arise from the day-to-day, gradual process of experimentation and data accumulation, but that revolutions in science, those breakthrough moments that disrupt accepted thinking and offer unanticipated ideas, occur outside of ""normal science,"" as he called it. Though Kuhn was writing when physics ruled the sciences, his ideas on how scientific revolutions bring order to the anomalies that amass over time in research experiments are still instructive in our biotech age. This new edition of Kuhn's essential work in the history of science includes an insightful introductory essay by Ian Hacking that clarifies terms popularized by Kuhn, including paradigm and incommensurability, and applies Kuhn's ideas to the science of today. Usefully keyed to the separate sections of the book, Hacking's essay provides important background information as well as a contemporary context. Newly designed, with an expanded index, this edition will be eagerly welcomed by the next generation of readers seeking to understand the history of our perspectives on science.",1963.0,"T. Kuhn, David Hawkins"
ef07defaf08123d5e1a8bd41ad6e2db5e5b225e3,https://www.semanticscholar.org/paper/ef07defaf08123d5e1a8bd41ad6e2db5e5b225e3,The spread of true and false news online,"Lies spread faster than the truth There is worldwide concern over false news and the possibility that it can influence political, economic, and social well-being. To understand how false news spreads, Vosoughi et al. used a data set of rumor cascades on Twitter from 2006 to 2017. About 126,000 rumors were spread by ∼3 million people. False news reached more people than the truth; the top 1% of false news cascades diffused to between 1000 and 100,000 people, whereas the truth rarely diffused to more than 1000 people. Falsehood also diffused faster than the truth. The degree of novelty and the emotional reactions of recipients may be responsible for the differences observed. Science, this issue p. 1146 A large-scale analysis of tweets reveals that false rumors spread further and faster than the truth. We investigated the differential diffusion of all of the verified true and false news stories distributed on Twitter from 2006 to 2017. The data comprise ~126,000 stories tweeted by ~3 million people more than 4.5 million times. We classified news as true or false using information from six independent fact-checking organizations that exhibited 95 to 98% agreement on the classifications. Falsehood diffused significantly farther, faster, deeper, and more broadly than the truth in all categories of information, and the effects were more pronounced for false political news than for false news about terrorism, natural disasters, science, urban legends, or financial information. We found that false news was more novel than true news, which suggests that people were more likely to share novel information. Whereas false stories inspired fear, disgust, and surprise in replies, true stories inspired anticipation, sadness, joy, and trust. Contrary to conventional wisdom, robots accelerated the spread of true and false news at the same rate, implying that false news spreads more than the truth because humans, not robots, are more likely to spread it.",2018.0,"Soroush Vosoughi, D. Roy, Sinan Aral"
da692ee969d9c33986196372c3f7cb87fa6b6f8f,https://www.semanticscholar.org/paper/da692ee969d9c33986196372c3f7cb87fa6b6f8f,Database resources of the National Center for Biotechnology Information,"Abstract The National Center for Biotechnology Information (NCBI) provides a large suite of online resources for biological information and data, including the GenBank® nucleic acid sequence database and the PubMed database of citations and abstracts for published life science journals. The Entrez system provides search and retrieval operations for most of these data from 39 distinct databases. The E-utilities serve as the programming interface for the Entrez system. Augmenting many of the Web applications are custom implementations of the BLAST program optimized to search specialized data sets. New resources released in the past year include PubMed Data Management, RefSeq Functional Elements, genome data download, variation services API, Magic-BLAST, QuickBLASTp, and Identical Protein Groups. Resources that were updated in the past year include the genome data viewer, a human genome resources page, Gene, virus variation, OSIRIS, and PubChem. All of these resources can be accessed through the NCBI home page at www.ncbi.nlm.nih.gov.",2017.0,"Richa Tanya Jeff Dennis A Colleen Evan Devon J Rodney St Agarwala Barrett Beck Benson Bollin Bolton Bourexi, R. Agarwala, T. Barrett, J. Beck, Dennis A. Benson, Colleen J. Bollin, Evan E. Bolton, Devon Bourexis, J. R. Brister, S. Bryant, Kathi Canese, Mark Cavanaugh, Chad Charowhas, Karen Clark, I. Dondoshansky, M. Feolo, Lawrence Fitzpatrick, Kathryn Funk, Lewis Y. Geer, V. Gorelenkov, Alan Graeff, W. Hlavina, Brad Holmes, Mark Johnson, B. Kattman, Viatcheslav Khotomlianski, Avi Kimchi, Michael Kimelman, Masato Kimura, P. Kitts, W. Klimke, A. Kotliarov, S. Krasnov, A. Kuznetsov, M. Landrum, D. Landsman, S. Lathrop, Jennifer M. Lee, Carl Leubsdorf, Zhiyong Lu, Thomas L. Madden, Aron Marchler-Bauer, Adriana Malheiro, Peter A. Meric, I. Karsch-Mizrachi, Anatoly Mnev, Terence D. Murphy, R. Orris, J. Ostell, Christopher O'Sullivan, Vasuki Palanigobu, A. Panchenko, Lon Phan, Borys Pierov, K. Pruitt, K. Rodarmer, E. Sayers, Valerie A. Schneider, C. Schoch, G. Schuler, S. Sherry, Karanjit Siyan, Alexandra Soboleva, Vladimir Soussov, G. Starchenko, T. Tatusova, F. Thibaud-Nissen, K. Todorov, B. Trawick, D. Vakatov, Minghong Ward, E. Yaschenko, A. Zasypkin, Kerry Zbicz"
73504bdcc93ffdfc65f33929d8e8a328c79eb0c6,https://www.semanticscholar.org/paper/73504bdcc93ffdfc65f33929d8e8a328c79eb0c6,Amazon's Mechanical Turk,"Amazon’s Mechanical Turk (MTurk) is a relatively new website that contains the major elements required to conduct research: an integrated participant compensation system; a large participant pool; and a streamlined process of study design, participant recruitment, and data collection. In this article, we describe and evaluate the potential contributions of MTurk to psychology and other social sciences. Findings indicate that (a) MTurk participants are slightly more demographically diverse than are standard Internet samples and are significantly more diverse than typical American college samples; (b) participation is affected by compensation rate and task length, but participants can still be recruited rapidly and inexpensively; (c) realistic compensation rates do not affect data quality; and (d) the data obtained are at least as reliable as those obtained via traditional methods. Overall, MTurk can be used to obtain high-quality data inexpensively and rapidly.",2011.0,"Michael D. Buhrmester, T. Kwang, S. Gosling"
49594776b8a1dce89e976c846266ccefafa948b7,https://www.semanticscholar.org/paper/49594776b8a1dce89e976c846266ccefafa948b7,Stochastic Processes,"Stochastic processes are probabilistic models of data streams such as speech, audio and video signals, stock market prices, and measurements of physical phenomena by digital sensors such as medical instruments, GPS receivers, or seismographs. A solid understanding of the mathematical basis of these models is essential for understanding phenomena and processing information in many branches of science and engineering including physics, communications, signal processing, automation, and structural dynamics.",2018.0,Dr. Gergely Záruba
4e7c457b21d8ba062fdbe4cefa16c0f29a2576de,https://www.semanticscholar.org/paper/4e7c457b21d8ba062fdbe4cefa16c0f29a2576de,Compositional data analysis : theory and applications,"It is difficult to imagine that the statistical analysis of compositional data has been a major issue of concern for more than 100 years. It is even more difficult to realize that so many statisticians and users of statistics are unaware of the particular problems affecting compositional data, as well as their solutions. The issue of ``spurious correlation'', as the situation was phrased by Karl Pearson back in 1897, affects all data that measures parts of some whole, such as percentages, proportions, ppm and ppb. Such measurements are present in all fields of science, ranging from geology, biology, environmental sciences, forensic sciences, medicine and hydrology. This book presents the history and development of compositional data analysis along with Aitchison's log-ratio approach. Compositional Data Analysis describes the state of the art both in theoretical fields as well as applications in the different fields of science.",2011.0,"V. Pawlowsky-Glahn, A. Buccianti"
136afcc7e8d0bb5c6b3dd68dcc0117c746c37d02,https://www.semanticscholar.org/paper/136afcc7e8d0bb5c6b3dd68dcc0117c746c37d02,Qualitative Case Study Methodology: Study Design and Implementation for Novice Researchers,"Qualitative case study methodology provides tools for researchers to study complex phenomena within their contexts. When the approach is applied correctly, it becomes a valuable method for health science research to develop theory, evaluate programs, and develop interventions. The purpose of this paper is to guide the novice researcher in identifying the key elements for designing and implementing qualitative case study research projects. An overview of the types of case study designs is provided along with general recommendations for writing the research questions, developing propositions, determining the “case” under study, binding the case and a discussion of data sources and triangulation. To facilitate application of these principles, clear examples of research questions, study propositions and the different types of case study designs",2008.0,"Pamela Baxter, S. Jack"
3a24b6a70aa8c88192cb4b584bd5cd0ec631c0ca,https://www.semanticscholar.org/paper/3a24b6a70aa8c88192cb4b584bd5cd0ec631c0ca,Heart Disease and Stroke Statistics—2015 Update: A Report From the American Heart Association,"STRIDE (Stanford Translational Research Integrated Database Environment) is a research and development project at Stanford University to create a standards-based informatics platform supporting clinical and translational research. STRIDE consists of three integrated components: a clinical data warehouse, based on the HL7 Reference Information Model (RIM), containing clinical information on over 1.3 million pediatric and adult patients cared for at Stanford University Medical Center since 1995; an application development framework for building research data management applications on the STRIDE platform and a biospecimen data management system. STRIDE’s semantic model uses standardized terminologies, such as SNOMED, RxNorm, ICD and CPT, to represent important biomedical concepts and their relationships. The system is in daily use at Stanford and is an important component of Stanford University’s CTSA (Clinical and Translational Science Award) Informatics Program.",2009.0,"D. Mozaffarian, E. Benjamin, A. Go, D. Arnett, M. Blaha, M. Cushman, S. D. de Ferranti, J. Despres, H. Fullerton, V. Howard, Mark D. Huffman, S. Judd, B. Kissela, D. Lackland, J. Lichtman, L. Lisabeth, Simin Liu, R. Mackey, D. Matchar, Darren K Mcguire, E. Mohler, C. Moy, P. Muntner, M. Mussolino, K. Nasir, R. Neumar, G. Nichol, L. Palaniappan, D. Pandey, M. Reeves, C. Rodriguez, P. Sorlie, J. Stein, A. Towfighi, T. Turan, S. Virani, J. Willey, D. Woo, R. Yeh, M. B. Turner"
3efd851140aa28e95221b55fcc5659eea97b172d,https://www.semanticscholar.org/paper/3efd851140aa28e95221b55fcc5659eea97b172d,The Graph Neural Network Model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.",2009.0,"F. Scarselli, M. Gori, A. Tsoi, M. Hagenbuchner, G. Monfardini"
d825b7ab9c0093c78a0d5c665815e0becc48cd9b,https://www.semanticscholar.org/paper/d825b7ab9c0093c78a0d5c665815e0becc48cd9b,Dynamic changes in motivation in collaborative citizen-science projects,"Online citizen science projects engage volunteers in collecting, analyzing, and curating scientific data. Existing projects have demonstrated the value of using volunteers to collect data, but few projects have reached the full collaborative potential of scientists and volunteers. Understanding the shared and unique motivations of these two groups can help designers establish the technical and social infrastructures needed to promote effective partnerships. We present findings from a study of the motivational factors affecting participation in ecological citizen science projects. We show that volunteers are motivated by a complex framework of factors that dynamically change throughout their cycle of work on scientific projects; this motivational framework is strongly affected by personal interests as well as external factors such as attribution and acknowledgment. Identifying the pivotal points of motivational shift and addressing them in the design of citizen-science systems will facilitate improved collaboration between scientists and volunteers.",2012.0,"D. Rotman, J. Preece, J. Hammock, Kezee Procita, Derek L. Hansen, C. Parr, Darcy Lewis, D. Jacobs"
85a8a97f614b2b6823e035bcc9abcb0f3d27be4d,https://www.semanticscholar.org/paper/85a8a97f614b2b6823e035bcc9abcb0f3d27be4d,An Introduction to the Bootstrap,"Statistics is the science of learning from experience, especially experience that arrives a little bit at a time. The earliest information 
science was statistics, originating in about 1650. This century has 
seen statistical techniques become the analytic methods of choice 
in biomedical science, psychology, education, economics, communications theory, sociology, genetic studies, epidemiology, and other 
areas. Recently, traditional sciences like geology, physics, and astronomy have begun to make increasing use of statistical methods 
as they focus on areas that demand informational efficiency, such as 
the study of rare and exotic particles or extremely distant galaxies. 
Most people are not natural-born statisticians. Left to our own 
devices we are not very good at picking out patterns from a sea 
of noisy data. To put it another way, we are all too good at picking out non-existent patterns that happen to suit our purposes. 
Statistical theory attacks the problem from both ends. It provides 
optimal methods for finding a real signal in a noisy background, 
and also provides strict checks against the overinterpretation of 
random patterns.",1995.0,"B. Efron, R. Tibshirani"
944eb4fc8737f5dbe72f4a73f9db58418eec2758,https://www.semanticscholar.org/paper/944eb4fc8737f5dbe72f4a73f9db58418eec2758,The structure of scientific collaboration networks.,"The structure of scientific collaboration networks is investigated. Two scientists are considered connected if they have authored a paper together and explicit networks of such connections are constructed by using data drawn from a number of databases, including MEDLINE (biomedical research), the Los Alamos e-Print Archive (physics), and NCSTRL (computer science). I show that these collaboration networks form ""small worlds,"" in which randomly chosen pairs of scientists are typically separated by only a short path of intermediate acquaintances. I further give results for mean and distribution of numbers of collaborators of authors, demonstrate the presence of clustering in the networks, and highlight a number of apparent differences in the patterns of collaboration between the fields studied.",2000.0,M. Newman
10a463bb00b44bdd3a8620f2bedb9e1564bfcf32,https://www.semanticscholar.org/paper/10a463bb00b44bdd3a8620f2bedb9e1564bfcf32,The Design and Analysis of Computer Algorithms,"From the Publisher: 
With this text, you gain an understanding of the fundamental concepts of algorithms, the very heart of computer science. It introduces the basic data structures and programming techniques often used in efficient algorithms. Covers use of lists, push-down stacks, queues, trees, and graphs. Later chapters go into sorting, searching and graphing algorithms, the string-matching algorithms, and the Schonhage-Strassen integer-multiplication algorithm. Provides numerous graded exercises at the end of each chapter. 
 
 
0201000296B04062001",1974.0,"A. Aho, J. Hopcroft, J. Ullman"
d56c1e5d37f9e71bb1b6a08e0661ebb51a9ec9ab,https://www.semanticscholar.org/paper/d56c1e5d37f9e71bb1b6a08e0661ebb51a9ec9ab,Narrative Inquiry: Experience and Story in Qualitative Research,"'The literature on narrative inquiry has been, until now, widely scattered and theoretically incomplete. Clandinin and Connelly have created a major tour de force. This book is lucid, fluid, beautifully argued, and rich in examples. Students will find a wealth of arguments to support their research, and teaching faculty will find everything they need to teach narrative inquiry theory and methods' - Yvonna S. Lincoln, professor, Department of Educational Administration, Texas A&M University.Understanding experience as lived and told stories - also known as narrative inquiry - has gained popularity and credence in qualitative research. Unlike more traditional methods, narrative inquiry successfully captures personal and human dimensions that cannot be quantified into dry facts and numerical data. In this definitive guide, Jean Clandinin and Michael Connelly draw from more than twenty years of field experience to show how narrative inquiry can be used in educational and social science research. Tracing the origins of narrative inquiry in the social sciences, they offer new and practical ideas for conducting fieldwork, composing field notes, and conveying research results. Throughout the book, stories and examples reveal a wide range of narrative methods. Engaging and easy to read, ""Narrative Inquiry"" is a practical resource from experts who have long pioneered the use of narrative in qualitative research.",1999.0,"D. Clandinin, F. Connelly"
6da5d24defba21364de4842d65666118e46edf12,https://www.semanticscholar.org/paper/6da5d24defba21364de4842d65666118e46edf12,A New Product Growth for Model Consumer Durables,"(This article originally appeared in Management Science, January 1969, Volume 15, Number 5, pp. 215-227, published by The Institute of Management Sciences.) 
 
A growth model for the timing of initial purchase of new products is developed and tested empirically against data for eleven consumer durables. The basic assumption of the model is that the timing of a consumer's initial purchase is related to the number of previous buyers. A behavioral rationale for the model is offered in terms of innovative and imitative behavior. The model yields good predictions of the sales peak and the timing of the peak when applied to historical data. A long-range forecast is developed for the sales of color television sets.",2004.0,F. Bass
5f3d1106094be3017046d7953c2a71e7f4559124,https://www.semanticscholar.org/paper/5f3d1106094be3017046d7953c2a71e7f4559124,Systematic Reviews in the Social Sciences: A Practical Guide,"Such diverse thinkers as Lao-Tze, Confucius, and U.S. Defense Secretary Donald Rumsfeld have all pointed out that we need to be able to tell the difference between real and assumed knowledge. The systematic review is a scientific tool that can help with this difficult task. It can help, for example, with appraising, summarising, and communicating the results and implications of otherwise unmanageable quantities of data. This is important because quite often there are so many studies, and their results are often so conflicting, that no policymaker or practitioner could possibly carry out this task themselves.Systematic review methods have been widely used in health care, and are becoming increasingly common in the social sciences (fostered, for example, by the work of the Campbell Collaboration). 
 
This book outlines the rationale and methods of systematic reviews, giving worked examples from social science and other fields. It requires no previous knowledge, but takes the reader through the process stage by stage. It draws on examples from such diverse fields as psychology, criminology, education, transport, social welfare, public health, and housing and urban policy, among others.The book includes detailed sections on assessing the quality of both quantitative, and qualitative research; searching for evidence in the social sciences;meta-analytic and other methods of evidence synthesis; publication bias; heterogeneity; and approaches to dissemination.",2005.0,"M. Petticrew, H. Roberts"
c4e2a160c5a4c0de3036935e95cb266d00546762,https://www.semanticscholar.org/paper/c4e2a160c5a4c0de3036935e95cb266d00546762,"Applied Logistic Regression, Second Edition","""A new edition of the definitive guide to logistic regression modeling for health science and other applicationsThis thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: A chapter on the analysis of correlated outcome data. A wealth of additional material for topics ranging from Bayesian methods to assessing model fit Rich data sets from real-world studies that demonstrate each method under discussion. Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines""--",1989.0,"D. Hosmer, S. Lemeshow"
ccaf829bffd0b1a55a67a6958dcfb7af4cd16641,https://www.semanticscholar.org/paper/ccaf829bffd0b1a55a67a6958dcfb7af4cd16641,Formal Concept Analysis: Mathematical Foundations,"From the Publisher: 
This is the first textbook on formal concept analysis. It gives a systematic presentation of the mathematical foundations and their relation to applications in computer science, especially in data analysis and knowledge processing. Above all, it presents graphical methods for representing conceptual systems that have proved themselves in communicating knowledge. Theory and graphical representation are thus closely coupled together. The mathematical foundations are treated thoroughly and illuminated by means of numerous examples.",1998.0,"B. Ganter, Rudolf Wille, C. Franzke"
1e52db1f61a5f0083cbe87845c019ab351bfe6c9,https://www.semanticscholar.org/paper/1e52db1f61a5f0083cbe87845c019ab351bfe6c9,Statistical learning theory,"A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more.",1998.0,V. Vapnik
1996013872df1d43bca8c693786c3853e99aa9ff,https://www.semanticscholar.org/paper/1996013872df1d43bca8c693786c3853e99aa9ff,"Research Design, Falsification, and the Qualitative–Quantitative Divide","R eceiving five serious reviews in this symposium is gratifying and confirms our belief that research design should be a priority for our discipline. We are pleased that our five distinguished reviewers appear to agree with our unified approach to the logic of inference in the social sciences, and with our fundamental point: that good quantitative and good qualitative research designs are based fundamentally on the same logic of inference. The reviewers also raised virtually no objections to the main practical contribution of our book-our many specific procedures for avoiding bias, getting the most out of qualitative data, and making reliable inferences. However, the reviews make clear that although our book may be the latest word on research design in political science, it is surely not the last. We are taxed for failing to include important issues in our analysis and for dealing inadequately with some of what we included. Before responding to the reviewers' most direct criticisms, let us explain what we emphasize in Designing Social Inquiry and how it relates to some of the points raised by the reviewers.",1995.0,"R. Alford, Gary King, R. Keohane, S. Verba"
7e4e5065f64fffe868640f05a9e7ed296cda4e0f,https://www.semanticscholar.org/paper/7e4e5065f64fffe868640f05a9e7ed296cda4e0f,Social Vulnerability to Environmental Hazards,"County-level socioeconomic and demographic data were used to construct an index of social vulnerability to environmental hazards, called the Social Vulnerability Index (SoVI) for the United States based on 1990 data. Copyright (c) 2003 by the Southwestern Social Science Association.",2003.0,"S. Cutter, B. Boruff, W. L. Shirley"
08b43d84e6747e370ef307e2ada50675b414514a,https://www.semanticscholar.org/paper/08b43d84e6747e370ef307e2ada50675b414514a,Survey of clustering algorithms,"Data analysis plays an indispensable role for understanding various phenomena. Cluster analysis, primitive exploration with little or no prior knowledge, consists of research developed across a wide variety of communities. The diversity, on one hand, equips us with many tools. On the other hand, the profusion of options causes confusion. We survey clustering algorithms for data sets appearing in statistics, computer science, and machine learning, and illustrate their applications in some benchmark data sets, the traveling salesman problem, and bioinformatics, a new field attracting intensive efforts. Several tightly related topics, proximity measure, and cluster validation, are also discussed.",2005.0,"Rui Xu, D. Wunsch"
d70e50a2cf1adb0a8de34e28de9cde267b931e26,https://www.semanticscholar.org/paper/d70e50a2cf1adb0a8de34e28de9cde267b931e26,The concepts of stress and stress system disorders. Overview of physical and behavioral homeostasis.,"OBJECTIVE
This article defines stress and related concepts and reviews their historical development. The notion of a stress system as the effector of the stress syndrome is suggested, and its physiologic and pathophysiologic manifestations are described. A new perspective on human disease states associated with dysregulation of the stress system is provided.


DATA SOURCES
Published original articles from human and animal studies and selected reviews. Literature was surveyed utilizing MEDLINE and the Index Medicus.


STUDY SELECTION
Original articles from the basic science and human literature consisted entirely of controlled studies based on verified methodologies and, with the exception of the most recent studies, replicated by more than one laboratory. Many of the basic science and clinical studies had been conducted in our own laboratories and clinical research units. Reviews cited were written by acknowledged leaders in the fields of neurobiology, endocrinology, and behavior.


DATA EXTRACTION
Independent extraction and cross-referencing by the authors.


DATA SYNTHESIS
Stress and related concepts can be traced as far back as written science and medicine. The stress system coordinates the generalized stress response, which takes place when a stressor of any kind exceeds a threshold. The main components of the stress system are the corticotropin-releasing hormone and locus ceruleus-norepinephrine/autonomic systems and their peripheral effectors, the pituitary-adrenal axis, and the limbs of the autonomic system. Activation of the stress system leads to behavioral and peripheral changes that improve the ability of the organism to adjust homeostasis and increase its chances for survival. There has been an exponential increase in knowledge regarding the interactions among the components of the stress system and between the stress system and other brain elements involved in the regulation of emotion, cognitive function, and behavior, as well as with the axes responsible for reproduction, growth, and immunity. This new knowledge has allowed association of stress system dysfunction, characterized by sustained hyperactivity and/or hypoactivity, to various pathophysiologic states that cut across the traditional boundaries of medical disciplines. These include a range of psychiatric, endocrine, and inflammatory disorders and/or susceptibility to such disorders.


CONCLUSIONS
We hope that knowledge from apparently disparate fields of science and medicine integrated into a working theoretical framework will allow generation and testing of new hypotheses on the pathophysiology and diagnosis of, and therapy for, a variety of human illnesses reflecting systematic alterations in the principal effectors of the generalized stress response. We predict that pharmacologic agents capable of altering the central apparatus that governs the stress response will be useful in the treatment of many of these illnesses.",1992.0,"G. Chrousos, P. Gold"
ad4f067304551c20b50a3c0f7e02e3e9d6946003,https://www.semanticscholar.org/paper/ad4f067304551c20b50a3c0f7e02e3e9d6946003,Automated data reduction workflows for astronomy,"Data from complex modern astronomical instruments often consist of a large number of different science and calibration files, and their reduction requires a variety of software tools. The execution chain of the tools represents a complex workflow that needs to be tuned and supervised, often by individual researchers that are not necessarily experts for any specific instrument. The efficiency of data reduction can be improved by using automatic workflows to organise data and execute the sequence of data reduction steps. To realize such efficiency gains, we designed a system that allows intuitive representation, execution and modification of the data reduction workflow, and has facilities for inspection and interaction with the data. The European Southern Observatory (ESO) has developed Reflex, an environment to automate data reduction workflows. Reflex is implemented as a package of customized components for the Kepler workflow engine. Kepler provides the graphical user interface to create an executable flowchart-like representation of the data reduction process. Key features of Reflex are a rule-based data organiser, infrastructure to re-use results, thorough book-keeping, data progeny tracking, interactive user interfaces, and a novel concept to exploit information created during data organisation for the workflow execution. Reflex includes novel concepts to increase the efficiency of astronomical data processing. While Reflex is a specific implementation of astronomical scientific workflows within the Kepler workflow engine, the overall design choices and methods can also be applied to other environments for running automated science workflows.",2013.0,"W. Freudling, M. Romaniello, D. Bramich, P. Ballester, V. Forchì, C. E. Garcia-Dablo, S. Moehler, M. Neeser"
a2b5e0c1d0b23d11fd8497f1b16fc9564246b482,https://www.semanticscholar.org/paper/a2b5e0c1d0b23d11fd8497f1b16fc9564246b482,The National Institutes of Health's Big Data to Knowledge (BD2K) initiative: capitalizing on biomedical big data,"Biomedical research has and will continue to generate large amounts of data (termed ‘big data’) in many formats and at all levels. Consequently, there is an increasing need to better understand and mine the data to further knowledge and foster new discovery. The National Institutes of Health (NIH) has initiated a Big Data to Knowledge (BD2K) initiative to maximize the use of biomedical big data. BD2K seeks to better define how to extract value from the data, both for the individual investigator and the overall research community, create the analytic tools needed to enhance utility of the data, provide the next generation of trained personnel, and develop data science concepts and tools that can be made available to all stakeholders.",2014.0,"Ronald Margolis, L. Derr, Michelle Dunn, Michael F. Huerta, Jennie Larkin, J. Sheehan, M. Guyer, E. Green"
f825b113bbb88ab40253169fc3480e13109348ec,https://www.semanticscholar.org/paper/f825b113bbb88ab40253169fc3480e13109348ec,The effect of human mobility and control measures on the COVID-19 epidemic in China,"Tracing infection from mobility data What sort of measures are required to contain the spread of severe acute respiratory syndrome–coronavirus 2 (SARS-CoV-2), which causes coronavirus disease 2019 (COVID-19)? The rich data from the Open COVID-19 Data Working Group include the dates when people first reported symptoms, not just a positive test date. Using these data and real-time travel data from the internet services company Baidu, Kraemer et al. found that mobility statistics offered a precise record of the spread of SARS-CoV-2 among the cities of China at the start of 2020. The frequency of introductions from Wuhan were predictive of the size of the epidemic sparked in other provinces. However, once the virus had escaped Wuhan, strict local control measures such as social isolation and hygiene, rather than long-distance travel restrictions, played the largest part in controlling SARS-CoV-2 spread. Science, this issue p. 493 Mobile phone data show that the spread of COVID-19 in China was driven by travel and mitigated substantially by local control measures. The ongoing coronavirus disease 2019 (COVID-19) outbreak expanded rapidly throughout China. Major behavioral, clinical, and state interventions were undertaken to mitigate the epidemic and prevent the persistence of the virus in human populations in China and worldwide. It remains unclear how these unprecedented interventions, including travel restrictions, affected COVID-19 spread in China. We used real-time mobility data from Wuhan and detailed case data including travel history to elucidate the role of case importation in transmission in cities across China and to ascertain the impact of control measures. Early on, the spatial distribution of COVID-19 cases in China was explained well by human mobility data. After the implementation of control measures, this correlation dropped and growth rates became negative in most locations, although shifts in the demographics of reported cases were still indicative of local chains of transmission outside of Wuhan. This study shows that the drastic control measures implemented in China substantially mitigated the spread of COVID-19.",2020.0,"M. Kraemer, Chia-Hung Yang, B. Gutiérrez, Chieh-Hsi Wu, Brennan Klein, D. Pigott, L. du Plessis, Nuno R. Faria, Ruoran Li, W. Hanage, J. Brownstein, Maylis Layan, A. Vespignani, H. Tian, C. Dye, O. Pybus, S. Scarpino"
e4fec57300d4033aa9372501a8b3a72c15a4384e,https://www.semanticscholar.org/paper/e4fec57300d4033aa9372501a8b3a72c15a4384e,Applied statistics for the behavioral sciences,"This introductory text provides students with a conceptual understanding of basic statistical procedures, as well as the computational skills needed to complete them. The clear presentation, accessible language, and step-by-step instruction make it easy for students from a variety of social science disciplines to grasp the material. The scenarios presented in chapter exercises span the curriculum, from political science to marketing, so that students make a connection between their own area of interest and the study of statistics. Unique coverage focuses on concepts critical to understanding current statistical research such as power and sample size, multiple comparison tests, multiple regression, and analysis of covariance. Additional SPSS coverage throughout the text includes computer printouts and expanded discussion of their contents in interpreting the results of sample exercises. 1. Introduction. 2. Organizing and Graphing Data. 3. Describing Distributions: Individual Scores, Central Tendency, and Variation. 4. The Normal Distribution. 5. Correlation: A Measure of Relationship. 6. Linear Regression: Prediction. 7. Sampling, Probability, and Sampling Distributions. 8. Hypothesis Testing: One-Sample Case for the Mean. 9. Estimation: One-Sample Case for the Mean. 10. Hypothesis Testing: One-Sample Case for Other Statistics. 11. Hypothesis Testing: Two-Sample Case for the Mean. 12. Hypothesis Testing: Two-Sample Case for Other Statistics. 13. Determining Power and Sample Size. 14. Hypothesis Testing, K-Sample Case: Analysis of Variance, One-Way Classification. 15. Multiple-Comparison Procedures. 16. Analysis of Variance, Two-Way Classification. 17. Linear Regression: Estimation and Hypothesis Testing. 18. Multiple Linear Regression. 19. Analysis of Covariance. 20. Other Correlation Coefficients. 21. Chi-Square (X2) Tests for Frequencies. 22. Other Nonparametric Tests.",1979.0,"D. Hinkle, W. Wiersma, S. Jurs"
b9bb6963c8291a9a3b697d30d8e8979c25c51f02,https://www.semanticscholar.org/paper/b9bb6963c8291a9a3b697d30d8e8979c25c51f02,Classical and modern regression with applications,"The author emphasizes applications with examples that illustrate nearly all the techniques discussed. Applications have been selected from physical sciences, engineering, biology, management science and economics. Emphasis is also placed on concepts with a blend between illustrations using real data sets and mathematical and conceptual development. Expanded coverage includes: simultaneous influence, maximum likelihood estimation of parameters, and the plotting of residuals, the use of the general linear hypothesis, indicator variables, the geometry of least squares, the relationship to ANOVA models, Box-Cox transformation with illustrations, categorical response, other nonnormal error situations, autocorrelated errors and logistic regression.",1986.0,R. H. Myers
67628543f7ae51979acadfbb8860568b25f263da,https://www.semanticscholar.org/paper/67628543f7ae51979acadfbb8860568b25f263da,Detecting Spacecraft Anomalies Using LSTMs and Nonparametric Dynamic Thresholding,"As spacecraft send back increasing amounts of telemetry data, improved anomaly detection systems are needed to lessen the monitoring burden placed on operations engineers and reduce operational risk. Current spacecraft monitoring systems only target a subset of anomaly types and often require costly expert knowledge to develop and maintain due to challenges involving scale and complexity. We demonstrate the effectiveness of Long Short-Term Memory (LSTMs) networks, a type of Recurrent Neural Network (RNN), in overcoming these issues using expert-labeled telemetry anomaly data from the Soil Moisture Active Passive (SMAP) satellite and the Mars Science Laboratory (MSL) rover, Curiosity. We also propose a complementary unsupervised and nonparametric anomaly thresholding approach developed during a pilot implementation of an anomaly detection system for SMAP, and offer false positive mitigation strategies along with other key improvements and lessons learned during development.",2018.0,"K. Hundman, V. Constantinou, Christopher Laporte, Ian Colwell, T. Söderström"
de96cb8ebe6119b28bb041bb3c57ccadc32a3997,https://www.semanticscholar.org/paper/de96cb8ebe6119b28bb041bb3c57ccadc32a3997,An effect size primer: A guide for clinicians and researchers.,"Increasing emphasis has been placed on the use of effect size reporting in the analysis of social science data. Nonetheless, the use of effect size reporting remains inconsistent, and interpretation of effect size estimates continues to be confused. Researchers are presented with numerous effect sizes estimate options, not all of which are appropriate for every research question. Clinicians also may have little guidance in the interpretation of effect sizes relevant for clinical practice. The current article provides a primer of effect size estimates for the social sciences. Common effect sizes estimates, their use, and interpretations are presented as a guide for researchers.",2009.0,C. Ferguson
3d9b385913f9d470e175051fc113eb2b8dc5981b,https://www.semanticscholar.org/paper/3d9b385913f9d470e175051fc113eb2b8dc5981b,Big data and its epistemology,"The article considers whether Big Data, in the form of data‐driven science, will enable the discovery, or appraisal, of universal scientific theories, instrumentalist tools, or inductive inferences. It points out, initially, that such aspirations are similar to the now‐discredited inductivist approach to science. On the positive side, Big Data may permit larger sample sizes, cheaper and more extensive testing of theories, and the continuous assessment of theories. On the negative side, data‐driven science encourages passive data collection, as opposed to experimentation and testing, and hornswoggling (“unsound statistical fiddling”). The roles of theory and data in inductive algorithms, statistical modeling, and scientific discoveries are analyzed, and it is argued that theory is needed at every turn. Data‐driven science is a chimera.",2015.0,Martin Frické
31af4b8793e93fd35e89569ccd663ae8777f0072,https://www.semanticscholar.org/paper/31af4b8793e93fd35e89569ccd663ae8777f0072,The Netflix Prize,"Netflix released a dataset containing 100 million anonymous movie ratings and challenged the data mining, machine learning and computer science communities to develop systems that could beat the accuracy of its recommendation system, Cinematch. We briefly describe the challenge itself, review related work and efforts, and summarize visible progress to date. Other potential uses of the data are outlined, including its application to the KDD Cup 2007.",2007.0,"J. Bennett, S. Lanning"
2fa9691d984a857a1ec24240a2d72425b8614c87,https://www.semanticscholar.org/paper/2fa9691d984a857a1ec24240a2d72425b8614c87,Foundations of Qualitative Research: Interpretive and Critical Approaches,"Chapter 1: World Views, Paradigms, and the Practice of Social Science Research Case 1. Quantitative Research Case 2. Qualitative Research Thinking about the Foundations and Practice of Research What This Book Is and Is Not About. What Warrants Our Attention? The Traditional Canon Alternative Paradigms New Techniques or New Paradigms? Chapter 2: History and Context of Paradigm Development Positivism: A Response to Metaphysical and Magical Explanations Critical Theory: A Response to Inequities in Society Interpretivism: A Response to the Excesses of ""Scientific"" Social Science The Special Cases of Postmodernism and Feminism Chapter 3: Foundational Issues: Postpositivist and Critical Perspectives Social Science Research: The View from the Postpositivist Paradigm Social Science Research: The View from the Critical Theory Paradigm Chapter 4: History and Foundations of Interpretivist Research (1) Nature of Reality. (2) Purpose of Research. (3) Acceptable Methodology/Data. (4) The Meaning of Data (5) Relationship of Research to Practice. The Implications of an Interpretivist Approach What Sorts of Research are Worthwhile? Examples of Interpretive Research Chapter 5: Frameworks for Qualitative Research Postpositivist Research ""Moments"" of Qualitative Research Some General Frameworks for Qualitative Research Chapter 6: General Guidelines for Qualitative Research Guidelines for Qualitative Research Situated or Contextual Understanding, Not Truth, is the Purpose of Research Accept Multiple Sources of Influence Take A Foundational Rather Than Technique Perspective Practice Recursive (Iterative ) and Emergent Data Collection and Analysis Use Multiple Sources of Data Think of Research as a Reflective Process The Researcher is the Primary Tool for Data Collection and Analysis An Emphasis on Participatory versus Nonparticipatory Research. Adopt an Open Approach Deal With Bias Directly Select Natural Contexts for Research Research Should be Holistic, Not Atomistic Research Involves More Than Induction and Deduction: Analogical Reasoning, Abduction, and Family Resemblances Alternatives to Postpositivist Criteria for Believability: Validity and Reliability Alternative Approaches to Validity and Reliability: Triangulation and More Conclusions? Aren't They Generalizations? Chapter 7: Methods of Qualitative Research Case 1: Action Research on a Pediatric Surgical Ward Established Qualitative Research Methods Ethnography Case Studies: Another Form of Qualitative Observation Interview Research Historigraphy Historiography: The Research Methods of History Innovative Methods Participatory Qualitative Research Emancipatory Research Critical Emancipatory Action Research Chapter 8: Approaches to Data Analysis and Intepretation The Purpose of Research General Theory Objective Description Hermeneutic (Verstehen) Understanding Story telling/Narrative Data Analysis Families Eyeballing the Data Connoisseurship: A Global Perspective Hermeneutics as a Data Analysis Method Grounded Theory Analytic Induction A Final Topic: The Ethics of Research Chapter 9: 21st Century Social Science: Peering into the Future Will the Cacophony Continue? Why Can't Social Science Converge on The Answer? Competition Linearity Dialog as an Alternative to Competition Three Approaches to Knowing in Greek Thought Plato Aristotle The Humanities Choices 20th Century Social Science Made Suppose We Chose Badly Two Theories That May Help Us Build 21st Century Social Science Poetic Logic Chaos and Complexity Theory: Another Route to a Nonlinea Social Science",2007.0,J. Willis
cc0019f2ad728bac89ae1586062f27a8018f5763,https://www.semanticscholar.org/paper/cc0019f2ad728bac89ae1586062f27a8018f5763,Reproducibility in science,"Multiple sources can lead to issues with reproducibility and reliability in scientific data. The issue of reproducibility and reliability in science has come to the forefront in light of several high-profile studies that could not be reproduced. Whereas some errors in reliability can be attributed to the application of new techniques that have unappreciated caveats, some problems with reproducibility lie in the climate of intense pressure for funding and to publish faced by many researchers.",2015.0,M. Yaffe
831e6a3edba368604297f3164855146a376d33c9,https://www.semanticscholar.org/paper/831e6a3edba368604297f3164855146a376d33c9,Dynamic Version of the Economic Lot Size Model,"(This article originally appeared in Management Science, October 1958, Volume 5, Number 1, pp. 89-96, published by The Institute of Management Sciences.) 
 
A forward algorithm for a solution to the following dynamic version of the economic lot size model is given: allowing the possibility of demands for a single item, inventory holding charges, and setup costs to vary over N periods, we desire a minimum total cost inventory management scheme which satisfies known demand in every period. Disjoint planning horizons are shown to be possible which eliminate the necessity of having data for the full N periods.",2004.0,"H. M. Wagner, T. M. Whitin"
f2c7aba9255a14a8e438b7b12934442b5d4fa146,https://www.semanticscholar.org/paper/f2c7aba9255a14a8e438b7b12934442b5d4fa146,Vegetation Description and Analysis: A Practical Approach,"The Nature of Quantitative Plant Ecology and Vegetation Science. The Description of Vegetation in the Field. The Nature and Properties of Vegetation Data. Basic Statistical Analysis of Vegetation and Environmental Data. Ordination Methods I, 1950-1970. Ordination Methods II, 1970-1992. Phytosociology and the Zurick-Montpellier (Braun-Blanquet) School of Subjective Classification. Numerical Classification and Phytosociology. Computer Programs for Vegetation and Environmental Data Analysis. Quantitative Plant Ecology, Vegetation Science and the Future. References. Index.",1992.0,"M. Kent, P. Coker"
6277dd84bd4ee42ab31a7afb3ef44e2b10f8d0dd,https://www.semanticscholar.org/paper/6277dd84bd4ee42ab31a7afb3ef44e2b10f8d0dd,Establishing the norms of scientific argumentation in classrooms,"Basing its arguments in current perspectives on the nature of the scientific enterprise, which see argument and argumentative practice as a core activity of scientists, this article develops the case for the inclusion and central role of argument in science education. Beginning with a review of the nature of argument, it discusses the function and purpose of dialogic argument in the social construction of scientific knowledge and the interpretation of empirical data. The case is then advanced that any education about science, rather than education in science, must give the role of argument a high priority if it is to give a fair account of the social practice of science, and develop a knowledge and understanding of the evaluative criteria used to establish scientific theories. Such knowledge is essential to enhance the public understanding of science and improve scientific literacy. The existing literature, and work that has attempted to use argument within science education, is reviewed to show that classroom practice does provide the opportunity to develop young people's ability to construct argument. Furthermore, the case is advanced that the lack of opportunities for the practice of argument within science classrooms, and lack of teacher's pedagogical skills in organizing argumentative discourse within the classroom are significant impediments to progress in the field. © 2000 John Wiley & Sons, Inc. Sci Ed84:287–312, 2000.",2000.0,"R. Driver, P. Newton, J. Osborne"
7c4a9643c701c0c91ea50fd587038f79187a0a5e,https://www.semanticscholar.org/paper/7c4a9643c701c0c91ea50fd587038f79187a0a5e,Artificial Intelligence: A Guide to Intelligent Systems,"From the Publisher: 
Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author. 
 
The book provides an introduction to the field of computer intelligence, covering 
 
rule-based expert systems, 
fuzzy expert systems, 
frame-based expert systems, 
artificail neural networks, 
evolutionary computation, 
hybrid intelligent systems, 
knowledge engineering, 
data mining. 
 
 
In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit",2001.0,M. Negnevitsky
ef9b8640efb2f6a3f059a8a72fbac4d9e7da4251,https://www.semanticscholar.org/paper/ef9b8640efb2f6a3f059a8a72fbac4d9e7da4251,A Continuous Satellite-Derived Measure of Global Terrestrial Primary Production,"Abstract Until recently, continuous monitoring of global vegetation productivity has not been possible because of technological limitations. This article introduces a new satellite-driven monitor of the global biosphere that regularly computes daily gross primary production (GPP) and annual net primary production (NPP) at 1-kilometer (km) resolution over 109,782,756 km2 of vegetated land surface. We summarize the history of global NPP science, as well as the derivation of this calculation, and current data production activity. The first data on NPP from the EOS (Earth Observing System) MODIS (Moderate Resolution Imaging Spectroradiometer) sensor are presented with different types of validation. We offer examples of how this new type of data set can serve ecological science, land management, and environmental policy. To enhance the use of these data by nonspecialists, we are now producing monthly anomaly maps for GPP and annual NPP that compare the current value with an 18-year average value for each pixel, clearly identifying regions where vegetation growth is higher or lower than normal.",2004.0,"S. Running, R. Nemani, F. Heinsch, Maosheng Zhao, M. Reeves, H. Hashimoto"
ca23dd4e9ab32bc29b79a4aa30f71ee6580a9f1c,https://www.semanticscholar.org/paper/ca23dd4e9ab32bc29b79a4aa30f71ee6580a9f1c,Information visualization: perception for design by Colin Ware,"Most designers know that yellow text presented against a blue background reads clearly and easily, but how many can explain why, and what really are the best ways to help others and ourselves clearly see key patterns in a bunch of data? When we use software, access a website, or view business or scientific graphics, our understanding is greatly enhanced or impeded by the way the information is presented. 
 
This book explores the art and science of why we see objects the way we do. Based on the science of perception and vision, the author presents the key principles at work for a wide range of applications--resulting in visualization of improved clarity, utility, and persuasiveness. The book offers practical guidelines that can be applied by anyone: interaction designers, graphic designers of all kinds (including web designers), data miners, and financial analysts. 
 
 
 
Complete update of the recognized source in industry, research, and academic for applicable guidance on information visualizing. 
 
Includes the latest research and state of the art information on multimedia presentation. 
 
More than 160 explicit design guidelines based on vision science. 
 
A new final chapter that explains the process of visual thinking and how visualizations help us to think about problems. 
 
Packed with over 400 informative full color illustrations, which are key to understanding of the subject. 
 
Table of Contents 
 
 
Chapter 1. Foundations for an Applied Science of Data Visualization 
 
Chapter 2. The Environment, Optics, Resolution, and the Display 
 
Chapter 3. Lightness, Brightness, Contrast and Constancy 
 
Chapter 4. Color 
 
Chapter 5. Visual Salience and Finding Information 
 
Chapter 6. Static and Moving Patterns 
 
Chapter 7. Space Perception 
 
Chapter 8. Visual Objects and Data Objects 
 
Chapter 9. Images, Narrative, and Gestures for Explanation 
 
Chapter 10. Interacting with Visualizations 
 
Chapter 11. Visual Thinking Processes",2000.0,Veit Jahns
2c8a9f92a8d969d052f2960e5b349ebbdeac97f1,https://www.semanticscholar.org/paper/2c8a9f92a8d969d052f2960e5b349ebbdeac97f1,The (Big) Data-security assemblage: Knowledge and critique,"The Snowden revelations and the emergence of ‘Big Data’ have rekindled questions about how security practices are deployed in a digital age and with what political effects. While critical scholars have drawn attention to the social, political and legal challenges to these practices, the debates in computer and information science have received less analytical attention. This paper proposes to take seriously the critical knowledge developed in information and computer science and reinterpret their debates to develop a critical intervention into the public controversies concerning data-driven security and digital surveillance. The paper offers a two-pronged contribution: on the one hand, we challenge the credibility of security professionals’ discourses in light of the knowledge that they supposedly mobilize; on the other, we argue for a series of conceptual moves around data, human–computer relations, and algorithms to address some of the limitations of existing engagements with the Big Data-security assemblage.",2015.0,"Claudia Aradau, Tobias Blanke"
7dcb61c32c9dcd4722037de73e9191d137504734,https://www.semanticscholar.org/paper/7dcb61c32c9dcd4722037de73e9191d137504734,Reflexive Methodology: New Vistas for Qualitative Research,"Introduction: The Intellectualization of Method (Post-)Positivism, Social Constructionism, Critical Realism: Three Reference Points in the Philosophy of Science Data-Oriented Methods: Empiricist Techniques and Procedures Hermeneutics: Interpretation and Insight Critical Theory: The Political and Ideological Dimension Post-Structuralism and Postmodernism: Destabilizing Subject and Text Language/Gender/Power: Discourse Analysis, Feminism and Genealogy On Reflexive Interpretation: The Play of Interpretive Levels Applications of Reflexive Methodology: Strategies, Criteria, Varieties",2000.0,"M. Alvesson, Kaj Sköldberg"
42bd119724bee894e0887bc1a27db26420e500ad,https://www.semanticscholar.org/paper/42bd119724bee894e0887bc1a27db26420e500ad,Approaches to social research,"Chapters 2-17 end with a Summary CHAPTER 1. INTRODUCTION Why Study Research Methods? Methodological Approaches to the Social World Conclusions I. THE SCIENTIFIC AND ETHICAL CONTEXTS OF SOCIAL RESEARCH CHAPTER 2. THE NATURE OF SCIENCE The Aim of Science Science as Product Science as Process Science: Ideal versus Reality CHAPTER 3. RESEARCH ETHICS Data Collection and Analysis Treatment of Human Subjects Making Ethical Decisions The Uses of Research: Science and Society II. RESEARCH DESIGN CHAPTER 4. ELEMENTS OF RESEARCH DESIGN Origins of Research Topics Units of Analysis Variables Relationships Formulating Questions and Hypotheses Research Purposes and Research Design Stages of Social Research CHAPTER 5. MEASUREMENT The Measurement Process Levels of Measurement Reliability and Validity Reliability Assessment Validity Assessment A Final Note on Reliability and Validity CHAPTER 6. SAMPLING Why Sample? Population Definition Sampling Designs Probability Sampling Nonprobability Sampling Other Sampling Designs Factors Affecting Choice of Sampling Design Factors Determining Sample Size Final Notes on Sampling Errors and Generalizability III. METHODS OF DATA COLLECTION CHAPTER 7. EXPERIMENTATION The Logic of Experimentation Staging Experiments The Experiment as a Social Occasion Experimentation Outside the Laboratory CHAPTER 8. EXPERIMENTAL DESIGNS Threats to Internal Validity Pre-experimental Designs True Experimental Designs Factorial Experimental Designs Quasi-experimental Designs CHAPTER 9. SURVEY RESEARCH General Features of Survey Research The Uses and Limitations of Surveys Survey Research Designs Steps in Survey Research: Planning Face-to-Face and Telephone Interviewing Paper-and-Pencil Mailed Questionnaires Computer-Assisted Interviews Mixed-Mode Surveys Field Administration CHAPTER 10. SURVEY INSTRUMENTATION The Survey as a Social Occasion Materials Available to the Survey Designer ""Sketches"" or Preliminaries Filling in the Sketch: Writing the Items Pretesting CHAPTER 11. FIELD RESEARCH The Potentials and Limitations of Field Research Research Design and Sampling Field Observation Field Interviewing Stages of Field Research CHAPTER 12. RESEARCH USING AVAILABLE DATA Sources of Available Data Advantages of Research Using Available Data General Methodological Issues in Available-Data Research Historical Analysis Content Analysis CHAPTER 13. MULTIPLE METHODS Triangulation Multiple Measures of Concepts within the Same Study Multiple Tests of Hypotheses across Different Studies A Comparison of the Four Basic Approaches to Social Research Meta-Analysis CHAPTER 14. EVALUATION RESEARCH Framework and Sample Studies Types of Evaluation Research Methodological Issues in Evaluation Research The Social and Political Context of Evaluation Research IV. DATA PROCESSING, ANALYSIS, AND INTERPRETATION CHAPTER 15. DATA PROCESSING AND ELEMENTARY DATA ANALYSIS Preview of Analysis Steps Data Processing Data Matrices and Documentation The Functions of Statistics in Social Research Inspecting and Modifying the Data Preliminary Hypothesis Testing CHAPTER 16. MULTIVARIATE ANALYSIS Modeling Relationships Elaboration: Tables and Beyond Multiple-Regression Analysis Other Modeling Techniques CHAPTER 17. WRITING RESEARCH REPORTS Searching the Literature Using the Internet Using the Library Outlining and Preparing to Write Major Headings Other Considerations Length",1993.0,"R. Singleton, B. Straits"
80c5d486c73bc43fcbbfcfbbb1971c1a72a8f27b,https://www.semanticscholar.org/paper/80c5d486c73bc43fcbbfcfbbb1971c1a72a8f27b,Artificial Intelligence in Human Resources Management: Challenges and a Path Forward,"There is a substantial gap between the promise and reality of artificial intelligence in human resource (HR) management. This article identifies four challenges in using data science techniques for HR tasks: complexity of HR phenomena, constraints imposed by small data sets, accountability questions associated with fairness and other ethical and legal constraints, and possible adverse employee reactions to management decisions via data-based algorithms. It then proposes practical responses to these challenges based on three overlapping principles—causal reasoning, randomization and experiments, and employee contribution—that would be both economically efficient and socially appropriate for using data science in the management of employees.",2019.0,"Prasanna Tambe, P. Cappelli, V. Yakubovich"
c223ee2e2117a5d2119070177279950f58b706d2,https://www.semanticscholar.org/paper/c223ee2e2117a5d2119070177279950f58b706d2,"A conceptual framework for managing very diverse data for complex, interdisciplinary science","Much attention has been given to the challenges of handling massive data volumes in modern data-intensive science. This paper examines an equally daunting challenge – the diversity of interdisciplinary data, notably research data, and the need to interrelate these data to understand complex systemic problems such as environmental change and its impact. We use the experience of the International Polar Year 2007–8 (IPY) as a case study to examine data management approaches seeking to address issues around complex interdisciplinary science. We find that, while technology is a critical factor in addressing the interdisciplinary dimension of the data intensive science, the technologies developing for exa-scale data volumes differ from those that are needed for extremely distributed and heterogeneous data. Research data will continue to be highly heterogeneous and distributed and will require technologies to be much simpler and more flexible. More importantly, there is a need for both technical and cultural adaptation. We describe a vision of discoverable, open, linked, useful, and safe collections of data, organized and curated using the best principles and practices of information and library science. This vision provides a framework for our discussion and leads us to suggest several short- and long-term strategies to facilitate a socio-technical evolution in the overall science data ecosystem.",2011.0,"M. Parsons, Ø. Godøy, E. LeDrew, T. D. Bruin, B. Danis, S. Tomlinson, David Carlson"
1c2bf15547506de420fb14a412c82506ab880975,https://www.semanticscholar.org/paper/1c2bf15547506de420fb14a412c82506ab880975,Planck 2015 results. I. Overview of products and scientific results,"The European Space Agency's Planck satellite, dedicated to studying the early Universe and its subsequent evolution, was launched 14{\textasciitilde}May 2009and scanned the microwave and submillimetre sky continuously between12{\textasciitilde}August 2009 and 23{\textasciitilde}October 2013. In February{\textasciitilde}2015, ESA and the PlanckCollaboration released the second set of cosmology products based ondata from the entire Planck mission, including both temperature andpolarization, along with a set of scientific and technical papers and aweb-based explanatory supplement. This paper gives an overview of themain characteristics of the data and the data products in the release,as well as the associated cosmological and astrophysical science resultsand papers. The science products include maps of the cosmic microwavebackground (CMB), the thermal Sunyaev-Zeldovich effect, and diffuseforegrounds in temperature and polarization, catalogues of compactGalactic and extragalactic sources (including separate catalogues ofSunyaev-Zeldovich clusters and Galactic cold clumps), and extensivesimulations of signals and noise used in assessing the performance ofthe analysis methods and assessment of uncertainties. The likelihoodcode used to assess cosmological models against the Planck data aredescribed, as well as a CMB lensing likelihood. Scientific resultsinclude cosmological parameters deriving from CMB power spectra,gravitational lensing, and cluster counts, as well as constraints oninflation, non-Gaussianity, primordial magnetic fields, dark energy, andmodified gravity.",2015.0,"Planck Collaboration R. Adam, P. Ade, N. Aghanim, Y. Akrami, M. Alves, M. Arnaud, F. Arroja, J. Aumont, C. Baccigalupi, M. Ballardini, A. Banday, R. B. Barreiro, J. Bartlett, N. Bartolo, S. Basak, P. Battaglia, E. Battaner, R. Battye, K. Benabed, A. Benoit, A. Benoit-Lévy, J. Bernard, M. Bersanelli, B. Bertincourt, P. Bielewicz, A. Bonaldi, L. Bonavera, J. Bond, J. Borrill, F. Bouchet, F. Boulanger, M. Bucher, C. Burigana, R. C. Butler, E. Calabrese, J. Cardoso, P. Carvalho, B. Casaponsa, G. Castex, A. Catalano, A. Challinor, A. Chamballu, R. Chary, H. Chiang, J. Chluba, P. Christensen, S. Church, M. Clemens, D. Clements, S. Colombi, L. Colombo, C. Combet, B. Comis, D. Contreras, F. Couchot, A. Coulais, B. Crill, M. Cruz, A. Curto, F. Cuttaia, L. Danese, R. Davies, R. Davis, P. Bernardis, A. D. Rosa, G. Zotti, J. Delabrouille, J. Delouis, F. D'esert, E. D. Valentino, C. Dickinson, J. Diego, K. Dolag, H. Dole, S. Donzelli, O. Dor'e, M. Douspis, A. Ducout, J. Dunkley, X. Dupac, G. Efstathiou, P. Eisenhardt, F. Elsner, T. Ensslin, H. Eriksen, E. Falgarone, Y. Fantaye, M. Farhang, S. Feeney, J. Fergusson, R. Fernández-Cobos, F. Feroz, F. Finelli, E. Florido, O. Forni, M. Frailis, A. Fraisse, C. Franceschet, E. Franceschi, A. Frejsel, A. Frolov, S. Galeotta, S. Galli, K. Ganga, C. Gauthier, R. G'enova-Santos, M. Gerbino, T. Ghosh, M. Giard, Y. Giraud-H'eraud, E. Giusarma, E. Gjerløw, J. Gonz'alez-Nuevo, K. M. G'orski, K. Grainge, S. Gratton, A. Gregorio, A. Gruppuso, J. Gudmundsson, J. Hamann, Will Handley, F. Hansen, D. Hanson, D. Harrison, A. Heavens, G. Helou, S. Henrot-Versill'e, C. Hern'andez-Monteagudo, D. Herranz, S. Hildebrandt, E. Hivon, M. Hobson, W. Holmes, A. Hornstrup, W. Hovest, Z. Huang, K. Huffenberger, G. Hurier, S. Ili'c, A. Jaffe, T. Jaffe, T. Jin, W. Jones, M. Juvela, A. Karakci, E. Keihanen, R. Keskitalo, K. Kiiveri, J. Kim, T. Kisner, R. Kneissl, J. Knoche, N. Krachmalnicoff, M. Kunz, H. Kurki-Suonio, F. Lacasa, G. Lagache, A. Lahteenmaki, J. Lamarre, M. Langer, A. Lasenby, M. Lattanzi, C. Lawrence, M. Jeune, J. Leahy, E. Lellouch, R. Leonardi, J. Le'on-Tavares, J. Lesgourgues, F. Levrier, A. Lewis, M. Liguori, P. Lilje, M. Linden-Vørnle, V. Lindholm, H. Liu, M. L'opez-Caniego, P. Lubin, Y.-Z. Ma, J. Mac'ias-P'erez, G. Maggio, D. Mak, N. Mandolesi, A. Mangilli, A. Marchini, A. Marcos-Caballero, D. Marinucci, D. Marshall, P. Martin, M. Martinelli, E. Mart'inez-Gonz'alez, S. Masi, S. Matarrese, P. Mazzotta, J. McEwen, P. McGehee, S. Mei, P. Meinhold, A. Melchiorri, J. Melin, L. Mendes, A. Mennella, M. Migliaccio, K. Mikkelsen, S. Mitra, M. Miville-Deschênes, D. Molinari, A. Moneti, L. Montier, R. Moreno, G. Morgante, D. Mortlock, A. Moss, S. Mottet, M. Muenchmeyer, D. Munshi, J. Murphy, A. Narimani, P. Naselsky, A. Nastasi, F. Nati, P. Natoli, M. Negrello, C. Netterfield, H. Nørgaard-Nielsen, F. Noviello, D. Novikov, I. Novikov, M. Olamaie, N. Oppermann, E. Orlando, C. A. Oxborrow, F. Paci, L. Pagano, F. Pajot, R. Paladini, S. Pandolfi, D. Paoletti, B. Partridge, F. Pasian, G. Patanchon, T. Pearson, M. Peel, H. Peiris, V. Pelkonen, O. Perdereau, L. Perotto, Y. Perrott, F. Perrotta, V. Pettorino, F. Piacentini, M. Piat, E. Pierpaoli, D. Pietrobon, S. Plaszczynski, D. Pogosyan, E. Pointecouteau, G. Polenta, L. Popa, G. Pratt, G. Pr'ezeau, S. Prunet, J. Puget, J. Rachen, B. Racine, W. Reach, R. Rebolo, M. Reinecke, M. Remazeilles, C. Renault, A. Renzi, I. Ristorcelli, G. Rocha, M. Roman, E. Romelli, C. Rosset, M. Rossetti, A. Rotti, G. Roudier, B. R. d'Orfeuil, M. Rowan‐Robinson, J. Rubino-Mart'in, B. Ruiz-Granados, C. Rumsey, B. Rusholme, N. Said, V. Salvatelli, L. Salvati, M. Sandri, H. Sanghera, D. Santos, R. Saunders, A. Sauv'e, M. Savelainen, G. Savini, B. Schaefer, M. Schammel, D. Scott, M. Seiffert, P. Serra, E. Shellard, T. W. Shimwell, M. Shiraishi, K. Smith, T. Souradeep, L. Spencer, M. Spinelli, S. Stanford, D. Stern, V. Stolyarov, R. Stompor, A. Strong, R. Sudiwala, R. Sunyaev, P. Sutter, D. Sutton, A.-S. Suur-Uski, J. Sygnet, J. Tauber, D. Tavagnacco, L. Terenzi, D. Texier, L. Toffolatti, M. Tomasi, M. Tornikoski, M. Tristram, A. Troja, T. Trombetti, M. Tucci, J. Tuovinen, M. Turler, G. Umana, L. Valenziano, J. Valiviita, B. Tent, T. Vassallo, M. Vidal, M. Viel, P. Vielva, F. Villa, L. Wade, B. Walter, Benjamin Dan Wandelt, R. Watson, I. Wehus, N. Welikala, J. Weller, M. White, S. White, A. Wilkinson, D. Yvon, A. Zacchei, J. Zibin, A. Zonca"
887ad080fbf279dd3c096d8722114c98759ce183,https://www.semanticscholar.org/paper/887ad080fbf279dd3c096d8722114c98759ce183,Astronomical Data Analysis Software and Systems X,"The Astronomical Data Analysis Software and Systems—ADASS—Conference series is now in its 10th year and continues to highlight advances across a wide range of topical areas. This year’s focus areas included “Enabling Technologies for Astronomy,” “Software Development Technologies,” “Science Data Pipelines, “Sky Surveys,” “Outreach,” and “Software History.” In addition, “Birds-of-a-Feather” sessions were held concerning the National Virtual Observatory (NVO), CORBA, IRAF, AIPS , DS9, IDL, Linux, and FITS. As evidence for the wide and continuing interest in astronomical software issues, 304 participants attended the conference from 21 countries. The theme of “enabling technologies” was addressed by A. Szalay (reviewing the genesis, challenges, and opportunities of the virtual observatory), J. Spyromilio (commissioning the VLT), and J. Tarter and D. Werthimer (the science and technical challenges of the SETI project). S. Murray and R. Brissenden described the early scientific successes and operational systems for theChandra X-Ray Observatory, and B. Glendenning gave an overview of the software engineering challenges facing the ALMA telescope project. Key papers on software development technologies included J. Graybeal (CORBA), G. Filippi (software development methodologies employed at ESO), C. Kesselman (computational grids), J. Manuel Filgueira (distributed objects), and V. Yodaiken (real-time Linux). In the area of science data processing pipelines, R. Lupton described the image reduction pipeline for the Sloan Digital Sky Survey, and R. Cutri described the 2MASS project under the topic of sky surveys. K. Shortridge gave an insightful and amusing historical perspective of astronomical software development. The oral program also included a number of excellent contributed papers, and daily poster sessions (with over 120 posters presented in total) included presentations on all areas of software development and systems. Fourteen groups demonstrated their latest software systems and astronomical information services. Over the past decade the emphasis of the conference has gradually changed, from the “Data Analysis” to the “Systems” part of the title. This change reflects the fact that astronomical software development has entered into the age of very large projects, either in support of large, complex, forefront telescopes (Gemini, VLT, ALMA, and NGST for example) or in the support of massive surveys and the associated terabytescale databases (e.g., 2MASS, GSC-II, and SDSS). The soft-",2001.0,"R. J. Hanisch, George H. Jacoby"
747207ccbb45f72facaedfd2e62e1ae02e096fa4,https://www.semanticscholar.org/paper/747207ccbb45f72facaedfd2e62e1ae02e096fa4,The science of sustainable supply chains,"Recent advances in the science and technology of global supply chain management offer near–real-time demand-response systems for decision-makers across production networks. Technology is helping propel “fast fashion” and “lean manufacturing,” so that companies are better able to deliver products consumers want most. Yet companies know much less about the environmental and social impacts of their production networks. The failure to measure and manage these impacts can be explained in part by limitations in the science of sustainability measurement, as well as by weaknesses in systems to translate data into information that can be used by decision-makers inside corporations and government agencies. There also remain continued disincentives for firms to measure and pay the full costs of their supply chain impacts. I discuss the current state of monitoring, measuring, and analyzing information related to supply chain sustainability, as well as progress that has been made in translating this information into systems to advance more sustainable practices by corporations and consumers. Better data, decision-support tools, and incentives will be needed to move from simply managing supply chains for costs, compliance, and risk reduction to predicting and preventing unsustainable practices.",2014.0,D. O'Rourke
33780e4aba639a97f9fb7f7e773853f74dd494b7,https://www.semanticscholar.org/paper/33780e4aba639a97f9fb7f7e773853f74dd494b7,The Bioperl toolkit: Perl modules for the life sciences.,"The Bioperl project is an international open-source collaboration of biologists, bioinformaticians, and computer scientists that has evolved over the past 7 yr into the most comprehensive library of Perl modules available for managing and manipulating life-science information. Bioperl provides an easy-to-use, stable, and consistent programming interface for bioinformatics application programmers. The Bioperl modules have been successfully and repeatedly used to reduce otherwise complex tasks to only a few lines of code. The Bioperl object model has been proven to be flexible enough to support enterprise-level applications such as EnsEMBL, while maintaining an easy learning curve for novice Perl programmers. Bioperl is capable of executing analyses and processing results from programs such as BLAST, ClustalW, or the EMBOSS suite. Interoperation with modules written in Python and Java is supported through the evolving BioCORBA bridge. Bioperl provides access to data stores such as GenBank and SwissProt via a flexible series of sequence input/output modules, and to the emerging common sequence data storage format of the Open Bioinformatics Database Access project. This study describes the overall architecture of the toolkit, the problem domains that it addresses, and gives specific examples of how the toolkit can be used to solve common life-sciences problems. We conclude with a discussion of how the open-source nature of the project has contributed to the development effort.",2002.0,"J. Stajich, David Block, K. Boulez, S. Brenner, S. Chervitz, Chris Dagdigian, Georg Fuellen, James G. R. Gilbert, I. Korf, H. Lapp, H. Lehväslaiho, Chad Matsalla, C. Mungall, B. Osborne, M. Pocock, P. Schattner, Martin Senger, L. Stein, E. Stupka, Mark D. Wilkinson, E. Birney"
df296559baaceb4a5efdb9fb7d6fbaf637fd9ccb,https://www.semanticscholar.org/paper/df296559baaceb4a5efdb9fb7d6fbaf637fd9ccb,The cost of doing science on the cloud: The Montage example,"Utility grids such as the Amazon EC2 cloud and Amazon S3 offer computational and storage resources that can be used on-demand for a fee by compute and data-intensive applications. The cost of running an application on such a cloud depends on the compute, storage and communication resources it will provision and consume. Different execution plans of the same application may result in significantly different costs. Using the Amazon cloud fee structure and a real-life astronomy application, we study via simulation the cost performance tradeoffs of different execution and resource provisioning plans. We also study these trade-offs in the context of the storage and communication fees of Amazon S3 when used for long-term application data archival. Our results show that by provisioning the right amount of storage and compute resources, cost can be significantly reduced with no significant impact on application performance.",2008.0,"E. Deelman, Gurmeet Singh, M. Livny, B. Berriman, J. Good"
6ebb3fb75e1a98dbc92945c76cb1d812004754c7,https://www.semanticscholar.org/paper/6ebb3fb75e1a98dbc92945c76cb1d812004754c7,The Emerging Role of Libraries in Data Curation and E-science,"ABSTRACT The role of libraries is to collect, preserve, and disseminate the intellectual output of the society. This output includes books and serials as well as the digital versions of the same. Scientists, other scholars, and all of society are now producing, storing, and disseminating digital data that underpin the aforementioned documents in much larger volumes than the text. The survival of this data is in question since the data are not housed in long-lived institutions such as libraries. This situation threatens the underlying principles of scientific replicability since in many cases data cannot readily be collected again. Libraries are the institutions that could best manage this intellectual output.",2011.0,"P. Heidorn, Bryan Heidorn, P. Heidorn"
09614af5489d4c7db873fe33a9153a1eff28a40e,https://www.semanticscholar.org/paper/09614af5489d4c7db873fe33a9153a1eff28a40e,What difference does quantity make? On the epistemology of Big Data in biology,"Is Big Data science a whole new way of doing research? And what difference does data quantity make to knowledge production strategies and their outputs? I argue that the novelty of Big Data science does not lie in the sheer quantity of data involved, but rather in (1) the prominence and status acquired by data as commodity and recognised output, both within and outside of the scientific community and (2) the methods, infrastructures, technologies, skills and knowledge developed to handle data. These developments generate the impression that data-intensive research is a new mode of doing science, with its own epistemology and norms. To assess this claim, one needs to consider the ways in which data are actually disseminated and used to generate knowledge. Accordingly, this article reviews the development of sophisticated ways to disseminate, integrate and re-use data acquired on model organisms over the last three decades of work in experimental biology. I focus on online databases as prominent infrastructures set up to organise and interpret such data and examine the wealth and diversity of expertise, resources and conceptual scaffolding that such databases draw upon. This illuminates some of the conditions under which Big Data needs to be curated to support processes of discovery across biological subfields, which in turn highlights the difficulties caused by the lack of adequate curation for the vast majority of data in the life sciences. In closing, I reflect on the difference that data quantity is making to contemporary biology, the methodological and epistemic challenges of identifying and analysing data given these developments, and the opportunities and worries associated with Big Data discourse and methods.",2014.0,S. Leonelli
f338304435183c446671010b4e87fc505fba0ae7,https://www.semanticscholar.org/paper/f338304435183c446671010b4e87fc505fba0ae7,Clickstream Data Yields High-Resolution Maps of Science,"Background Intricate maps of science have been created from citation data to visualize the structure of scientific activity. However, most scientific publications are now accessed online. Scholarly web portals record detailed log data at a scale that exceeds the number of all existing citations combined. Such log data is recorded immediately upon publication and keeps track of the sequences of user requests (clickstreams) that are issued by a variety of users across many different domains. Given these advantages of log datasets over citation data, we investigate whether they can produce high-resolution, more current maps of science. Methodology Over the course of 2007 and 2008, we collected nearly 1 billion user interactions recorded by the scholarly web portals of some of the most significant publishers, aggregators and institutional consortia. The resulting reference data set covers a significant part of world-wide use of scholarly web portals in 2006, and provides a balanced coverage of the humanities, social sciences, and natural sciences. A journal clickstream model, i.e. a first-order Markov chain, was extracted from the sequences of user interactions in the logs. The clickstream model was validated by comparing it to the Getty Research Institute's Architecture and Art Thesaurus. The resulting model was visualized as a journal network that outlines the relationships between various scientific domains and clarifies the connection of the social sciences and humanities to the natural sciences. Conclusions Maps of science resulting from large-scale clickstream data provide a detailed, contemporary view of scientific activity and correct the underrepresentation of the social sciences and humanities that is commonly found in citation data.",2009.0,"J. Bollen, H. Van de Sompel, A. Hagberg, Luís M. A. Bettencourt, Ryan Chute, Michael A. Rodriguez, Lyudmila Balakireva"
54fd5a995a46f026860aede2aff6629fdae66a38,https://www.semanticscholar.org/paper/54fd5a995a46f026860aede2aff6629fdae66a38,Requirements for Science Data Bases and SciDB,"For the past year, we have been assembling requirements from a collection of scientific data base users from astronomy, particle physics, fusion, remote sensing, oceanography, and biology. The intent has been to specify a common set of requirements for a new science data base system, which we call SciDB. In addition, we have discovered that very complex business analytics share most of the same requirements as “big science”. We have also constructed a partnership of companies to fund the development of SciDB, including eBay, the Large Synoptic Survey Telescope (LSST), Microsoft, the Stanford Linear Accelerator Center (SLAC) and Vertica. Lastly, we have identified two “lighthouse customers” (LSST and eBay) who will run the initial system, once it is constructed. In this paper, we report on the requirements we have identified and briefly sketch some of the SciDB design.",2009.0,"M. Stonebraker, J. Becla, D. DeWitt, Kian-Tat Lim, D. Maier, Oliver Ratzesberger, S. Zdonik"
497d3e9006db04242649672eb5fd9ca41f95b89a,https://www.semanticscholar.org/paper/497d3e9006db04242649672eb5fd9ca41f95b89a,Consultative Committee For Space Data Systems,"The major space agencies of the world recognize that there are benefits in using standard techniques for handling space data and that, by cooperatively developing these techniques, future data system interoperability will be enhanced. In order to assure that work towards standardization of space-related information technologies provides the maximum benefi t for 
the interested agencies, both individually and collectively, an international Consultative Committee for Space Data Systems (CCSDS) was established in 1982 as a forum for international cooperation in the development of data handling techniques supporting space research, including space science and 
applications. In 1991, the committee was incorporated as a subcommittee of the International Organization for Standardization (ISO). 
The article describs the work of CCSDS till its beginning in the 80s till today (2009).",2009.0,M. Pilgram
81da5bd490672b1fbf64826a11888e6c59a6f61e,https://www.semanticscholar.org/paper/81da5bd490672b1fbf64826a11888e6c59a6f61e,Evaluating replicability of laboratory experiments in economics,"Another social science looks at itself Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values. Science, this issue p. 1433 By several metrics, economics experiments do replicate, although not as often as predicted. The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90% to detect the original effect size at the 5% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61%); on average, the replicated effect size is 66% of the original. The replicability rate varies between 67% and 78% for four additional replicability indicators, including a prediction market measure of peer beliefs.",2016.0,"Colin Camerer, Anna Dreber, Eskil Forsell, Teck-Hua Ho, J. Huber, M. Johannesson, Michael Kirchler, Johan Almenberg, Adam Altmejd, Taizan Chan, Emma Heikensten, Felix Holzmeister, Taisuke Imai, Siri Isaksson, G. Nave, T. Pfeiffer, Michael Razen, Hang Wu"
bf245b7c072d3cd3f45ed9f9b9f80fb417395d47,https://www.semanticscholar.org/paper/bf245b7c072d3cd3f45ed9f9b9f80fb417395d47,The INTEGRAL Science Data Centre (ISDC),The INTEGRAL Science Data Centre (ISDC) provides the INTEGRAL data and means to analyse them to the scientific community. The ISDC runs a gamma ray burst alert system that provides the position of gamma ray bursts on the sky within seconds to the community. It operates a quick-look analysis of the data within few hours that detects new and unexpected sources as well as it monitors the instruments. The ISDC processes the data through a standard analysis the results of which are provided to the observers together with their data.,2001.0,"T. Courvoisier, R. Walter, V. Beckmann, A. Dean, P. Dubath, R. Hudec, P. Kretschmar, S. Mereghetti, T. Montmerle, N. Mowlavi, S. Paltani, A. Martinez, Nicolas Produit, R. Staubert, A. Strong, J. Swings, N. Westergaard, N. White, C. Winkler, A. Zdziarski"
621e8b30252ba4b6370382915a0586b437b82aca,https://www.semanticscholar.org/paper/621e8b30252ba4b6370382915a0586b437b82aca,Statistical Methods for Categorical Data Analysis,"This book provides a comprehensive introduction to methods and models for categorical data analysis and their applications in social science research. Companion website also available, at https://webspace.utexas.edu/dpowers/www/",1999.0,D. Powers
d7c535ae48cc29f5e87f0a64c43a0c728080241e,https://www.semanticscholar.org/paper/d7c535ae48cc29f5e87f0a64c43a0c728080241e,Epistemologically Authentic Inquiry in Schools: A Theoretical Framework for Evaluating Inquiry Tasks,"A main goal of science education is to help students learn to reason scien- tifically. A main way to facilitate learning is to engage students in inquiry activities such as conducting experiments. This article presents a theoretical framework for evaluating inquiry tasks in terms of how similar they are to authentic science. The framework helps identify the respects in which these reasoning tasks are similar to and different from real scientific research. The framework is based on a recent theory of reasoning, models-of-data theory. We argue that inquiry tasks commonly used in schools evoke reasoning processes that are qualitatively different from the processes employed in real scientific inquiry. More- over, school reasoning tasks appear to be based on an epistemology that differs from the epistemology of authentic science. Inquiry tasks developed by researchers have increas- ingly captured features of authentic science, but further improvement is still possible. We conclude with a discussion of the implications of our analysis for research, assessment, and instruction. C",2002.0,"C. Chinn, Betina A. Malhotra"
e3e133f6894cf03514a3e626545fbfd46491503e,https://www.semanticscholar.org/paper/e3e133f6894cf03514a3e626545fbfd46491503e,Euclid. I. Overview of the Euclid mission,"The current standard model of cosmology successfully describes a variety of measurements, but the nature of its main ingredients, dark matter and dark energy, remains unknown. is a medium-class mission in the Cosmic Vision 2015--2025 programme of the European Space Agency (ESA) that will provide high-resolution optical imaging, as well as near-infrared imaging and spectroscopy, over about 14\,000\,deg$^2$ of extragalactic sky. In addition to accurate weak lensing and clustering measurements that probe structure formation over half of the age of the Universe, its primary probes for cosmology, these exquisite data will enable a wide range of science. This paper provides a high-level overview of the mission, summarising the survey characteristics, the various data-processing steps, and data products. We also highlight the main science objectives and expected performance.",2024.0,"Euclid Collaboration Y. Mellier, Abdurro’uf, J. A. Barroso, A. Ach'ucarro, J. Adamek, R. Adam, G. E. Addison, N. Aghanim, M. Aguena, V. Ajani, Y. Akrami, A. Al-Bahlawan, A. Alavi, I. S. Albuquerque, G. Alestas, G. Alguero, A. Allaoui, S. W. Allen, V. Allevato, A. V. Alonso-Tetilla, B. Altieri, A. Alvarez-Candal, A. Amara, L. Amendola, J. Amiaux, I. Andika, S. Andreon, A. Andrews, G. Angora, R. E. Angulo, F. Annibali, A. Anselmi, S. Anselmi, S. Arcari, M. Archidiacono, G. Arico, M. Arnaud, S. Arnouts, M. Asgari, J. Asorey, L. Atayde, H. Atek, F. Atrio-Barandela, M. Aubert, É. Aubourg, T. Auphan, N. Auricchio, B. Aussel, H. Aussel, P. Avelino, A. Avgoustidis, S. Ávila, S. Awan, R. Azzollini, C. Baccigalupi, E. Bachelet, D. Bacon, M. Baes, M. Bagley, B. Bahr-Kalus, A. Balaguera-Antolínez, E. Balbinot, M. Balcells, M. Baldi, I. Baldry, A. Balestra, M. Ballardini, O. Ballester, M. Balogh, E. Bañados, R. Barbier, S. Bardelli, T. Barreiro, J. Barrière, B. J. Barros, A. Barthelemy, N. Bartolo, A. Basset, P. Battaglia, A. J. Battisti, C. M. Baugh, L. Baumont, L. Bazzanini, J. Beaulieu, V. Beckmann, A. N. Belikov, J. Bel, F. Bellagamba, M. Bella, E. Bellini, K. Benabed, R. Bender, G. Benevento, C. L. Bennett, K. Benson, P. Bergamini, J. Bermejo-Climent, F. Bernardeau, D. Bertacca, M. Berthé, J. Berthier, M. Béthermin, F. Beutler, C. Bevillon, S. Bhargava, R. Bhatawdekar, L. Bisigello, A. Biviano, R. Blake, A. Blanchard, J. Blazek, L. Blot, A. Bosco, C. Bodendorf, T. Boenke, H. Bohringer, M. Bolzonella, A. Bonchi, M. Bonici, D. Bonino, L. Bonino, C. Bonvin, W. Bon, J. T. Booth, S. Borgani, A. Borlaff, E. Borsato, B. Bose, M. Botticella, A. Boucaud, F. Bouche, J. Boucher, D. Boutigny, T. Bouvard, H. Bouy, R. Bowler, V. Bozza, E. Bozzo, E. Branchini, S. Brau-Nogué, P. Brekke, M. Bremer, M. Brescia, M.-A. Breton, J. Brinchmann, T. Brinckmann, C. Brockley-Blatt, M. Brodwin, L. Brouard, M. L. Brown, S. Bruton, J. Bucko, H. Buddelmeijer, G. Buenadicha, F. Buitrago, P. Burger, C. Burigana, V. Busillo, D. Busonero, R. Cabanac, L. Cabayol-Garcia, M. S. Cagliari, A. Caillat, L. Caillat, M. Calabrese, A. Calabrò, G. Calderone, F. Calura, B. C. Quevedo, S. Camera, L. Campos, G. Cañas-Herrera, G. Candini, M. Cantiello, V. Capobianco, E. Cappellaro, N. Cappelluti, A. Cappi, K. Caputi, C. Cara, C. Carbone, V. Cardone, E. Carella, R. Carlberg, M. Carle, L. Carminati, F. Caro, J. M. Carrasco, J. Carretero, P. Carrilho, J. C. Duque, B. Carry, A. Carvalho, C. Carvalho, R. Casas, S. Casas, P. Casenove, C. M. Casey, P. Cassata, F. Castander, D. Castelão, M. Castellano, L. Castiblanco, G. Castignani, T. Castro, C. Cavet, S. Cavuoti, P. Chabaud, K. Chambers, Y. Charles, S. Charlot, N. Chartab, R. Chary, F. Chaumeil, H. Cho, G. Chon, E. Ciancetta, P. Ciliegi, A. Cimatti, M. Cimino, M. Cioni, R. Claydon, C. Cleland, B. Cl'ement, D. Clements, N. Clerc, S. Clesse, S. Codis, F. Cogato, J. Colbert, R. Cole, P. Coles, T. Collett, R. S. Collins, C. Colodro-Conde, C. Colombo, F. Combes, V. Conforti, G. Congedo, S. Conseil, C. Conselice, S. Contarini, T. Contini, L. Conversi, A. Cooray, Y. Copin, Pier Stefano Corasaniti, P. Corcho-Caballero, L. Corcione, O. Cordes, O. Corpace, M. Correnti, M. Costanzi, A. Costille, F. Courbin, L. C. Mifsud, H. Courtois, M. Cousinou, G. Covone, T. Cowell, C. Cragg, G. Cresci, S. Cristiani, M. Crocce, M. Cropper, P. Crouzet, B. Csizi, J. Cuby, E. Cucchetti, O. Cucciati, J. Cuillandre, P. Cunha, V. Cuozzo, E. Daddi, M. D'Addona, C. Dafonte, N. Dagoneau, E. Dalessandro, G. Dalton, G. D'Amico, H. Dannerbauer, P. Danto, I. Das, A. D. Silva, R. D. Silva, G. Daste, J. E. Davies, S. Davini, T. D. Boer, R. Decarli, B. D. Caro, H. Degaudenzi, G. Degni, J. D. Jong, L. F. D. L. Bella, S. D. Torre, F. Delhaise, D. Delley, G. Delucchi, G. Lucia, J. Denniston, F. Paolis, M. Petris, A. Derosa, S. Desai, V. Desjacques, G. Despali, G. Desprez, J. D. Vicente-Albendea, Y. Deville, J. Dias, A. D'iaz-S'anchez, J. Diaz, S. Domizio, J. M. Diego, D. Ferdinando, A. D. Giorgio, P. Dimauro, J. Dinis, K. Dolag, C. Dolding, H. Dole, H. D. S'anchez, O. Dor'e, F. Dournac, M. Douspis, H. Dreihahn, B. Droge, B. Dryer, F. Dubath, P. Duc, F. Ducret, C. Duffy, F. Dufresne, C. Duncan, X. Dupac, V. Duret, R. Durrer, F. Durret, S. Dusini, A. Ealet, A. Eggemeier, P. Eisenhardt, D. Elbaz, M. Y. Elkhashab, A. Ellien, J. Endicott, A. Enia, T. Erben, J. Vigo, S. Escoffier, I. E. Sanz, J. Essert, S. Ettori, M. Ezziati, G. Fabbian, M. Fabricius, Y. Fang, A. Farina, M. Farina, R. Farinelli, S. Farrens, F. Faustini, A. Feltre, A. Ferguson, P. Ferrando, A. G. Ferrari, A. Ferr'e-Mateu, P. G. Ferreira, I. Ferreras, I. Ferrero, S. Ferriol, P. Ferruit, D. Filleul, F. Finelli, S. Finkelstein, A. Finoguenov, B. Fiorini, F. Flentge, P. Focardi, J. Fonseca, A. Fontana, F. Fontanot, F. Fornari, P. Fosalba, M. Fossati, S. Fotopoulou, D. Fouchez, N. Fourmanoit, M. Frailis, D. Fraix-Burnet, E. Franceschi, A. Franco, P. Franzetti, J. Freihoefer, G. Frittoli, P. Frugier, N. Frusciante, A. Fumagalli, M. Fumagalli, M. Fumana, Y. Fu, L. Gabarra, S. Galeotta, L. Galluccio, K. Ganga, H. Gao, J. Garc'ia-Bellido, K. Garcia, J. P. Gardner, B. Garilli, L.-M. Gaspar-Venancio, T. Gasparetto, V. Gautard, R. Gavazzi, E. Gaztañaga, L. Genolet, R. G. Santos, F. Gentile, K. George, Z. Ghaffari, F. Giacomini, F. Gianotti, G. P. S. Gibb, W. Gillard, B. Gillis, M. Ginolfi, C. Giocoli, M. Girardi, S. Giri, L. W. K. Goh, P. G'omez-Alvarez, A. H. Gonzalez, E. J. Gonzalez, J. C. Gonzalez, S. G. Beauchamps, G. Gozaliasl, J. Graciá-Carpio, S. Grandis, B. Granett, M. Granvik, A. Grazian, A. Gregorio, C. Grenet, C. Grillo, F. Grupp, C. Gruppioni, A. Gruppuso, C. Guerbuez, S. Guerrini, M. Guidi, P. Guillard, C. M. Gutierrez, P. Guttridge, L. Guzzo, S. Gwyn, J. Haapala, J. Haase, C. Haddow, M. Hailey, A. Hall, D. Hall, N. Hamaus, B. S. Haridasu, J. Harnois-D'eraps, C. Harper, W. Hartley, G. Hasinger, F. Hassani, N. A. Hatch, S. Haugan, B. Haussler, A. Heavens, L. Heisenberg, A. Helmi, G. Helou, S. Hemmati, K. Henares, O. Herent, C. Hern'andez-Monteagudo, T. Heuberger, P. Hewett, S. Heydenreich, H. Hildebrandt, M. Hirschmann, J. Hjorth, J. Hoar, H. Hoekstra, A. Holland, M. Holliman, W. Holmes, I. Hook, B. Horeau, F. Hormuth, A. Hornstrup, S. Hosseini, D. Hu, P. Hudelot, M. Hudson, M. Huertas-Company"
c2eb2c7166b34c5eb8daaf8209a1a235a00c53d6,https://www.semanticscholar.org/paper/c2eb2c7166b34c5eb8daaf8209a1a235a00c53d6,The Data Revolution and Economic Analysis,"Many believe that “big data” will transform business, government, and other aspects of the economy. In this article we discuss how new data may impact economic policy and economic research. Large-scale administrative data sets and proprietary private sector data can greatly improve the way we measure, track, and describe economic activity. They can also enable novel research designs that allow researchers to trace the consequences of different events or policies. We outline some of the challenges in accessing and making use of these data. We also consider whether the big data predictive modeling tools that have emerged in statistics and computer science may prove useful in economics.",2013.0,"L. Einav, Jonathan Levin"
0d10f2efad55f6669376058b17bf00017e704aa4,https://www.semanticscholar.org/paper/0d10f2efad55f6669376058b17bf00017e704aa4,Math and science motivation: A longitudinal examination of the links between choices and beliefs.,"This study addresses the longitudinal associations between youths' out-of-school activities, expectancies-values, and high school course enrollment in the domains of math and science. Data were collected on 227 youth who reported on their activity participation in 5th grade, expectancies-values in 6th and 10th grade, and courses taken throughout high school. Math and science course grades at 5th and 10th grade were gathered through school record data. Results indicated youths' math and science activity participation predicted their expectancies and values, which, in turn, predicted the number of high school courses above the predictive power of grades. Although there were mean-level differences between boys and girls on some of these indicators, relations among indicators did not significantly differ by gender.",2006.0,"S. Simpkins, P. Davis‐Kean, J. Eccles"
71cb78088052d49bc93032636ef9e56bc7274e09,https://www.semanticscholar.org/paper/71cb78088052d49bc93032636ef9e56bc7274e09,Statistics: The Art and Science of Learning from Data,"Part 1: Gathering and Exploring Data 1. Statistics: The Art and Science of Learning from Data 1.1 Using Data to Answer Statistical Questions 1.2 Sample Versus Population 1.3 Using Calculators and Computers Chapter Summary Chapter Problems 2. Exploring Data with Graphs and Numerical Summaries 2.1 Different Types of Data 2.2 Graphical Summaries of Data 2.3 Measuring the Center of Quantitative Data 2.4 Measuring the Variability of Quantitative Data 2.5 Using Measures of Position to Describe Variability 2.6 Recognizing and Avoiding Misuses of Graphical Summaries Chapter Summary Chapter Problems 3. Association: Contingency, Correlation, and Regression 3.1 The Association Between Two Categorical Variables 3.2 The Association Between Two Quantitative Variables 3.3 Predicting the Outcome of a Variable 3.4 Cautions in Analyzing Associations Chapter Summary Chapter Problems 4. Gathering Data 4.1 Experimental and Observational Studies 4.2 Good and Poor Ways to Sample 4.3 Good and Poor Ways to Experiment 4.4 Other Ways to Conduct Experimental and Nonexperimental Studies Chapter Summary Chapter Problems Part 1 Review Part 1 Questions Part 1 Exercises Part 2: Probability, Probability Distributions, and Sampling Distributions 5. Probability in Our Daily Lives 5.1 How Probability Quantifies Randomness 5.2 Finding Probabilities 5.3 Conditional Probability: The Probability of A Given B 5.4 Applying the Probability Rules Chapter Summary Chapter Problems 6. Probability Distributions 6.1 Summarizing Possible Outcomes and Their Probabilities 6.2 Probabilities for Bell-Shaped Distributions 6.3 Probabilities When Each Observation Has Two Possible Outcomes Chapter Summary Chapter Problems 7. Sampling Distributions 7.1 How Sample Proportions Vary Around the Population Proportion 7.2 How Sample Means Vary Around the Population Mean 7.3 The Binomial Distribution Is a Sampling Distribution (Optional) Chapter Summary Chapter Problems Part 2 Review Part 2 Questions Part 2 Exercises Part 3: Inferential Statistics 8. Statistical Inference: Confidence Intervals 8.1 Point and Interval Estimates of Population Parameters 8.2 Constructing a Confidence Interval to Estimate a Population Proportion 8.3 Constructing a Confidence Interval to Estimate a Population Mean 8.4 Choosing the Sample Size for a Study 8.5 Using Computers to Make New Estimation Methods Possible Chapter Summary Chapter Problems 9. Statistical Inference: Significance Tests about Hypotheses 9.1 Steps for Performing a Significance Test 9.2 Significance Tests about Proportions 9.3 Significance Tests about Means 9.4 Decisions and Types of Errors in Significance Tests 9.5 Limitations of Significance Tests 9.6 The Likelihood of a Type II Error (Not Rejecting H0, Even Though It's False) Chapter Summary Chapter Problems 10. Comparing Two Groups 10.1 Categorical Response: Comparing Two Proportions 10.2 Quantitative Response: Comparing Two Means 10.3 Other Ways of Comparing Means and Comparing Proportions 10.4 Analyzing Dependent Samples 10.5 Adjusting for the Effects of Other Variables Chapter Summary Chapter Problems Part 3 Review Part 3 Questions Part 3 Exercises Part 4: Analyzing Association and Extended Statistical Methods 11. Analyzing the Association Between Categorical Variables 11.1 Independence and Association 11.2 Testing Categorical Variables for Independence 11.3 Determining the Strength of the Association 11.4 Using Residuals to Reveal the Pattern of Association 11.5 Small Sample Sizes: Fisher's Exact Test Chapter Summary Chapter Problems 12. Analyzing the Association Between Quantitative Variables: Regression Analysis 12.1 Model How Two Variables Are Related 12.2 Describe Strength of Association 12.3 Make Inference About the Association 12.4How the Data Vary Around the Regression Line 12.5 Exponential Regression: A Model for Nonlinearity Chapter Summary Chapter Problems 13. Multiple Regression 13.1 Using Several Variables to Predict a Response 13.2 Extending the Correlation and R-squared for Multiple Regression 13.3 Using Multiple Regression to Make Inferences 13.4 Checking a Regression Model Using Residual Plots 13.5 Regression and Categorical Predictors 13.6 Modeling a Categorical Response Chapter Summary Chapter Problems 14. Comparing Groups: Analysis of Variance Methods 14.1 One-Way ANOVA: Comparing Several Means 14.2 Estimating Differences in Groups for a Single Factor 14.3 Two-Way ANOVA Chapter Summary Chapter Problems 15. Nonparametric Statistics 15.1 Compare Two Groups by Ranking 15.2 Nonparametric Methods For Several Groups and for Matched Pairs Chapter Summary Chapter Problems PART 4 Review Part 4 Questions Part 4 Exercises Tables Answers Index Index of Applications Photo Credits",2005.0,"A. Agresti, C. Franklin"
1110eb8dadaa11b15a5b5e7f31d67d9edea4de1f,https://www.semanticscholar.org/paper/1110eb8dadaa11b15a5b5e7f31d67d9edea4de1f,An empirical test of a taxonomy of responses to anomalous data in science.,"The purpose of this study was to test a taxonomy of seven proposed responses to anomalous data. Our results generally supported the taxonomy but indicated that one additional type of response should be added to the taxonomy. We conclude that there are eight possible responses to anomalous data: (a) ignoring the data, (b) rejecting the data, (c) professing uncertainty about the validity of the data, (d) excluding the data from the domain of the current theory, (e) holding the data in abeyance, (f) reinter- preting the data, (g) accepting the data and making peripheral changes to the current theory, and (h) ac- cepting the data and changing theories. We suggest that this taxonomy could help science teachers in two ways. First, science teachers could use the taxonomy to try to anticipate how students might react to anom- alous data so as to make theory change more likely. Second, science teachers could use the taxonomy as a framework to guide classroom discussion about the nature of scientific rationality. In addition, the tax- onomy suggests directions for future research. © 1998 John Wiley & Sons, Inc. J Res Sci Teach 35: 623-654, 1998.",1998.0,"C. Chinn, W. Brewer"
22158fa7321ef520c13f50c32b50feccfc1d5aa9,https://www.semanticscholar.org/paper/22158fa7321ef520c13f50c32b50feccfc1d5aa9,The Science of Real-Time Data Capture: Self-Reports in Health Research,"PART I: THE SCIENCE AND THEORY OF REAL-TIME DATA CAPTURE: A FOCUS ON ECOLOGICAL MOMENTARY ASSESSMENT (EMA) 1. Historical Roots and Rationale of Ecological Momentary Assessment (EMA) 2. Retrospective and Concurrent Self-Reports: The Rationale for Real-Time Data Capture 3. Designing Protocols for Ecological Momentary Assessment 4. Special Methodological Challenges and Opportunities in Ecological Momentary Assessment 5. The Analysis of Real-Time Momentary Data: A Practical Guide PART II: APPLICATION OF REAL-TIME DATA CAPTURE: EXEMPLARS OF REAL-TIME DATA RESEARCH 6. Real-Time Data Capture and Adolescent Cigarette Smoking: Moods and Smoking 7. Ecological Momentary Assessment of Physical Activity in Hispanics/Latinos Using Pedometers and Diaries 8. Dietary Assessment and Monitoring in Real-Time 9. Real-Time Data Capture: Ecological Momentary Assessment of Behavioral Symptoms Associated with Eating Disorders 10. Ecological Momentary Assessment for Alcohol Consumption 11. Assessing the Impact of Fibromyalgia Syndrome in Real-Time 12. Evaluating Fatigue of Ovarian Cancer Patients Using Ecological Momentary Assessment 13. Personality, Mood States, and Daily Health 14. Ecological Momentary Assessment as a Resource for Social Epidemiology PART III: FUTURE DEVELOPMENTS IN REAL-TIME DATA CAPTURE 15. Momentary Health Interventions: Where are we and where are we going? 16. Technological Innovations Enabling Automatic, Context-Sensitive Ecological Momentary Assessment 17. Statistical Issues in Intensive Longitudinal Data Analysis 18. Thoughts on the Present State of Real-Tmie Data Capture",2007.0,Audie A Atienza
164578a8d62985888e99d85154ff52dbea034a08,https://www.semanticscholar.org/paper/164578a8d62985888e99d85154ff52dbea034a08,Data Mining Techniques,"Methods for knowledge discovery in data bases (KDD) have been studied for more than a decade. New methods are required owing to the size and complexity of data collections in administration, business and science. They include procedures for data query and extraction, for data cleaning, data analysis, and methods of knowledge representation. The part of KDD dealing with the analysis of the data has been termed data mining. Common data mining tasks include the induction of association rules, the discovery of functional relationships (classification and regression) and the exploration of groups of similar data objects in clustering. This review provides a discussion of and pointers to efficient algorithms for the common data mining tasks in a mathematical framework. Because of the size and complexity of the data sets, efficient algorithms and often crude approximations play an important role.",2001.0,M. Hegland
17141fb5e4630ddf1e9fb20757f439ae79ffc3f3,https://www.semanticscholar.org/paper/17141fb5e4630ddf1e9fb20757f439ae79ffc3f3,The ESA Climate Change Initiative: Satellite Data Records for Essential Climate Variables,"Observations of Earth from space have been made for over 40 years and have contributed to advances in many aspects of climate science. However, attempts to exploit this wealth of data are often hampered by a lack of homogeneity and continuity and by insufficient understanding of the products and their uncertainties. There is, therefore, a need to reassess and reprocess satellite datasets to maximize their usefulness for climate science. The European Space Agency has responded to this need by establishing the Climate Change Initiative (CCI). The CCI will create new climate data records for (currently) 13 essential climate variables (ECVs) and make these open and easily accessible to all. Each ECV project works closely with users to produce time series from the available satellite observations relevant to users' needs. A climate modeling users' group provides a climate system perspective and a forum to bring the data and modeling communities together. This paper presents the CCI program. It outlines its benefit and presents approaches and challenges for each ECV project, covering clouds, aerosols, ozone, greenhouse gases, sea surface temperature, ocean color, sea level, sea ice, land cover, fire, glaciers, soil moisture, and ice sheets. It also discusses how the CCI approach may contribute to defining and shaping future developments in Earth observation for climate science.",2013.0,"R. Hollmann, C. Merchant, R. Saunders, C. Downy, M. Buchwitz, A. Cazenave, E. Chuvieco, P. Defourny, G. Leeuw, R. Forsberg, T. Holzer-Popp, F. Paul, S. Sandven, S. Sathyendranath, M. Roozendael, W. Wagner"
db22407602c7406015cd278773ec80aace490e69,https://www.semanticscholar.org/paper/db22407602c7406015cd278773ec80aace490e69,Data Analysis in Forensic Science A Bayesian Decision Perspective,"If you really want to be smarter, reading can be one of the lots ways to evoke and realize. Many people who like reading will have more knowledge and experiences. Reading can be a way to gain information from economics, politics, science, fiction, literature, religion, and many others. As one of the part of book categories, data analysis in forensic science a bayesian decision perspective always becomes the most wanted book. Many people are absolutely searching for this book. It means that many love to read this kind of book.",2010.0,"F. Taroni, S. Bozza, A. Biedermann, Paolo Garbolino, C. Aitken"
3627e71e8ca6d87b10699ed5432424afd921d3fa,https://www.semanticscholar.org/paper/3627e71e8ca6d87b10699ed5432424afd921d3fa,Landolt-Börnstein: Numerical Data and Functional Relationships in Science and Technology - New Series,"This book provides an introduction to Quantum Field Theory (QFT) at an elementary level—with only special relativity, electromagnetism and quantum mechanics as prerequisites. For this fresh approach to teaching QFT, based on numerous lectures and courses given by the authors, a representative sample of topics has been selected containing some of the more innovative, challenging or subtle concepts. They are presented with a minimum of technical details, the discussion of the main ideas being more important than the presentation of the typically very technical mathematical details necessary to obtain the final results.",1965.0,"J. W. Baars, H. Beer, C. J. Durrant, U. Graser, B. Guinot, M. Hoffmann, U. Hopp, W. Ip, E. Jessberger, B. Klecker, D. Lemke, K. Meisenheimer, Eberhard Mobius, H. Palme, J. Rahe, H. Röser, J. Schubart, R. Schwenn, J. Solf, G. Soltau, R. Staubert, R. Stewart, J. Trümper, V. Vanýsek, G. Weigelt, R. Wolf"
12747823c0346f027da877a0d98509f6984fee2c,https://www.semanticscholar.org/paper/12747823c0346f027da877a0d98509f6984fee2c,Does Practical Work Really Work? A study of the effectiveness of practical work as a teaching and learning method in school science,"Many within the science education community and beyond see practical work carried out by students as an essential feature of science education. Questions have, however, been raised by some science educators about its effectiveness as a teaching and learning strategy. This study explored the effectiveness of practical work by analysing a sample of 25 ‘typical’ science lessons involving practical work in English secondary schools. Data took the form of observational field notes and tape‐recorded interviews with teachers and students. The analysis used a model of effectiveness based on the work of Millar et al. and Tiberghien. The teachers’ focus in these lessons was predominantly on developing students’ substantive scientific knowledge, rather than on developing understanding of scientific enquiry procedures. Practical work was generally effective in getting students to do what is intended with physical objects, but much less effective in getting them to use the intended scientific ideas to guide their actions and reflect upon the data they collect. There was little evidence that the cognitive challenge of linking observables to ideas is recognized by those who design practical activities for science lessons. Tasks rarely incorporated explicit strategies to help students to make such links, or were presented in class in ways that reflected the size of the learning demand. The analytical framework used in this study offers a means of assessing the learning demand of practical tasks, and identifying those that require specific support for students’ thinking and learning in order to be effective.",2008.0,"I. Abrahams, R. Millar"
2c2142a28eab7569c1c75a93c185d3c8c5152e51,https://www.semanticscholar.org/paper/2c2142a28eab7569c1c75a93c185d3c8c5152e51,Where's the evidence that active learning works?,"Calls for reforms in the ways we teach science at all levels, and in all disciplines, are wide spread. The effectiveness of the changes being called for, employment of student-centered, active learning pedagogy, is now well supported by evidence. The relevant data have come from a number of different disciplines that include the learning sciences, cognitive psychology, and educational psychology. There is a growing body of research within specific scientific teaching communities that supports and validates the new approaches to teaching that have been adopted. These data are reviewed, and their applicability to physiology education is discussed. Some of the inherent limitations of research about teaching and learning are also discussed.",2006.0,J. Michael
9849ff80a6e3d0ede2f726940901c70a88cb7116,https://www.semanticscholar.org/paper/9849ff80a6e3d0ede2f726940901c70a88cb7116,An online algorithm for segmenting time series,"In recent years, there has been an explosion of interest in mining time-series databases. As with most computer science problems, representation of the data is the key to efficient and effective solutions. One of the most commonly used representations is piecewise linear approximation. This representation has been used by various researchers to support clustering, classification, indexing and association rule mining of time-series data. A variety of algorithms have been proposed to obtain this representation, with several algorithms having been independently rediscovered several times. In this paper, we undertake the first extensive review and empirical comparison of all proposed techniques. We show that all these algorithms have fatal flaws from a data-mining perspective. We introduce a novel algorithm that we empirically show to be superior to all others in the literature.",2001.0,"Eamonn J. Keogh, Selina Chu, D. M. Hart, M. Pazzani"
835a0560a2a29fe396b99992699911ad9c18e553,https://www.semanticscholar.org/paper/835a0560a2a29fe396b99992699911ad9c18e553,The LIGO Open Science Center,"The LIGO Open Science Center (LOSC) fulfills LIGO's commitment to release, archive, and serve LIGO data in a broadly accessible way to the scientific community and to the public, and to provide the information and tools necessary to understand and use the data. In August 2014, the LOSC published the full dataset from Initial LIGO's “S5” run at design sensitivity, the first such large-scale release and a valuable testbed to explore the use of LIGO data by non-LIGO researchers and by the public, and to help teach gravitational-wave data analysis to students across the world. In addition to serving the S5 data, the LOSC web portal (losc.ligo.org) now offers documentation, data-location and data-quality queries, tutorials and example code, and more. We review the mission and plans of the LOSC, focusing on the S5 data release.",2014.0,"M. Vallisneri, J. Kanner, Roy Williams, A. Weinstein, B. Stephens"
c7c3db5525baf365570e0a3ccc781a9bc01ca57d,https://www.semanticscholar.org/paper/c7c3db5525baf365570e0a3ccc781a9bc01ca57d,The EBI RDF platform: linked open data for the life sciences,"Motivation: Resource description framework (RDF) is an emerging technology for describing, publishing and linking life science data. As a major provider of bioinformatics data and services, the European Bioinformatics Institute (EBI) is committed to making data readily accessible to the community in ways that meet existing demand. The EBI RDF platform has been developed to meet an increasing demand to coordinate RDF activities across the institute and provides a new entry point to querying and exploring integrated resources available at the EBI. Availability: http://www.ebi.ac.uk/rdf Contact: jupp@ebi.ac.uk",2014.0,"S. Jupp, J. Malone, Jerven T. Bolleman, Marco Brandizi, M. Davies, L. García, A. Gaulton, S. Gehant, Camille Laibe, Nicole Redaschi, Sarala M. Wimalaratne, M. Martin, N. Novère, H. Parkinson, E. Birney, Andrew M. Jenkinson"
104ddf3dbb59096999ae310eea6771dcd3d7f252,https://www.semanticscholar.org/paper/104ddf3dbb59096999ae310eea6771dcd3d7f252,Data Mining with Decision Trees: Theory and Applications,"Decision trees have become one of the most powerful and popular approaches in knowledge discovery and data mining; it is the science of exploring large and complex bodies of data in order to discover useful patterns. Decision tree learning continues to evolve over time. Existing methods are constantly being improved and new methods introduced. This 2nd Edition is dedicated entirely to the field of decision trees in data mining; to cover all aspects of this important technique, as well as improved or new methods and techniques developed after the publication of our first edition. In this new edition, all chapters have been revised and new topics brought in. New topics include Cost-Sensitive Active Learning, Learning with Uncertain and Imbalanced Data, Using Decision Trees beyond Classification Tasks, Privacy Preserving Decision Tree Learning, Lessons Learned from Comparative Studies, and Learning Decision Trees for Big Data. A walk-through guide to existing open-source data mining software is also included in this edition. This book invites readers to explore the many benefits in data mining that decision trees offer: Self-explanatory and easy to follow when compacted Able to handle a variety of input data: nominal, numeric and textual Scales well to big data Able to process datasets that may have errors or missing values High predictive performance for a relatively small computational effort Available in many open source data mining packages over a variety of platforms Useful for various tasks, such as classification, regression, clustering and feature selection Readership: Researchers, graduate and undergraduate students in information systems, engineering, computer science, statistics and management.",2007.0,"L. Rokach, O. Maimon"
e8556def981f064bfafafa98f3bd36a9aa32b044,https://www.semanticscholar.org/paper/e8556def981f064bfafafa98f3bd36a9aa32b044,Citation analysis as a tool in journal evaluation.,"As a communications system, the network of journals that play a paramount role in the exchange of scientific and technical information is little understood. Periodically since 1927, when Gross and Gross published their study (1) of references in 1 year’s issues of the Journal of the American Chemical Socie/y, pieces of the network have been illuminated by the work of Bradford (2), Allen (3), Gross and Woodford (4), Hooker (5), Henkle (6), Fussier (7), Brown (8), and others (9). Nevertheless, there is still no map of the journal network as a whok. To date, studies of the network and of the interrelation of its components have been limited in the number of journak, the areas of scientific study, and the periods of time their authors were able to consider, Such shortcomings have not been due to any lack of purpose, insight, or energy on the part of investigators, but to the practical difficulty of compiling and manipulating manually the enormous amount of necessary data. A solution to this problem of data is available in the data base used to produce the Science Citation Index ( SCI ) (10). The coverage of the SCI is international and multidisciplinary; it has grown from 600 journals in 1964 to 2400 journals in 1972, and now includes the world’s most important scientific and technical journals in mow disciplines. The SCI is published quarterly and is cumulated annually and quinquennially, but the data base from which the volumes are compiled is maintained on magnetic tape and is updated weekly. At the end of 1971, this data base contained more than 27 mi[tion references to about 10 million different published items. These references appeared over the past decade in the footnotes and bibliographies of more than 2 million journal articles, communications, letters, and so on. The data base is, thus, not only multidisciplinary, it covers a substantial period of time and, being in machine-readable form, is amenable to extensive manipulation by computer. In 1971, the Institute for Scientfic Information (1S1) decided to undertake a systematic analysis of journal citation patterns across the whole of science and technology. It began by extracting from the data base all references pobIished during the last quarter of 1969 in the 2200 journals then covered by the SCL The resultant sample was about 1 million citations of journals, books, reports, theses, and so forth. To test whether this 3-month sample was representative of the year as a whole, it was matched against another sample made by selecting every 27th reference from the approximately 4 million references collected over the entire year. The two samples were similar enough in scope (number of diflerent items cited) and detail (relative frequency of their citation by different journals) to",1972.0,E. Garfield
7484ddebd54457816d916f7c016b549a24f2b0e6,https://www.semanticscholar.org/paper/7484ddebd54457816d916f7c016b549a24f2b0e6,Mastering Data Mining: The Art and Science of Customer Relationship Management,"From the Publisher: 
""Berry and Linoff lead the reader down an enlightened path of best practices."" -Dr. Jim Goodnight, President and Cofounder, SAS Institute Inc.""This is a great book, and it will be in my stack of four or five essential resources for my professional work."" -Ralph Kimball, Author of The Data Warehouse Lifecycle ToolkitMastering Data MiningIn this follow-up to their successful first book, Data Mining Techniques, Michael J. A. Berry and Gordon S. Linoff offer a case study-based guide to best practices in commercial data mining. Their first book acquainted you with the new generation of data mining tools and techniques and showed you how to use them to make better business decisions. Mastering Data Mining shifts the focus from understanding data mining techniques to achieving business results, placing particular emphasis on customer relationship management.In this book, you'll learn how to apply data mining techniques to solve practical business problems. After providing the fundamental principles of data mining and customer relationship management, Berry and Linoff share the lessons they have learned through a series of warts-and-all case studies drawn from their experience in a variety of industries, including e-commerce, banking, cataloging, retailing, and telecommunications.Through the cases, you will learn how to formulate the business problem, analyze the data, evaluate the results, and utilize this information for similar business problems in different industries.Berry and Linoff show you how to use data mining to:* Retain customer loyalty* Target the right prospects* Identify new markets for products and services* Recognize cross-selling opportunities on and off the Web. Thecompanion Web site features:* Updated information on data mining products and service providers* Information on data mining conferences, courses, and other sources of information* Full-color versions of the illustrations used in the book",1999.0,"Michael J. A. Berry, G. Linoff, Paolo Cirio, Alessandro Ludovico neural.it"
cab9848f517e2b328f4e338120423260909579d4,https://www.semanticscholar.org/paper/cab9848f517e2b328f4e338120423260909579d4,ROC Curves for Continuous Data,"Bringing together all the relevant material to impart a clear understanding of how to analyze ROC curves, this book covers the fundamental theory as well as various special topics. It provides illustrative examples of the major methodological developments and includes as much of the mathematical theory as necessary without making the treatment too dense. The authors survey the uses made of the methodology across a range of different areas, from atmospheric science and geoscience to experimental psychology and sociology. They also list a number of websites from which software implementing the various techniques can be downloaded.",2009.0,"W. Krzanowski, D. Hand"
da559bd90f2490f58ad91f7d43bab26823239f2c,https://www.semanticscholar.org/paper/da559bd90f2490f58ad91f7d43bab26823239f2c,A rational analysis of the selection task as optimal data selection.,"Human reasoning in hypothesis-testing tasks like P. C. Wason's (1968) selection task has been depicted as prone to systematic biases. However, performance on this task has been assessed against a now outmoded falsificationist philosophy of science. Therefore, the experimental data is reassessed in the light of a Bayesian model of optimal data selection in inductive hypothesis testing. The model provides a rational analysis (J. R. Anderson, 1990) of the selection task that fits well with people's performance on both abstract and thematic versions of the task. The model suggests that reasoning in these tasks may be rational rather than subject to systematic bias.",1994.0,"M. Oaksford, N. Chater"
16c9a99680bda420806a6df5e34ede696868db94,https://www.semanticscholar.org/paper/16c9a99680bda420806a6df5e34ede696868db94,Privacy-Preserving Data Mining: Models and Algorithms,"Advances in hardware technology have increased the capability to store and record personal data about consumers and individuals, causing concerns that personal data may be used for a variety of intrusive or malicious purposes. Privacy-Preserving Data Mining: Models and Algorithms proposes a number of techniques to perform the data mining tasks in a privacy-preserving way. These techniques generally fall into the following categories: data modification techniques, cryptographic methods and protocols for data sharing, statistical techniques for disclosure and inference control, query auditing methods, randomization and perturbation-based techniques. This edited volume contains surveys by distinguished researchers in the privacy field. Each survey includes the key research content as well as future research directions. Privacy-Preserving Data Mining: Models and Algorithms is designed for researchers, professors, and advanced-level students in computer science, and is also suitable for industry practitioners.",2008.0,"C. Aggarwal, Philip S. Yu"
a9bf700aa5b6ebaa4d8a8f575d9ecb4a8f67627d,https://www.semanticscholar.org/paper/a9bf700aa5b6ebaa4d8a8f575d9ecb4a8f67627d,Gaming for (Citizen) Science: Exploring Motivation and Data Quality in the Context of Crowdsourced Science through the Design and Evaluation of a Social-Computational System,"Citizen Sort, currently under development, is a web-based social-computational system designed to support a citizen science task, the taxonomic classification of various insect, animal, and plant species. In addition to supporting this natural science objective, the Citizen Sort platform will also support information science research goals on motivation for participation in social-computation and citizen science. In particular, this research program addresses the use of games to motivate participation in social-computational citizen science, and explores the effects of system design on motivation and data quality. A design science approach, where IT artifacts are developed to solve problems and answer research questions is described. Research questions, progress on Citizen Sort planning and implementation, and key challenges are discussed.",2011.0,"Nathan R. Prestopnik, Kevin Crowston"
a6a3ba5fd69fb8d4b5618cff865fed5870772eea,https://www.semanticscholar.org/paper/a6a3ba5fd69fb8d4b5618cff865fed5870772eea,Enhancing the Quality and Trust of Citizen Science Data,"The Internet, Web 2.0 and Social Networking technologies are enabling citizens to actively participate in “citizen science” projects by contributing data to scientific programs. However, the limited expertise of contributors can lead to poor quality or misleading data being submitted. Subsequently, the scientific community often perceive citizen science data as not worthy of being used in serious scientific research. In this paper, we describe a technological framework that combines data quality improvements and trust metrics to enhance the reliability of citizen science data. We describe how trust models provide a simple and effective mechanism for measuring the reliability of community-generated data. We also describe filtering services that remove untrustworthy data, and enable confident re-use of the data. The resulting services are evaluated in the context of the Coral Watch project which uses volunteers to collect data on coral reef bleaching.",2010.0,"A. Alabri, J. Hunter"
83bda2860fb4005b0316e8d5850f68100e3cdeb7,https://www.semanticscholar.org/paper/83bda2860fb4005b0316e8d5850f68100e3cdeb7,Lab Experiments Are a Major Source of Knowledge in the Social Sciences,"Experimental Economics The disciplines of social science, with the notable exception of psychology, have traditionally steered clear of laboratory. The field of economics, and in particular econometrics, has amassed an imposing arsenal of quantitative and statistical methods for analyzing observational data in assessing economic theory and in making causal inferences. More recently, laboratory experiments carried out under controlled conditions and randomized field experiments carried out under natural conditions have gained some currency as complementary approaches. Falk and Heckman (p. 535) review the strengths and shortfalls of these recent developments. Laboratory experiments are a widely used methodology for advancing causal knowledge in the physical and life sciences. With the exception of psychology, the adoption of laboratory experiments has been much slower in the social sciences, although during the past two decades the use of lab experiments has accelerated. Nonetheless, there remains considerable resistance among social scientists who argue that lab experiments lack “realism” and generalizability. In this article, we discuss the advantages and limitations of laboratory social science experiments by comparing them to research based on nonexperimental data and to field experiments. We argue that many recent objections against lab experiments are misguided and that even more lab experiments should be conducted.",2009.0,"A. Falk, J. Heckman"
4f2e84f1c5ea7a0d5d8ebfa12a78a869f13d7b59,https://www.semanticscholar.org/paper/4f2e84f1c5ea7a0d5d8ebfa12a78a869f13d7b59,Deep learning for neural networks,"Machine learning algorithms are designed to improve as they encounter more data, making them a versatile technology for understanding large sets of photos such as those accessible from Google Images. Elizabeth Holm, professor of materials science and engineering at Carnegie Mellon University, is leveraging this technology to better understand the enormous number of research images accumulated in the field of materials science. [13]",2018.0,"David Kauchak CS158 – Fall, 1. Admin, Adam Coates"
0166d107c091e2ea0c0d2ea172f48ab010677e4f,https://www.semanticscholar.org/paper/0166d107c091e2ea0c0d2ea172f48ab010677e4f,Anatomy of STEM teaching in North American universities,"Lecture is prominent, but practices vary A large body of evidence demonstrates that strategies that promote student interactions and cognitively engage students with content (1) lead to gains in learning and attitudinal outcomes for students in science, technology, engineering, and mathematics (STEM) courses (1, 2). Many educational and governmental bodies have called for and supported adoption of these student-centered strategies throughout the undergraduate STEM curriculum. But to the extent that we have pictures of the STEM undergraduate instructional landscape, it has mostly been provided through self-report surveys of faculty members, within a particular STEM discipline [e.g., (3–6)]. Such surveys are prone to reliability threats and can underestimate the complexity of classroom environments, and few are implemented nationally to provide valid and reliable data (7). Reflecting the limited state of these data, a report from the U.S. National Academies of Sciences, Engineering, and Medicine called for improved data collection to understand the use of evidence-based instructional practices (8). We report here a major step toward a characterization of STEM teaching practices in North American universities based on classroom observations from over 2000 classes taught by more than 500 STEM faculty members across 25 institutions.",2018.0,"M. Stains, Jordan T. Harshman, Megan K. Barker, S. Chasteen, Renee Cole, S. E. DeChenne-Peters, M. K. Eagan, Joan M. Esson, Jennifer K. Knight, F. A. Laski, M. Levis-Fitzgerald, Christopher Lee, S. M. Lo, Lisa McDonnell, T. A. McKay, N. Michelotti, A. Musgrove, M. S. Palmer, Kathryn M. Plank, T. Rodela, Erin R. Sanders, N. Schimpf, Patricia M. Schulte, Michelle K. Smith, M. R. Stetzer, B. Valkenburgh, Erin Vinson, Laura K Weir, Paul J. Wendel, L. B. Wheeler, Anna M Young"
3e40ed4ba57a911d5780610ca31a358084669f11,https://www.semanticscholar.org/paper/3e40ed4ba57a911d5780610ca31a358084669f11,"Analysis of Multivariate Social Science Data, Second Edition","When four of the leading researchers in the field of quantitative social sciences team up to write a book together, you can expect nothing less than a brilliant work. That is what the first edition of “Analysis of Multivariate Social Science Data” from 2002 was, and that’s what the current second edition is. This new edition contains additional chapters on regression analysis, confirmatory factor analysis including structural equation models, and multilevel models.",2008.0,"D. Bartholomew, F. Steele, I. Moustaki, J. Galbraith"
475bbf493d8246031a5152c8005a5c567231307c,https://www.semanticscholar.org/paper/475bbf493d8246031a5152c8005a5c567231307c,Basis Set Exchange: A Community Database for Computational Sciences,"Basis sets are some of the most important input data for computational models in the chemistry, materials, biology, and other science domains that utilize computational quantum mechanics methods. Providing a shared, Web-accessible environment where researchers can not only download basis sets in their required format but browse the data, contribute new basis sets, and ultimately curate and manage the data as a community will facilitate growth of this resource and encourage sharing both data and knowledge. We describe the Basis Set Exchange (BSE), a Web portal that provides advanced browsing and download capabilities, facilities for contributing basis set data, and an environment that incorporates tools to foster development and interaction of communities. The BSE leverages and enables continued development of the basis set library originally assembled at the Environmental Molecular Sciences Laboratory.",2007.0,"K. Schuchardt, B. T. Didier, Todd O. Elsethagen, Lisong Sun, V. Gurumoorthi, Jared M. Chase, Jun Yu Li, T. Windus"
451ce6775150a829c0513a7e4fc86a1ec8095bf0,https://www.semanticscholar.org/paper/451ce6775150a829c0513a7e4fc86a1ec8095bf0,From Big Data to Knowledge in the Social Sciences,"One of the challenges associated with high-volume, diverse datasets is whether synthesis of open data streams can translate into actionable knowledge. Recognizing that challenge and other issues related to these types of data, the National Institutes of Health developed the Big Data to Knowledge or BD2K initiative. The concept of translating “big data to knowledge” is important to the social and behavioral sciences in several respects. First, a general shift to data-intensive science will exert an influence on all scientific disciplines, but particularly on the behavioral and social sciences given the wealth of behavior and related constructs captured by big data sources. Second, science is itself a social enterprise; by applying principles from the social sciences to the conduct of research, it should be possible to ameliorate some of the systemic problems that plague the scientific enterprise in the age of big data. We explore the feasibility of recalibrating the basic mechanisms of the scientific enterprise so that they are more transparent and cumulative; more integrative and cohesive; and more rapid, relevant, and responsive.",2015.0,"B. Hesse, R. Moser, W. Riley"
622dc72bc883dd96a3538f45ba16491e39bd53cf,https://www.semanticscholar.org/paper/622dc72bc883dd96a3538f45ba16491e39bd53cf,"The impacts of an invasive species citizen science training program on participant attitudes, behavior, and science literacy","Citizen science can make major contributions to informal science education by targeting participants’ attitudes and knowledge about science while changing human behavior towards the environment. We examined how training associated with an invasive species citizen science program affected participants in these areas. We found no changes in science literacy or overall attitudes between tests administered just before and after a one-day training program, matching results from other studies. However, we found improvements in science literacy and knowledge using context-specific measures and in self-reported intention to engage in pro-environmental activities. While we noted modest change in knowledge and attitudes, we found comparison and interpretation of these data difficult in the absence of other studies using similar measures. We suggest that alternative survey instruments are needed and should be calibrated appropriately to the pre-existing attitudes, behavior, and levels of knowledge in these relatively sophisticated target groups.",2013.0,"Alycia Crall, R. Jordan, Kirstin A. Holfelder, Greg Newman, J. Graham, D. Waller"
0476ddfada4efc840f38a71588afb6a88874e5dc,https://www.semanticscholar.org/paper/0476ddfada4efc840f38a71588afb6a88874e5dc,Data at work: supporting sharing in science and engineering,"Data are a fundamental component of science and engineering work, and the ability to share data is critical to the validation and progress of science. Data sharing and reuse in some fields, however, has proven to be a difficult problem. This paper argues that the development of effective CSCW systems to support data sharing in work groups requires a better understanding of the use of data in practice. Drawing on our work with three scientific disciplines, we show that data play two general roles in scientific communities: 1) they serve as evidence to support scientific inquiry, and 2) they make a social contribution to the establishment and maintenance of communities of practice. A clearer consideration and understanding of these roles can contribute to the design of more effective data sharing systems. We suggest that this can be achieved through supporting social interaction around data abstractions, reaching beyond current metadata models, and supporting the social roles of data.",2003.0,"Jeremy P. Birnholtz, Matthew J. Bietz"
e6e7bd03b1fca6e11e19c1c03fad0880c35d385f,https://www.semanticscholar.org/paper/e6e7bd03b1fca6e11e19c1c03fad0880c35d385f,Irena: tool suite for modeling and analysis of small‐angle scattering,"Irena, a tool suite for analysis of both X-ray and neutron small-angle scattering (SAS) data within the commercial Igor Pro application, brings together a comprehensive suite of tools useful for investigations in materials science, physics, chemistry, polymer science and other fields. In addition to Guinier and Porod fits, the suite combines a variety of advanced SAS data evaluation tools for the modeling of size distribution in the dilute limit using maximum entropy and other methods, dilute limit small-angle scattering from multiple non-interacting populations of scatterers, the pair-distance distribution function, a unified fit, the Debye–Bueche model, the reflectivity (X-ray and neutron) using Parratt's formalism, and small-angle diffraction. There are also a number of support tools, such as a data import/export tool supporting a broad sampling of common data formats, a data modification tool, a presentation-quality graphics tool optimized for small-angle scattering data, and a neutron and X-ray scattering contrast calculator. These tools are brought together into one suite with consistent interfaces and functionality. The suite allows robust automated note recording and saving of parameters during export.",2009.0,"J. Ilavsky, P. Jemian"
c51f841b17cbb5b521f0a6709597c5dd86127ff7,https://www.semanticscholar.org/paper/c51f841b17cbb5b521f0a6709597c5dd86127ff7,"Nonseparable, Stationary Covariance Functions for Space–Time Data","Geostatistical approaches to spatiotemporal prediction in environmental science, climatology, meteorology, and related fields rely on appropriate covariance models. This article proposes general classes of nonseparable, stationary covariance functions for spatiotemporal random processes. The constructions are directly in the space–time domain and do not depend on closed-form Fourier inversions. The model parameters can be associated with the data's spatial and temporal structures, respectively; and a covariance model with a readily interpretable space–time interaction parameter is fitted to wind data from Ireland.",2002.0,T. Gneiting
100d5c0e10af860e734c61ca70222cf2fc6ec125,https://www.semanticscholar.org/paper/100d5c0e10af860e734c61ca70222cf2fc6ec125,"Conceptual approaches for defining data, information, and knowledge","The field of Information Science is constantly changing. Therefore, information scientists are required to regularly review-and if necessary-redefine its fundamental building blocks. This article is one of a group of four articles, which resulted from a Critical Delphi study conducted in 2003-2005. The study, ""Knowledge Map of Information Science,"" was aimed at exploring the foundations of information science. The international panel was composed of 57 leading scholars from 16 countries, who represent (almost) all the major subfields and important aspects of the field. This particular article documents 130 definitions of data, information, and knowledge formulated by 45 scholars, and maps the major conceptual approaches for defining these three key concepts.",2007.0,Chaim Zins
113a347b4d0682067d14685500a1689cdccb50e3,https://www.semanticscholar.org/paper/113a347b4d0682067d14685500a1689cdccb50e3,Analysis of Multivariate Social Science Data,"Preface Setting the Scene Structure of the book Our limited use of mathematics Variables The geometry of multivariate analysis Use of examples Data inspection, transformations, and missing data Cluster Analysis Classification in social sciences Some methods of cluster analysis Graphical presentation of results Derivation of the distance matrix Example on English dialects Comparisons Clustering variables Further examples and suggestions for further work Multidimensional Scaling Introduction Examples Classical, ordinal, and metrical multidimensional scaling Comments on computational procedures Assessing fit and choosing the number of dimensions A worked example: dimensions of color vision Further examples and suggestions for further work Correspondence Analysis Aims of correspondence analysis Carrying out a correspondence analysis: a simple numerical example Carrying out a correspondence analysis: the general method The biplot Interpretation of dimensions Choosing the number of dimensions Example: confidence in purchasing from European Community countries Correspondence analysis of multiway tables Further examples and suggestions for further work Principal Components Analysis Introduction Some potential applications Illustration of PCA for two variables An outline of PCA Examples Component scores The link between PCA and multidimensional scaling and between PCA and correspondence analysis Using principal component scores to replace the original variables Further examples and suggestions for further work NEW! Regression Analysis Basic ideas Simple linear regression A probability model for simple linear regression Inference for the simple linear regression model Checking the assumptions Multiple regression Examples of multiple regression Estimation and inference about the parameters Interpretation of the regression coefficients Selection of regressor variables Transformations and interactions Logistic regression Path analysis Further examples and suggestions for further work Factor Analysis Introduction to latent variable models The linear single-factor model The general linear factor model Interpretation Adequacy of the model and choice of the number of factors Rotation Factor scores A worked example: the test anxiety inventory How rotation helps interpretation A comparison of factor analysis and principal components analysis Further examples and suggestions for further work Software Factor Analysis for Binary Data Latent trait models Why is the factor analysis model for metrical variables invalid for binary responses? Factor model for binary data using the item response theory approach Goodness-of-fit Factor scores Rotation Underlying variable approach Example: sexual attitudes Further examples and suggestions for further work Software Factor Analysis for Ordered Categorical Variables The practical background Two approaches to modeling ordered categorical data Item response function approach Examples The underlying variable approach Unordered and partially ordered observed variables Further examples and suggestions for further work Software Latent Class Analysis for Binary Data Introduction The latent class model for binary data Example: attitude to science and technology data How can we distinguish the latent class model from the latent trait model? Latent class analysis, cluster analysis, and latent profile analysis Further examples and suggestions for further work Software NEW! Confirmatory Factor Analysis and Structural Equation Models Introduction Path diagram Measurement models Adequacy of the model Introduction to structural equation models with latent variables The linear structural equation model A worked example Extensions Further examples Software NEW! Multilevel Modeling Introduction Some potential applications Comparing groups using multilevel modeling Random intercept model Random slope model Contextual effects Multilevel multivariate regression Multilevel factor analysis Further examples and suggestions for further work Further topics Estimation procedures and software References Index Further reading sections appear at the end of each chapter.",2008.0,"D. Bartholomew, F. Steele, I. Moustaki, J. Galbraith"
3b59a9cbe019062c5afb2f7b0dfe19828f593853,https://www.semanticscholar.org/paper/3b59a9cbe019062c5afb2f7b0dfe19828f593853,THE USE OF CITATION DATA IN WRITING THE HISTORY OF SCIENCE,"Abstract : A study is reported which tested the hypothesis that citation indexes are useful heuristic tools for the historian. In this approach, the history of science is regarded as a chronological sequence of events in which each new discovery is dependent upon earlier discoveries. Models of history were constructed consisting of chronological maps or topological network diagrams. Two such models were used here. The first is based on the events in the history of DNA as described by Dr. Isaac Asimov in the Genetic Code. The second is based on the bibliographic citation data contained in the documents which are the original published studies of events represented in the Asimov book. The interdependencies of linkages among 40 major events (nodes) included in both network diagrams were mapped and compared. The study confirmed 65% (28 of 43) of the historical dependencies in the Asimov network by corresponding linkages established by citations. In addition, 31 citation connections were found which did not correspond to any historical dependencies noted in The Genetic Code.",1964.0,"E. Garfield, I. H. Sher, R. Torpie"
d441ac8a4da15b1ebd9321f33c914647d99b780d,https://www.semanticscholar.org/paper/d441ac8a4da15b1ebd9321f33c914647d99b780d,Lack of group-to-individual generalizability is a threat to human subjects research,"Significance The current study quantified the degree to which group data are able to describe individual participants. We utilized intensive repeated-measures data—data that have been collected many times, across many individuals—to compare the distributions of bivariate correlations calculated within subjects vs. those calculated between subjects. Because the vast majority of social and medical science research aggregates across subjects, we aimed to assess how closely such aggregations reflect their constituent individuals. We provide evidence that conclusions drawn from aggregated data may be worryingly imprecise. Specifically, the variance in individuals is up to four times larger than in groups. These data call for a focus on idiography and open science that may substantially alter best-practice guidelines in the medical and behavioral sciences. Only for ergodic processes will inferences based on group-level data generalize to individual experience or behavior. Because human social and psychological processes typically have an individually variable and time-varying nature, they are unlikely to be ergodic. In this paper, six studies with a repeated-measure design were used for symmetric comparisons of interindividual and intraindividual variation. Our results delineate the potential scope and impact of nonergodic data in human subjects research. Analyses across six samples (with 87–94 participants and an equal number of assessments per participant) showed some degree of agreement in central tendency estimates (mean) between groups and individuals across constructs and data collection paradigms. However, the variance around the expected value was two to four times larger within individuals than within groups. This suggests that literatures in social and medical sciences may overestimate the accuracy of aggregated statistical estimates. This observation could have serious consequences for how we understand the consistency between group and individual correlations, and the generalizability of conclusions between domains. Researchers should explicitly test for equivalence of processes at the individual and group level across the social and medical sciences.",2018.0,"A. Fisher, J. Medaglia, B. Jeronimus"
e53a4e19d7329965de877d0dfde5872f5cb007c7,https://www.semanticscholar.org/paper/e53a4e19d7329965de877d0dfde5872f5cb007c7,Reasoning from data: How students collect and interpret data in science investigations,"This study explored the understandings of data and measurement that school students draw upon, and the ways that they reason from data, when carrying out a practical science inquiry task. The two practical tasks used in the study each involved investigations of the relationships between two independent variables (IVs) and a dependent variable (DV); in both tasks, one IV covaried with the DV, whereas the other did not. Each was undertaken by 10 students, aged 10, 12, and 14 years (total n = 60 students), working individually. Their actions were video-recorded for analysis. In a subsequent interview, each student was asked to discuss and interpret data collected by two other students, undertaking a similar (but different) practical task, shown on a video-recording. An analysis of the sample students' performance on the practical tasks and their interview responses showed few differences across task contexts, or with age, in students' reasoning, but significant differences in performance when investigating situations of covariation and non-covariation. Few students in the sample displayed sufficient understanding of measurement error to deal effectively with the latter. Investigations of non-covariation cases revealed, much more clearly than investigations of covariation cases, the students' ideas about data and measurement, and their ways of reasoning from data. Such investigations therefore provide particularly valuable contexts for teaching and research.",2004.0,"Zoe Kanari, R. Millar"
1d0588ab31f2e29c6029cfa0762bad255c30d94a,https://www.semanticscholar.org/paper/1d0588ab31f2e29c6029cfa0762bad255c30d94a,"An Evaluation of Amazon’s Mechanical Turk, Its Rapid Rise, and Its Effective Use","Over the past 2 decades, many social scientists have expanded their data-collection capabilities by using various online research tools. In the 2011 article “Amazon’s Mechanical Turk: A new source of inexpensive, yet high-quality, data?” in Perspectives on Psychological Science, Buhrmester, Kwang, and Gosling introduced researchers to what was then considered to be a promising but nascent research platform. Since then, thousands of social scientists from seemingly every field have conducted research using the platform. Here, we reflect on the impact of Mechanical Turk on the social sciences and our article’s role in its rise, provide the newest data-driven recommendations to help researchers effectively use the platform, and highlight other online research platforms worth consideration.",2018.0,"Michael D. Buhrmester, Sanaz Talaifar, S. Gosling"
922e397a4f37ed5e4b0cdde100f733472b3fb390,https://www.semanticscholar.org/paper/922e397a4f37ed5e4b0cdde100f733472b3fb390,Geographical information science,Abstract. Research papers at conferences such as EGIS and the International Symposia on Spatial Data Handling address a set of intellectual and scientific questions which go well beyond the limited technical capabilities of current technology in geographical information systems. This paper reviews the topics which might be included in a science of geographical information. Research on these fundamental issues is a better prospect for long-term survival and acceptance in the academy than the development of technical capabilities. This paper reviews the current state of research in a series of key areas and speculates on why progress has been so uneven. The final section of the paper looks to the future and to new areas of significant potential in this area of research.,1992.0,M. Goodchild
21da991da90631cdfaaede7a24dd719d3fcd1adf,https://www.semanticscholar.org/paper/21da991da90631cdfaaede7a24dd719d3fcd1adf,Information as thing,"Three meanings of “information” are distinguished: “Information‐as‐process”; “information‐as‐knowledge”; and “information‐as‐thing,” the attributive use of “information” to denote things regarded as informative. The nature and characteristics of “information‐as‐thing” are discussed, using an indirect approach (“What things are informative?”). Varieties of “information‐as‐thing” include data, text, documents, objects, and events. On this view “information” includes but extends beyond communication. Whatever information storage and retrieval systems store and retrieve is necessarily “information‐as‐thing.” These three meanings of “information,” along with “information processing,” offer a basis for classifying disparate information‐related activities (e.g., rhetoric, bibliographic retrieval, statistical analysis) and, thereby, suggest a topography for “information science.” © 1991 John Wiley & Sons, Inc.",1991.0,M. Buckland
de6fcef4febe3d8acd6f7ecc272baeee41355640,https://www.semanticscholar.org/paper/de6fcef4febe3d8acd6f7ecc272baeee41355640,Beyond the Data Deluge,The demands of data-intensive science represent a challenge for diverse scientific communities.,2009.0,"Gordon Bell, Tony (Anthony) John Grenville Hey, A. Szalay"
6b82497a15d7c66e2227de3973d38c65b50385f1,https://www.semanticscholar.org/paper/6b82497a15d7c66e2227de3973d38c65b50385f1,Estimating Dynamic Panel Data Models in Political Science,"Panel data are a very valuable resource for finding empirical solutions to political science puzzles. Yet numerous published studies in political science that use panel data to estimate models with dynamics have failed to take into account important estimation issues, which calls into question the inferences we can make from these analyses. The failure to account explicitly for unobserved individual effects in dynamic panel data induces bias and inconsistency in cross-sectional estimators. The purpose of this paper is to review dynamic panel data estimators that eliminate these problems. I first show how the problems with cross-sectional estimators arise in dynamic models for panel data. I then show how to correct for these problems using generalized method of moments estimators. Finally, I demonstrate the usefulness of these methods with replications of analyses in the debate over the dynamics of party identification.",2002.0,Gregory J. Wawro
4f0d47b625c09a0206511f425256b7dc1aa09922,https://www.semanticscholar.org/paper/4f0d47b625c09a0206511f425256b7dc1aa09922,Statistical Procedures and the Justification of Knowledge in Psychological Science,"Justification, in the vernacular language of philosophy of science, refers to the evaluation, defense, and confirmation of claims of truth. In this article, we examine some aspects of the rhetoric of justification, which in part draws on statistical data analysis to shore up facts and inductive inferences. There are a number of problems of methodological spirit and substance that in the past have been resistant to attempts to correct them. The major problems are discussed, and readers are reminded of ways to clear away these obstacles to justification.",1989.0,"R. L. Rosnow, R. Rosenthal"
c473f93e6b5729bbb21c0eb061c8af1f03d539be,https://www.semanticscholar.org/paper/c473f93e6b5729bbb21c0eb061c8af1f03d539be,Interpretation and Method : Empirical Research Methods and the Interpretive Turn,"This book demonstrates the relevance, rigor, and creativity of interpretive research methodologies for the social and human sciences. The book situates methods questions within the context of broader methodological questions--specifically, the character of social realities and their ""know-ability."" Exceptionally clear and well-written chapters provide engaging discussions of the methods of accessing, generating, and analyzing social science data, using methods ranging from reflexive historical analysis to critical ethnography. Reflecting on their own research experiences, the contributors offer an inside, applied perspective on how research topics, evidence, and methods intertwine to produce knowledge in the social sciences.",2006.0,"D. Yanow, Peregrine Schwartz-Shea"
428922e0d2bf4b2f90dea64b7ea3b88645985d19,https://www.semanticscholar.org/paper/428922e0d2bf4b2f90dea64b7ea3b88645985d19,"New Politics and Class Politics in the Context of Austerity and Globalization: Welfare State Regress in 18 Countries, 1975–95","The relevance of socioeconomic class and of class-related parties for policymaking is a recurring issue in the social sciences. The “new politics” perspective holds that in the present era of austerity, class-based parties once driving welfare state expansion have been superseded by powerful new interest groups of welfare-state clients capable of largely resisting retrenchment pressures emanating from postindustrial forces. We argue that retrenchment can fruitfully be analyzed as distributive conflict involving a remaking of the early postwar social contract based on the full employment welfare state, a conflict in which partisan politics and welfare-state institutions are likely to matter. Pointing to problems of conceptualization and measurement of the dependent variable in previous research, we bring in new data on the extent of retrenchment in social citizenship rights and show that the long increase in social rights has been turned into a decline and that significant retrenchment has taken place in several countries. Our analyses demonstrate that partisan politics remains significant for retrenchment also when we take account of contextual indictors, such as constitutional veto points, economic factors, and globalization.Author names are in alphabetical order and they share equal responsibility for the manuscript. Early versions of this paper were presented at annual meetings of the Nordic Political Science Association in Aalborg, 2002, and the American Political Science Association in San Francisco, 2001, the International Sociological Association RC 28 meeting in Mannheim, 2001, the International Sociological Association RC 19 meeting in Tilburg 2000, and the American Sociological Association in Washington, DC, 2000, as well as at various seminars. For constructive comments on different versions of the manuscript we thank Rainer Lepsius, Anders Lindbom, Ingalill Montanari, John Myles, Michael Shalev, Sheila Shaver, and Robin Stryker, as well as other participants in these meetings. We want to thank Olof Bäckman, Stefan Englund, Ingrid Esser, Helena Höög, and Annita Näsström for very valuable help and Dennis Quinn for providing us his data on international financial deregulation. Our thanks are also due to three anonymous referees for careful reading. This research has been supported by grants from the Bank of Sweden Tercentennial Foundation and the Swedish Council for Social Research.",2003.0,"W. Korpi, Joakim Palme"
2ebd442043d8dfd92b832a5ec9670b98f7087ba7,https://www.semanticscholar.org/paper/2ebd442043d8dfd92b832a5ec9670b98f7087ba7,Sorting and searching,"Typical computer science students study the basic sorting algorithms at least three times before they graduate:first in introductory programming,then in data structures, and finally in their algorithms course.",2007.0,N. Govindaraju
2f2a2e5bf046cfe076a62a102117b74cae0e788e,https://www.semanticscholar.org/paper/2f2a2e5bf046cfe076a62a102117b74cae0e788e,Visualizing Science by Citation Mapping,"Science mapping is discussed in the general context of information visualization. Attempts to construct maps of science using citation data are reviewed, focusing on the use of co-citation clusters. New work is reported on a dataset of about 36,000 documents using simplified methods for ordination, and nesting maps hierarchically. An overall map of the dataset shows the multidisciplinary breadth of the document sample, and submaps allow drilling down to the document level. An effort to visualize these data using advanced virtual reality software is described, and the creation of document pathways through the map is seen as a realization of Bush's (1945) associative trails.",1999.0,H. Small
d751a404697db6de97c364092598e385c9e63c72,https://www.semanticscholar.org/paper/d751a404697db6de97c364092598e385c9e63c72,Data Mining: Data Mining Concepts and Techniques,Data mining is a field of intersection of computer science and statistics used to discover patterns in the information bank. The main aim of the data mining process is to extract the useful information from the dossier of data and mold it into an understandable structure for future use. There are different process and techniques used to carry out data mining successfully.,2013.0,Shivam Agarwal
8b345f5d227cc0ad445084d419bb0ed014e71237,https://www.semanticscholar.org/paper/8b345f5d227cc0ad445084d419bb0ed014e71237,Braun-Blanquet's legacy and data analysis in vegetation science,"Abstract This article investigates whether the Braun-Blanquet abundance/dominance (AD) scores that commonly appear in phytosociological tables can properly be analysed by conventional multivariate analysis methods such as Principal Components Analysis and Correspondence Analysis. The answer is a definite NO. The source of problems is that the AD values express species performance on a scale, namely the ordinal scale, on which differences are not interpretable. There are several arguments suggesting that no matter which methods have been preferred in contemporary numerical syntaxonomy and why, ordinal data should be treated in an ordinal way. In addition to the inadmissibility of arithmetic operations with the AD scores, these arguments include interpretability of dissimilarities derived from ordinal data, consistency of all steps throughout the analysis and universality of the method which enables simultaneous treatment of various measurement scales. All the ordination methods that are commonly used, for example, Principal Components Analysis and all variants of Correspondence Analysis as well as standard cluster analyses such as Ward's method and group average clustering, are inappropriate when using AD data. Therefore, the application of ordinal clustering and scaling methods to traditional phytosociological data is advocated. Dissimilarities between relevés should be calculated using ordinal measures of resemblance, and ordination and clustering algorithms should also be ordinal in nature. A good ordination example is Non-metric Multidimensional Scaling (NMDS) as long as it is calculated from an ordinal dissimilarity measure such as the Goodman & Kruskal γ coefficient, and for clustering the new OrdClAn-H and OrdClAn-N methods. Abbreviations: AD = Abundance/dominance; NMDS = Non-metric Multidimensional Scaling.",2006.0,J. Podani
d1ed3cf4fe0e4612448c9564214cf1019cdc58a0,https://www.semanticscholar.org/paper/d1ed3cf4fe0e4612448c9564214cf1019cdc58a0,Earth System Science Workbench: a data management infrastructure for earth science products,"The Earth System Science Workbench (ESSW) is a non-intrusive data management infrastructure for researchers who are also data publishers. An implementation of ESSW to track the processing of locally received satellite imagery is presented, demonstrating the Workbench's transparent and robust support for archiving and publishing data products. ESSW features a Lab Notebook metadata service, an ND-WORM (No Duplicate-Write Once Read Many) storage service, and Web user interface tools. The Lab Notebook logs processes (experiments) and their relationships via a custom API to XML documents stored in a relational database. The ND-WORM provides a managed storage archive for the Lab Notebook by keeping unique file digests and name-space meta-data, also in a relational database. ESSW Notebook tools allow project searching and ordering, and file and meta-data management.",2001.0,"J. Frew, R. Bose"
29421664e97c275cfc3711228b22a05a3bfb3835,https://www.semanticscholar.org/paper/29421664e97c275cfc3711228b22a05a3bfb3835,Science fraud: from patchwork mouse to patchwork data,"Summerlin pulled two white mice from the container. While they wriggled and squeaked in protest, he inspected the sites of the black skin grafts. Impulsively, Summerlin took his felttipped pen out of the breast pocket of his white coat and applied it briefly to the grafted patches on the two white animals. The ink made them look darker. Then he replaced the mice in the bin and strode out . . . From The Patchwork Mouse, an account of William T. Summerlin's 1974 false claim of skin transplantation without immunosuppression (1).",2006.0,G. Weissmann
4b2b0821d9d6c9535f4f1aaf80b1e6be79a3ca3e,https://www.semanticscholar.org/paper/4b2b0821d9d6c9535f4f1aaf80b1e6be79a3ca3e,Networks,"The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in recent years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyse network data on an unprecendented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social science. This book brings together the most important breakthroughts in each of these fields and presents them in a unified fashion, highlighting the strong interconnections between work in different areas. Topics covered include the measurement of networks; methods for analysing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms, including spectral algorithms and community detection; mathematical models of networks such as random graph models and generative models; and models of processes taking place on networks.",1995.0,Jessen T. Havill
a6ffce203cb7e587a5f4d36ca7442a7b26c65b07,https://www.semanticscholar.org/paper/a6ffce203cb7e587a5f4d36ca7442a7b26c65b07,Deep-learning seismology,"Seismic waves from earthquakes and other sources are used to infer the structure and properties of Earth’s interior. The availability of large-scale seismic datasets and the suitability of deep-learning techniques for seismic data processing have pushed deep learning to the forefront of fundamental, long-standing research investigations in seismology. However, some aspects of applying deep learning to seismology are likely to prove instructive for the geosciences, and perhaps other research areas more broadly. Deep learning is a powerful approach, but there are subtleties and nuances in its application. We present a systematic overview of trends, challenges, and opportunities in applications of deep-learning methods in seismology. Description Large-scale learning The large amount and availability of datasets in seismology creates a great opportunity to apply machine learning and artificial intelligence to data processing. Mousavi and Beroza provide a comprehensive review of the deep-learning techniques being applied to seismic datasets, covering approaches, limitations, and opportunities. The trends in data processing and analysis can be instructive for geoscience and other research areas more broadly. —BG The ways in which deep learning can help process and analyze large seismological datasets are reviewed. BACKGROUND Seismology is the study of seismic waves to understand their origin—most obviously, sudden fault slip in earthquakes, but also explosions, volcanic eruptions, glaciers, landslides, ocean waves, vehicular traffic, aircraft, trains, wind, air guns, and thunderstorms, for example. Seismology uses those same waves to infer the structure and properties of planetary interiors. Because sources can generate waves at any time, seismic ground motion is recorded continuously, at typical sampling rates of 100 points per second, for three components of motion, and on arrays that can include thousands of sensors. Although seismology is clearly a data-rich science, it often is a data-driven science as well, with new phenomena and unexpected behavior discovered with regularity. And for at least some tasks, the careful and painstaking work of seismic analysts over decades and around the world has also made seismology a data label–rich science. This facet makes it fertile ground for deep learning, which has entered almost every subfield of seismology and outperforms classical approaches, often dramatically, for many seismological tasks. ADVANCES Seismic wave identification and onset-time, first-break determination for seismic P and S waves within continuous seismic data are foundational to seismology and are particularly well suited to deep learning because of the availability of massive, labeled datasets. It has received particularly close attention, and that has led, for example, to the development of deep learning–based earthquake catalogs that can feature more than an order of magnitude more events than are present in conventional catalogs. Deep learning has shown the ability to outperform classical approaches for other important seismological tasks as well, including the discrimination of earthquakes from explosions and other sources, separation of seismic signals from background noise, seismic image processing and interpretation, and Earth model inversion. OUTLOOK The development of increasingly cost-effective sensors and emerging ground-motion sensing technologies, such as fiber optic cable and accelerometers in smart devices, portend a continuing acceleration of seismological data volumes, so that deep learning is likely to become essential to seismology’s future. Deep learning’s nonlinear mapping ability, sequential data modeling, automatic feature extraction, dimensionality reduction, and reparameterization are all advantageous for processing high-dimensional seismic data, particularly because those data are noisy and, from the point of view of mathematical inference, incomplete. Deep learning for scientific discovery and direct extraction of insight into seismological processes is clearly just getting started. Aspects of seismology pose interesting additional challenges for deep learning. Many of the most important problems in earthquake seismology—such as earthquake forecasting, ground motion prediction, and rapid earthquake alerting—concern large and damaging earthquakes that are (fortunately) rare. That rarity poses a fundamental challenge for the data-hungry methods of deep learning: How can we train reliable models, and how do we validate them well enough to rely on them when data are scarce and opportunities to test models are infrequent? Further, how can we operationalize deep-learning techniques in such a situation, when the mechanisms by which they make predictions from data may not be easily explained, and the consequences of incorrect models are high? Incorporating domain knowledge through physics-based and explainable deep learning and setting up standard benchmarking and evaluation protocols will help ensure progress, as is the nascent emergence of a seismological data science ecosystem. More generally, a combination of data science literacy for geoscientists as well as recruiting data science expertise will help to ensure that deep-learning seismology reaches its full potential. Deep-learning processing of seismic data and incorporation of domain knowledge can lead to new capabilities and new insights across seismology. A. MASTIN/SCIENCE, TOP RIGHT: CARA HARWOOD/UNIVERSITY OF CALIFORNIA-DAVIS/CC BY-NC-SA 3.0 MIDDLE RIGHT: JOHAN SWANEPOEL/SCIENCE SOURCE",2022.0,"S. Mousavi, G. Beroza"
64bf542854f74a24bd68e264267bc0af664de995,https://www.semanticscholar.org/paper/64bf542854f74a24bd68e264267bc0af664de995,Open Data in Science,"Abstract Open Data (OD) is an emerging term in the process of defining how scientific data may be published and re-used without price or permission barriers. Scientists generally see published data as belonging to the scientific community, but many publishers claim copyright over data and will not allow its re-use without permission. This is a major impediment to the progress of scholarship in the digital age. This article reviews the need for Open Data, shows examples of why Open Data are valuable, and summarizes some early initiatives in formalizing the right of access to and re-use of scientific data.",2008.0,Peter Murray-Rust
1bcdf65d707f7111dbe0f7c89545871e6523066b,https://www.semanticscholar.org/paper/1bcdf65d707f7111dbe0f7c89545871e6523066b,Vegetation Description and Data Analysis: A Practical Approach,"Preface to the second edition ix Acknowledgements xi Safety in the field xiii Chapter 1 The nature of quantitative plant ecology and vegetation science 1 Chapter 2 Environmental gradients, plant communities and vegetation dynamics 23 Chapter 3 The description of vegetation in the field 49 Chapter 4 The nature and properties of vegetation data 101 Chapter 5 Basic statistical methods for understanding multivariate analysis 139 Chapter 6 Ordination methods 171 Chapter 7 Phytosociology and the Zurich-Montpellier (Braun-Blanquet) School of subjective classification 273 Chapter 8 Numerical classification, cluster analysis and phytosociology 307 Chapter 9 Computer software for the analysis of vegetation and environmental/biotic data 359 Chapter 10 Future developments in vegetation science and quantitative plant ecology 369 References 371 Index 403",2011.0,M. Kent
c2d3a1661a97f10ad29d2ea56e09981e4cb758e1,https://www.semanticscholar.org/paper/c2d3a1661a97f10ad29d2ea56e09981e4cb758e1,A Data Repository for the EDM Community: The PSLC DataShop,"In recent years, educational data mining has emerged as a burgeoning new area for scientific investigation. One reason for the emerging excitement about educational data mining is the increasing availability of fine-grained, extensive, and longitudinal data on student learning. These data come from many sources, including standardized tests combined with student demographic data (for instance, www.icpsr.umich.edu/IAED), and videos of classroom interactions [22]. Extensive new data sources have been transformational in science [5] and business (being a major part of the success of key businesses such as Google, FedEx, and WalMart).",2010.0,"K. Koedinger, R. Baker, Kyle Cunningham, A. Skogsholm, Brett Leber, John C. Stamper"
40a8bd9fdcfa5ae1d7b91894719d37ea861f2c2d,https://www.semanticscholar.org/paper/40a8bd9fdcfa5ae1d7b91894719d37ea861f2c2d,Influencing student understanding of the nature of science: Data from a case of curriculum development,"The purpose of this study was to investigate the effects of the first-year field test BSCS middle school science program on student understanding of the creative, developmental, testable, and unified nature of science. The experimental group, which was exposed to the BSCS program, and the control group, which was taught using a more traditional middle school science curriculum, were administered a pretest and posttest using the Modified Nature of Scientific Knowledge Scale (MNSKS). Analyses of the results showed that the understanding of students who experienced the BSCS science program decreased significantly in regard to the developmental and testable nature of science. The understanding of students who experienced the control-group science program decreased significantly in regard to the creative nature of science. Analyses of covariance indicated that students in the control group possessed a significantly better understanding of the testable nature of science than did students who used the BSCS science program. Implications of these results are related to the constructivist view of learning, the development of curricula designed to facilitate scientific literacy, and future research endeavors.",1992.0,Yvonne J. Meichtry
9759ed3befc96caa5035b7176671efea99cd3493,https://www.semanticscholar.org/paper/9759ed3befc96caa5035b7176671efea99cd3493,A Brief Review on Leading Big Data Models,"Today, science is passing through an era of transformation, where the inundation of data, dubbed data deluge is influencing the decision making process. The science is driven by the data and is being termed as data science. In this internet age, the volume of the data has grown up to petabytes, and this large, complex, structured or unstructured, and heterogeneous data in the form of “Big Data” has gained significant attention. The rapid pace of data growth through various disparate sources, especially social media such as Facebook, has seriously challenged the data analytic capabilities of traditional relational databases. The velocity of the expansion of the amount of data gives rise to a complete paradigm shift in how new age data is processed. Confidence in the data engineering of the existing data processing systems is gradually fading whereas the capabilities of the new techniques for capturing, storing, visualizing, and analyzing data are evolving. In this review paper, we discuss some of the modern Big Data models that are leading contributors in the NoSQL era and claim to address Big Data challenges in reliable and efficient ways. Also, we take the potential of Big Data into consideration and try to reshape the original operationaloriented definition of “Big Science” (Furner, 2003) into a new data-driven definition and rephrase it as “The science that deals with Big Data is Big Science.”",2014.0,"Sugam Sharma, U. S. Tim, Johnny S. Wong, S. Gadia, Subhash Chander Sharma"
e31f0c56082f1887844dc8d8f80993c4fbcf6919,https://www.semanticscholar.org/paper/e31f0c56082f1887844dc8d8f80993c4fbcf6919,Open Data for Global Science,"he digital revolution has transformed the accumulation of properly curated public research data into an essential upstream resource whose value increases with use. The potential contributions of such data to the creation of new knowledge and downstream economic and social goods can in many cases be multiplied exponentially when the data are made openly available on digital networks. Most developed countries spend large amounts of public resources on research and related scientific facilities and instruments that generate massive amounts of data. Yet precious little of that investment is devoted to promoting the value of the resulting data by preserving and making them broadly available. The largely ad hoc approach to managing such data, however, is now beginning to be understood as inadequate to meet the exigencies of the national and international research enterprise. The time has thus come for the research community to establish explicit responsibilities for these digital resources. This article reviews the opportunities and challenges to the global science system associated with establishing an open data policy.",2007.0,"P. Uhlir, P. Schröder"
4cc445dfc181101552971c2342a06a24bc4bbc88,https://www.semanticscholar.org/paper/4cc445dfc181101552971c2342a06a24bc4bbc88,"Student performances on the science processes of recording data, analyzing data, drawing conclusions, and providing evidence","The National Committee on Science Education Standards and Assessment (1994 draft) viewed several science processes as important to an understanding of science as inquiry: formulating usable questions, planning experiments, conducting systematic observations, interpreting and analyzing data, drawing conclusions, communicating, and coordinating and implementing a full investigation. This study is one of three undertaken to develop research rubrics for a performance assessment of science processes and to evaluate seventh-grade science students' ability to perform them. Specifically, this article focuses on the processes of recording data, analyzing data, drawing conclusions, and providing evidence. A total of 364 students field tested the Alternative Assessment of Science Process Skills. Their responses were used to develop a research rubric, and then this rubric was used to determine response patterns that could inform both instruction and assessment of science process skills. Only 61% of students performed the activity and recorded data successfully. Sixty-nine percent of students did not attend to the hypothesis in drawing their conclusions. Eighty-one percent did not provide specific evidence for their conclusions. These results were discussed in light of relevant theories and models as well as their implications for instruction and assessment. © 1996 John Wiley & Sons, Inc.",1996.0,"P. Germann, Roberta J. Aram"
6a04847a2e6875142d594abfd0c977fa42a5a95a,https://www.semanticscholar.org/paper/6a04847a2e6875142d594abfd0c977fa42a5a95a,Data curation + process curation=data integration + science,"In bioinformatics, we are familiar with the idea of curated data as a prerequisite for data integration. We neglect, often to our cost, the curation and cataloguing of the processes that we use to integrate and analyse our data. Programmatic access to services, for data and processes, means that compositions of services can be made that represent the in silico experiments or processes that bioinformaticians perform. Data integration through workflows depends on being able to know what services exist and where to find those services. The large number of services and the operations they perform, their arbitrary naming and lack of documentation, however, mean that they can be difficult to use. The workflows themselves are composite processes that could be pooled and reused but only if they too can be found and understood. Thus appropriate curation, including semantic mark-up, would enable processes to be found, maintained and consequently used more easily. This broader view on semantic annotation is vital for full data integration that is necessary for the modern scientific analyses in biology. This article will brief the community on the current state of the art and the current challenges for process curation, both within and without the Life Sciences.",2008.0,"C. Goble, R. Stevens, Duncan Hull, K. Wolstencroft, R. Lopez"
6765332ea2813d8b0c744fe20fb02269c23bfec8,https://www.semanticscholar.org/paper/6765332ea2813d8b0c744fe20fb02269c23bfec8,Registered Reports A Method to Increase the Credibility of Published Results,"Ignoring replications and negative results is bad for science. This special issue presents a novel publishing format – Registered Reports – as a partial solution. Peer review occurs prior to data collection, design and analysis plans are preregistered, and results are reported regardless of outcome. Fourteen Registered Reports of replications of important published results in social psychology are reported with strong confirmatory tests. Further, the articles demonstrate open science practices such as open data, open materials, and disclosure of research process, conflicts of interest, and contributions. The credibility of published science will increase with cultural shifts that accept replications and negative results as viable research outcomes, and when transparency and reproducibility are part of standard research practice.",2014.0,"Brian A. Nosek, D. Lakens"
128c49a834e06409af15001de12c351fcacbf74b,https://www.semanticscholar.org/paper/128c49a834e06409af15001de12c351fcacbf74b,Information Sciences,"The information sciences provide tools for deductive reasoning to supplement the classifications made by the data sciences and the explanations made by explanatory models. Formal ontologies provide a unifying framework for organizing definitions, research findings, and theories. One of the primary purposes of a formal ontology is to use deductive reasoning to answer questions submitted to computer. A general or upper oncology is required to integrate more specialized domain ontologies. The Suggested Upper Merged Ontology is particularly helpful because it consists of 20,000 concepts with connections to both WordNet and FrameNet. WordNet is an electronic dictionary while FrameNet captures co-occurrences of words to provide a thematic context in which words occur. Together, WordNet, FrameNet, and the Suggested Upper Merged Ontology provide an integration of three major information science tools.",2020.0,Stephen K. Reed
bcce25437e5e259bde08b32d7764d50cbebf0406,https://www.semanticscholar.org/paper/bcce25437e5e259bde08b32d7764d50cbebf0406,Ensuring the Data-Rich Future of the Social Sciences,"Massive increases in the availability of informative social science data are making dramatic progress possible in analyzing, understanding, and addressing many major societal problems. Yet the same forces pose severe challenges to the scientific infrastructure supporting data sharing, data management, informatics, statistical methodology, and research ethics and policy, and these are collectively holding back progress. I address these changes and challenges and suggest what can be done.",2011.0,Gary King
d01378408cc72d65ec6fd0c912b7f6dba44c6288,https://www.semanticscholar.org/paper/d01378408cc72d65ec6fd0c912b7f6dba44c6288,Developing a science of land change: challenges and methodological issues.,"Land-change science has emerged as a foundational element of global environment change and sustainability science. It seeks to understand the human and environment dynamics that give rise to changed land uses and covers, not only in terms of their type and magnitude but their location as well. This focus requires the integration of social, natural, and geographical information sciences. Each of these broad research communities has developed different ways to enter the land-change problem, each with different means of treating the locational specificity of the critical variables, such as linking the land manager to the parcel being managed. The resulting integration encounters various data, methodological, and analytical problems, especially those concerning aggregation and inference, land-use pixel links, data and measurement, and remote sensing analysis. Here, these integration problems, which hinder comprehensive understanding and theory development, are addressed. Their recognition and resolution are required for the sustained development of land-change science.",2004.0,"R. Rindfuss, S. Walsh, B. Turner, J. Fox, Vinod K. Mishra"
f26b402b75675c4aec5e9b932c6622cdc7407e25,https://www.semanticscholar.org/paper/f26b402b75675c4aec5e9b932c6622cdc7407e25,KDD for Science Data Analysis: Issues and Examples,"The analysis of the massive data sets collected by scientific instruments demands automation as a prerequisite to analysis. There is an urgent need to create an intermediate level at which scientists can operate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even remotely approach humans—namely, creative data analysis, theory and hypothesis formation, and drawing insights into underlying phenomena. We give an overview of the main issues in the exploitation of scientific datasets, present five case studies where KDD tools play important and enabling roles, and conclude with future challenges for data mining and KDD techniques in science data analysis.",1996.0,"U. Fayyad, D. Haussler, P. Stolorz"
caf174b2da16efaff0be53a1f5e8bb27453c2efa,https://www.semanticscholar.org/paper/caf174b2da16efaff0be53a1f5e8bb27453c2efa,Data-intensive e-science frontier research,"Large-scale e-science, including high-energy and nuclear physics, biomedical informatics, and Earth science, depend on an increasingly integrated, distributed cyberinfrastructure serving virtual organizations on a global scale.",2003.0,"Harvey B. Newman, Mark Ellisman, J. Orcutt"
3d45e1d46928827f61b1841f9b2420f922abb92e,https://www.semanticscholar.org/paper/3d45e1d46928827f61b1841f9b2420f922abb92e,The Appearance of Data,"This paper explores conditions that allow data to appear, to come into being, in both conventional and more radical approaches in empirical social science research. Conventional qualitative inquiry that uses a positivist ontology—even when it claims to be interpretive—treats qualitative data, words, as brute, existing independent of an interpretive frame, waiting to be “collected” by a human. However, a Deleuzo-Guattarian ontology that does not assume the subject/object binary might not think the concept data at all. The author resists recuperating data in the collapse of the old empiricism and is content to pause in the curious possibilities of a normative ontology that imagines a superior, affirmative, and experimental empiricism in which all concepts, including data, must be re-thought.",2013.0,Elizabeth Adams St. Pierre
7c37727568c324cb2b613a457eca9a31d49604a4,https://www.semanticscholar.org/paper/7c37727568c324cb2b613a457eca9a31d49604a4,Technical note: Application of the Box-Cox data transformation to animal science experiments.,"In the use of ANOVA for hypothesis testing in animal science experiments, the assumption of homogeneity of errors often is violated because of scale effects and the nature of the measurements. We demonstrate a method for transforming data so that the assumptions of ANOVA are met (or violated to a lesser degree) and apply it in analysis of data from a physiology experiment. Our study examined whether melatonin implantation would affect progesterone secretion in cycling pony mares. Overall treatment variances were greater in the melatonin-treated group, and several common transformation procedures failed. Application of the Box-Cox transformation algorithm reduced the heterogeneity of error and permitted the assumption of equal variance to be met.",1998.0,"M R Peltier, C J Wilcox, D C Sharp"
10c39539ed74cf239cf6f7f4eea7d4a57e947aad,https://www.semanticscholar.org/paper/10c39539ed74cf239cf6f7f4eea7d4a57e947aad,Young People's Images of Science,"This book should be of interest to all science educators, whether they teach students, advise or inspect or are involved in planning a science curriculum. The content is essentially an account of a research project undertaken to find out and to report the range and nature of school students' understanding of the nature of science. The authors designed a cross-age study, giving the same task to samples of pupils of three different ages: 9, 12 and 16 years. The collected data were analysed to see the ways in which understanding seemed to change with age and experience. The researchers' interest in students' ideas about science has grown from their observation that learners' responses to observations and ideas are constrained and limited in significant ways by their perception of the nature of scientific work and of scientific knowledge. The result is that students often misinterpret information and experiences presented in the classroom and laboratory. The authors hope that, by knowing more about these misperceptions, we may understand better the processes of science content learning and hence achieve more effective teaching. The book is divided into three sections. In the first 70 pages the authors set out the arguments for giving the nature of science a more prominent place in the curriculum. Next they present an overview of the major schools of thought about science and scientific knowledge, followed by a summary of previous research on students' ideas about the nature of science. The section concludes with the questions, methods and tools used in the research. In the next 60 pages the main findings of the study are presented and discussed. In the final chapter the authors bring together the theoretical arguments for the place of teaching about the nature of science with the results of the research study and consider the possible implications for science teaching in schools, discussing ways in which the curriculum could be adapted to assist students to become better citizens in a modern technological world. The book is well laid out, carefully planned and argued at every stage, with excellent clear headings and summaries. It is possible to read it on a superficial level, dwelling more on the conclusions, or to select sections for closer study using either the index or section headings. There are several appendices, one of which contains the 174 references to other publications quoted in the text. There is considerable food for thought and/or discussion or debate, not only for new teachers but as timely reminders to those who have been in the classroom, laboratory or management for many years, about their aims and objectives and whether they are being realized. This is a volume which should find its way into the resource library of every science teaching department.",1996.0,R. Hackett
07975bba7a9f710c3ce23d90953d180faa323d60,https://www.semanticscholar.org/paper/07975bba7a9f710c3ce23d90953d180faa323d60,Modern analytical ultracentrifugation in protein science: A tutorial review,"Analytical ultracentrifugation (AU) is reemerging as a versatile tool for the study of proteins. Monitoring the sedimentation of macromolecules in the centrifugal field allows their hydrodynamic and thermodynamic characterization in solution, without interaction with any matrix or surface. The combination of new instrumentation and powerful computational software for data analysis has led to major advances in the characterization of proteins and protein complexes. The pace of new advancements makes it difficult for protein scientists to gain sufficient expertise to apply modern AU to their research problems. To address this problem, this review builds from the basic concepts to advanced approaches for the characterization of protein systems, and key computational and internet resources are provided. We will first explore the characterization of proteins by sedimentation velocity (SV). Determination of sedimentation coefficients allows for the modeling of the hydrodynamic shape of proteins and protein complexes. The computational treatment of SV data to resolve sedimenting components has been achieved. Hence, SV can be very useful in the identification of the oligomeric state and the stoichiometry of heterogeneous interactions. The second major part of the review covers sedimentation equilibrium (SE) of proteins, including membrane proteins and glycoproteins. This is the method of choice for molar mass determinations and the study of self‐association and heterogeneous interactions, such as protein–protein, protein–nucleic acid, and protein–small molecule binding.",2002.0,"J. Lebowitz, M. Lewis, P. Schuck"
f650deee6a7f2b5b5c1643ddb9e56f2e2dd8294d,https://www.semanticscholar.org/paper/f650deee6a7f2b5b5c1643ddb9e56f2e2dd8294d,The contribution of data mining to information science,"The information explosion is a serious challenge for current information institutions. On the other hand, data mining, which is the search for valuable information in large volumes of data, is one of the solutions to face this challenge. In the past several years, data mining has made a significant contribution to the field of information science. This paper examines the impact of data mining by reviewing existing applications, including personalized environments, electronic commerce, and search engines. For these three types of application, how data mining can enhance their functions is discussed. The reader of this paper is expected to get an overview of the state of the art research associated with these applications. Furthermore, we identify the limitations of current work and raise several directions for future research.",2004.0,"Sherry Y. Chen, Xiaohui Liu"
5d1b6b28cd0dafcf73d80419eb9ffdf54e546bf4,https://www.semanticscholar.org/paper/5d1b6b28cd0dafcf73d80419eb9ffdf54e546bf4,Statistics and the Evaluation of Evidence for Forensic Scientists,Preface to the first edition. Preface to the second edition. Uncertainty in forensic science. Variation. The evaluation of evidence. Historical review. Bayesian inference. Sampling. Interpretation. Transfer evidence. Discrete data. Continuous data. Multivariate analysis. Fibres. DNA profiling. Bayesian networks. References. Notation. Cases.,2004.0,"C. Aitken, F. Taroni"
890d7f077bb5dce137aa41e784c4aea22e899027,https://www.semanticscholar.org/paper/890d7f077bb5dce137aa41e784c4aea22e899027,When Do Scientists Become Entrepreneurs? The Social Structural Antecedents of Commercial Activity in the Academic Life Sciences1,"The authors examine the conditions prompting university‐employed life scientists to become entrepreneurs, defined to occur when a scientist (1) founds a biotechnology company, or (2) joins the scientific advisory board of a new biotechnology firm. This study draws on theories of social influence, socialization, and status dynamics to examine how proximity to colleagues in commercial science influences individuals’ propensity to transition to entrepreneurship. To expose the mechanisms at work, this study also assesses how proximity effects change over time as for‐profit science diffuses through the academy. Using adjusted proportional hazards models to analyze case‐cohort data, the authors find evidence that the orientation toward commercial science of individuals’ colleagues and coauthors, as well as a number of other workplace attributes, significantly influences scientists’ hazards of transitioning to for‐profit science.",2006.0,"Toby E. Stuart, Waverly W. Ding"
aa449c6b6614387fbf783f0d490c466ab2044109,https://www.semanticscholar.org/paper/aa449c6b6614387fbf783f0d490c466ab2044109,Data-Driven Modeling & Scientific Computation: Methods for Complex Systems & Big Data,"The burgeoning field of data analysis is expanding at an incredible pace due to the proliferation of data collection in almost every area of science. The enormous data sets now routinely encountered in the sciences provide an incentive to develop mathematical techniques and computational algorithms that help synthesize, interpret and give meaning to the data in the context of its scientific setting. A specific aim of this book is to integrate standard scientific computing methods with data analysis. By doing so, it brings together, in a self-consistent fashion, the key ideas from:DT statistics,DT time-frequency analysis, and DT low-dimensional reductions The blend of these ideas provides meaningful insight into the data sets one is faced with in every scientific subject today, including those generated from complex dynamical systems. This is a particularly exciting field and much of the final part of the book is driven by intuitive examples from it, showing how the three areas can be used in combination to give critical insight into the fundamental workings of various problems.Data-Driven Modeling and Scientific Computation is a survey of practical numerical solution techniques for ordinary and partial differential equations as well as algorithms for data manipulation and analysis. Emphasis is on the implementation of numerical schemes to practical problems in the engineering, biological and physical sciences. An accessible introductory-to-advanced text, this book fully integrates MATLAB and its versatile and high-level programming functionality, while bringing together computational and data skills for both undergraduate and graduate students in scientific computing.",2013.0,N. Kutz
ba552db747386562f0f054e9ade71f2fbb7db947,https://www.semanticscholar.org/paper/ba552db747386562f0f054e9ade71f2fbb7db947,The resolution of multipeak data in fibre science,"A computer program based on a procedure discussed by Powell has been developed for the resolution of overlapping peaks in data output from x-ray diffraction, ion-exchange chromatography with spectroscopic detection, and infrared spectroscopy. A polynomial background is also fitted to the data so that recourse to arbitrary graphical methods for separating peaks and background is no longer necessary.",1971.0,"A. M. Hindeleh, D. J. Johnson"
5d5829723fb240543ff15ffeda1f63fff47f628d,https://www.semanticscholar.org/paper/5d5829723fb240543ff15ffeda1f63fff47f628d,"Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning","In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.",2022.0,Maad M. Mijwil
3c123bf62763d45c2c55fc06365dc2142103a729,https://www.semanticscholar.org/paper/3c123bf62763d45c2c55fc06365dc2142103a729,Inductive Risk and Values in Science,"Although epistemic values have become widely accepted as part of scientific reasoning, non-epistemic values have been largely relegated to the ""external"" parts of science (the selection of hypotheses, restrictions on methodologies, and the use of scientific technologies). I argue that because of inductive risk, or the risk of error, non-epistemic values are required in science wherever non-epistemic consequences of error should be considered. I use examples from dioxin studies to illustrate how non-epistemic consequences of error can and should be considered in the internal stages of science: choice of methodology, characterization of data, and interpretation of results.",2000.0,Heather Douglas
52c9e15facda9999e54b8876d8ad5f2edd885d9f,https://www.semanticscholar.org/paper/52c9e15facda9999e54b8876d8ad5f2edd885d9f,A Bayesian Truth Serum for Subjective Data,"Subjective judgments, an essential information source for science and policy, are problematic because there are no public criteria for assessing judgmental truthfulness. I present a scoring method for eliciting truthful subjective data in situations where objective truth is unknowable. The method assigns high scores not to the most common answers but to the answers that are more common than collectively predicted, with predictions drawn from the same population. This simple adjustment in the scoring criterion removes all bias in favor of consensus: Truthful answers maximize expected score even for respondents who believe that their answer represents a minority view.",2004.0,D. Prelec
94b295e711ec744583597432c19749a5cd61038e,https://www.semanticscholar.org/paper/94b295e711ec744583597432c19749a5cd61038e,"Multicategory Support Vector Machines , Theory , and Application to the Classification of Microarray Data and Satellite Radiance Data","2002 Ph.D. in Statistics, University of Wisconsin-Madison Dissertation: Multicategory Support Vector Machines, Theory, and Application to the Classification of Microarray Data and Satellite Radiance Data Advisor: Grace Wahba, Ph.D. (Co-advisor: Yi Lin, Ph.D.) 1996 M.Sc. in Statistics, Seoul National University, Korea Thesis: Image Data Analysis by Markov Random Field Models Advisor: Jong Woo Jeon, Ph.D. 1994 B.Sc. in Computer Science and Statistics, Seoul National University, Korea",2004.0,"Yoonkyung Lee, Cheol-Koo Lee"
bb8d94b70e3db414406265406491d30e92cf9f70,https://www.semanticscholar.org/paper/bb8d94b70e3db414406265406491d30e92cf9f70,Citation data as science indicators,"Attempts to appraise the condition of science as intellectual activity or social institution have involved data compilation—reports on amounts of money spent on scientific research, magnitudes of scientific manpower, number ot students enrolled as science majors at universities, and number of scientific papers produced or number of patents issued. For avariety of reasons, these all fail to indicate the “condition” of science. Perhaps the problem is that such data are compiled and presented without regard toa specific set of questions or set of hypotheses; thus a coherent framework for estimating the social or intellectual condition of science is missing. Science Indicators 1972 (.S/-72) is an improvement over mere data compilation, and it should be applauded as a step, however preliminary and tentative, in the right direction. The purpose of this paper is to suggest further indicators relevant for measuring scientific activity, in the hope that this will lead to a better estimate of the condition of science. At the Institute for Scientific Information (1S1), we operate on the fundamental as-",1978.0,"E. Garfield, Morton V. Malin, H. Small"
7e0960d5a68ed9be5ad1ada6dab29aed17637aa1,https://www.semanticscholar.org/paper/7e0960d5a68ed9be5ad1ada6dab29aed17637aa1,Stereotypic images of the scientist: The draw‐a‐scientist test,"The present work intends to present the analysis of representations referring science and scientist of children who are part of a Junior Scientific Initiation project. The instruments used for the data collection were questionnaires and drawings, in which the analysis of this study were carried out from the Content Analysis the Laurence Bardin. With the results it was observed that the three students involved in the research have some stereotyped representations both in relation to the knowledge identified as science and in relation to the profile of scientist. It is expected, with the study, it is hoped that teachers will reflect on the information and activities they propose to students in science classes, being of great importance the previous knowledge of the students on these representations. Besides that, should teachers consider the fundamental Scientific Initiation Junior so that the students can express and extend their representations in relation to science and scientist, it is necessary that the school context modifies the ways of approaching subjects related to science and to scientists",1983.0,D. W. Chambers
ea11efe27e029e391ea52609468353f98d9f946b,https://www.semanticscholar.org/paper/ea11efe27e029e391ea52609468353f98d9f946b,Machine learning on Big Data,"Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.",2013.0,"Tyson Condie, Paul Mineiro, N. Polyzotis, Markus Weimer"
89227d150b531658bbcd23a5b02c8d147b96ed10,https://www.semanticscholar.org/paper/89227d150b531658bbcd23a5b02c8d147b96ed10,"Geometric representation of high dimension, low sample size data","Summary.  High dimension, low sample size data are emerging in various areas of science. We find a common structure underlying many such data sets by using a non‐standard type of asymptotics: the dimension tends to ∞ while the sample size is fixed. Our analysis shows a tendency for the data to lie deterministically at the vertices of a regular simplex. Essentially all the randomness in the data appears only as a random rotation of this simplex. This geometric representation is used to obtain several new statistical insights.",2005.0,"Hall Peter Gavin, J. Marron, A. Neeman"
d6d953cc22c8f9c33bdda11301cf4c6c9c0c4a41,https://www.semanticscholar.org/paper/d6d953cc22c8f9c33bdda11301cf4c6c9c0c4a41,An online repository of Swift/XRT light curves of Γ-ray bursts,"Context. Swift data are revolutionising our understanding of Gamma Ray Bursts. Since bursts fade rapidly, it is desirable to create and disseminate accurate light curves rapidly. Aims. To provide the community with an online repository of X-ray light curves obtained with Swift . The light curves should be of the quality expected of published data, but automatically created and updated so as to be self-consistent and rapidly available. Methods. We have produced a suite of programs which automatically generates Swift /XRT light curves of GRBs. Effects of the damage to the CCD, automatic readout-mode switching and pile-up are appropriately handled, and the data are binned with variable bin durations, as necessary for a fading source. Results. The light curve repository website (http://www.swift.ac.uk/xrt_curves) contains light curves, hardness ratios and deep images for every GRB which Swift 's XRT has observed. When new GRBs are detected, light curves are created and updated within minutes of the data arriving at the UK Swift Science Data Centre.",2007.0,"P. Evans, A. Beardmore, K. Page, L. Tyler, J. Osborne, M. Goad, P. O’Brien, L. Vetere, J. Racusin, D. Morris, D. Burrows, M. Capalbi, M. Perri, N. Gehrels, P. U. O. Leicester, P. S. University, A. D. Center, N. J. S. Center, I. Brera, U. Milano"
655b232d4dc0b73f36944d42b40922b8220f5c5b,https://www.semanticscholar.org/paper/655b232d4dc0b73f36944d42b40922b8220f5c5b,Why Linked Data is Not Enough for Scientists,"Scientific data stands to represent a significant portion of the linked open data cloud and science itself stands to benefit from the data fusion capability that this will afford. However, simply publishing linked data into the cloud does not necessarily meet the requirements of reuse. Publishing has requirements of provenance, quality, credit, attribution, methods in order to provide the \emph{reproducibility} that allows validation of results. In this paper we make the case for a scientific data publication model on top of linked data and introduce the notion of \emph{Research Objects} as first class citizens for sharing and publishing.",2010.0,"S. Bechhofer, I. Buchan, D. D. Roure, P. Missier, J. Ainsworth, Jiten Bhagat, P. Couch, Don Cruickshank, M. Delderfield, I. Dunlop, Matthew Gamble, D. Michaelides, S. Owen, David R. Newman, Shoaib Sufi, C. Goble"
bf5a13c1a5096e86eaa316b97ff43df0078a2042,https://www.semanticscholar.org/paper/bf5a13c1a5096e86eaa316b97ff43df0078a2042,Probability Theory: The Logic of Science,"This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. Now results are discussed, along with the application of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book is not restricted to one particular discipline but rather will be of interest to scientists working in any area where inference from incomplete information is necessary.",2004.0,R. Baierlein
24a25e012c15706947294e93ec4a4b82475dd8a4,https://www.semanticscholar.org/paper/24a25e012c15706947294e93ec4a4b82475dd8a4,Buckets of Resistance: Standards and the Effectiveness of Citizen Science,"In light of arguments that citizen science has the potential to make environmental knowledge and policy more robust and democratic, this article inquires into the factors that shape the ability of citizen science to actually influence scientists and decision makers. Using the case of community-based air toxics monitoring with ‘‘buckets,’’ it argues that citizen science’s effectiveness is significantly influenced by standards and standardized practices. It demonstrates that, on one hand, standards serve a boundary-bridging function that affords bucket monitoring data a crucial measure of legitimacy among experts. On the other hand, standards simultaneously serve a boundary-policing function, allowing experts to dismiss bucket data as irrelevant to the central project of air quality assessment. The article thus calls attention to standard setting as an important site of intervention for citizen science-based efforts to democratize science and policy.",2010.0,G. Ottinger
f9edcd99423749b9936d3a749d47db52005cde8d,https://www.semanticscholar.org/paper/f9edcd99423749b9936d3a749d47db52005cde8d,What Kind of a Girl Does Science? The Construction of School Science Identities,"A view of science as a culturally-mediated way of thinking and knowing suggests that learning can be defined as engagement with scientific practices. How students engage in school science is influenced by whether and how students view themselves and whether or not they are the kind of person who engages in science. It is therefore crucial to understand students' identities and how they do or do not overlap with school science identities. In this paper, we describe four middle school African American girls' engagement with science. They were selected in the 7th grade because they expressed a fondness for science in school or because they had science-related hobbies outside of school. The data were collected from the following sources: interviews of students, their parents and their teachers; observations in science classes; journal writing; and focus groups. These girls' stories provide us with a better understanding of the variety of ways girls choose to engage in science and how this engagement is shaped by their views of what kind of girl they are. © 2000 John Wiley & Sons, Inc. J Res Sci Teach 37: 441–458, 2000.",2000.0,"N. Brickhouse, Patricia A. Lowery, Katherine Schultz"
654b16fc452219c1b681d75ed045cf905855bd5e,https://www.semanticscholar.org/paper/654b16fc452219c1b681d75ed045cf905855bd5e,Hyperlinks as a data source for science mapping,"Hyperlinks between academic web sites, like citations, can potentially be used to map disciplinary structures and identify evidence of connections between disciplines. In this paper we classified a sample of links originating in three different disciplines: maths, physics and sociology. Links within a discipline were found to be different in character to links between pages in different disciplines. There were also disciplinary differences in both types of link. As a consequence, we argue that interpretations of web science maps covering multiple disciplines will need to be sensitive to the contexts of the links mapped.",2004.0,"G. Harries, David Wilkinson, Liz Price, Ruth Fairclough, M. Thelwall"
bc5337b0a9c8e82ad3490409adf423a56863dc66,https://www.semanticscholar.org/paper/bc5337b0a9c8e82ad3490409adf423a56863dc66,The Protein Data Bank: a historical perspective.,"The Protein Data Bank began as a grassroots effort in 1971. It has grown from a small archive containing a dozen structures to a major international resource for structural biology containing more than 40000 entries. The interplay of science, technology and attitudes about data sharing have all played a role in the growth of this resource.",2008.0,H. Berman
689178ea158205d3bd47549031e4a277926d14ac,https://www.semanticscholar.org/paper/689178ea158205d3bd47549031e4a277926d14ac,The application of Principal Component Analysis to materials science data,"The relationship between apparently disparate sets of data is a critical component of interpreting materials' behavior, especially in terms of assessing the impact of the microscopic characteristics of materials on their macroscopic or engineering behavior. In this paper we demonstrate the value of principal component analysis of property data associated with high temperature superconductivity to examine the statistical impact of the materials' intrinsic characteristics on high temperature superconducting behavior",2002.0,"C. Suh, A. Rajagopalan, Xiang Li, K. Rajan"
62cd3123ddd65dff935c6c03d90788d71bec90a9,https://www.semanticscholar.org/paper/62cd3123ddd65dff935c6c03d90788d71bec90a9,Predictive Data Mining: A Practical Guide,1 What is Data Mining? 2 Statistical Evaluation for Big Data 3 Preparing the Data 4 Data Reduction 5 Looking for Solutions 6 What's Best for Data Reduction and Mining? 7 Art or Science? Case Studies in Data Mining,1997.0,"S. Weiss, N. Indurkhya"
5cd126a9a657004562d7a7326e39bde9ddeab1f8,https://www.semanticscholar.org/paper/5cd126a9a657004562d7a7326e39bde9ddeab1f8,OVERVIEW OF THE KEPLER SCIENCE PROCESSING PIPELINE,"The Kepler Mission Science Operations Center (SOC) performs several critical functions including managing the ∼156,000 target stars, associated target tables, science data compression tables and parameters, as well as processing the raw photometric data downlinked from the spacecraft each month. The raw data are first calibrated at the pixel level to correct for bias, smear induced by a shutterless readout, and other detector and electronic effects. A background sky flux is estimated from ∼4500 pixels on each of the 84 CCD readout channels, and simple aperture photometry is performed on an optimal aperture for each star. Ancillary engineering data and diagnostic information extracted from the science data are used to remove systematic errors in the flux time series that are correlated with these data prior to searching for signatures of transiting planets with a wavelet-based, adaptive matched filter. Stars with signatures exceeding 7.1σ are subjected to a suite of statistical tests including an examination of each star's centroid motion to reject false positives caused by background eclipsing binaries. Physical parameters for each planetary candidate are fitted to the transit signature, and signatures of additional transiting planets are sought in the residual light curve. The pipeline is operational, finding planetary signatures and providing robust eliminations of false positives.",2010.0,"J. Jenkins, D. Caldwell, H. Chandrasekaran, J. Twicken, S. Bryson, E. Quintana, B. Clarke, J. Li, C. Allen, P. Tenenbaum, Hayley Wu, T. Klaus, C. Middour, M. Cote, S. McCauliff, F. Girouard, Jay P. Gunter, B. Wohler, J. Sommers, J. Hall, K. Uddin, Michael S. Wu, Paresh A. Bhavsar, J. Cleve, D. Pletcher, J. Dotson, M. Haas, R. Gilliland, D. Koch, W. Borucki"
4bd0c123f3d77a10cec974533263d5e0da20ab4a,https://www.semanticscholar.org/paper/4bd0c123f3d77a10cec974533263d5e0da20ab4a,A Stratosphere-Troposphere Data Assimilation System,"Abstract A data assimilation system has been developed at the UK Meteorological Office to analyze the mix of observations available in the troposphere and stratosphere. The data assimilation system is based on the analysis correction scheme used at the UK Meteorological Office for operational weather forecasting. The assimilation system is currently being used to supply near real-time analyses of meteorological fields from the troposphere and stratosphere to the Upper Atmosphere Research Satellite (UARS) Science Team. At this stage, these analyses are based on a similar set of observations to the operational analyses, so they provide an independent check of the UARS observations. In the stratosphere they are largely based on soundings from the National Oceanic and Atmospheric Administration polar orbiters. Some results from the assimilation system are presented for periods in January and August 1992. They are compared with equivalent products from the National Meteorological Center. A particular study is ...",1994.0,"R. Swinbank, A. O'Neill"
1fd799775cc90b4bf04424eb4d7352c10cd1ffb0,https://www.semanticscholar.org/paper/1fd799775cc90b4bf04424eb4d7352c10cd1ffb0,Database Paper - The IRI Marketing Data Set,"This paper describes a new data set available to academic researchers at the following website: http://mktsci.pubs.informs.org . These data are comprised of store sales and consumer panel data for 30 product categories. The store sales data contain 5 years of product sales, pricing, and promotion data for all items sold in 47 U.S. markets. In two U.S. markets, the store level data are supplemented with panel-level purchase data and cover the entire population of stores. Further information is available regarding store characteristics in these markets. We address several potential applications of these data, as well as the access protocol. 
 
The data set described in this paper is maintained by IRI. Any fees charged by IRI for the distribution of the data set will be used for the continual maintenance and updating of the data. Scholarships to cover IRI's fees for those who need it are available through the INFORMS Society for Marketing Science ISMS. Please see the website above for further details.",2008.0,"Bart J. Bronnenberg, Michael W. Kruger, C. Mela"
56125ef1ea46916f3155d0fbe3968bca344fd818,https://www.semanticscholar.org/paper/56125ef1ea46916f3155d0fbe3968bca344fd818,Strategies for analyzing ecological momentary assessment data.,"Studies incorporating repeated observations of momentary phenomena are becoming more common in behavioral and medical science. Analysis of such data requires the use of statistical techniques that are unfamiliar to many investigators. Some common ways of analyzing momentary data are reviewed--aggregation strategies, repeated measures analysis of variance, pooled within-person regression, and two-stage estimation procedures for multilevel models--and are found to be usually suboptimal, possibly leading to incorrect inferences. A broad class of statistical models for multilevel data that can address many research questions typically asked of momentary data are then described. Analytic issues that merit careful consideration include the scaling of momentary variables, allowance for serial autocorrelation of residuals, and the treatment of coefficients that vary across individuals as fixed versus random effects.",1998.0,"J. Schwartz, A. Stone"
991b6e7ada8f6538cde1debb3b890c614b9bdbc5,https://www.semanticscholar.org/paper/991b6e7ada8f6538cde1debb3b890c614b9bdbc5,"Big Data, data integrity, and the fracturing of the control zone","Despite all the attention to Big Data and the claims that it represents a “paradigm shift” in science, we lack understanding about what are the qualities of Big Data that may contribute to this revolutionary impact. In this paper, we look beyond the quantitative aspects of Big Data (i.e. lots of data) and examine it from a sociotechnical perspective. We argue that a key factor that distinguishes “Big Data” from “lots of data” lies in changes to the traditional, well-established “control zones” that facilitated clear provenance of scientific data, thereby ensuring data integrity and providing the foundation for credible science. The breakdown of these control zones is a consequence of the manner in which our network technology and culture enable and encourage open, anonymous sharing of information, participation regardless of expertise, and collaboration across geographic, disciplinary, and institutional barriers. We are left with the conundrum—how to reap the benefits of Big Data while re-creating a trust fabric and an accountable chain of responsibility that make credible science possible.",2014.0,C. Lagoze
dc994a4ca68c4ce94bb1c5e601bc51054f4556f1,https://www.semanticscholar.org/paper/dc994a4ca68c4ce94bb1c5e601bc51054f4556f1,Measuring Resemblance in Sequence Data: An Optimal Matching Analysis of Musicians' Careers,"This article introduces a method that measures resemblance between sequences using a simple metric based on the insertions, deletions, and substitutions required to transform one sequence into another. The method, called optimal matching, is widely used in natural science. The article reviews the literature on sequence analysis, then discusses the optimal matching algorithm in some detail. Applying this technique to a data set detailing careers of musicians active in Germany in the 18th century demonstrates the practical steps involved in the application of the technique and develops a set of typical careers that successfully categorize most of the actual careers studied by the authors.",1990.0,"A. Abbott, Alexandra Hrycak"
938d8e55a4262a611e8b5979ae92e1f4b01074b4,https://www.semanticscholar.org/paper/938d8e55a4262a611e8b5979ae92e1f4b01074b4,Road Traffic Data: Collection Methods and Applications,The mission of the IPTS is to provide customer-driven support to the EU policy-making process by researching science-based responses to policy challenges that have both a socioeconomic and a scientific or technological dimension. Legal Notice Neither the European Commission nor any person acting on behalf of the Commission is responsible for the use which might be made of this publication. (*) Certain mobile telephone operators do not allow access to 00 800 numbers or these calls may be billed.,2008.0,G. Leduc
b0c5efdf2f90322784283290a052797eb073b554,https://www.semanticscholar.org/paper/b0c5efdf2f90322784283290a052797eb073b554,The World Ocean Database,"The World Ocean Database (WOD) is the most comprehensive global ocean profile-plankton database available internationally without restriction. All data are in one well-documented format and are available both on DVDs for a minimal charge and on-line without charge. The latest DVD version of the WOD is the World Ocean Database 2009 (WOD09). All data in the WOD are associated with as much metadata as possible, and every ocean data value has a quality control flag associated with it. The WOD is a product of the U.S. National Oceanographic Data Center and its co-located World Data Center for Oceanography. However, the WOD exists because of the international oceanographic data exchange that has occurred under the auspices of the Intergovernmental Oceanographic Commission (IOC) and the International Council of Science (ICSU) World Data Center (WDC) system. World Data Centers are part of the ICSU World Data System.",2013.0,"S. Levitus, J. Antonov, O. Baranova, T. Boyer, C. L. Coleman, H. E. Garcia, A. Grodsky, D. R. Johnson, R. Locarnini, A. Mishonov, J. Reagan, C. L. Sazama, D. Seidov, I. Smolyar, E. Yarosh, M. Zweng"
3646a27c7271bc02164cbc6512607e5e9f20b223,https://www.semanticscholar.org/paper/3646a27c7271bc02164cbc6512607e5e9f20b223,Data quality and quality control of a population-based cancer registry. Experience in Finland.,"Cancer registries should pay great attention to the quality of their data, both in terms of completeness (all cancer patients in the population are registered) and accuracy (data on individual cancer patients must be correct). In addition to technical measures in the data processing, different types of checks and comparisons should be routine practice. Active research policy and ambitious, research-oriented staff with competence in medicine, biostatistics and computer science are essential in terms of maintaining good data quality.",1994.0,"L. Teppo, E. Pukkala, M. Lehtonen"
e346b6781fcd1ce694befddd4fd063da87db2d06,https://www.semanticscholar.org/paper/e346b6781fcd1ce694befddd4fd063da87db2d06,Statistics for Sensory and Consumer Science,"Preface. Acknowledgements. 1 Introduction. 1.1 The Distinction between Trained Sensory Panels and Consumer Panels. 1.2 The Need for Statistics in Experimental Planning and Analysis. 1.3 Scales and Data Types. 1.4 Organisation of the Book. 2 Important Data Collection Techniques for Sensory and Consumer Studies. 2.1 Sensory Panel Methodologies. 2.2 Consumer Tests. PART I PROBLEM DRIVEN. 3 Quality Control of Sensory Profile Data. 3.1 General Introduction. 3.2 Visual Inspection of Raw Data. 3.3 Mixed Model ANOVA for Assessing the Importance of the Sensory Attributes. 3.4 Overall Assessment of Assessor Differences Using All Variables Simultaneously. 3.5 Methods for Detecting Differences in Use of the Scale. 3.6 Comparing the Assessors Ability to Detect Differences between the Products. 3.7 Relations between Individual Assessor Ratings and the Panel Average. 3.8 Individual Line Plots for Detailed Inspection of Assessors. 3.9 Miscellaneous Methods.- 4 Correction Methods and Other Remedies for Improving Sensory Profile Data. 4.1 Introduction. 4.2 Correcting for Different Use of the Scale. 4.3 Computing Improved Panel Averages. 4.4 Pre-processing of Data for Three-Way Analysis. 5 Detecting and Studying Sensory Differences and Similarities between Products. 5.1 Introduction. 5.2 Analysing Sensory Profile Data: Univariate Case. 5.3 Analysing Sensory Profile Data: Multivariate Case. 6 Relating Sensory Data to Other Measurements. 6.1 Introduction. 6.2 Estimating Relations between Consensus Profiles and External Data. 6.3 Estimating Relations between Individual Sensory Profiles and External Data. 7 Discrimination and Similarity Testing. 7.1 Introduction. 7.2 Analysis of Data from Basic Sensory Discrimination Tests. 7.3 Examples of Basic Discrimination Testing. 7.4 Power Calculations in Discrimination Tests. 7.5 Thurstonian Modelling: What Is It Really? 7.6 Similarity versus Difference Testing. 7.7 Replications: What to Do? 7.8 Designed Experiments, Extended Analysis and Other Test Protocols. 8 Investigating Important Factors Influencing Food Acceptance and Choice. 8.1 Introduction. 8.2 Preliminary Analysis of Consumer Data Sets (Raw Data Overview). 8.3 Experimental Designs for Rating Based Consumer Studies. 8.4 Analysis of Categorical Effect Variables. 8.5 Incorporating Additional Information about Consumers. 8.6 Modelling of Factors as Continuous Variables. 8.7 Reliability/Validity Testing for Rating Based Methods. 8.8 Rank Based Methodology. 8.9 Choice Based Conjoint Analysis. 8.10 Market Share Simulation. 9 Preference Mapping for Understanding Relations between Sensory Product Attributes and Consumer Acceptance. 9.1 Introduction. 9.2 External and Internal Preference Mapping. 9.3 Examples of Linear Preference Mapping. 9.4 Ideal Point Preference Mapping. 9.5 Selecting Samples for Preference Mapping. 9.6 Incorporating Additional Consumer Attributes. 9.7 Combining Preference Mapping with Additional Information about the Samples. 10 Segmentation of Consumer Data. 10.1 Introduction. 10.2 Segmentation of Rating Data. 10.3 Relating Segments to Consumer Attributes. PART II METHOD ORIENTED. 11 Basic Statistics. 11.1 Basic Concepts and Principles. 11.2 Histogram, Frequency and Probability. 11.3 Some Basic Properties of a Distribution (Mean, Variance and Standard Deviation). 11.4 Hypothesis Testing and Confidence Intervals for the Mean . 11.5 Statistical Process Control. 11.6 Relationships between Two or More Variables. 11.7 Simple Linear Regression. 11.8 Binomial Distribution and Tests. 11.9 Contingency Tables and Homogeneity Testing. 12 Design of Experiments for Sensory and Consumer Data. 12.1 Introduction. 12.2 Important Concepts and Distinctions. 12.3 Full Factorial Designs. 12.4 Fractional Factorial Designs: Screening Designs. 12.5 Randomised Blocks and Incomplete Block Designs. 12.6 Split-Plot and Nested Designs. 12.7 Power of Experiments. 13 ANOVA for Sensory and Consumer Data. 13.1 Introduction. 13.2 One-Way ANOVA. 13.3 Single Replicate Two-Way ANOVA. 13.4 Two-Way ANOVA with Randomised Replications. 13.5 Multi-Way ANOVA. 13.6 ANOVA for Fractional Factorial Designs. 13.7 Fixed and Random Effects in ANOVA: Mixed Models. 13.8 Nested and Split-Plot Models. 13.9 Post Hoc Testing. 14 Principal Component Analysis. 14.1 Interpretation of Complex Data Sets by PCA. 14.2 Data Structures for the PCA. 14.3 PCA: Description of the Method. 14.4 Projections and Linear Combinations. 14.5 The Scores and Loadings Plots. 14.6 Correlation Loadings Plot. 14.7 Standardisation. 14.8 Calculations and Missing Values. 14.9 Validation. 14.10 Outlier Diagnostics. 14.11 Tucker-1. 14.12 The Relation between PCA and Factor Analysis (FA). 15 Multiple Regression, Principal Components Regression and Partial Least Squares Regression. 15.1 Introduction. 15.2 Multivariate Linear Regression. 15.3 The Relation between ANOVA and Regression Analysis. 15.4 Linear Regression Used for Estimating Polynomial Models. 15.5 Combining Continuous and Categorical Variables. 15.6 Variable Selection for Multiple Linear Regression. 15.7 Principal Components Regression (PCR). 15.8 Partial Least Squares (PLS) Regression. 15.9 Model Validation: Prediction Performance. 15.10 Model Diagnostics and Outlier Detection. 15.11 Discriminant Analysis. 15.12 Generalised Linear Models, Logistic Regression and Multinomial Regression. 16 Cluster Analysis: Unsupervised Classification. 16.1 Introduction. 16.2 Hierarchical Clustering. 16.3 Partitioning Methods. 16.4 Cluster Analysis for Matrices. 17 Miscellaneous Methodologies. 17.1 Three-Way Analysis of Sensory Data. 17.2 Relating Three-Way Data to Two-Way Data. 17.3 Path Modelling. 17.4 MDS-Multidimensional Scaling. 17.5 Analysing Rank Data. 17.6 The L-PLS Method. 17.7 Missing Value Estimation. Nomenclature, Symbols and Abbreviations. Index.",2010.0,"T. Næs, P. Brockhoff, O. Tomic"
34d0f17694eb50bd8365e13dde6d93cd82dd9881,https://www.semanticscholar.org/paper/34d0f17694eb50bd8365e13dde6d93cd82dd9881,An overview of MODIS capabilities for ocean science observations,"The Moderate Resolution Imaging Spectroradiometer (MODIS) will add a significant new capability for investigating the 70% of the Earth's surface that is covered by oceans, in addition to contributing to the continuation of a decadal scale time series necessary for climate change assessment in the oceans. Sensor capabilities of particular importance for improving the accuracy of ocean products include high SNR and high stability for narrow or spectral bands, improved onboard radiometric calibration and stability monitoring, and improved science data product algorithms. Spectral bands for resolving solar-stimulated chlorophyll fluorescence and a split window in the 4-/spl mu/m region for SST will result in important new global ocean science products for biology and physics. MODIS will return full global data at 1-km resolution. The complete suite of Levels 2 and 3 ocean products is reviewed, and many areas where MODIS data are expected to make significant, new contributions to the enhanced understanding of the oceans' role in understanding climate change are discussed. In providing a highly complementary and consistent set of observations of terrestrial, atmospheric, and ocean observations, MODIS data will provide important new information on the interactions between Earth's major components.",1998.0,"W. Esaias, M. Abbott, I. Barton, O. Brown, Janet W. Campbell, K. Carder, D. Clark, R. Evans, F. Hoge, H. Gordon, W. Balch, R. Letelier, P. Minnett"
db59c6111788c9d4fa40c9de3daf7cf2625e4bc7,https://www.semanticscholar.org/paper/db59c6111788c9d4fa40c9de3daf7cf2625e4bc7,On the economics of information,"On November 11, 1971, the Special Interest Group of the American Society for Information Science on Behavioral and Social Sciences held a panel discussion a t the annual meeting in Denver. The audience found the ideas generated of sufficient importance to request that the highlights be published and disseminated to policy makers and other key people. In this paper, Robin Crickman' has summarized the statements of the five panelists. The primary message from these and other comments made during the panel discussion was the need to probe more deeply into the utility of knowledge for decision-making, to shift priorities toward making data more usable to policy-makers than to collectors and disseminators.",1972.0,M. Kochen
b8691ae58baa4019e8fac7b59e5c129b3cc4aa84,https://www.semanticscholar.org/paper/b8691ae58baa4019e8fac7b59e5c129b3cc4aa84,Towards FAIR principles for research software,"The FAIR Guiding Principles, published in 2016, aim to improve the findability, accessibility,
interoperability and reusability of digital research objects for both humans and machines.
The FAIR principles are also directly relevant to research software. In this position paper
“Towards FAIR principles for research software”, we summarised and developed a basis for
community discussion. At the start, we discussed what makes software different from data
concerning the application of the FAIR principles, and which desired characteristics of
research software go beyond FAIR. Then, we presented an analysis of where the existing
principles can directly apply to software, and where they need to be adapted or
reinterpreted. Our next step after the position paper is to prompt for community-agreed
identifiers for FAIR research software.Acknowledgments
To all the authors of Towards FAIR principles for research software
https://doi.org/10.3233/DS-190026, and the numerous people who contributed to the
discussions around FAIR research software at different occasions preceding the work on this
paper. References
Lamprecht, Anna-Lena, et al. (2019) Towards FAIR principles for research software. Data
Science. https://doi.org/10.3233/DS-190026 ABOUT THE AUTHOR(S) Dr Paula Andrea Martinez is leading the National Training Program for the Characterisation
Community in Australia since 2019. She works for the National Image Facility (NIF). Last year
she worked at ELIXIR Europe coordinating the Bioinformatics and Data Science training
program in Belgium and collaborated with multiple ELIXIR nodes in the development of
Software best practices. Her career, spanning Sweden, Australia and Belgium nurtured her
experience in Bioinformatics and Research Software development for complex and dataintensive science. She started a career in Computer Science, later on, interested in research
methods development and now outreach and advocacy in data and software best practices",2020.0,"Anna-Lena Lamprecht, L. García, Mateusz Kuzak, Carlos Martinez, Ricardo Arcila, Eva Martin Del Pico, Victoria Dominguez Del Angel, Stephanie van de Sandt, J. Ison, P. Martínez, Peter McQuilton, A. Valencia, J. Harrow, Fotis Psomopoulos, J. Gelpi, Neil Philippe Chue Hong, C. Goble, S. Capella-Gutiérrez"
41e2f85dd49bc997396bc27b630f8d10e24b4235,https://www.semanticscholar.org/paper/41e2f85dd49bc997396bc27b630f8d10e24b4235,Addressing Big Data challenges for Scientific Data Infrastructure,"This paper discusses the challenges that are imposed by Big Data Science on the modern and future Scientific Data Infrastructure (SDI). The paper refers to different scientific communities to define requirements on data management, access control and security. The paper introduces the Scientific Data Lifecycle Management (SDLM) model that includes all the major stages and reflects specifics in data management in modern e-Science. The paper proposes the SDI generic architecture model that provides a basis for building interoperable data or project centric SDI using modern technologies and best practices. The paper explains how the proposed models SDLM and SDI can be naturally implemented using modern cloud based infrastructure services provisioning model.",2012.0,"Y. Demchenko, Zhiming Zhao, P. Grosso, A. Wibisono, C. D. Laat"
9b41e7559902658b5a6a85c2d06bdc8dd3d8806e,https://www.semanticscholar.org/paper/9b41e7559902658b5a6a85c2d06bdc8dd3d8806e,The Open Science Grid,"The U.S. LHC Tier-1 and Tier-2 laboratories and universities are developing production Grids to support LHC applications running across a worldwide Grid computing system. Together with partners in computer science, physics grid projects and (active experiments, we will build a common national production grid infrastructure which is open in its architecture, implementation and use. The Open Science Grid (OSG) model builds upon the successful approach of last year's joint Grid2003 project. The Grid3 shared infrastructure has for over eight months provided significant computational resources and throughput to a range of applications, including ATLAS and CMS data challenges, SDSS, LIGO, and biology analyses, and computer science demonstrators and experiments. To move towards LHC-scale data management, access and analysis capabilities, we must increase the scale, services, and sustainability of the current infrastructure by an order of magnitude or more. Thus, we must achieve a significant upgrade in its functionalities and technologies. The initial OSG partners will build upon a fully usable, sustainable and robust grid. Initial partners include the US LHC collaborations, DOE & NSF Laboratories and Universities & Trillium Grid projects. The approach is to federate with other application communities in the U.S. to build a shared infrastructure open to other sciences and capable of being modified and improved to respond to needs of other applications, including CDF, D0, BaBar, and RHIC experiments. We describe the application-driven, engineered services of the OSG, short term plans and status, and the roadmap for a consortium, its partnerships and national focus. The partners in the Open Science Grid Consortium have come together to build a sustainable national production Grid infrastructure in the United States that will be open to scientific collaborations. The Consortium will build on and evolve the existing Grid3 [2] common shared infrastructure together with the distributed computing facilities at the Laboratories and Universities, including the Fermilab SAMGrid [3] system. The Open Science Grid Consortium will act to integrate, deploy, maintain and operate a shared common infrastructure to the benefit of all its users. To meet the long-term needs of the experimental physics community in the US, existing grid infrastructures must evolve by an order of magnitude or more in size, performance, and capabilities. The Consortium plans to evolve the grid infrastructure to meet both these needs and computational needs of other science partners. Our vision is to make a significant step forward in cooperative development and Grid interoperation on a global …",2005.0,Paul Avery
85705ada0f98731a51b76ce8a49441b6f3785046,https://www.semanticscholar.org/paper/85705ada0f98731a51b76ce8a49441b6f3785046,"Data, Relativism and Archaeological Science",Discussion du mode de production des donnees : le modele ethnographique de production des donnees n'est pas adapte a l'archeologie et conduit a des erreurs d'appreciation du passe,1987.0,L. Binford
90faebb8c0e629202ab8a33ba44c78c68c312ba0,https://www.semanticscholar.org/paper/90faebb8c0e629202ab8a33ba44c78c68c312ba0,Student conceptualizations of the nature of science in response to a socioscientific issue,"This study investigates student conceptualizations of the nature of science (NOS) and how students interpret and evaluate conflicting evidence regarding a socioscientific issue. Eighty‐four high school students participated in the study by reading contradictory reports about the status of global warming and responding to questions designed to elicit ideas pertinent to the research goals. A subsample of 30 students was interviewed in order to triangulate data from the written responses. Data were analyzed using a qualitative methodological approach. The participants displayed a range of views on three distinct aspects of NOS: empiricism, tentativeness, and social embeddedness. Findings indicate that interpretation and evaluation of conflicting evidence in a socioscientific context is influenced by a variety of factors related to NOS such as data interpretation and social interactions including individuals' own articulation of personal beliefs and scientific knowledge. Implications for science teaching and learning are discussed.",2004.0,"T. Sadler, F. William Chambers, D. Zeidler"
cab479743ab6c15bc7a6245296c5b9f11ae8acfa,https://www.semanticscholar.org/paper/cab479743ab6c15bc7a6245296c5b9f11ae8acfa,Making Social Science Work Across Space and Time: A Critical Reflection on Robert Putnam's Making Democracy Work,"Political scientists are becoming more self-conscious about how they connect quantitative and qualitative data in social science and about the role of systematic country studies in comparative research. As the most striking example of both practices in recent years, Robert Putnam and his collaborators' Making Democracy Work deserves more serious criticism than it has received. While Putnam's original project aimed at a precise goal—studying how a new administrative reform is institutionalized—his ultimate project aimed at nothing less than examining how differently democracy works in different sociopolitical contexts, operationalized cross-sectionally in southern and northern Italy. The sources of these differences he found in the two regions' histories, which led him to employ the quantitative interregional data he had collected for one purpose to support a model of historical development of North and South. This historical reconstruction rests largely on qualitative data; but it also rests on a set of comparative inferences about individual values and community cohesiveness in the two regions that is of questionable historical validity and innocent of structural grounding. This article applauds Putnam's joining qualitative and quantitative data but attacks his reconstruction of Italian history to fit his model of social capital.",1996.0,S. Tarrow
6cadb82f9cd0daf03c39155fccc0435c55e690b0,https://www.semanticscholar.org/paper/6cadb82f9cd0daf03c39155fccc0435c55e690b0,State of the art of graph-based data mining,The need for mining structured data has increased in the past few years. One of the best studied data structures in computer science and discrete mathematics are graphs. It can therefore be no surprise that graph based data mining has become quite popular in the last few years.This article introduces the theoretical basis of graph based data mining and surveys the state of the art of graph-based data mining. Brief descriptions of some representative approaches are provided as well.,2003.0,"T. Washio, H. Motoda"
c9144c907de1cd5096d1496058782aa7a05ae52e,https://www.semanticscholar.org/paper/c9144c907de1cd5096d1496058782aa7a05ae52e,Suggestions for presenting the results of data analyses,"We give suggestions for the presentation of research results from frequentist, information-theoretic, and Bayesian analysis paradigms, followed by several general suggestions. The information-theoretic and Bayesian methods offer alternative approaches to data analysis and inference compared to traditionally used methods. Guidance is lacking on the presentation of results under these alternative procedures and on nontesting aspects of classical frequentist methods of statistical analysis. Null hypothesis testing has come under intense criticism. We recommend less reporting of the results of statistical tests of null hypotheses in cases where the null is surely false anyway, or where the null hypothesis is of little interest to science or management.",2001.0,"David R. Anderson, W. Link, Douglas H. Johnson, K. Burnham"
df477cf83a7e76f599f5e62b08e6c130bfd80f0c,https://www.semanticscholar.org/paper/df477cf83a7e76f599f5e62b08e6c130bfd80f0c,Advances in Machine Learning and Data Mining for Astronomy,"Advances in Machine Learning and Data Mining for Astronomy documents numerous successful collaborations among computer scientists, statisticians, and astronomers who illustrate the application of state-of-the-art machine learning and data mining techniques in astronomy. Due to the massive amount and complexity of data in most scientific disciplines, the material discussed in this text transcends traditional boundaries between various areas in the sciences and computer science. The books introductory part provides context to issues in the astronomical sciences that are also important to health, social, and physical sciences, particularly probabilistic and statistical aspects of classification and cluster analysis. The next part describes a number of astrophysics case studies that leverage a range of machine learning and data mining technologies. In the last part, developers of algorithms and practitioners of machine learning and data mining show how these tools and techniques are used in astronomical applications. With contributions from leading astronomers and computer scientists, this book is a practical guide to many of the most important developments in machine learning, data mining, and statistics. It explores how these advances can solve current and future problems in astronomy and looks at how they could lead to the creation of entirely new algorithms within the data mining community.",2012.0,"M. Way, J. Scargle, Kamal M. Ali, Ashok N. Srivastava"
057fd4df2816fdc5c19c0c26ca80921324c98123,https://www.semanticscholar.org/paper/057fd4df2816fdc5c19c0c26ca80921324c98123,"Interpretation of Inaccurate, Insufficient and Inconsistent Data","Sumntary Many problems in physical science involve the estimation of a number of unknown parameters which bear a linear or quasi-linear relationship to a set of experimental data. The data may be contaminated by random errors, insufficient to determine the unknowns, redundant, or all of the above. This paper presents a method of optimizing the conclusions from such a data set. The problem is formulated as an ill-posed matrix equation, and general criteria are established for constructing an ‘ inverse ’ matrix. The ‘ solution ’ to the problem is defined in terms of a set of generalized eigenvectors of the matrix, and may be chosen to optimize the resolution provided by the data, the expected error in the solution, the fit to the data, the proximity of the solution to an arbitrary function, or any combination of the above. The classical ‘ least-squares ’ solution is discussed as a special case.",1972.0,D. Jackson
ef6b6d214289d19f98dc38cdb9406001b921edd9,https://www.semanticscholar.org/paper/ef6b6d214289d19f98dc38cdb9406001b921edd9,Qualitative Research for the Social Sciences,"PART I. CONCEPTUAL AND HISTORICAL CONTEXT Chapter 1. Introduction Chapter 2. Qualitative Research-A Reflexive Stance Chapter 3. Ethical Issues in Qualitative Research Chapter 4. Conceptualizing Research Approaches Chapter 5. A Detailed Examination of Common Approaches Chapter 6. A Review of Additional Research Approaches PART II. THE QUALITATIVE RESEARCH PROCESS Chapter 7. Planning and Conceptualizing a Qualitative Research Study Chapter 8. Social Media, the Internet, and Technology Chapter 9. A Review of Research Literature Chapter 10. Interviewing Chapter 11. Additional Methods of Gathering Data PART III. THE FINAL PRODUCT Chapter 12. Drawing Meaning from the Data Chapter 13. Communicating Your Ideas Chapter 14. Judging the Research Process and Product Epilogue: Social Science and the Future of Qualitative Research Glossary",2013.0,M. Lichtman
7f54287fa97b1b0e1b6780e684312c0384fee675,https://www.semanticscholar.org/paper/7f54287fa97b1b0e1b6780e684312c0384fee675,The Quality of Students' Use of Evidence in Written Scientific Explanations,"Drawing on sociological and philosophical studies of science, science educators have begun to view argumentation as a central scientific practice that students should learn. In this article, we extend recent work to understand the structure of students' arguments to include judgments about their quality through content analyses of high school students' written explanations for 2 problems of natural selection. In these analyses, we aim to explicate the relations between students' conceptual understanding of specific domains and their epistemic understanding of scientific practices of argumentation as they try to learn science through inquiry. We present a method that assesses the warrant of explanatory claims, the sufficiency of the evidence explicitly cited for claims, and students' rhetorical use of specific inscriptions in their arguments. Students were attentive to the need to cite data, yet they often failed to cite sufficient evidence for claims. Students' references to specific inscriptions in their arguments often failed to articulate how specific data related to particular claims. We discuss these patterns of data citation in terms of what they suggest about students' epistemological ideas about explanation and consequent implications for inquiry-oriented, science education reforms.",2005.0,"W. Sandoval, Kelli A. Millwood"
c152c6358d89e4a0a64a876d1d54e1a6b9291b4a,https://www.semanticscholar.org/paper/c152c6358d89e4a0a64a876d1d54e1a6b9291b4a,Mixed method data collection strategies,"Social scientists have long relied on a wide range of tools to collect information about the social world, but as individual fields have become more specialised, researchers are trained to use a narrow range of the possible data collection methods. This book, first published in 2006, draws on a broad range of available social data collection methods to formulate a set of data collection approaches. The approaches described here are ideal for social science researchers who plan to collect new data about people, organisations, or social processes. Axinn and Pearce present methods designed to create a comprehensive empirical description of the subject being studied, with an emphasis on accumulating the information needed to understand what causes what with a minimum of error. In addition to providing methodological motivation and underlying principles, the book is filled with detailed instructions and concrete examples for those who wish to apply the methods to their research.",2006.0,"W. Axinn, Lisa D. Pearce"
469c93bd8a79c6a400405437cbb97cbfa3a27abe,https://www.semanticscholar.org/paper/469c93bd8a79c6a400405437cbb97cbfa3a27abe,Data Streams: Models and Algorithms (Advances in Database Systems),"This book primarily discusses issues related to the mining aspects of data streams and it is unique in its primary focus on the subject. This volume covers mining aspects of data streams comprehensively: each contributed chapter contains a survey on the topic, the key ideas in the field for that particular topic, and future research directions. The book is intended for a professional audience composed of researchers and practitioners in industry. This book is also appropriate for advanced-level students in computer science.",2006.0,Charu C. Aggarwal
642e65252035e32ea1dd2e3ced3ae7f62b754cd8,https://www.semanticscholar.org/paper/642e65252035e32ea1dd2e3ced3ae7f62b754cd8,Principles of research in behavioral science,"1. THE SCIENCE OF PSYCHOLOGY: THEORY, RESEARCH, AND APPLICATION Science / Theories / Research / Theory, Research, and Application2. RESEARCH STRATEGIES: AN OVERVIEW Purposes of Research / Quantitative and Qualitative Research / Research Strategies / Time Perspectives: Short-Term versus Long-Term / Research Settings: Laboratory versus Field / Research as a Set of Tradeoffs3. THE ETHICAL TREATMENT OF RESEARCH PARTICIPANTS Responsibility for Ethical Research / Ethical Considerations in Planning Research / Ethical Considerations During Data Collection / Ethical Considerations Following Data Collection4. FORMULATING A RESEARCH QUESTION Formulating Research Hypotheses / Replication Research / Designing Research for Utilization / Bias in the Formulation of Research Questions5. DEVELOPING A MEASUREMENT STRATEGY Reliability and Validity / Modalities of Measurement / Evaluating and Selecting Measures 6. THE INTERNAL VALIDITY OF RESEARCH Confounds / Threats to Internal Validity / Reactivity / Demand Characteristics / Experimenter Expectancies7. THE EXPERIMENTAL RESEARCH STRATEGY A Note on Statistics / Manipulating the Independent Variable / Controlling Extraneous Variance / Multiple-Group Designs / Factorial Designs8. THE CORRELATIONAL (PASSIVE) RESEARCH STRATEGY The Nature of Correlational Research / Simple and Partial Correlation Analysis / Multiple Regression Analysis (MRA) / Some Other Correlational Techniques / Testing Mediational Hypotheses / Factor Analysis9. THE SINGLE-CASE RESEARCH STRATEGY The Role of Single-Case Research in Psychology / Validity Criteria in Single-Case Research / Case Study Research / Single-Case Experiments / Data Analysis in Single-Case Research10. RESEARCH IN NATURAL SETTINGS The Problem of Control in Natural Settings / Field Experiments / Natural Experiments and Quasi-Experiments / Naturalistic Observation / Interviews / Archival Data / Coding Open-Ended Data11. SURVEY RESEARCH Asking Questions / Obtaining Answers / Multi-Item Scales / Response Biases / Questionnaire Design / Questionnaire Administration / Survey Data Archives12. DATA COLLECTION Research Participants / Research Procedures / Using the Internet to Collect Data13. INTERPRETING RESEARCH RESULTS Describing Results / Interpreting the Results / Fitting Results into the Big Picture14. THE EXTERNAL VALIDITY OF RESEARCH The Concept of External Validity / The Structural Component of External Validity / The Functional and Conceptual Components of External Validity / Assessing External Validity / Laboratory Research, Natural-Setting Research, and External Validity15. EVALUATION RESEARCH Goal Definition / Program Monitoring / Impact Assessment / Efficiency Analysis / Information Utilization / Measuring Change16. INTEGRATIVE LITERATURE REVIEWING Defining the Research Question / Data Collection / Data Evaluation / Data Analysis / Data Interpretation / A Sample Meta-Analysis17. WRITING RESEARCH REPORTS The Research Report / Journal Articles and Convention Presentations / Ethical Issues in Publication18. THE PROFESSIONAL AND SOCIAL RESPONSIBILITIES OF SCIENTISTS Malpractice in Research / Mistakes and Error in Research / Using the Results of Research / Research and the Common Good",1996.0,B. Whitley
03ef2d4f591667920039d7777e1e53b24505aa12,https://www.semanticscholar.org/paper/03ef2d4f591667920039d7777e1e53b24505aa12,The open science grid,"The Open Science Grid (OSG) provides a distributed facility where the Consortium members provide guaranteed and opportunistic access to shared computing and storage resources. OSG provides support for and evolution of the infrastructure through activities that cover operations, security, software, troubleshooting, addition of new capabilities, and support for existing and engagement with new communities. The OSG SciDAC-2 project provides specific activities to manage and evolve the distributed infrastructure and support it's use. The innovative aspects of the project are the maintenance and performance of a collaborative (shared & common) petascale national facility over tens of autonomous computing sites, for many hundreds of users, transferring terabytes of data a day, executing tens of thousands of jobs a day, and providing robust and usable resources for scientific groups of all types and sizes. More information can be found at the OSG web site: www.opensciencegrid.org.",2007.0,"R. Pordes, D. Petravick, B. Kramer, D. Olson, M. Livny, A. Roy, P. Avery, K. Blackburn, T. Wenaus, F. Würthwein, Ian T Foster, R. Gardner, M. Wilde, Alan R. Blatecky, J. McGee, R. Quick"
9c01c4ec48dcfe0d15629264b1e6adc52d0a5764,https://www.semanticscholar.org/paper/9c01c4ec48dcfe0d15629264b1e6adc52d0a5764,Integrating vessel monitoring systems (VMS) data with daily catch data from logbooks to explore the spatial distribution of catch and effort at high resolution,"This is a pre-copy-editing, author-produced PDF of an article accepted for publication in ICES Journal of Marine Science following peer review. The definitive publisher-authenticated version “Gerritsen H., Lordan C., Integrating vessel monitoring systems (VMS) data with daily catch data from logbooks to explore the spatial distribution of catch and effort at high resolution. (2011) ICES Journal of Marine Science, 68 (1): 245-252” is available online at: http://icesjms.oxfordjournals.org/content/68/1/245",2011.0,"H. Gerritsen, C. Lordan"
7b73e8701e94899a99d25741079d0dff554f3ab2,https://www.semanticscholar.org/paper/7b73e8701e94899a99d25741079d0dff554f3ab2,Service-Oriented Science,"New information architectures enable new approaches to publishing and accessing valuable data and programs. So-called service-oriented architectures define standard interfaces and protocols that allow developers to encapsulate information tools as services that clients can access without knowledge of, or control over, their internal workings. Thus, tools formerly accessible only to the specialist can be made available to all; previously manual data-processing and analysis tasks can be automated by having services access services. Such service-oriented approaches to science are already being applied successfully, in some cases at substantial scales, but much more effort is required before these approaches are applied routinely across many disciplines. Grid technologies can accelerate the development and adoption of service-oriented science by enabling a separation of concerns between discipline-specific content and domain-independent software and hardware infrastructure.",2005.0,Ian T Foster
937287b0c249add46f61718c170542294b226ccf,https://www.semanticscholar.org/paper/937287b0c249add46f61718c170542294b226ccf,Astronomical Data Analysis Software and Systems Xv,"Until 23 Nov 2003, no total solar eclipse (TSE) had ever been observed from the Antarctic. Yet, interest in securing observations of that event, visible only from the Antarctic, was extremely high and provided the impetus for breaking that paradigm of elusivity in the historical record of science and exploration. The execution of a lunar shadow intercept and the conduction of an observing program from a Boeing 747-400 ER aircraft over the Antarctic interior permitted the previously unobtainable to be accomplished. The unique computational and navigational requirements for this flight are discussed from the enabling perspective of control and data acquisition S/W specifically developed for this task.",,"C. Gabriel, C. Arviset, D. Ponz, E. Solano, Eds, Glenn Schneider"
621398d6fc7b7f98191695ae427f58f52abba5c2,https://www.semanticscholar.org/paper/621398d6fc7b7f98191695ae427f58f52abba5c2,OECD Principles and Guidelines for Access to Research Data from Public Funding,"Ministers of science and technology asked the OECD in January 2004 to develop international guidelines on access to research data from public funding. The resulting Principles and Guidelines for Access to Research Data from Public Funding were recently approved by OECD governments and are discussed below. They are intended to promote data access and sharing among researchers, research institutions, and national research agencies. OECD member countries have committed to taking these principles and guidelines into account in developing their own national laws and research policies, taking account of differences in their respective national context.",2007.0,"D. Pilat, Yukiko Fukasaku"
716899800f8c04722603ef3bcee557f401e0ca10,https://www.semanticscholar.org/paper/716899800f8c04722603ef3bcee557f401e0ca10,Statistical Methods for Communication Science,Contents: Preface. Statistics and Communication Science. Fundamentals of Measurement. Sampling. Data Description and Visualization. Fundamentals of Probability. Assessing and Quantifying Reliability. Parameter Estimation. Hypothesis Testing Concepts. Testing a Hypothesis About a Single Mean. Comparing Two Independent Groups. Some Tests for Categorical Variables. Simple Linear Regression. Multiple Linear Regression. Single Factor Analysis of Variance. Analysis of Covariance: ANOVA With Statistical Controls. Interaction. Appendices.,2005.0,A. Hayes
24a2ccdf4629f85f23ea39e39ea96f763ccd34e8,https://www.semanticscholar.org/paper/24a2ccdf4629f85f23ea39e39ea96f763ccd34e8,The WFCAM Science Archive,"We describe the WFCAM Science Archive, which is the primary point of access for users of data from the wide-field infrared camera WFCAM on the United Kingdom Infrared Telescope (UKIRT), especially science catalogue products from the UKIRT Infrared Deep Sky Survey. We describe the database design with emphasis on those aspects of the system that enable users to fully exploit the survey data sets in a variety of different ways. We give details of the database-driven curation applications that take data from the standard nightly pipeline-processed and calibrated files for the production of science-ready survey data sets. We describe the fundamentals of querying relational databases with a set of astronomy usage examples, and illustrate the results.",2006.0,"N. Hambly, R. Collins, N. Cross, R. Mann, M. Read, E. Sutorius, I. Bond, J. Bryant, J. Emerson, A. Lawrence, L. Rimoldini, Jonathan M. Stewart, Peredur M. Williams, A. Adamson, P. Hirst, S. Dye, S. Warren"
12ec8bc371e72fc2613829daf26b25b2c001c30a,https://www.semanticscholar.org/paper/12ec8bc371e72fc2613829daf26b25b2c001c30a,Comparative Welfare States Data Set,"The data contained in this data set were collected by a project entitled ""Comparative Welfare States in the 21 Century"" directed by David Brady, Evelyne Huber, and John D. Stephens. The project was supported in 2011-13 by grants from the National Science Foundation (""Collaborative Research: Comparative Welfare States: A Public Use Archival Dataset,” SES 1059959 and 1061007). Additional support was provided by the Morehead Alumni Distinguished Professorship and the Margaret and Paul A. Johnston Professorships (funding the Gerhard E. Lenski, Jr. Distinguished Professorship) in the College of Arts and Sciences at the University of North Carolina at Chapel Hill. Some further support was provided by Duke University and the WZB Berlin Social Science Center. An earlier version of this dataset was assembled by Evelyne Huber, Charles Ragin, and John Stephens in the 1990s. That project was supported in 1990-92 by a grant from the National Science Foundation (Grant # SES 9108716). Please direct correspondence to David Brady at david.brady@ucr.edu.",2004.0,"E. Huber, Charles C. Ragin, J. Stephens, David J. Brady, Jason Beckfield"
e0ffacf66497c7dfc4e5c58a49f2af77ab3c24cf,https://www.semanticscholar.org/paper/e0ffacf66497c7dfc4e5c58a49f2af77ab3c24cf,An assimilated dataset for Earth science applications,"The Data Assimilation Office at NASA's Goddard Space Flight Center is currently producing a multiyear gridded global atmospheric dataset for use in climate research, including tropospheric chemistry applications. The data, which are being made available to the scientific community, are well suited for climate research since they are produced by a fixed assimilation system designed to minimize the spinup in the hydrological cycle. By using a nonvarying system, the variability due to algorithm change is eliminated and geophysical variability can be more confidently isolated. The analysis incorporates rawinsonde reports, satellite retrievals of geopotential thickness, cloud-motion winds, and aircraft, ship, and rocketsonde reports. At the lower boundary, the assimilating atmospheric general circulation model is constrained by the observed sea surface temperature and soil moisture derived from observed surface air temperature and precipitation fields. The available output data include all prognostic variables...",1993.0,"S. Schubert, R. Rood, J. Pfaendtner"
07ecb2b8de1cd5f61936c60b9fdc52c31d4b3f22,https://www.semanticscholar.org/paper/07ecb2b8de1cd5f61936c60b9fdc52c31d4b3f22,Unraveling the Complexities of Life Sciences Data,"The life sciences have entered into the realm of big data and data-enabled science, where data can either empower or overwhelm. These data bring the challenges of the 5 Vs of big data: volume, veracity, velocity, variety, and value. Both independently and through our involvement with DELSA Global (Data-Enabled Life Sciences Alliance, DELSAglobal.org), the Kolker Lab ( kolkerlab.org ) is creating partnerships that identify data challenges and solve community needs. We specialize in solutions to complex biological data challenges, as exemplified by the community resource of MOPED (Model Organism Protein Expression Database, MOPED.proteinspire.org ) and the analysis pipeline of SPIRE (Systematic Protein Investigative Research Environment, PROTEINSPIRE.org ). Our collaborative work extends into the computationally intensive tasks of analysis and visualization of millions of protein sequences through innovative implementations of sequence alignment algorithms and creation of the Protein Sequence Universe tool (PSU). Pushing into the future together with our collaborators, our lab is pursuing integration of multi-omics data and exploration of biological pathways, as well as assigning function to proteins and porting solutions to the cloud. Big data have come to the life sciences; discovering the knowledge in the data will bring breakthroughs and benefits.",2013.0,"R. Higdon, W. Haynes, L. Stanberry, Elizabeth Stewart, Gregory Yandl, C. Howard, William Broomall, N. Kolker, E. Kolker"
afcae65c578d36201cc5a691ab8a5d83eeae16b5,https://www.semanticscholar.org/paper/afcae65c578d36201cc5a691ab8a5d83eeae16b5,“Paradigms Lost”: On Theory and Method in Research in Marketing,"Only recently have marketing scientists become concerned with issues in the philosophy of science. This paper points to one neglected area—the implications of a theoretical tradition for the selection of research methods (design, data collection, and data analysis). It is argued that marketing has been relying primarily on only one theoretical tradition. The dominance of this philosophy has led to marketing science growing more rapidly in the area of hypothesis testing than in the development of new, rich explanatory theories. Several suggestions are made to achieve a balance in theory construction and testing, with implications for reducing methods bias by a process of triangulating methodologies.",1983.0,Rohit Deshpandé
b79b8703a9807ab19ab993d309c650859c171ba7,https://www.semanticscholar.org/paper/b79b8703a9807ab19ab993d309c650859c171ba7,Administrative Science as Socially Constructed Truth.,"(?) 1985 by Cornell University. 0001 -8392/85/3004-0497/$1 .00. This paper argues that the body of knowledge that constitutes administrative science is a socially constructed product. Because empirical observations are inevitably mediated by theoretical preconceptions, our knowledge of organizations is fundamentally shaped by the subjective world views through which we perceive data. Truth is defined in terms of the theoretical constructs and conceptual vocabulary that guide research and mediate access to organizational phenomena. The chief product of research is, consequently, theoretical language, rather than objective data. The knowledge of administrative science is not built from objective truths but is, instead, an artifact-the product of social definition. Institutional mechanisms reinforce these social definitions of truth by investing them with the stamp of scientific authenticity.",1985.0,Graham Astley
46fd40358f55f062ca37ec05f3f74dd08d88499b,https://www.semanticscholar.org/paper/46fd40358f55f062ca37ec05f3f74dd08d88499b,Some Methodological Issues in Cohort Analysis of Archival Data,"Walton, John 1966a ""Substance and artifact: the current status of research on community power structure."" American Journal of Sociology 71 (January) :430-8. 1966b ""Discipline, method and community power: a note on the sociology of knowledge."" American Sociological Review 31 (October): 684-99. 1967 ""The vertical axis of community organization and the structure of power."" Southwestern Social Science Quarterly 48 (December) :353-68. Wolfinger, Raymond E. 1962 ""Reputation and reality in the study of community power."" American Sociological Review 25 (October) :636-44.",1973.0,"K. Mason, W. Mason, H. Winsborough, W. Poole"
fa23c195b5ac15e571d386f37da3afe912d42f88,https://www.semanticscholar.org/paper/fa23c195b5ac15e571d386f37da3afe912d42f88,Longitudinal and Panel Data,"This focuses on models and data that arise from repeated observations of a cross-section of individuals, households or companies. These models have found important applications within business, economics, education, political science and other social science disciplines. The author introduces the foundations of longitudinal and panel data analysis at a level suitable for quantitatively oriented graduate social science students as well as individual researchers. He emphasizes mathematical and statistical fundamentals but also describes substantive applications from across the social sciences, showing the breadth and scope that these models enjoy. The applications are enhanced by real-world data sets and software programs in SAS and Stata.",2004.0,E. Frees
221da285b72f6272c2d7332a547d1e034a60e154,https://www.semanticscholar.org/paper/221da285b72f6272c2d7332a547d1e034a60e154,Data modelling versus ontology engineering,"Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. Unlike data models, the fundamental asset of ontologies is their relative independence of particular applications, i.e. an ontology consists of relatively generic knowledge that can be reused by different kinds of applications/tasks. The first part of this paper concerns some aspects that help to understand the differences and similarities between ontologies and data models. In the second part we present an ontology engineering framework that supports and favours the genericity of an ontology. We introduce the DOGMA ontology engineering approach that separates ""atomic"" conceptual relations from ""predicative"" domain rules. A DOGMA ontology consists of an ontology base that holds sets of intuitive context-specific conceptual relations and a layer of ""relatively generic"" ontological commitments that hold the domain rules. This constitutes what we shall call the double articulation of a DOGMA ontology 1.",2002.0,"Peter Spyns, R. Meersman, Mustafa Jarrar"
7467866259be9bcad9a9f03a21c9a0c594b828c0,https://www.semanticscholar.org/paper/7467866259be9bcad9a9f03a21c9a0c594b828c0,Biostatistics for animal science,Presenting and summarizing data Probability Random variables and their distributions Population and sample Hypotheses testing Correlation Two independent variables Concepts of experimental design Blocking Split-plot design Analysis of covariance Repeated measures Lack of fit,2004.0,"M. Kaps, W. Lamberson"
9e7a7c605264f886bee1e0b53909f825ed6e20e6,https://www.semanticscholar.org/paper/9e7a7c605264f886bee1e0b53909f825ed6e20e6,Making Data Maximally Available,"Science is driven by data. New technologies have vastly increased the ease of data collection and consequently the amount of data collected, while also enabling data to be independently mined and reanalyzed by others. And society now relies on scientific data of diverse kinds; for example, in responding to disease outbreaks, managing resources, responding to climate change, and improving transportation. It is obvious that making data widely available is an essential element of scientific research. The scientific community strives to meet its basic responsibilities toward transparency, standardization, and data archiving. Yet, as pointed out in a special section of this issue (pp. 692–729), scientists are struggling with the huge amount, complexity, and variety of the data that are now being produced.",2011.0,"B. Hanson, A. Sugden, B. Alberts"
ea5e1f456870cb6352799f604e3e7717b5ddbc46,https://www.semanticscholar.org/paper/ea5e1f456870cb6352799f604e3e7717b5ddbc46,Statistics for engineering and the sciences,"CHAPTER 1: INTRODUCTION 1.1 Statistics: The Science of Data 1.2 Fundamental Elements of Statistics 1.3 Types of Data 1.4 The Role of Statistics in Critical Thinking 1.5 A Guide to Statistical Methods Presented in this Text Statistics in Action: Contamination of Fish in the Tennessee River Collecting theData CHAPTER 2: DESCRIPTIVE STATISTICS 2.1 Graphical and Numerical Methods for Describing Qualitative Data 2.2 Graphical Methods for Describing Quantitative Data 2.3 Numerical Methods for Describing Quantitative Data 2.4 Measures of Central Tendency 2.5 Measures of Variation 2.6 Measures of Relative Standing 2.7 Methods for Detecting Outliers 2.8 Distorting the Truth with Descriptive Statistics Statistics in Action: Characteristics of Contaminated Fish in the Tennessee River CHAPTER 3: PROBABILITY 3.1 The Role of Probability in Statistics 3.2 Events, Sample Spaces, and Probability 3.3 Compound Events 3.4 Complementary Events 3.5 Conditional Probability 3.6 Probability Rules for Unions and Intersections 3.7 Bayes' Rule (Optional) 3.8 Some Counting Rules 3.9 Probability and Statistics: An Example 3.10 Random Sampling Statistics in Action: Assessing Predictors of Software Defects CHAPTER 4: DISCRETE RANDOM VARIABLES 4.1 Discrete Random Variables 4.2 The Probability Distribution for a Discrete Random Variable 4.3 Expected Values for Random Variables 4.4 Some Useful Expectation Theorems 4.5 Bernoulli Trials 4.6 The Binomial Probability Distribution 4.7 The Multinomial Probability Distribution 4.8 The Negative Binomial and the Geometric Probability Distributions 4.9 The Hypergeometric Probability Distribution 4.10 The Poisson Probability Distribution 4.11 Moments and Moment Generating Functions (Optional) Statistics in Action: The Reliability of a ""One-Shot"" Device CHAPTER 5: CONTINUOUS RANDOM VARIABLES 5.1 Continuous Random Variables 5.2 The Density Function for a Continuous Random Variable 5.3 Expected Values for Continuous Random Variables 5.4 The Uniform Probability Distribution 5.5 The Normal Probability Distribution 5.6 Descriptive Methods for Assessing Normality 5.7 Gamma-Type Probability Distributions 5.8 The Weibull Probability Distriibution 5.9 Beta-Type Probability Distributions 5.10 Moments and Moment Generating Functions (Optional) Statistics in Action: Super Weapons Development: Optimizing the Hit Ratio CHAPTER 6: JOINT PROBABILITY DISTRIBUTIONS AND SAMPLING DISTRIBUTIONS 6.1 Bivariate Probability Distributions for Discrete Random Variables 6.2 Bivariate Probability Distributions for Continuous Random Variables 6.3 The Expected Value of Functions of Two Random Variables 6.4 Independence 6.5 The Covariance and Correlation of Two Random Variables 6.6 Probability Distributions and Expected Values of Functions of Random Variables (Optional) 6.7 Sampling Distributions 6.8 Approximating a Sampling Distribution by Monte Carlo Simulation 6.9 The Sampling Distributions of Means and Sums 6.10 Normal Approximation to the Binomial Distribution 6.11 Sampling Distributions Related to the Normal Distribution Statistics in Action: Availability of an Up/Down System CHAPTER 7: ESTIMATION USING CONFIDENCE INTERVALS 7.1 Point Estimators and their Properties 7.2 Finding Point Estimators: Classical Methods of Estimation 7.3 Finding Interval Estimators: The Pivotal Method 7.4 Estimation of Population Mean 7.5 Estimation of the Difference Between Two Population Means: Independent Samples 7.6 Estimation of the Difference Between Two Population Means: Matched Pairs 7.7 Estimation of a Poulation Proportion 7.8 Estimation of the Difference Between Two Population Proportions 7.9 Estimation of a Population Variance 7.10 Estimation of the Ratio of Two Population Variances 7.11 Choosing the Sample Size 7.12 Alternative Estimation Methods: Bootstrapping and Bayesian Methods (Optional) Statistics in Action: Bursting Strength of PET Beverage Bottles CHAPTER 8: TESTS OF HYPOTHESES 8.1 The Relationship Between Statistical Tests of Hypotheses and Confidence Intervals 8.2 Elements and Properties of a Statistical Test 8.3 Finding Statistical Tests: Classical Methods 8.4 Choosing the Null and Alternative Hypotheses 8.5 Testing a Population Mean 8.6 The Observed Significance Level for a Test 8.7 Testing the Difference Between Two Population Means: Independent Samples 8.8 Testing the Difference Between Two Population Means: Independent Samples 8.9 Testing a Population Proportion 8.10 Testing the Difference Between Two Population Proportions 8.11 Testing a Population Variance 8.12 Testing the Ration of Two Population Variances 8.13 Alternative Testing Procedures: Bootstrapping and Bayesian Methods (Optional) Statistics in Action: Comparing Methods for Dissolving Drug Tablets - Dissolution Method Equivalence Testing CHAPTER 9: CATEGORICAL DATA ANALYSIS 9.1 Categorical Data and Multinomial Probabilities 9.2 Estimating Category Probabilities in a One-Way Table 9.3 Testing Category Probabilities in a One-Way Table 9.4 Inferences About Category Probabilities in a Two-Way (Contingency) Table 9.5 Contingency Tables with Fixed Marginal Totals 9.6 Exact Tests for Independence in a Contingency Table Analysis (Optional) Statistics in Action: The Public's Perception of Engineers and Engineering CHAPTER 10: SIMPLE LINEAR REGRESSION 10.1 Regression Models 10.2 Model Assumptions 10.3 Estimating ss0 and ss1: The Method of Least Squares 10.4 Properties of the Least Squares Estimators 10.5 An Estimator of d2 10.6 Assessing the Utility of the Model: Making Inferences About the Slope ss1 10.7 The Coefficient of Correlation 10.8 The Coefficient of Determination 10.9 Using the Model for Estimation and Pediction 10.10 A Complete Example 10.11 A Summary of the Steps to Follow in Simple Linear Regression Statistics in Action: Can Dowser's Really Detect Water? CHAPTER 11: MULTIPLE REGRESSION ANALYSIS 11.1 General Form of a Multiple Regression Model 11.2 Model Assumptions 11.3 Fitting the Model: The Method of Least Squares 11.4 Computations using Matrix Algebra Estimating and Making Inferences about the ss Parameters 11.5 Assessing Overall Model Adequacy 11.6 A Confidence Interval for E(y) and a prediction interval for a Future Value of y 11.7 A First-Order Model with Quantitative Predictors 11.8 An Interaction Model with Quantitative Predictors 11.9 A Quadratic (Second-Order) Model with a Quantitative Predictor 11.10 Checking Assumptions: Residual Analysis 11.11 Some Pitfalls: Estimability, Multicollinearity, and Extrapolation 11.12 A Summary of the Steps to Follow in a Multiple Regression Analysis Statistics in Action: Bid-Rigging in the Highway Construction Industry CHAPTER 12: MODEL BUILDING 12.1 Introduction: Why Model Building is Important 12.2 The Two Types of Independent Variables: Quantitative and Qualitative 12.3 Models with a Single Quantitative Independent Variable 12.4 Models with Two Quantitative Independent Variables 12.5 Coding Quantitative Independent Variables (Optional) 12.6 Models with One Qualitative Independent Variable 12.7 Models with Both Quantitative and Qualitative Independent Variables 12.8 Tests for Comparing Nested Models 12.9 External Model Validation (Optional) 12.10 Stepwise Regression Statistics in Action: Deregulation of the Intrastate Trucking Industry CHAPTER 13: PRINCIPLES OF EXPERIMENTAL DESIGN 13.1 Introduction 13.2 Experimental Design Terminology 13.3 Controlling the Information in an Experiment 13.4 Noise-Reducing Designs 13.5 Volume-Increasing Designs 13.6 Selecting the Sample Size 13.7 The Importance of Randomization Statistics in Action: Anti-Corrosive Behavior of Epoxy Coatings Augmented with Zinc CHAPTER 14: ANALYSIS OF VARIANCE FOR DESIGNED EXPERIMENTS 14.1 Introduction 14.2 The Logic Behind an Analysis of Variance 14.3 One-Factor Completely Randomized Designs 14.4 Randomized Block Designs 14.5 Two-Factor Factorial Experiments 14.6 More Complex Factorial Designs (Optional) 14.7 Nested Sampling Designs (Optional) 14.8 Multiple Comparisons of Teatment Means 14.9 Checking ANOVA Assumptions Statistics in Action: On the Trail of the Cockroach CHAPTER 15: NONPARAMETRIC STATISTICS 15.1 Introduction: Distribution-Free Tests 15.2 Testing for Location of a Single Population 15.3 Comparing Two Populations: Independent Random Samples 15.4 Comparing Two Populations: Matched-Pair Design 15.5 Comparing Three or More Populations: Completely Randomized Design 15.6 Comparing Three or More Populations: Randomized Block Design 15.7 Nonparametric Regression Statistics in Action: Agent Orange and Vietnam Vets CHAPTER 16: STATISTICAL PROCESS AND QUALITY CONTROL 16.1 Total Quality Management 16.2 Variable Control Charts 16.3 Control Chart for Means: x-Chart 16.4 Control Chart for Process Variation: R-Chart 16.5 Detecting Trends in a Control Chart: Runs Analysis 16.6 Control Chart for Percent Defective: p-Chart 16.7 Control Chart for number of Defectives per item: c-Chart 16.8 Tolerance Limits 16.9 Capability Analysis (Optional) 16.10 Acceptance Sampling for Defectives 16.11 Other Sampling Plans (Optional) 16.12 Evolutionary Operations (Optional) Statistics in Action: Testing Jet Fuel Additive for Safety CHAPTER 17: PRODUCT AND SYSTEM RELIABILITY 17.1 Introduction 17.2 Failure Time Distributions 17.3 Hazard Rates 17.4 Life Testing: Censored Sampling 17.5 Estimating the Parameters of an Exponential Failure Time Distribution 17.6 Estimating the Parameters of a Weibull Failure Time Distribution 17.7 System Reliability Statistics in Action: Modeling the Hazard Rate of Reinforced Concrete Bridge Deck Deterioration APPENDIX A: MATRIX ALGEBRA APPENDIX B: USEFUL STATISTICAL TABLES APPENDIX C: SAS FOR WINDOWS TUTORIAL APPENDIX D: MINITAB FOR WINDOWS TUTORIAL APPENDIX E: SPSS FOR WINDOWS TUTORIAL ANSWERS TO SELECTED EXERCISES INDEX",1984.0,"W. Mendenhall, T. Sincich"
907e0a1242aa0ab1415b57c5154884ba74a099c4,https://www.semanticscholar.org/paper/907e0a1242aa0ab1415b57c5154884ba74a099c4,Environmental Science : A Global Concern,"This book is intended for use in a one- or two-semester course in environmental science, human ecology, or environmental studies at the college or advanced placement high school level. Because most students who will use this book are freshman or sophomore nonscience majors, the authors have tried to make the text readable and accessible without technical jargon or a presumption of prior science background. At the same time, enough data and depth are presented to make this book suitable for many upper-division classes and a valuable resource for students who will keep it in their personal libraries after their formal studies are completed. The goal of this book is to provide an up-to-date, introductory view of essential themes in environmental science along with emphasis on details and case studies that will help students process and retain the general principles.",1990.0,"W. Cunningham, B. W. Saigo"
e5cfb20df1a1abb9b425fb85a4529855017714db,https://www.semanticscholar.org/paper/e5cfb20df1a1abb9b425fb85a4529855017714db,The Analysis and Interpretation of Multivariate Data for Social Scientists,"Based on a longtime course for master's level students at the London School of Economics and Politics, where the authors are based, this text concentrates on the multivariate methods so useful to social science problems involving correlational rather than causal relationships. Chapters with application examples and further readings cover data preliminaries, cluster analysis, multidimensional scaling, correspondence analysis, principal components analysis, factor analysis, and latent variable methods. While mathematical demands are minimal, these methods require use of a computer software package; an auxiliary website supplies data sets and code for use with SPSS.",2002.0,"D. Bartholomew, I. Moustaki, J. Galbraith, F. Steele"
7a649ce3304aa787b91d2c59a09c5dc8ad1a3e96,https://www.semanticscholar.org/paper/7a649ce3304aa787b91d2c59a09c5dc8ad1a3e96,Computational science and engineering,"Computational Science and Engineering (CSE) is the multi-disciplinary field of computer-based modelling and simulation for studying scientific phenomena and engineering designs. Modelling and simulation helps to validate theory, and makes it possible to analyse scenarios that would otherwise be too time-consuming, expensive, or dangerous to study by experiment. Data exploration helps to turn numbers into insight which is especially challenging in times of Big Data.",1996.0,Ahmed H. Sameh
99275cf0d56914361636d7c16c5101b8de5ecc44,https://www.semanticscholar.org/paper/99275cf0d56914361636d7c16c5101b8de5ecc44,"Validity of the Computer Science and Applications, Inc. (CSA) activity monitor.","The validity of the Computer Science and Applications, Inc. (CSA) accelerometer in assessing physical activity was assessed during treadmill walking and running at three different grades. Energy expenditure (EE) served as the criterion measure. CSA data were compared to data collected with the Caltrac accelerometer. Both accelerometers were sensitive to changes in treadmill speed, but neither discriminated changes in treadmill grade. Caltrac and CSA activity counts were significantly and similarly correlated with EE (r = 0.66-0.82), relative VO2 (r = 0.77-0.89), heart rate (r = 0.66-0.80), treadmill speed (r = 0.82-0.92), and with each other (r = 0.77-0.82). CSA data were used to develop models to predict EE (kcal.min-1). Cross-validation resulted in a mean difference between actual and predicted EE of 0.02 kcal.min-1 (SEE = 0.85 kcal.min-1). The range of individual differences in the validation group was large for both the CSA model (-2.86 to +3.86 kcal.min-1) and Caltrac (-4.17 to +2.04 kcal.min-1). It is concluded that the CSA and Caltrac accelerometers have similar validity and that either instrument can be used to estimate EE of groups.",1995.0,"Edward L. Melanson, Patty S. Freedson"
22f9e174bf51476efcf9cd8b14a674b1da5c2b92,https://www.semanticscholar.org/paper/22f9e174bf51476efcf9cd8b14a674b1da5c2b92,The Data Avalanche is Here: Shouldn’t We Be Digging?,"We have access to an unprecedented amount of fine-grained data on cities, transportation, economies, and societies, much of these data referenced in geo-space and time. There is a tremendous opportunity to discover new knowledge about spatial economies that can inform theory and modeling in regional science. However, there is little evidence of computational methods for discovering knowledge from databases in the regional science literature. This paper addresses this gap by clarifying the geospatial knowledge discovery process, its relation to scientific knowledge construction, and identifying challenges to a greater role in regional science.",2010.0,H. R. Miller
49a966258dfd23b8a398209d50a7773bb81b7b9f,https://www.semanticscholar.org/paper/49a966258dfd23b8a398209d50a7773bb81b7b9f,Using the science writing heuristic as a tool for learning from laboratory investigations in secondary science,"This article presents and discusses preliminary research on a new heuristic tool for learning from laboratory activities in secondary science. The tool, called the science writing heuristic, can be used by teachers as a framework from which to design classroom activities. Theoretically, the science writing heuristic represents a bridge between traditional laboratory reports and types of writing that promote personal construction of meaning. Two eighth-grade classes participated in using the science writing heuristic during an 8-week stream study. The teacher and one of the researchers collaboratively developed activities based on the science writing heuristic that the teacher implemented. Nineteen target students were studied in depth. Characteristics of report writing and students' understanding of the nature of science were investigated, using interpretive techniques. There is evidence that use of the science writing heuristic facilitated students to generate meaning from data, make connections among procedures, data, evidence, and claims, and engage in metacognition. Students' vague understandings of the nature of science at the beginning of the study were modified to more complex, rich, and specific understandings. The implications of the study for writing in science classrooms is discussed. © 1999 John Wiley & Sons, Inc. J Res Sci Teach 36: 1065–1084, 1999",1999.0,"Carolyn W. Keys, B. Hand, V. Prain, Susan Collins"
ea5c82d066cc20a9cf90fb5921071c7f501e4337,https://www.semanticscholar.org/paper/ea5c82d066cc20a9cf90fb5921071c7f501e4337,Handling Data Skew in MapReduce,"MapReduce systems have become popular for processing large data sets and are increasingly being used in e-science applications. In contrast to simple application scenarios like word count, e-science applications involve complex computations which pose new challenges to MapReduce systems. In particular, (a) the runtime complexity of the reducer task is typically high, and (b) scientific data is often skewed. This leads to highly varying execution times for the reducers. Varying execution times result in low resource utilisation and high overall execution time since the next MapReduce cycle can only start after all reducers are done. In this paper we address the problem of efficiently processing MapReduce jobs with complex reducer tasks over skewed data. We define a new cost model that takes into account non-linear reducer tasks and we provide an algorithm to estimate the cost in a distributed environment. We propose two load balancing approaches, fine partitioning and dynamic fragmentation, that are based on our cost model and can deal with both skewed data and complex reduce tasks. Fine partitioning produces a fixed number of data partitions, dynamic fragmentation dynamically splits large partitions into smaller portions and replicates data if necessary. Our approaches can be seamlessly integrated into existing MapReduce systems like Hadoop. We empirically evaluate our solution on both synthetic data and real data from an e-science application.",2011.0,"B. Gufler, Nikolaus Augsten, Angelika Reiser, A. Kemper"
ea5c82d066cc20a9cf90fb5921071c7f501e4337,https://www.semanticscholar.org/paper/ea5c82d066cc20a9cf90fb5921071c7f501e4337,Handling Data Skew in MapReduce,"MapReduce systems have become popular for processing large data sets and are increasingly being used in e-science applications. In contrast to simple application scenarios like word count, e-science applications involve complex computations which pose new challenges to MapReduce systems. In particular, (a) the runtime complexity of the reducer task is typically high, and (b) scientific data is often skewed. This leads to highly varying execution times for the reducers. Varying execution times result in low resource utilisation and high overall execution time since the next MapReduce cycle can only start after all reducers are done. In this paper we address the problem of efficiently processing MapReduce jobs with complex reducer tasks over skewed data. We define a new cost model that takes into account non-linear reducer tasks and we provide an algorithm to estimate the cost in a distributed environment. We propose two load balancing approaches, fine partitioning and dynamic fragmentation, that are based on our cost model and can deal with both skewed data and complex reduce tasks. Fine partitioning produces a fixed number of data partitions, dynamic fragmentation dynamically splits large partitions into smaller portions and replicates data if necessary. Our approaches can be seamlessly integrated into existing MapReduce systems like Hadoop. We empirically evaluate our solution on both synthetic data and real data from an e-science application.",2011.0,"B. Gufler, Nikolaus Augsten, Angelika Reiser, A. Kemper"
d944ebc2103355b8fddd9c05e5cc8a9eccee875b,https://www.semanticscholar.org/paper/d944ebc2103355b8fddd9c05e5cc8a9eccee875b,Open Access and Global Participation in Science,"Previous investigations into the impact of open-access journals on subsequent citations confounded open and electronic access and failed to track availability over time. With new data, we separated these effects. We demonstrate that a journal receives a modest increase in citations when it comes online freely, but the jump is larger when it first comes online through commercial sources. This effect reverses for poor countries where free-access articles are much more likely to be cited. Together, findings suggest that free Internet access widens the circle of those who read and make use of scientists' investigations.",2009.0,"James A. Evans, Jacob Reimer"
8743ad5d9ce5e08b8d81a52ac0820c92c4f65afe,https://www.semanticscholar.org/paper/8743ad5d9ce5e08b8d81a52ac0820c92c4f65afe,"Stories as data, data as stories: making sense of narrative inquiry in clinical education *","Background  Narrative inquiry is a form of qualitative research that takes story as either its raw data or its product. Science and narrative can be seen as two kinds of knowing, reflected in the distinction between evidence‐based medicine derived from population studies and narrative‐based medicine focused upon the single case. A similar tension exists in the field of narrative inquiry between cognitive‐orientated analytical methods and affective‐orientated methods of synthesis.",2005.0,A. Bleakley
173e0355f87b33f3829f80e4c852dbcefe6a7974,https://www.semanticscholar.org/paper/173e0355f87b33f3829f80e4c852dbcefe6a7974,Combinatorial Materials Science and Catalysis.,"After forever changing the drug discovery process in the pharmaceutical industry, combinatorial chemistry methodologies are increasingly being applied to the discovery and optimization of more efficient catalysts and materials (see picture). With the advent of new combinatorial synthesis and screening technologies, coupled with integrated data management systems, the application of these technologies to materials science and catalyst research holds tremendous potential and brings high expectations to this new and exciting field.",1999.0,"B. Jandeleit, Dieter J. Schaefer, Timothy Powers, Howard W. Turner, W. Weinberg"
cfc1ab5e9effeac607a1760c08daaefeffb1d1a5,https://www.semanticscholar.org/paper/cfc1ab5e9effeac607a1760c08daaefeffb1d1a5,MapReduce in the Clouds for Science,"The utility computing model introduced by cloud computing combined with the rich set of cloud infrastructure services offers a very viable alternative to traditional servers and computing clusters. MapReduce distributed data processing architecture has become the weapon of choice for data-intensive analyses in the clouds and in commodity clusters due to its excellent fault tolerance features, scalability and the ease of use. Currently, there are several options for using MapReduce in cloud environments, such as using MapReduce as a service, setting up one’s own MapReduce cluster on cloud instances, or using specialized cloud MapReduce runtimes that take advantage of cloud infrastructure services. In this paper, we introduce Azure MapReduce, a novel MapReduce runtime built using the Microsoft Azure cloud infrastructure services. Azure MapReduce architecture successfully leverages the high latency, eventually consistent, yet highly scalable Azure infrastructure services to provide an efficient, on demand alternative to traditional MapReduce clusters. Further we evaluate the use and performance of MapReduce frameworks, including Azure MapReduce, in cloud environments for scientific applications using sequence assembly and sequence alignment as use cases.",2010.0,"Thilina Gunarathne, T. Wu, J. Qiu, G. Fox"
11dbcc3b66789b2791b1d1a780692df32c5fc120,https://www.semanticscholar.org/paper/11dbcc3b66789b2791b1d1a780692df32c5fc120,Data Visualization : Principles and Practice,"The goal of data visualization is to use images to improve our understanding of a dataset, drawing on techniques from mathematics, computer science, cognitive and perception science, and physics. In this introductory text, the author provides a compact introduction to the field that allows readers to learn about visualization techniques. The material focuses on those techniques and methods that have a broad applicability in visualization applications, occur in most practical problems in various guises, and do not demand a specialized background to be understood. However, the author has also included a number of less mainstream visualization techniques. With these methods, the book gives the reader an idea of the large variety of applications of data visualizations, illustrates the wide range of problems that can be tackled by such methods, and emphasizes the strong connections between visualization and related disciplines such as imaging and computer graphics.",2007.0,A. Telea
25732139300e03b2dd2ad07e2500b356b061ae43,https://www.semanticscholar.org/paper/25732139300e03b2dd2ad07e2500b356b061ae43,User-friendly web mapping: lessons from a citizen science website,"Citizen science websites are emerging as a common way for volunteers to collect and report geographic ecological data. Engaging the public in citizen science is challenging and, when involving online participation, data entry, and map use, becomes even more daunting. Given these new challenges, citizen science websites must be easy to use, result in positive overall satisfaction for many different users, support many different tasks, and ensure data quality. To begin reaching these goals, we built a geospatially enabled citizen science website, evaluated its usability, and gained experience by working with and listening to citizens using the website. We sought to determine general perceptions, discover potential problems, and iteratively improve website features. Although the website was rated positively overall, map-based tasks identified a wide range of problems. Given our results, we redesigned the website, improved the content, enhanced the ease of use, simplified the map interface, and added features. We discuss citizen science websites in relation to online Public Participation Geographic Information Systems, examine the role(s) websites may play in the citizen science research model, discuss how citizen science research advances GIScience, and offer guidelines to improve citizen-based web mapping applications.",2010.0,"Greg Newman, Don Zimmerman, Alycia Crall, M. Laituri, J. Graham, L. Stapel"
5fafc89d30ab130905e769a953c09e69352af588,https://www.semanticscholar.org/paper/5fafc89d30ab130905e769a953c09e69352af588,Explaining Science*,"Following the revolutionary book by Thomas Kuhn [1962], philosophical accounts of the development of science have attended more or less closely to actual episodes of scientific inquiry. Lakatos and his students launched their historiographic research programme as a means of testing his scientific methodology, and both Laudan [1977] and Hull [1988] likewise appeal to data from past or contemporary scientific practice. In his new book Explaining Science [1988] Ronald Giere would take us beyond this limited test-bed for the philosophy of science by directly applying results from the cognitive sciences. The main features of this new approach are perhaps best expressed in the preface:",1991.0,K. Korb
4e6aa0488ddd0b99ca8d51e4872db53d74e452cd,https://www.semanticscholar.org/paper/4e6aa0488ddd0b99ca8d51e4872db53d74e452cd,Science and the Precautionary Principle,"The Precautionary Principle has become enshrined in international law, and is the basis for European environmental legislation. However, ""precautionary"" decisions have been controversial, and the principle itself lacks clear definition. A recent commentary by the European Commission offers guidelines for politically transparent application of the principle, while emphasizing the need for careful review of relevant scientific data. Recent precautionary policies for limiting public exposure to radio-frequency energy shows that the principle can be applied in a way that does not conflict with traditional exposure guidelines. Major uncertainties still remain in the standard of proof needed to invoke the principle.",2000.0,"K. Foster, P. Vecchia, M. Repacholi"
58661dccf9c996e8ab0bb0afd1a5455e14ed1a99,https://www.semanticscholar.org/paper/58661dccf9c996e8ab0bb0afd1a5455e14ed1a99,"Promoting Access to Public Research Data for Scientific, Economic, and Social Development","Access to and sharing of data are essential for the conduct and advancement of science. This article argues that publicly funded research data should be openly available to the maximum extent possible. To seize upon advancements of cyberinfrastructure and the explosion of data in a range of scientific disciplines, this access to and sharing of publicly funded data must be advanced within an international framework, beyond technological solutions. The authors, members of an OECD Follow-up Group, present their research findings, based closely on their report to OECD, on key issues in data access, as well as operating principles and management aspects necessary to successful data access regimes.",2004.0,"P. Arzberger, Peter Schroeder, A. Beaulieu, G. Bowker, Kathleen Casey, Leif Laaksonen, David Moorman, Paul Uhlir, P. Wouters"
35150893834d1b260e9a83c7823e40b1de24ca41,https://www.semanticscholar.org/paper/35150893834d1b260e9a83c7823e40b1de24ca41,UNDERGRADUATE SCIENCE STUDENTS' IMAGES OF SCIENCE,"This article describes views about the nature of science held by a small sample of science students in their final year at the university. In a longitudinal interview study, 11 students were asked questions about the nature of science during the time they were involved in project work. Statements about the nature of science were characterized and coded using a framework drawing on aspects of the epistemology and sociology of science. The framework in this study has three distinct areas: the relationship between data and knowledge claims, the nature of lines of scientific enquiry, and science as a social activity. The students in our sample tended to see knowledge claims as resting solely on empirical grounds, although some students mentioned social factors as also being important. Many of the students showed significant development in their understanding of how lines of scientific enquiry are influenced by theoretical developments within a discipline, over the 5–8 month period of their project work. Issues relating to scientists working as a community were underrepresented in the students' discussions about science. Individual students drew upon a range of views about the nature of science, depending on the scientific context being discussed. © 1999 John Wiley & Sons, Inc. J Res Sci Teach 36: 201–219, 1999",1999.0,"J. Ryder, J. Leach, R. Driver"
ccc4e83642d7563790e5ad8be3c193ce92407023,https://www.semanticscholar.org/paper/ccc4e83642d7563790e5ad8be3c193ce92407023,Deep learning methods for drug response prediction in cancer: Predominant and emerging trends,"Cancer claims millions of lives yearly worldwide. While many therapies have been made available in recent years, by in large cancer remains unsolved. Exploiting computational predictive models to study and treat cancer holds great promise in improving drug development and personalized design of treatment plans, ultimately suppressing tumors, alleviating suffering, and prolonging lives of patients. A wave of recent papers demonstrates promising results in predicting cancer response to drug treatments while utilizing deep learning methods. These papers investigate diverse data representations, neural network architectures, learning methodologies, and evaluations schemes. However, deciphering promising predominant and emerging trends is difficult due to the variety of explored methods and lack of standardized framework for comparing drug response prediction models. To obtain a comprehensive landscape of deep learning methods, we conducted an extensive search and analysis of deep learning models that predict the response to single drug treatments. A total of 61 deep learning-based models have been curated, and summary plots were generated. Based on the analysis, observable patterns and prevalence of methods have been revealed. This review allows to better understand the current state of the field and identify major challenges and promising solution paths.",2022.0,"A. Partin, T. Brettin, Yitan Zhu, Oleksandr Narykov, Austin R. Clyde, Jamie Overbeek, Rick L. Stevens Division of Data Science, Learning, A. N. Laboratory, Argonne, Il, Usa, Department of Materials Science, T. Chicago, Chicago."
bbeb0f3ce758b1301231f11ed9656a0278cbd987,https://www.semanticscholar.org/paper/bbeb0f3ce758b1301231f11ed9656a0278cbd987,Data sharing in the sciences,"ion, resolving file/element naming and data protection issues, and resource allocation. The cloud provides technology as a service; the two major services are Software-as-a-Service (SaaS) and Infrastructureas-a-Service (IaaS) both of which can simplify data sharing. Software-asa-Service provides access to a suite of software tools, either as a monthly or per-use cost for commercial software, or at no cost for non-commercial software. This enables a computing environment with no capital investment, few set-up costs, and no maintenance. Infrastructure-as-a-Service provides on-demand computing cycles and data storage via a virtual machine that can be configured to execute requests. The impact of the cloud is not fully understood and the future is unclear. Although Geambasu and colleagues argue that the cloud infrastructure will eliminate the need for traditional data centers within organizations, Arms and colleagues (2009) argue that local systems operations help to diffuse expertise across an organization and thus will continue to be valuable. Access and Discovery Metadata, Ontologies, and Vocabularies Access to and discovery of data resources require sufficient, targeted description of the resources themselves. This description is generally accomplished with metadata, ontologies, and vocabularies. The words metadata and ontologies have been used in multiple ways, but it is worth defining them briefly for our purposes. Ontologies are hierarchical, formal representations of the objects that constitute a specific area of interest (Gruber, 1995). In general, metadata has been used in information science to mean information about a particular representation of data (a document or set of documents, a dataset, images, or similar digital objects) (Duval, Hodgins, Sutton, & Weibel, 2002). A vocabulary consists of the properties that characterize the particular metadataset. For example, Dublin Core, which may be the most common metadata vocabulary, is applied to data to describe them for resource discovery and content management. It includes thirteen properties—such as title, author, and publication data—that characterize a particular digital resource. Metadata can be informal or maintained and created in a more structured way as part of the dataset itself. Metadata can be as straightforward as the date a particular dataset was generated, the creators/ custodians of the data, and the experimental or observational procedures that generated the data, to much more complex information about potential uses, geographic location of the data, its quality, or its state of completeness. Some metadata are also used to describe the kinds of web services that generated and maintain the dataset, associated tasks, and previous uses by others. Metadata can be human-generated or automatically generated and can be intended either for human use or for use by automated processes. Scientific communities, national scientific bodies, and international scientific organizations have all created metadata 268 Annual Review of Information Science and Technology",2011.0,"Stacy T. Kowalczyk, K. Shankar"
110cd53ef4b63aa6951a4897c4a53e04c465883c,https://www.semanticscholar.org/paper/110cd53ef4b63aa6951a4897c4a53e04c465883c,A Primer for Panel Data Analysis,"Panel data analysis is a method of studying a particular subject within multiple sites, periodically observed over a defined time frame. Within the social sciences, panel analysis has enabled researchers to undertake longitudinal analyses in a wide variety of fields. In economics, panel data analysis is used to study the behavior of firms and wages of people over time. In political science, it is used to study political behavior of parties and organizations over time. It is used in psychology, sociology, and health research to study characteristics of groups of people followed over time. In educational research, researchers study classes of students or graduates over time.",2003.0,R. Yaffee
9d076765e9b46eee7aee32a6013e4ee809df73a4,https://www.semanticscholar.org/paper/9d076765e9b46eee7aee32a6013e4ee809df73a4,Data Mining and Knowledge Discovery: Making Sense Out of Data,"Find loads of the data mining and knowledge discovery making sense out of data book catalogues in this site as the choice of you visiting this page. You can also join to the website book library that will show you numerous books from any types. Literature, science, politics, and many more catalogues are presented to offer you the best book to find. The book that really makes you feels satisfied. Or that's the book that will save you from your job deadline.",1996.0,
293da404614d89f0eb9621d26d3b23afd6c9d0ff,https://www.semanticscholar.org/paper/293da404614d89f0eb9621d26d3b23afd6c9d0ff,"Scientific Data Management - Challenges, Technology, and Deployment","We introduce and describe scientific workflows, i.e., executable descriptions of automatable scientific processes such as computational science simulations and data analyses. Scientific workflows are often expressed in terms of tasks and their (data ow) dependencies. This chapter first provides an overview of the characteristic features of scientific workflows and outlines their life cycle. A detailed case study highlights workflow challenges and solutions in simulation management. We then provide a brief overview of how some concrete systems support the various phases of the workflow life cycle, i.e., design, resource management, execution, and provenance management. We conclude with a discussion on community-based workflow sharing.",2009.0,"Bertram Ludäscher, I. Altintas, S. Bowers, J. Cummings, T. Critchlow, E. Deelman, D. D. Roure, J. Freire, C. Goble, Matthew B. Jones, S. Klasky, T. McPhillips, N. Podhorszki, Cláudio T. Silva, I. Taylor, M. Vouk"
8440f7d0811ada6c3f0a0025b27a8fc3e6675faa,https://www.semanticscholar.org/paper/8440f7d0811ada6c3f0a0025b27a8fc3e6675faa,The knowledge pyramid: a critique of the DIKW hierarchy,"The paper evaluates the data—information—knowledge—wisdom (DIKW) hierarchy. This hierarchy, also known as the `knowledge hierarchy', is part of the canon of information science and management. Arguments are offered that the hierarchy is unsound and methodologically undesirable. The paper identifies a central logical error that DIKW makes. The paper also identifies the dated and unsatisfactory philosophical positions of operationalism and inductivism as the philosophical backdrop to the hierarchy. The paper concludes with a sketch of some positive theories, of value to information science, on the nature of the components of the hierarchy: that data is anything recordable in a semantically and pragmatically sound way, that information is what is known in other literature as `weak knowledge', that knowledge also is `weak knowledge' and that wisdom is the possession and use, if required, of wide practical knowledge, by an agent who appreciates the fallible nature of that knowledge.",2009.0,Martin Frické
a5fa73ec0bb416c7a345bbeedc512085623ba847,https://www.semanticscholar.org/paper/a5fa73ec0bb416c7a345bbeedc512085623ba847,Data mining,"Abstract A student's ability to complete a study according to a designated time is an important factor in assessing university accreditation. A good accreditation shows the image of a university. The problem that arises is that many students miss the completion of their studies, which hinders the certification of their learning programs. The purpose of this study is to design an application program that can support student graduation-related decisions. This study applies the Nave Bayesian method, which can predict future opportunities based on past experience at the University of Al- Ashalia Mander, School of Computer Science, and Information Systems Research Program. This study is a data mining application program for student graduation classification using the Nave Bayes method, which is expected to help educators complete their studies on time and develop their ability to increase accreditation in their research programs. The design was successful.",1996.0,"P. Adriaans, Dolf Zantinge"
57d791eb2cd5fe8ed9cfe8a7167f7a4439e3b11e,https://www.semanticscholar.org/paper/57d791eb2cd5fe8ed9cfe8a7167f7a4439e3b11e,Data Management Challenges of Data-Intensive Scientific Workflows,"Scientific workflows play an important role in today's science. Many disciplines rely on workflow technologies to orchestrate the execution of thousands of computational tasks. Much research to-date focuses on efficient, scalable, and robust workflow execution, especially in distributed environments. However, many challenges remain in the area of data management related to workflow creation, execution, and result management. In this paper we examine some of these issues in the context of the entire workflow lifecycle.",2008.0,"E. Deelman, A. Chervenak"
63f11ef0c83e3567f74e6e199267240317bc1c92,https://www.semanticscholar.org/paper/63f11ef0c83e3567f74e6e199267240317bc1c92,The INTEGRAL/IBIS scientific data analysis,"The gamma-ray astronomical observatory INTEGRAL, succesfully launched on 17th October 2002, carries two large gamma-ray telescopes. One of them is the coded-mask imaging gamma-ray telescope onboard the INTEGRAL satellite (IBIS) which provides high-resolution ( ≈ 12 ' ) sky images of $29^{\circ}\times29^{\circ}$ in the energy range from 15 keV to 10 MeV with typical on-axis sensitivity of ≈ 1 mCrab at 100 keV (3 σ , 10 6  s exposure). We report here the general description of the IBIS coded-mask imaging system and of the standard IBIS science data analysis procedures. These procedures reconstruct, clean and combine IBIS sky images providing at the same time detection, identification and preliminary analysis of point-like sources present in the field. Spectral extraction has also been implemented and is based on simultaneous fitting of source and background shadowgram models to detector images. The procedures are illustrated using some of the IBIS data collected during the inflight calibrations and present performance is discussed. The analysis programs described here have been integrated as instrument specific software in the Integral Science Data Center (ISDC) analysis software packages currently used for the Quick Look, Standard and Off-line Scientific Analysis.",2003.0,"A. Goldwurm, P. David, L. Foschini, A. Gros, P. Laurent, A. Sauvageon, A. Bird, L. Lerusse, N. P. SApCEA-Saclay, France IASFCNR-Bologna, Italy Southampton Un., UK ISDC-Versoix, Switzerland."
3fa51d53df3e1049cebfec5734927521efbb6dd9,https://www.semanticscholar.org/paper/3fa51d53df3e1049cebfec5734927521efbb6dd9,"How hard is hard science, how soft is soft science? The empirical cumulativeness of research.",""" Research results in the social and behavioral sciences are often conceded to be less replicable than research results in the physical sciences. However, direct empirical comparisons of the cumulativeness of research in the social and physical sciences have not been made to date. This article notes the parallels between methods used in the quantitative synthesis of research in the social and in the physical sciences. Essentially identical methods are used to test the consistency of research results in physics and in psychology. These methods can be used to compare the consistency of replicated research results in physics and in the social sciences. The methodology is illustrated with 13 exemplary reviews from each domain. The exemplary comparison suggests that the results of physical experiments may not be strikingly more consistent than those of social or behavioral experiments. The data suggest that even the results of physical experiments may not be cumulative in the absolute sense by statistical criteria. It is argued that the study of the actual cumulativeness found in physical data could inform social scientists about what to expect from replicated experiments under good conditions. Psychologists and other social scientists have often compared their fields to the natural (the ""hard"") sciences with a tinge of dismay. Those of us in the social and behavioral sciences know intuitively that there is something ""softer"" and less cumulative about our research results than about those of the physical sciences. It is easy to chronicle the differences between soft and hard sciences that might lead to less cumulative research results in the soft sciences. One such chronicle is provided by Meehl (1978), who listed 20 such differences and went on to argue that reliance on tests of statistical significance also contributes to the poorer cumulativeness of research results in the social sciences. Other distinguished researchers have cited the pervasive presence of interactions (Cronbach, 1975) or historical influences (Gergen, 1973, 1982) as reasons not to expect a cumulative social science. Still others (Kruskal, 1978, 1981) have cited the low quality of data in the social sciences as a barrier to truly cumulative social inquiry. These pessimistic views have been accompanied by a tendency to reconceptualize the philosophy of inquiry into a format that implies less ambitious aspirations for social knowledge (e.g., Cronbach, 1975; Gergen, 1982). Cumulativeness in the scientific enterprise can mean at least two things. In the broadest sense scientific results are cumulative if empirical laws and theoretical structures build on one another so that later developments extend and unify earlier work. This idea might be called conceptual or theoretical cumulativeness. The assessment of theoretical cumulativeness must be rather subjective. A narrower and less subjective indicator of cumulativeness is the degree of agreement among replicated experiments or the degree to which related experimental results fit into a simple pattern that makes conceptual sense. This idea might be called empirical cumulativeness. The purpose of this article is to suggest that it may be possible to compare at least the empirical cumulativeness of psychological research with that of research in the physical sciences. An exemplary comparison suggests that the differences may be less striking than previously imagined. The mechanism for this comparison is derived from recent developments in methods for the quantitative synthesis of research in the social sciences. Some of the methods used in meta-analysis are analogous to methods used in the quantitative synthesis of research in the physical sciences. In particular, physicists and psychologists use analogous methods for assessing the consistency of research results, a fact that makes possible comparisons among quantitative reviews in physics and in psychology. One such comparison is reported in this article. This comparison was not chosen in a way that guarantees it to be representative of either social science research or physical science research. However, some effort was exerted to prevent the comparison from obviously favoring one domain or the other, and additional examples are provided to suggest that the case for the empirical cumulativeness of physical science could have been made to look far worse. More data would obviously be needed to support strong conclusions. It seems, however, that the ""obvious"" conclusion that the results of physical science experiments are more cumulative than those of social science experiments does not have much empirical sup-",1987.0,L. Hedges
3e6e98b35872d2011c296e3b64cafc55e18d16f6,https://www.semanticscholar.org/paper/3e6e98b35872d2011c296e3b64cafc55e18d16f6,A Proposed Standard for the Scholarly Citation of Quantitative Data,"An essential aspect of science is a community of scholars cooperating and competing in the pursuit of common goals. A critical component of this community is the common language of and the universal standards for scholarly citation, credit attribution, and the location and retrieval of articles and books. We propose a similar universal standard for citing quantitative data that retains the advantages of print citations, adds other components made possible by, and needed due to, the digital form and systematic nature of quantitative data sets, and is consistent with most existing subfield-specific approaches. Although the digital library field includes numerous creative ideas, we limit ourselves to only those elements that appear ready for easy practical use by scientists, journal editors, publishers, librarians, and archivists.",2008.0,"Micah Altman, Gary King"
d57f9d5b641a86ac2ea235bbe1cb4d5a3432d182,https://www.semanticscholar.org/paper/d57f9d5b641a86ac2ea235bbe1cb4d5a3432d182,Keynote address - data abstraction and hierarchy,"Data abstraction is a valuable method for organizing programs to make them easier to modify and maintain. Inheritance allows one implementation of a data abstraction to be related to another hierarchically. This paper investigates the usefulness of hierarchy in program development, and concludes that although data abstraction is the more important idea, hierarchy does extend its usefulness in some situations. This research was supported by the NEC Professorship of Software Science and Engineering October 1987 OOPSLA ‘87 Addendum to the Proceedings 17",1988.0,B. Liskov
6041496b16718fcbc471c079c33bae57c7af8ad5,https://www.semanticscholar.org/paper/6041496b16718fcbc471c079c33bae57c7af8ad5,A Qualitative Study of Factors Influencing Science Teaching Self-Efficacy of Elementary Level Teachers.,"Science teaching self-efficacy may be one area of importance which has been over-looked in implementing change to improve science teaching in elementary schools. This qualitative study was designed to examine factors which influence personal science teaching efficacy and science teaching outcome expectancy in elementary teachers. Based on Bandura's psychological construct of self-efficacy, science teaching self-efficacy has been related to teachers' belief in their ability to teach science, called personal science teaching efficacy (PSTE), and their belief in students' ability to learn, called science teaching outcome expectancy (STOE). Data were collected from 23 elementary teachers involved in a project to enhance science, mathematics, and technology education. Initially, data on variables identified as related to science teaching self-efficacy were collected and triangulated from several self-reporting instruments, including the Science Teaching Efficacy Beliefs Instrument, In-service version (STEBI-A) Teachers' scores on personal science teaching efficacy (PSTE) and science teaching outcome expectancy (STOE), two subscales of the STEBI-A, along with other data were used to develop in depth interview questions. Ten of the teachers, with varying PSTE and STOE levels (high, moderate, and low), were purposefully selected for interviews regarding their teacher preparation, professional development, and science-related antecedent experiences. The qualitative data analysis methods of constant-comparison and clustering were used to identify patterns and themes within the interview data. Qualitative analysis of triangulated data provided insights as to development of PSTE and STOE. Results of the interview analysis revealed more definitive findings for the dimension of personal science teaching efficacy than for science teaching outcome expectancy. Theme in the data indicated that antecedent experiences influenced interest in science teaching. Preservice and in-service experiences such as success in high quality science courses and workshops, access to a lesser degree STOE. Findings on experiences which influenced STOE were limited. Implications for early science experiences, teacher preparation and teacher professional development are presented. Recommendations for further research are made for PSTE and STOE. © 1996 John Wiley & Sons, Inc.",1996.0,"L. Ramey-gassert, M. Shroyer, John R. Staver"
7d6571232e2006ca2301439d5c9ad858e2d35735,https://www.semanticscholar.org/paper/7d6571232e2006ca2301439d5c9ad858e2d35735,Measurement in the Social Sciences: The Link Between Theory and Data,1. Introduction to measurement 2. Factor analysis 3. Reliability 4. Validity 5. Evaluating systematic error 6. Integrating reliability and validity Appendix: multiple indicators Bibliography Index.,1980.0,"R. Zeller, E. Carmines"
3f4558f0526a7491e2597941f99c14fea536288d,https://www.semanticscholar.org/paper/3f4558f0526a7491e2597941f99c14fea536288d,Author cocitation: A literature measure of intellectual structure,"It is shown that the mapping of a particular area of science, in this case information science, can be done using authors as units of analysis and the cocitations of pairs of authors as the variable that indicates their “distances” from each other. The analysis assumes that the more two authors are cited together, the closer the relationship between them. The raw data are cocitation counts drawn online from Social Scisearch (Social Sciences Citation Index) over the period 1972–1979. The resulting map shows (1) identifiable author groups (akin to “schools”) of information science, (2) locations of these groups with respect to each other, (3) the degree of centrality and peripherality of authors within groups, (4) proximities of authors within group and across group boundaries (“border authors” who seem to connect various areas of research), and (5) positions of authors with respect to the map's axes, which were arbitrarily set spanning the most divergent groups in order to aid interpretation. Cocitation analysis of authors offers a new technique that might contribute to the understanding of intellectual structure in the sciences and possibly in other areas to the extent that those areas rely on serial publications. The technique establishes authors, as well as documents, as an effective unit in analyzing subject specialties.",1981.0,"Howard D. White, B. C. Griffith"
a025360e9b43f45b8a0446af275f9a654cbbe692,https://www.semanticscholar.org/paper/a025360e9b43f45b8a0446af275f9a654cbbe692,Voyager Radio Science Observations of Neptune and Triton,"The Voyager 2 encounter with the Neptune system included radio science investigations of the masses and densities of Neptune and Triton, the low-order gravitational harmonics of Neptune, the vertical structures of the atmospheres and ionospheres of Neptune and Triton, the composition of the atmosphere of Neptune, and characteristics of ring material. Demanding experimental requirements were met successfully, and study of the large store of collected data has begun. The initial search of the data revealed no detectable effects of ring material with optical depth τ [unknown] 0.01. Preliminary representative results include the following: 1.0243 x 1026 and 2.141 x 1022 kilograms for the masses of Neptune and Triton; 1640 and 2054 kilograms per cubic meter for their respective densities; 1355 � 7 kilometers, provisionally, for the radius of Triton; and J2 = 3411 � 10(x 10-6) and J4 = -26+12-20(x10-6) for Neptune's gravity field (J>2 and J4 are harmonic coefficients of the gravity field). The equatorial and polar radii of Neptune are 24,764 � 20 and 24,340 � 30 kllometers, respectively, at the 105-pascal (1 bar) pressure level. Neptune's atmosphere was probed to a pressure level of about 5 x 105 pascals, and effects of a methane cloud region and probable ammonia absorption below the cloud are evident in the data. Results for the mixing ratios of helium and ammonia are still being investigated; the methane abundance below the clouds is at least 1 percent by volume. Derived temperature-pressure profiles to 1.2 x 105 pascals and 78 kelvins (K) show a lapse rate corresponding to ""frozen"" equilibrium of the para- and ortho-hydrogen states. Neptune's ionosphere exhibits an extended topside at a temperature of 950 � 160 K if H+ is the dominant ion, and narrow ionization layers of the type previously seen at the other three giant planets. Triton has a dense ionosphere with a peak electron concentration of 46 x 109 per cubic meter at an altitude of 340 kilometers measured during occultation egress. Its topside plasma temperature is about 80 � 16 K if N2+ is the principal ion. The tenuous neutral atmosphere of Triton produced distinct signatures in the occultation data; however, the accuracy of the measurements is limited by uncertainties in the frequency of the spacecraft reference oscillator. Preliminary values for the surface pressure of 1.6 � 0.3 pascals and an equivalent isothermal temperature of 48 � 5 K are suggested, on the assumption that molecular nitrogen dominates the atmosphere. The radio data may be showing the effects of a thermal inversion near the surface; this and other evidence imply that the Triton atmosphere is controlled by vapor-pressure equilibrium with surface ices, at a temperature of 38 K and a methane mixing ratio of about 10-4.",1989.0,"G. Tyler, D. Sweetnam, J. Anderson, S. Borutzki, J. Campbell, V. Eshleman, D. Gresh, E. Gurrola, D. Hinson, N. Kawashima, E. Kursinski, G. Levy, G. F. Lindal, J. Lyons, E. Marouf, P. Rosen, R. A. Simpson, G. E. Wood"
992c22c3ddcc142c63be1edbd7584bf2d983546a,https://www.semanticscholar.org/paper/992c22c3ddcc142c63be1edbd7584bf2d983546a,"""Science Citation Index""--A New Dimension in Indexing.","ing Services Recently Bennett (47) has reiterated my earlier recommendation that an index to the abstracts in specialty journals and to abstracts prepared by the smaller abstracting services be compiled (21). Frequently these are abstracts which include criticism. As such, they constitute original publications. Every author should know of such critical abstracts of his papers. In literature searches, abstracts may serve in lieu of the original articles, particularly when the original article is in a foreign language, or when it is not readily available. Citation indexes can be used to locate these abstracts quickly and to identify unabstracted articles (31). Author Citation Index A random-access computer memory does not require special ordering for data storage. In such memories the arbitrarily assigned data-addresses need not be known to the user. By contrast, a printed citation index must have a",1964.0,E. Garfield
eb3cb17fc02f4be76f3c8039d8a599efbf24a37c,https://www.semanticscholar.org/paper/eb3cb17fc02f4be76f3c8039d8a599efbf24a37c,Data Documentation Initiative: Toward a Standard for the Social Sciences,"The Data Documentation Initiative (DDI) is an emerging metadata standard for the social sciences. The DDI is in active use by many data specialists and archivists, but researchers themselves have been slow to recognize the benefits of the standards approach to metadata. This paper outlines how the DDI has evolved since its inception in 1995 and discusses ways to broaden its impact in the social science research community.",2008.0,"Mary Vardigan, P. Heus, Wendy L. Thomas"
b4e273b8bdf2385f0445e7780ec4eac02fbc281e,https://www.semanticscholar.org/paper/b4e273b8bdf2385f0445e7780ec4eac02fbc281e,An introduction to the WEKA data mining system,"This is a proposal for a half day tutorial on Weka, an open source Data Mining software package written in Java and available from www.cs.waikato.ac.nz/~ml/weka/index.html. The goal of the tutorial is to introduce faculty to the package and to the pedagogical possibilities for its use in the undergraduate computer science and engineering curricula. The Weka system provides a rich set of powerful Machine Learning algorithms for Data Mining tasks, some not found in commercial data mining systems. These include basic statistics and visualization tools, as well as tools for pre-processing, classification, and clustering, all available through an easy to use graphical user interface.",2006.0,"Z. Markov, I. Russell"
a08e42db0257e5a25458253ebb4491b7e6c1afd0,https://www.semanticscholar.org/paper/a08e42db0257e5a25458253ebb4491b7e6c1afd0,"OPeNDAP: Accessing data in a distributed, heterogeneous environment","In the process of implementing a protocol for the transport of science data, the Open Source Project for a Network Data Access Protocol (OPeNDAP) group has learned a considerable amount about the internal anatomy of what are commonly considered monolithic concepts. In order to communicate among our group, we have adopted a collection of deinitions and observations about data and the metadata that make them useful: differentiating between ""semantic"" and ""syntactic"" metadata, and deining categories such as ""translational"" and ""use"" metadata. We share the deinitions and categorizations here in the hope that others will ind them as useful as we do.",2003.0,"P. Cornillon, J. Gallagher, Tom Sgouros"
ca84d0bc7791d027d63c663d61cf95be41cecdb0,https://www.semanticscholar.org/paper/ca84d0bc7791d027d63c663d61cf95be41cecdb0,"Dimensions of evidence, the public understanding of science and science education","This paper explores the nature and type of evidence employed by participants in an issue of public concern. By examining documents and interviewing members of the public involved in the debate, the way in which evidence was used in the arguments for and against the issue was determined. Three dimensions of evidence emerged from the data: formal scientific evidence based on the data; informal evidence (e.g. common sense, personal experience) and wider issues which impinge on the evidence (e.g. environmental or legal concerns). In this particular controversy, it was the questioning of the formal evidence by local scientists which became the 'magic bullet' but pertinent questioning by local nonscientists also framed the debate. The authors suggest that school science curricula should include practice in questioning and manipulating different sorts of real data in a variety of ways so that pupils are equipped and empowered to tackle contemporary issues of this kind.",2001.0,R. Tytler
bdece7df1e2808ccb947f3cc2d16e9ad6d1fcd4b,https://www.semanticscholar.org/paper/bdece7df1e2808ccb947f3cc2d16e9ad6d1fcd4b,What Are Data? The Many Kinds of Data and Their Implications for Data Re-Use,"One key feature of e-science is to encourage archiving and release of data so that they are available in digitally-processable forms for re-use almost from the point of collection. This assumes particular processes of translation by which data can be made visible in transportable and intelligible forms. It also requires mechanisms by which data quality and provenance can be trusted once ""disconnected"" from their producers. By analyzing the ""life stages"" of data in four academic projects, we show that these requirements create difficulties for disciplines where tacit knowledge and craft-like methods are deeply embedded in researchers, as well as for disciplines producing non-digital heterogeneous data or data derived from people rather than from material phenomena. While craft practices and tacit knowledges are a feature of most scientific endeavors, some disciplines currently appear more inclined to attempt to formalize or at least record these knowledges. We discuss the implications this has for the e-science objective of widespread data re-use.",2007.0,"S. Carlson, B. Anderson"
5c779f032e8d1a7595d32ef16f3f9b8b70246125,https://www.semanticscholar.org/paper/5c779f032e8d1a7595d32ef16f3f9b8b70246125,Data Analysis and Decision Making with Microsoft Excel,"From the Publisher: 
In response to the growing market trend in quantitative education, Albright, Winston, and Zappe's integrated business-statistics and management-science text presents core statistics and management-science methods in a modern, unified spreadsheet-oriented approach. 
With a focus on applications, not on mathematical techniques, the book covers business statistics with some essential management-science topics included. The example-based, Excel spreadsheet approach is useful in courses that combine traditional statistics and management-science topics, though it can be easily used for a one-term business statistics only course. The modeling and application focus, together with the Excel spreadsheet add-ins, provides a complete learning source for both students and practicing managers.",1999.0,"S. Albright, Wayne L. Winston, C. Zappe"
8f306edfaf6ebc326604f99d3116f4dfd25c6b06,https://www.semanticscholar.org/paper/8f306edfaf6ebc326604f99d3116f4dfd25c6b06,Gender Differences in Patenting in the Academic Life Sciences,We analyzed longitudinal data on academic careers and conducted interviews with faculty members to determine the scope and causes of the gender gap in patenting among life scientists. Our regressions on a random sample of 4227 life scientists over a 30-year period show that women faculty members patent at about 40% of the rate of men. We found that the gender gap has improved over time but remains large.,2006.0,"Waverly W. Ding, Fiona E. Murray, Toby E. Stuart"
28ee52385ce6998008132a824dfc1d97893b91a5,https://www.semanticscholar.org/paper/28ee52385ce6998008132a824dfc1d97893b91a5,Some Frontiers in Social Science,"The fundamental challenge in the social sciences is moving from complicated correlations to useful prediction. Progress usually reflects an interplay between theory, data, and tools. Six areas of innovation, principally data and tools, are now pushing at the frontiers of these sciences: longitudinal data, laboratory experimentation, improved statistical methods, geographic information tools, biosocial science, and international replication. These innovations are gaining power as they cross disciplinary boundaries, helping to attribute causality to observed relationships, to understand their nature, and thereby to improve the accuracy and usefulness of predictions.",2006.0,"W. Butz, B. B. Torrey"
ff8c339a5b24e47246cd30ea399b44f0946cb25f,https://www.semanticscholar.org/paper/ff8c339a5b24e47246cd30ea399b44f0946cb25f,The data analysis handbook,"Analyzing observed or measured data is an important step in applied sciences. The recent increase in computer capacity has resulted in a revolution both in data collection and data analysis. An increasing number of scientists, researchers and students are venturing into statistical data analysis; hence the need for more guidance in this field, which was previously dominated mainly by statisticians. This handbook fills the gap in the range of textbooks on data analysis. Written in a dictionary format, it will serve as a comprehensive reference book in a rapidly growing field. However, this book is more structured than an ordinary dictionary, where each entry is a separate, self-contained entity. The authors provide not only definitions and short descriptions, but also offer an overview of the different topics. Therefore, the handbook can also be used as a companion to textbooks for undergraduate or graduate courses. 1700 entries are given in alphabetical order grouped into 20 topics and each topic is organized in a hierarchical fashion. Additional specific entries on a topic can be easily found by following the cross-references in a top-down manner. Several figures and tables are provided to enhance the comprehension of the topics and a list of acronyms helps to locate the full terminologies. The bibliography offers suggestions for further reading.",1994.0,"I. E. Frank, R. Todeschini"
517d4518040fd3e33e451adab37275cea60728ae,https://www.semanticscholar.org/paper/517d4518040fd3e33e451adab37275cea60728ae,Bioinformatics--Trying to Swim in a Sea of Data,"The massive quantities of data generated by genomic research have given rise to the field of bioinformatics--an emerging discipline that seeks to integrate computer science with applications derived from molecular biology.
 Roos
 identifies two major challenges to the advancement of bioinformatics research: how to define permissible use of data released to the community before publication, and restrictions on the further analysis and reposting of published data available on proprietary Web sites.",2001.0,David S. Roos
d9c73428e9bda201985d84374b7d0d80e004393b,https://www.semanticscholar.org/paper/d9c73428e9bda201985d84374b7d0d80e004393b,"Social Sciences and Modern States: Policy research: data, ideas, or arguments?","We have delayed examination of the effects of social science on public policy long enough. This, and the succeeding chapter by Wittrock, now address ways in which the social sciences influence the development of policies in the modern state. Where Majone (chapter 13) drew ideas from the philosophy of science, I adopt an idea from the legal system, the idea of argumentation. In the first half of the chapter, I examine the influence on policy of three types of research products: data and findings, ideas and criticism, and arguments or briefs for policy action. Whereas the traditional output of a policy study is a report of the first kind, heavy on data, conclusions, statistics, and findings, a review of the sketchy evidence available suggests that in some settings research has greater impact when it becomes part of advocacy for a preferred position. The second part of the chapter then wrestles with the normative question: what stance should researchers adopt? It confronts the question of whether advocacy has a place in the policy researcher's kit. Policy research is a close relative of social science, and even though it has put on its working clothes and gone out to labour in the offices and chambers of government, it has not relinquished the ‘science’ label: thus, policy sciences. There is something uncomfortable in the thought of abandoning the norms of objectivity that characterize a science and embracing a notion of advocacy more suitable to an interest group or lobbyist.",1991.0,C. Weiss
548e2b8d76e0624f95246bd28ce279fcae569da5,https://www.semanticscholar.org/paper/548e2b8d76e0624f95246bd28ce279fcae569da5,Data Mining on the Web,"We read with great interest the Perspective “Creating a science of the Web” by T. Berners-Lee et al. (11 Aug, p. 769). We agree that evolving Web technologies enable the creation of novel structures of information, whose properties and dynamics can be fruitfully studied. More generally, we would like to point out that the Web is a specific phenomenon associated with the increasing prevalence of information being digitized and linked together into complicated structures. The complexity of these structures underscores the need for systematic, large-scale data mining both to uncover new patterns in social interactions and to make discoveries in science through connecting disparate findings. For this vision to be realized, we have to develop a new science of practical data mining focusing on questions answerable with the existing digital libraries of information. In particular, today, free-text search (as embodied by Google) is the primary means of mining the Web, but there are many kinds of information requests it cannot handle. Queries combining general, standardized annotation about pages (such as from the semantic Web) with free-text search within them are often not supported—e.g., doing a full-text search of all biophysics blogs emanating just from governmental institutions within 100 miles of Chicago. Furthermore, it would be useful to develop ways of leveraging the small amounts of highly structured information in the semantic Web as “gold-standard training sets” to help bootstrap the querying and clustering of the large bodies of unstructured information on the Web as a whole. Thus, the science of the Web should enumerate the range of information requests that can be fruitfully made and the kinds of information infrastructure and data-mining techniques needed to fulfill them.

# Response {#article-title-2}

We agree with Smith and Gerstein's view that data mining is among the many important areas of research that are considering the Web as an object of scientific inquiry. They are correct in pointing out the importance of “text mining,” the basis of current Web search, for providing new Web capabilities. However, with the increasing amount of directly machine-readable data that are available on the Web (coming from, for example, database-producing equipment such as modern scientific devices and data-oriented applications), it is also clear that text mining needs to be augmented with new data technologies that work more directly with data and meta-data. Data mining is also an excellent case in point for the main focus of our Perspective in relation to the interdisciplinary nature of the emerging science of the Web. Analytic modeling techniques will be needed to understand where Web data reside and how they can best be accessed and integrated. Engineering and language development are needed if we are to be able to perform data mining without having to pull all the information into centralized data servers of a scale that only the few largest search companies can currently afford. In addition, data mining provides not just opportunities for better search, but also real policy issues with respect to information access and user privacy, especially where multiple data sources are aggregated into searchable forms.",2006.0,"Andrew K. Smith, M. Gerstein"
1502f80367837e0580c9fae07f73a2ad1643cafd,https://www.semanticscholar.org/paper/1502f80367837e0580c9fae07f73a2ad1643cafd,"Science and Ethics in Conducting, Analyzing, and Reporting Psychological Research","The relationship between scientific quality and ethical quality is considered for three aspects of the research process: conduct of the research, data analysis, and reporting of results. In the area of conducting research, issues discussed involve design, recruitment, causism, scientific quality, and costs and utilities. The discussion of data analysis considers data dropping, data exploitation, and meta-analysis. Issues regarding reporting of results include misrepresentation of findings, misrepresentation of credit, and failure to report results as a result of self-censoring or external censoring.",1994.0,R. Rosenthal
eadeb224797c2184367970f363dbdb49a60ffcd7,https://www.semanticscholar.org/paper/eadeb224797c2184367970f363dbdb49a60ffcd7,Remote Sensing in Geology,"Describes the theory, techniques, and applications of remote sensing in the geological sciences. Prepared by 20 leading experts in the field and written within an integrated context, the text has been organized into 4 sections; the science of the interaction of light with surfaces and the acquisition of data; optical and digital processing of data in preparation for analysis; interpretive techniques; and the application of remotely sensed data to different disciplines within geology.",1980.0,"B. Siegal, A. Gillespie"
79020730b41ec3b29a204c4c9be756276b6e3086,https://www.semanticscholar.org/paper/79020730b41ec3b29a204c4c9be756276b6e3086,LETTING THE DATA SPEAK FOR THEMSELVES,"ABSTRACT In Science, great difficulty is sometimes experienced in giving up hypothesized structures. The inadequacies of Freudian hypotheses are highlighted, and attention is directed to Dasein-analysis, which stays close to the data. This perspective focuses attention upon the phenomenological tradition, and suggests that certain mathematical frameworks in human geography are inappropriate. The adequacy of a priori models is also questioned from a Heideggerian perspective, and more general qualitative algebras are suggested to replace the distorted functional thinking inherited from the physical sciences.",1981.0,P. Gould
a17ae7090e12106e12aafeb1c3fd79458f321cb8,https://www.semanticscholar.org/paper/a17ae7090e12106e12aafeb1c3fd79458f321cb8,Remote sensing for the earth sciences,"SPECTRAL CHARACTERISTICS. Spectroscopy of Rocks and Minerals and Principles of Spectroscopy (R. Clark). Multispectral Thermal Infrared Data in Geological Studies (S. Hook, et al.). Soil Reflectance (E. Ben-Dor, et al.). Geobotany: Vegetation Mapping in Earth Science (S. Ustin, et al.). ANALYSIS. Spectral Analysis for Earth Science Investigation (J. Mustard & J. Sunshine). Integration and Visualization of Geoscience Data (J. Harris, et al.). APPLICATIONS. Stratigraphy (H. Lang). Strategies for Mineral Exploration (C. Sabine). Hydrocarbon Exploration (J. Berry & G. Prost). Planetary Geology (J. Bell, et al.). SENSORS/CASE STUDIES. Visible and Infrared: Sensors and Case Studies (F. Kruse). Radar: Sensors and Case Studies (J. Plaut, et al.). Geophysical Methods (J. Broome). Index.",1999.0,A. Rencz
b59b81c3d26f9f119faae83b64daea629f737237,https://www.semanticscholar.org/paper/b59b81c3d26f9f119faae83b64daea629f737237,NONGEOSPATIAL METADATA FOR THE ECOLOGICAL SCIENCES,"Issues related to data preservation and sharing are receiving increased at- tention from scientific societies, funding agencies, and the broad scientific community. Ecologists, for example, are increasingly using data collected by other scientists to address questions at broader spatial, temporal, and thematic scales (e.g., global change, biodiversity, sustainability). No data set is perfect and self-explanatory. Ecologists must, therefore, rely upon a set of instructions or documentation to acquire a specific data set, determine its suitability for meeting specific research objectives, and accurately interpret results from subsequent processing, analysis, and modeling. ""Metadata"" represent the set of instructions or documentation that describe the content, context, quality, structure, and accessibility of a data set. Although geospatial metadata standards have been developed and widely endorsed by the geographical science community, such standards do not yet exist for the ecological sciences. In this paper, we examine potential benefits and costs associated with developing and implementing metadata for nongeospatial ecological data. We present a set of generic metadata descriptors that could serve as the basis for a ""metadata standard"" for nongeospatial ecological data. Alternative strategies for metadata implementation that meet differing organizational or investigator- specific objectives are presented. Finally, we conclude with several recommendations related to future development and implementation of ecological metadata.",1997.0,"K., Michener, James, W., Brunt, John, J., Helly, Thomas, B., Kirchner, '. SUSANG.STAFFORD, Joseph W. Jones"
867e112fbf0851641f723ea5b17e134d14043bb8,https://www.semanticscholar.org/paper/867e112fbf0851641f723ea5b17e134d14043bb8,Learning to Interview in the Social Sciences,"A large proportion of social science investigations rely on interview data, yet few researchers received formal training in interviewing. The authors investigated how novice researchers developed their interview skills, reporting on postgraduate students' experiences and reflections during an intensive 15-day interview course. Data analyzed for the article include audiotapes and transcripts of in-depth interviews and students' written critiques and journal reflections. Challenges faced by novice interviewers conducting in-depth interviews included unexpected participant behaviors, dealing with the consequences of the interviewers' own actions and subjectivities, constructing and delivering questions, and handling sensitive research topics. The authors also discuss the transcription of audio-recorded talk and include their own and students' reflections concerning the learning and teaching of interviewing. Finally, the authors provide recommendations for teaching interview skills for the purpose of doing social science research. This study informs teachers of qualitative research and researchers who seek to develop their interview skills.",2003.0,"K. Roulston, Kathleen P. deMarrais, Jamie B. Lewis"
4d91d032d1961eee08aba75e6466457e2ca74198,https://www.semanticscholar.org/paper/4d91d032d1961eee08aba75e6466457e2ca74198,Life Sciences,"The radiobiological properties of the heavy ions of cosmic radiation were investigated on Spacelab 1 by use of biostacks, monolayers of biological test organisms sandwiched between thin foils of different types of nuclear track detectors. Biostacks were exposed to cosmic radiation at several locations with different shielding environments in the module and on the pallet. Evaluations of the physical and biological components of the experiment to date indicate that in general they survived the spaceflight in good condition. Dosimetric data are presented for the different shielding environments.",1984.0,"H. Bücker, G. Horneck, R. Facius, Günther Reitz, M. Schäfer, J. Schott, R. Beaujean, W. Enge, E. Schopper, H. Heinrich, J. Beer, B. Wiegel, R. Pfohl, H. Francois, G. Portal, S. L. Bonting, E. Graul, W. Rüther, A. R. Kranz, U. Bork, K. Koller-Lampert, B. Kirchheim, M. Starke, H. Planel, M. Delpoux"
1a20c6e39eba4e874f221eaf385519ab8a025483,https://www.semanticscholar.org/paper/1a20c6e39eba4e874f221eaf385519ab8a025483,States of consciousness and state-specific sciences.,"Charles T. Tart not in tenrns of any particular wn'tent of consciousness, or ,specific lbeh,avior or physiological chan,ge, ,but in terms of ' the overall patterning of psychological functioning. An analogy with computer funotioning can clarify this definition. A c m puter has a com'plex program of many subroutines. If we repro,Dam it quite differently, the same sorts of input data may be han,dled in qui,te different ways; we will be able to prediot very little from our knowledge of the old program about the effects of varying the input, even though old anld new programs have some su,brout.ines in common. The new program with its input-output in(teractions mlust be stjudied Blackburn ( I ) recently noted' that and have his antiscientific attitude furin and of itself. AX is many of our most talented young people ther reinforced. It is clear to him that to =hanging temporarily *he program of are ''turned off"" to science: as a salnthe scientist has no real understanding a computer. tion, he proposed that we recognize the 'of what marijuana intoxication is all ~ h , A ~ c ' ~ experienced by al,most validity of a more sensuous-intui,tive apabout (3). ordinary people are drealming states proach to nature, treating it as WmMore formally, an increasin'gly and the hypnogogic and hypnopompic plamentary to the classical in.tellect.ua1 significant num~ber of people are experistates, the transi,tional states (&tween approach. menting with ASC's in themselves, and sleeping and waking. M~~~ other I have seen the same rejection of finding the experiences thus gained of experience another A x , alcohol science by many of the brightest extreme i,mportance in their philosophy intoxication. students in California, and the problem and style of life. The conflict between he relatively new (to our cul:ture) is indeed serious. Blaoklburn's analysis experiences in these ASC's and the atASS that are now h,aving rnch an is valid, but not deep enough. A more titudes and intellectual-emotional sysimpact are those produced by mari6",1972.0,C. Tart
ace35be3d1033a03c9258cff232b6517b1bc73c6,https://www.semanticscholar.org/paper/ace35be3d1033a03c9258cff232b6517b1bc73c6,Materials Informatics,"The purpose of this two-part Special Issue on Materials Informatics is to bring focus on the exciting developments and opportunities in the application of data mining and advanced statistical analysis in materials discovery and design. It might seem natural to assume that large amounts of data are critical for any serious informatics study. In materials science applications, however, what constitutes ‘enough’ data can vary significantly. In structural ceramics, for instance, it is difficult to obtain measurements of ‘fracture toughness’, and, in fact, just a few careful measurements can be of great value for some of the more complex materials. Similarly, reliable measurements of fundamental constants or properties for a given material require the use of very detailed measurement and/or computational techniques. Unlike astronomers or biologists who look at the world (environment) around them to gather data and then analyze it to find out what is important, materials scientists do a great deal of analysis (and/or experimentation) to get their data. The result is a number of challenges that are unique to materials science: lack of sufficient data, skewed datasets, and missing information, among others. On the other hand, the emergence of high-throughput dataacquisition techniques in materials science, such as combinatorial experimentation, offers unprecedented opportunities as well as challenges in data-driven discovery techniques. With such widely different issues in data characteristics, materials science offers an exciting domain for the application of the science of data mining as demonstrated with the papers in this Special Issue. The papers in this issue cover a broad range of applications in data-driven research in materials science covering a range of length and time scales. We have strived to introduce, through these papers, the challenges and more importantly, the opportunities that exist for both the materials science community and the computer science/statistics community in the emerging field of materials informatics. The papers cover different aspects of materials science and are not based on the classification of one type of material over another, but rather, the types of materials science studies where data-driven discovery can provide great benefits. Many of the papers deal with data acquisition or data mining associated with the processing of materials and their impact on properties, e.g. steel, polymeric fibers, and catalysts (Bhadeshia, Chakraborti, Datta, Chen and Mentges et al.), while others deal with the impact that processing has on microstructure (Sundararaghavan and Zabaras), or the influence of chemistry on crystal structure (Broderick and Rajan). In some cases, similar techniques are used for different problems, and the needs in linking materials informatics to engineering problems (LeSar) are also discussed. Finally the description of materials science problems in the context of high-dimensional information and the different approaches to reduce the dimensionality are discussed (Rajan and Mendez). We hope this set of publications will serve as a template for generating a long and rigorous research collaboration between the fields of data mining and materials science.",2009.0,"Krishna Rajan, Patricio F. Mendez"
155391192091a99f44b377db1e0e7819f2317498,https://www.semanticscholar.org/paper/155391192091a99f44b377db1e0e7819f2317498,Estimating the reproducibility of psychological science,"Empirically analyzing empirical evidence One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study. Science, this issue 10.1126/science.aac4716 A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired. INTRODUCTION Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error. RATIONALE There is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science. RESULTS We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P < .05). Thirty-six percent of replications had significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams. CONCLUSION No single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here. Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. Original study effect size versus replication effect size (correlation coefficients). Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects. Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.",2015.0,"Alexander A. Aarts, Joanna E. Anderson, Christopher J. Anderson, Peter Raymond Attridge, A. Attwood, Jordan R. Axt, Molly Babel, Š. Bahník, E. Baranski, M. Barnett-Cowan, E. Bartmess, Jennifer S. Beer, Raoul Bell, Heather Bentley, Leah Beyan, Grace Binion, D. Borsboom, Annick Bosch, Frank Bosco, Sara D. Bowman, M. Brandt, E. L. Braswell, Hilmar Brohmer, B. T. Brown, Kristina Brown, Jovita Brüning, Ann Calhoun-Sauls, Shannon P. Callahan, E. Chagnon, Jesse J. Chandler, Christopher R. Chartier, F. Cheung, Cody Daniel Christopherson, Linda Cillessen, R. Clay, Hayley M. D. Cleary, M. Cloud, Michael Conn, J. Cohoon, Simon Columbus, A. Cordes, G. Costantini, Leslie D Cramblet Alvarez, E. Cremata, J. Crusius, J. DeCoster, M. DeGaetano, Nicolás Delia Penna, Bobby Den Bezemer, M. Deserno, Olivia Devitt, L. Dewitte, David G. Dobolyi, Geneva T. Dodson, M. Donnellan, Ryan Donohue, Rebecca A. Dore, A. Dorrough, Anna Dreber, M. Dugas, Elizabeth W. Dunn, Kayleigh E. Easey, Sylvia Eboigbe, Casey M Eggleston, Jo Embley, S. Epskamp, Timothy M. Errington, Vivien Estel, Frank J. Farach, J. Feather, A. Fedor, Belén Fernández-Castilla, S. Fiedler, James G. Field, Stanka A. Fitneva, Taru Flagan, Amanda L. Forest, Eskil Forsell, J. D. Foster, Michael C. Frank, Rebecca S. Frazier, Heather M. Fuchs, P. Gable, Jeff Galak, E. Galliani, Anup Gampa, Sara Garcia, Douglas Gazarian, E. Gilbert, Roger Giner-Sorolla, A. Glöckner, Lars Goellner, Jin X. Goh, R. Goldberg, Patrick T. Goodbourn, S. Gordon-McKeon, Bryan H. Gorges, Jessie Gorges, J. Goss, J. Graham, J. Grange, J. Gray, C.H.J. Hartgerink, Joshua K. Hartshorne, F. Hasselman, Timothy Hayes, Emma Heikensten, Felix Henninger, J. Hodsoll, Taylor Holubar, G. Hoogendoorn, D. Humphries, C. O. Hung, N. Immelman, Vanessa C. Irsik, G. Jahn, F. Jäkel, Marc Jekel, M. Johannesson, L. Johnson, David J. Johnson, Kate M. Johnson, William Johnston, K. Jonas, Jennifer A. Joy-Gaba, H. Kappes, Kim Kelso, Mallory C. Kidwell, Seung K. Kim, M. Kirkhart, Bennett Kleinberg, G. Knežević, F. Kolorz, J. Kossakowski, R. Krause, J.M.T. Krijnen, T. Kuhlmann, Y. Kunkels, Megan M. Kyc, Calvin K. Lai, Aamir Laique, D. Lakens, Kristin A. Lane, Bethany Lassetter, L. Lazarević, E. Bel, Key Jung Lee, Minha Lee, K. Lemm, C. Levitan, Melissa Lewis, Lin Lin, Stephanie C. Lin, Matthias Lippold, Darren Loureiro, Ilse Luteijn, S. Mackinnon, Heather N. Mainard, Denise C. Marigold, D. P. Martin, Tylar Martinez, E. Masicampo, Joshua J. Matacotta, Maya B. Mathur, Michael May, Nicole C Mechin, Pranjal H. Mehta, Johannes M. Meixner, Alissa Melinger, Jeremy K. Miller, Mallorie Miller, K. Moore, Marcus Möschl, Matt Motyl, S. Muller, M. Munafo, K. Neijenhuijs, Taylor Nervi, Gandalf Nicolas, G. Nilsonne, Brian A. Nosek, Michèle B. Nuijten, Catherine Olsson, C. Osborne, Lutz Ostkamp, M. Pavel, I. Penton-Voak, O. Perna, C. Pernet, M. Perugini, R. Pipitone, M. Pitts, F. Plessow, J. Prenoveau, R. Rahal, Kate A. Ratliff, David A. Reinhard, Frank Renkewitz, Ashley A. Ricker, A. Rigney, Andrew M Rivers, Mark A. Roebke, Abraham M. Rutchick, Robert S. Ryan, O. Şahin, Anondah Saide, Gillian M. Sandstrom, David Santos, R. Saxe, René Schlegelmilch, Kathleen Schmidt, Sabine Scholz, L. Seibel, Dylan Selterman, S. Shaki, W. B. Simpson, H. Sinclair, Jeanine L. M. Skorinko, A. Slowik, J. Snyder, C. Soderberg, Carina M. Sonnleitner, N. Spencer, Jeffrey R. Spies, S. Steegen, S. Stieger, Nina Strohminger, G. Sullivan, Thomas Talhelm, M. Tapia, A. T. Dorsthorst, M. Thomae, Sarah L. Thomas, Pia Tio, Frits Traets, Steve Tsang, F. Tuerlinckx, Paul Turchan, Milan Valášek, A. V. Veer, R. V. Aert, M. V. Assen, R. V. Bork, Mathijs van de Ven, D. V. D. Bergh, M. Hulst, R. V. Dooren, J. Doorn, D. V. Renswoude, H. Rijn, Wolf Vanpaemel, Alejandro Echeverría, Melissa Vazquez, Natalia Vélez, Marieke Vermue, M. Verschoor, M. Vianello, M. Voracek, Gina Vuu, E. Wagenmakers, Joanneke Weerdmeester, A. Welsh, Erin C. Westgate, Joeri Wissink, M. Wood, A. Woods, Emily M. Wright, Sining Wu, M. Zeelenberg, Kellylynn Zuni"
71aac04dbe2c31b39f6fbdaac10feacb446c9139,https://www.semanticscholar.org/paper/71aac04dbe2c31b39f6fbdaac10feacb446c9139,"Using Synthetic Controls: Feasibility, Data Requirements, and Methodological Aspects","Probably because of their interpretability and transparent nature, synthetic controls have become widely applied in empirical research in economics and the social sciences. This article aims to provide practical guidance to researchers employing synthetic control methods. The article starts with an overview and an introduction to synthetic control estimation. The main sections discuss the advantages of the synthetic control framework as a research design, and describe the settings where synthetic controls provide reliable estimates and those where they may fail. The article closes with a discussion of recent extensions, related methods, and avenues for future research. (JEL B41, C32, C54, E23, F15, O47)",2021.0,Alberto Abadie
5d150cec2775f9bc863760448f14104cc8f42368,https://www.semanticscholar.org/paper/5d150cec2775f9bc863760448f14104cc8f42368,Discovering governing equations from data by sparse identification of nonlinear dynamical systems,"Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.",2015.0,"S. Brunton, J. Proctor, J. Kutz"
0e779fd59353a7f1f5b559b9d65fa4bfe367890c,https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c,Geometric Deep Learning: Going beyond Euclidean data,"Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",2016.0,"M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, P. Vandergheynst"
b1b825efc9f4c4a577d9bd7909e19c8758c06bb1,https://www.semanticscholar.org/paper/b1b825efc9f4c4a577d9bd7909e19c8758c06bb1,Data Reduction And Error Analysis For The Physical Sciences, ,2016.0,Christina Kluge
5cca605ccbda79502210edd94a227d897389de4c,https://www.semanticscholar.org/paper/5cca605ccbda79502210edd94a227d897389de4c,Grand challenges in the science of wind energy,"A multifaceted future for wind power Modern wind turbines already represent a tightly optimized confluence of materials science and aerodynamic engineering. Veers et al. review the challenges and opportunities for further expanding this technology, with an emphasis on the need for interdisciplinary collaboration. They highlight the need to better understand atmospheric physics in the regions where taller turbines will operate as well as the materials constraints associated with the scale-up. The mutual interaction of turbine sites with one another and with the evolving features of the overall electricity grid will furthermore necessitate a systems approach to future development. Science, this issue p. eaau2027 BACKGROUND A growing global population and an increasing demand for energy services are expected to result in substantially greater deployment of clean energy sources. Wind energy is already playing a role as a mainstream source of electricity, driven by decades of scientific discovery and technology development. Additional research and exploration of design options are needed to drive innovation to meet future demand and functionality. The growing scale and deployment expansion will, however, push the technology into areas of both scientific and engineering uncertainty. This Review explores grand challenges in wind energy research that must be addressed to enable wind energy to supply one-third to one-half, or even more, of the world’s electricity needs. ADVANCES Drawing from a recent international workshop, we identify three grand challenges in wind energy research that require further progress from the scientific community: (i) improved understanding of the physics of atmospheric flow in the critical zone of wind power plant operation, (ii) materials and system dynamics of individual wind turbines, and (iii) optimization and control of fleets of wind plants comprising hundreds of individual generators working synergistically within the larger electric grid system. These grand challenges are interrelated, so progress in each domain must build on concurrent advances in the other two. Characterizing the wind power plant operating zone in the atmosphere will be essential to designing the next generation of even-larger wind turbines and achieving dynamic control of the machines. Enhanced forecasting of the nature of the atmospheric inflow will subsequently enable control of the plant in the manner necessary for grid support. These wind energy science challenges bridge previously separable geospatial and temporal scales that extend from the physics of the atmosphere to flexible aeroelastic and mechanical systems more than 200 m in diameter and, ultimately, to the electrical integration with and support for a continent-sized grid system. OUTLOOK Meeting the grand research challenges in wind energy science will enable the wind power plant of the future to supply many of the anticipated electricity system needs at a low cost. The interdependence of the grand challenges requires expansion of integrated and cross-disciplinary research efforts. Methods for handling and streamlining exchange of vast quantities of information across many disciplines (both experimental and computational) will also be crucial to enabling successful integrated research. Moreover, research in fields related to computational and data science will support the research community in seeking to further integrate models and data across scales and disciplines. The cascade of scales underlying wind energy scientific grand challenges. Length scales from weather systems at a global level down the boundary layer of a wind turbine airfoil and time scales from seasonal fluctuations in weather to subsecond dynamic control and balancing of electrical generation and demand must be understood and managed. ILLUSTRATION: JOSH BAUER AND BESIKI KAZAISHVILI, NREL Harvested by advanced technical systems honed over decades of research and development, wind energy has become a mainstream energy resource. However, continued innovation is needed to realize the potential of wind to serve the global demand for clean energy. Here, we outline three interdependent, cross-disciplinary grand challenges underpinning this research endeavor. The first is the need for a deeper understanding of the physics of atmospheric flow in the critical zone of plant operation. The second involves science and engineering of the largest dynamic, rotating machines in the world. The third encompasses optimization and control of fleets of wind plants working synergistically within the electricity grid. Addressing these challenges could enable wind power to provide as much as half of our global electricity needs and perhaps beyond.",2019.0,"P. Veers, K. Dykes, E. Lantz, Stephan Barth, C. Bottasso, O. Carlson, A. Clifton, Johney B. Green, P. Green, H. Holttinen, D. Laird, Ville Lehtomäki, J. Lundquist, J. Lundquist, J. Manwell, M. Marquis, C. Meneveau, P. Moriarty, X. Munduate, M. Muskulus, J. Naughton, L. Pao, J. Paquette, J. Peinke, A. Robertson, J. Rodrigo, A. Sempreviva, J. Smith, A. Tuohy, R. Wiser"
6a8f0593165afacae9e1bce383810e8da4af820e,https://www.semanticscholar.org/paper/6a8f0593165afacae9e1bce383810e8da4af820e,THE ELEVENTH AND TWELFTH DATA RELEASES OF THE SLOAN DIGITAL SKY SURVEY: FINAL DATA FROM SDSS-III,"The third generation of the Sloan Digital Sky Survey (SDSS-III) took data from 2008 to 2014 using the original SDSS wide-field imager, the original and an upgraded multi-object fiber-fed optical spectrograph, a new near-infrared high-resolution spectrograph, and a novel optical interferometer. All of the data from SDSS-III are now made public. In particular, this paper describes Data Release 11 (DR11) including all data acquired through 2013 July, and Data Release 12 (DR12) adding data acquired through 2014 July (including all data included in previous data releases), marking the end of SDSS-III observing. Relative to our previous public release (DR10), DR12 adds one million new spectra of galaxies and quasars from the Baryon Oscillation Spectroscopic Survey (BOSS) over an additional 3000 deg2 of sky, more than triples the number of H-band spectra of stars as part of the Apache Point Observatory (APO) Galactic Evolution Experiment (APOGEE), and includes repeated accurate radial velocity measurements of 5500 stars from the Multi-object APO Radial Velocity Exoplanet Large-area Survey (MARVELS). The APOGEE outputs now include the measured abundances of 15 different elements for each star. In total, SDSS-III added 5200 deg2 of ugriz imaging; 155,520 spectra of 138,099 stars as part of the Sloan Exploration of Galactic Understanding and Evolution 2 (SEGUE-2) survey; 2,497,484 BOSS spectra of 1,372,737 galaxies, 294,512 quasars, and 247,216 stars over 9376 deg2; 618,080 APOGEE spectra of 156,593 stars; and 197,040 MARVELS spectra of 5513 stars. Since its first light in 1998, SDSS has imaged over 1/3 of the Celestial sphere in five bands and obtained over five million astronomical spectra.",2015.0,"S. Alam, F. D. Albareti, C. Prieto, F. Anders, S. Anderson, B. Andrews, E. Armengaud, 'Eric Aubourg, S. Bailey, J. Bautista, R. Beaton, T. Beers, Chad F. Bender Andreas A. Berlind, F. Beutler, V. Bhardwaj, J. Bird, D. Bizyaev, C. Blake, M. Blanton, M. Blomqvist, J. Bochanski, A. Bolton, J. Bovy, A. Bradley, W. Brandt, D. Brauer, J. Brinkmann, P. Brown, J. Brownstein, A. Burden, É. Burtin, N. Busca, Z. Cai, D. Capozzi, A. Rosell, R. Carrera, Yen-Chi Chen, C. Chiappini, S. Chojnowski, C. Chuang, N. Clerc, J. Comparat, K. Covey, R. Croft, A. Cuesta, K. Cunha, L. Costa, N. D. Rio, J. Davenport, K. Dawson, N. D. Lee, T. Delubac, R. Deshpande, L. Dutra-Ferreira, T. Dwelly, A. Ealet, G. Ebelke, E. Edmondson, D. Eisenstein, S. Escoffier, M. Esposito, Xiaohui Fan, E. Fern'andez-Alvar, D. Feuillet, N. F. Ak, H. Finley, A. Finoguenov, K. Flaherty, S. Fleming, A. Font-Ribera, J. Foster, P. Frinchaboy, J. Galbraith-Frew, D. A. Garc'ia-Hern'andez, A. P'erez, P. Gaulme, J. Ge, R. G'enova-Santos, L. Ghezzi, B. Gillespie, L. Girardi, D. Goddard, S. Gontcho, J. G. Hern'andez, E. Grebel, J. Grieb, N. Grieves, J. Gunn, Hong Guo, P. Harding, S. Hasselquist, S. Hawley, M. Hayden, F. Hearty, S. Ho, D. Hogg, K. Holley-Bockelmann, J. Holtzman, K. Honscheid, J. Huehnerhoff, Linhua Jiang, Jennifer A. Johnson, K. Kinemuchi, D. Kirkby, F. Kitaura, M. Klaene, J. Kneib, X. Koenig, Charles R. Lam Ting-Wen Lan, D. Lang, Pierre Laurent, J. Goff, A. Leauthaud, K. Lee, Y. S. Lee, Timothy C. Licquia, Jian Liu, D. Long, M. L'opez-Corredoira, D. Lorenzo-Oliveira, S. Lucatello, B. Lundgren, R. Lupton, C. E. Mack, S. Mahadevan, M. Maia, S. Majewski, E. Malanushenko, V. Malanushenko, A. Manchado, M. Manera, Qingqing Mao, C. Maraston, R. Marchwinski, D. Margala, S. Martell, M. Martig, K. Masters, C. McBride, P. McGehee, I. McGreer, R. McMahon, B. M'enard, M. Menzel, A. Merloni, S. M'esz'aros, Adam A. Miller, Jordi Miralda-Escud'e Hironao Miyatake, A. Montero-Dorta, S. More, X. Morice-Atkinson, H. Morrison, D. Muna, A. Myers, J. Newman, M. Neyrinck, D. Nguyen, R. Nichol, D. Nidever, P. Noterdaeme, S. E. Nuza, J. O’Connell, R. O’Connell, R. O’Connell, R. Ogando, M. Olmstead, A. Oravetz, D. Oravetz, Keisuke Osumi, R. Owen, D. Padgett, N. Padmanabhan, M. Paegert, N. Palanque-Delabrouille, K. Pan, J. Parejko, Changbom Park, I. Pâris, P. Pattarakijwanich, M. Pellejero-Ibáñez, J. Pepper, W. Percival, I. P'erez-Fournon, I. P'erez-Rafols, P. Petitjean, M. Pieri, M. Pinsonneault, G. P. D. Mello, F. Prada, A. Prakash, A. Price-Whelan, M. Raddick, Mubdi Rahman, B. Reid, J. Rich, H. Rix, A. Robin, C. Rockosi, T. Rodrigues, Sergio Rodr'iguez-Rottes, N. Roe, A. Ross, N. Ross, G. Rossi, J. Ruan, J. Rubino-Mart'in, E. Rykoff, Salvador Salazar-Albornoz, M. Salvato, L. Samushia, Ariel G. S'anchez, B. Santiago, C. Sayres, R. Schiavon, D. Schlegel, S. Schmidt, D. Schneider, M. Schultheis, A. Schwope, C. Sc'occola, K. Sellgren, H. Seo, N. Shane, Yue Shen, M. Shetrone, Y. Shu, T. Sivarani, M. Skrutskie, A. Slosar, V. Smith, F. Sobreira, K. Stassun, M. Steinmetz, M. Strauss, A. Streblyanska, M. Swanson, J. Tan, J. Tayar, R. Terrien, A. Thakar, D. Thomas, B. Thompson, J. Tinker, R. Tojeiro, N. Troup, M. Vargas-Magaña, J. Vázquez, L. Verde, M. Viel, N. Vogt, D. Wake, Ji Wang, B. Weaver, D. Weinberg, B. Weiner, M. White, John C. Wilson, J. Wisniewski, W. M. Wood-Vasey, C. Yéche, D. York, N. Zakamska, O. Zamora, G. Zasowski, I. Zehavi, Gong-Bo Zhao, Zheng Zheng, Xu Zhou, Zhi-min Zhou, G. Zhu, H. Bruce, A. Cosmology, D. Physics, C. University, 5000 Forbes Ave, Pittsburgh, PA 15213, Usa, Instituto de F'isica Te'orica, U. A. D. Madrid, Cantoblanco, E. Madrid, Spain., Instituto de Astrof'isica de Canarias, C. L'actea, sn., E-38200, La Laguna, Tenerife, D. Astrof'isica, U. L. Laguna, E-38206, L. I. A. Potsdam, 16 AnderSternwarte, D. Potsdam, H Germany, D. O. Astronomy, U. Washington, Box 351580, Seattle, WA 98195, O. University, 140 West 18th Avenue, Columbus, OH 43210, Pitt Pacc, Astronomy, U. Pittsburgh, 3941 O'Hara Street, PA 15260, Cea, C. Saclay, IrfuSPP, F-91191 Gif-sur-Yvette, France., Apc, U. P. Diderot, CNRSIN2p3, CEAIrfu, O. Paris, Sorbonne Paris Cit'e, France Paris, L. B. N. Laboratory, O. Road, Berkeley, CA 94720, U. Virginia, P. O. B. 400325, Charlottesville, VA 22904-4325, Observatories of the Carnegie Institution of Washington, 813 Santa Barbara Street, Pasadena, CA 91101, Jina Center for the Evolution of the Elements, U. N. Dame, Notre Dame, IN 47907 Usa, Astrophysics, 525 Davey Laboratory, T. O. S. University, University Park, PA 16802, Center for Exoplanets, Habitable Worlds, P. S. University, V. University, VU Station 1807, Nashville, TN 37235, A. P. Observatory, 59 P.O.Box, Sunspot, NM 88349, Msc 4500, N. University, P. O. B. 30001, L. Cruces, NM 88003, U. Pennsylvania, 2. S. 3. St., Philadelphia., PA 19104, Center for Cosmology, P. Physics, N. University, 4. W. Place, New York., NY 10003, U. California, Irvine, CA 92697, R. University, 2083 Lawrenceville Road, Lawrenceville, NJ 08648, U. Utah, Salt lake City, UT 84112, Institute for Advanced Study, E. Drive, Princeton, NJ 08540, John Bahcall fellow., Institute for Gravitation, The Cosmos, P. George, C. N. Y. I. F. T. Physics, A. Texas, M. University, 4242 Tamu, College Station, TX 77843, Institute of Cosmology, Gravitation, Dennis Sciama Building, U. Portsmouth, Portsmouth, PO1 3FX, UK., O. Nacional, 77 RuaGal.Jos'eCristino, R. Janeiro, RJ - 20921-400, Brazil, Laboratorio Interinstitucional de e-Astronomia, -. LIneA, S. Observatory, 950 N. Cherry Avenue, Tucson, AZ 85721, Department of Statistics, Bruce, M. F. Physik, Postfach 1312, G. Garching, L. Observatory, 1400 W. Mars Hill Road, Flagstaff AZ 86001, Western Washington University, D. O. Astronomy, 516 High Street, Bellingham WA 98225, Institut de Ci'encies del Cosmos, Universitat de BarcelonaIEEC, Barcelona E-08028, Y. C. F. Astronomy, Yale University, New Haven, Ct, 06520, U. Florida, B. S. S. Center, Gainesville, FL 32611-2055, Geology, Northern Kentucky University, Highland Heights, KY 41099, Laboratoire d'astrophysique, 'Ecole polytechnique f'ed'erale de Lausanne, Observatoire de Sauverny, 1290, Versoix, Switzerland., U. F. R. D. Janeiro, Observat'orio do Valongo, 43 LadeiradoPedroAntonio, 20080-090 Rio de Janeiro, D. D. F'isica, Universidade Federal do Rio Grande do Norte, 59072-970, Natal, Rn, Brazil, Centre de Physique des Particules de Marseille, A. Universit'e, E-13288 Marseille, H. C. F. Astrophysics, 60 Garden Street, Cambridge MA 02138, F. O. Sciences, S. Sciences, E. University, 38039 Kayseri, Turkey, I. Paris, Upmc-Cnrs, UMR7095, 98 bis bd Arago, F-75014, París, U. Helsinki, 2. GustafHallstrominkatu, Helsinki FI-00140, Finland., Van Vleck Observatory, Wesleyan University, Middletown, CT 06459, Space Telescope Science Institute, 3700 San Martin Dr., Baltimore., MD 21218, Computer Sciences Corporation, Texas Christian University, 2800 South University Drive, F. Worth, TX 76129, Center for Computational Sciences, J. University, 3. N. C. Street, -INAF, O. Padova, 5. vicolodell'Osservatorio, I-35131 Padova, Italy., A. Rechen-Institut, Zentrum fur Astronomie der Universitat Heidelberg, Monchhofstr. 12--14"
f72d3f58ff73353978e224af348448b34d27cf7b,https://www.semanticscholar.org/paper/f72d3f58ff73353978e224af348448b34d27cf7b,Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems,"Data-driven dynamical systems is a burgeoning field-it connects how measurements of nonlinear dynamical systems and/or complex systems can be used with well-established methods in dynamical systems theory. This is a critically important new direction because the governing equations of many problems under consideration by practitioners in various scientific fields are not typically known. Thus, using data alone to help derive, in an optimal sense, the best dynamical system representation of a given application allows for important new insights. The recently developed dynamic mode decomposition (DMD) is an innovative tool for integrating data with dynamical systems theory. The DMD has deep connections with traditional dynamical systems theory and many recent innovations in compressed sensing and machine learning. Dynamic Mode Decomposition: Data-Driven Modeling of Complex Systems, the first book to address the DMD algorithm, presents a pedagogical and comprehensive approach to all aspects of DMD currently developed or under development; blends theoretical development, example codes, and applications to showcase the theory and its many innovations and uses; highlights the numerous innovations around the DMD algorithm and demonstrates its efficacy using example problems from engineering and the physical and biological sciences; and provides extensive MATLAB code, data for intuitive examples of key methods, and graphical presentations. Audience: The core audience for this book is engineers and applied mathematicians working in the physical and biological sciences. It can be used in courses that integrate data analysis with dynamical systems.",2016.0,"J. Kutz, S. Brunton, Bingni W. Brunton, J. Proctor"
2711117464ccbb23a310b9de727c9bcfec86ba2e,https://www.semanticscholar.org/paper/2711117464ccbb23a310b9de727c9bcfec86ba2e,Geneious Basic: An integrated and extendable desktop software platform for the organization and analysis of sequence data,"Summary: The two main functions of bioinformatics are the organization and analysis of biological data using computational resources. Geneious Basic has been designed to be an easy-to-use and flexible desktop software application framework for the organization and analysis of biological data, with a focus on molecular sequences and related data types. It integrates numerous industry-standard discovery analysis tools, with interactive visualizations to generate publication-ready images. One key contribution to researchers in the life sciences is the Geneious public application programming interface (API) that affords the ability to leverage the existing framework of the Geneious Basic software platform for virtually unlimited extension and customization. The result is an increase in the speed and quality of development of computation tools for the life sciences, due to the functionality and graphical user interface available to the developer through the public API. Geneious Basic represents an ideal platform for the bioinformatics community to leverage existing components and to integrate their own specific requirements for the discovery, analysis and visualization of biological data. Availability and implementation: Binaries and public API freely available for download at http://www.geneious.com/basic, implemented in Java and supported on Linux, Apple OSX and MS Windows. The software is also available from the Bio-Linux package repository at http://nebc.nerc.ac.uk/news/geneiousonbl. Contact: peter@biomatters.com",2012.0,"Matthew D. Kearse, R. Moir, Amy Wilson, Steven Stones-Havas, M. Cheung, S. Sturrock, S. Buxton, A. Cooper, S. Markowitz, Chris Duran, T. Thierer, Bruce Ashton, Peter L. Meintjes, A. Drummond"
656fc9ee80901678859bbf310b92118bb4a21a07,https://www.semanticscholar.org/paper/656fc9ee80901678859bbf310b92118bb4a21a07,Coding qualitative data: a synthesis guiding the novice,"Qualitative research has gained in importance in the social sciences. General knowledge about qualitative data analysis, how to code qualitative data and decisions concerning related research design in the analytical process are all important for novice researchers. This article offers researchers who are new to qualitative research a thorough yet practical introduction to the vocabulary and craft of coding.",2020.0,M. S. Linneberg
35aab48e1045740b1a8b3992e541f51f624130bc,https://www.semanticscholar.org/paper/35aab48e1045740b1a8b3992e541f51f624130bc,Solving inverse problems using data-driven models,"Recent research in inverse problems seeks to develop a mathematically coherent foundation for combining data-driven models, and in particular those based on deep learning, with domain-specific knowledge contained in physical–analytical models. The focus is on solving ill-posed inverse problems that are at the core of many challenging applications in the natural sciences, medicine and life sciences, as well as in engineering and industrial applications. This survey paper aims to give an account of some of the main contributions in data-driven inverse problems.",2019.0,"S. Arridge, P. Maass, O. Öktem, C. Schönlieb"
0f07c5ce71d2e4d8940821ce16642e599fa789d3,https://www.semanticscholar.org/paper/0f07c5ce71d2e4d8940821ce16642e599fa789d3,CNSA: a data repository for archiving omics data,"With the application and development of high-throughput sequencing technology in life and health sciences, massive multi-dimensional biological data brings the problem of efficient management and utilization. Database development and biocuration are the prerequisites for the reuse of these big data. Here, relying on China National GeneBank (CNGB), we present CNGB Sequence Archive (CNSA) for archiving omics data, including raw sequencing data and its analytical data and related metadata which are organized into six objects, namely Project, Sample, Experiment, Run, Assembly, and Variation at present. Moreover, CNSA has created the correlation model of living samples, sample information, and analytical data on some projects, so that all data can be traced throughout the life cycle from the living sample to the sample information to the analytical data. Complying with the data standards commonly used in the life sciences, CNSA is committed to building a comprehensive and curated data repository for the storage, management and sharing of omics data, improving the data standards, and providing free access to open data resources for worldwide scientific communities to support academic research and the bio-industry. Database URL: https://db.cngb.org/cnsa/",2020.0,"Xueqin Guo, Fengzhen Chen, Fei Gao, Ling Li, Ke Liu, L. You, Cong Hua, Fan Yang, Wanliang Liu, Chunhua Peng, Lina Wang, Xiao-xia Yang, Feiyu Zhou, Jiawei Tong, Jia Cai, Zhiyong Li, Bo Wan, Lei Zhang, Tao Yang, Minwen Zhang, Linlin Yang, Yawen Yang, W. Zeng, Bo Wang, Xiaofeng Wei, Xun Xu"
d5424a5f93f9d410cf17c5393eb5e6a1821dc135,https://www.semanticscholar.org/paper/d5424a5f93f9d410cf17c5393eb5e6a1821dc135,Observational Health Data Sciences and Informatics (OHDSI): Opportunities for Observational Researchers,"The vision of creating accessible, reliable clinical evidence by accessing the clincial experience of hundreds of millions of patients across the globe is a reality. Observational Health Data Sciences and Informatics (OHDSI) has built on learnings from the Observational Medical Outcomes Partnership to turn methods research and insights into a suite of applications and exploration tools that move the field closer to the ultimate goal of generating evidence about all aspects of healthcare to serve the needs of patients, clinicians and all other decision-makers around the world.",2015.0,"G. Hripcsak, J. Duke, N. Shah, C. Reich, V. Huser, M. Schuemie, M. Schuemie, M. Suchard, R. W. Park, I. Wong, P. Rijnbeek, J. Lei, N. Pratt, G. N. Norén, Y. Li, P. Stang, D. Madigan, P. Ryan"
0a66086a2f23ef968f65395f88cdb2f4d458923a,https://www.semanticscholar.org/paper/0a66086a2f23ef968f65395f88cdb2f4d458923a,Progressive statistics for studies in sports medicine and exercise science.,"Statistical guidelines and expert statements are now available to assist in the analysis and reporting of studies in some biomedical disciplines. We present here a more progressive resource for sample-based studies, meta-analyses, and case studies in sports medicine and exercise science. We offer forthright advice on the following controversial or novel issues: using precision of estimation for inferences about population effects in preference to null-hypothesis testing, which is inadequate for assessing clinical or practical importance; justifying sample size via acceptable precision or confidence for clinical decisions rather than via adequate power for statistical significance; showing SD rather than SEM, to better communicate the magnitude of differences in means and nonuniformity of error; avoiding purely nonparametric analyses, which cannot provide inferences about magnitude and are unnecessary; using regression statistics in validity studies, in preference to the impractical and biased limits of agreement; making greater use of qualitative methods to enrich sample-based quantitative projects; and seeking ethics approval for public access to the depersonalized raw data of a study, to address the need for more scrutiny of research and better meta-analyses. Advice on less contentious issues includes the following: using covariates in linear models to adjust for confounders, to account for individual differences, and to identify potential mechanisms of an effect; using log transformation to deal with nonuniformity of effects and error; identifying and deleting outliers; presenting descriptive, effect, and inferential statistics in appropriate formats; and contending with bias arising from problems with sampling, assignment, blinding, measurement error, and researchers' prejudices. This article should advance the field by stimulating debate, promoting innovative approaches, and serving as a useful checklist for authors, reviewers, and editors.",2009.0,"W. Hopkins, S. Marshall, A. Batterham, J. Hanin"
efa13986a1a4df6fcc379e3b40701da07b057576,https://www.semanticscholar.org/paper/efa13986a1a4df6fcc379e3b40701da07b057576,Data Mining: The Textbook,"This textbook explores the different aspects of data mining from the fundamentals to the complex data types and their applications, capturing the wide diversity of problem domains for data mining issues. It goes beyond the traditional focus on data mining problems to introduce advanced data types such as text, time series, discrete sequences, spatial data, graph data, and social networks. Until now, no single book has addressed all these topics in a comprehensive and integrated way. The chapters of this book fall into one of three categories: Fundamental chapters: Data mining has four main problems, which correspond to clustering, classification, association pattern mining, and outlier analysis. These chapters comprehensively discuss a wide variety of methods for these problems. Domain chapters: These chapters discuss the specific methods used for different domains of data such as text data, time-series data, sequence data, graph data, and spatial data. Application chapters: These chapters study important applications such as stream mining, Web mining, ranking, recommendations, social networks, and privacy preservation. The domain chapters also have an applied flavor. Appropriate for both introductory and advanced data mining courses, Data Mining: The Textbook balances mathematical details and intuition. It contains the necessary mathematical details for professors and researchers, but it is presented in a simple and intuitive style to improve accessibility for students and industrial practitioners (including those with a limited mathematical background). Numerous illustrations, examples, and exercises are included, with an emphasis on semantically interpretable examples. Praise for Data Mining: The Textbook - As I read through this book, I have already decided to use it in my classes. This is a book written by an outstanding researcher who has made fundamental contributions to data mining, in a way that is both accessible and up to date. The book is complete with theory and practical use cases. Its a must-have for students and professors alike!"" -- Qiang Yang, Chair of Computer Science and Engineering at Hong Kong University of Science and Technology""This is the most amazing and comprehensive text book on data mining. It covers not only the fundamental problems, such as clustering, classification, outliers and frequent patterns, and different data types, including text, time series, sequences, spatial data and graphs, but also various applications, such as recommenders, Web, social network and privacy. It is a great book for graduate students and researchers as well as practitioners."" -- Philip S. Yu, UIC Distinguished Professor and Wexler Chair in Information Technology at University of Illinois at Chicago",2015.0,C. Aggarwal
2309a3231cd2622e2d815f1811bdcea24de06d1f,https://www.semanticscholar.org/paper/2309a3231cd2622e2d815f1811bdcea24de06d1f,Qualitative and descriptive research: Data type versus data analysis,"Qualitative and descriptive research methods have been very common procedures for conducting research in many disciplines, including education, psychology, and social sciences. These types of research have also begun to be increasingly used in the field of second language teaching and learning. The interest in such methods, particularly in qualitative research, is motivated in part by the recognition that L2 teaching and learning is complex. To uncover this complexity, we need to not only examine how learning takes place in general or what factors affect it, but also provide more in-depth examination and understanding of individual learners and their behaviors and experiences. Qualitative and descriptive research is well suited to the study of L2 classroom teaching, where conducting tightly controlled experimental research is hardly possible, and even if controlled experimental research is conducted in such settings, the generalizability of its findings to real classroom contexts are questionable. Therefore, Language Teaching Research receives many manuscripts that report qualitative or descriptive research. The terms qualitative research and descriptive research are sometimes used interchangeably. However, a distinction can be made between the two. One fundamental characteristic of both types of research is that they involve naturalistic data. That is, they attempt to study language learning and teaching in their naturally occurring settings without any intervention or manipulation of variables. Nonetheless, these two types of research may differ in terms of their goal, degree of control, and the way the data are analyzed. The goal of descriptive research is to describe a phenomenon and its characteristics. This research is more concerned with what rather than how or why something has happened. Therefore, observation and survey tools are often used to gather data (Gall, Gall, & Borg, 2007). In such research, the data may be collected qualitatively, but it is often analyzed quantitatively, using frequencies, percentages, averages, or other statistical analyses to determine relationships. Qualitative research, however, is more holistic and often involves a rich collection of data from various sources to gain a deeper understanding of individual participants, including their opinions, perspectives, and attitudes. Qualitative research collects data qualitatively, and the method of analysis is 572747 LTR0010.1177/1362168815572747Language Teaching ResearchEditorial editorial2015",2015.0,Hossein Nassaji
0e8a5df8824feef18d44d7d3b009ef9df800e097,https://www.semanticscholar.org/paper/0e8a5df8824feef18d44d7d3b009ef9df800e097,"Genome, transcriptome and proteome: the rise of omics data and their integration in biomedical sciences","Abstract Advances in the technologies and informatics used to generate and process large biological data sets (omics data) are promoting a critical shift in the study of biomedical sciences. While genomics, transcriptomics and proteinomics, coupled with bioinformatics and biostatistics, are gaining momentum, they are still, for the most part, assessed individually with distinct approaches generating monothematic rather than integrated knowledge. As other areas of biomedical sciences, including metabolomics, epigenomics and pharmacogenomics, are moving towards the omics scale, we are witnessing the rise of inter-disciplinary data integration strategies to support a better understanding of biological systems and eventually the development of successful precision medicine. This review cuts across the boundaries between genomics, transcriptomics and proteomics, summarizing how omics data are generated, analysed and shared, and provides an overview of the current strengths and weaknesses of this global approach. This work intends to target students and researchers seeking knowledge outside of their field of expertise and fosters a leap from the reductionist to the global-integrative analytical approach in research.",2016.0,"C. Manzoni, D. Kia, J. Vandrovcova, J. Hardy, N. Wood, P. Lewis, R. Ferrari"
43e2159db4d0e66c0f07e78bac1dc0c4c69558ac,https://www.semanticscholar.org/paper/43e2159db4d0e66c0f07e78bac1dc0c4c69558ac,Science with the Cherenkov Telescope Array,"The Cherenkov Telescope Array, CTA, will be the major global observatory for very high energy gamma-ray astronomy over the next decade and beyond. The scientific potential of CTA is extremely broad: from understanding the role of relativistic cosmic particles to the search for dark matter. CTA is an explorer of the extreme universe, probing environments from the immediate neighbourhood of black holes to cosmic voids on the largest scales. Covering a huge range in photon energy from 20 GeV to 300 TeV, CTA will improve on all aspects of performance with respect to current instruments. The observatory will operate arrays on sites in both hemispheres to provide full sky coverage and will hence maximize the potential for the rarest phenomena such as very nearby supernovae, gamma-ray bursts or gravitational wave transients. With 99 telescopes on the southern site and 19 telescopes on the northern site, flexible operation will be possible, with sub-arrays available for specific tasks. CTA will have important synergies with many of the new generation of major astronomical and astroparticle observatories. Multi-wavelength and multi-messenger approaches combining CTA data with those from other instruments will lead to a deeper understanding of the broad-band non-thermal properties of target sources. The CTA Observatory will be operated as an open, proposal-driven observatory, with all data available on a public archive after a pre-defined proprietary period. Scientists from institutions worldwide have combined together to form the CTA Consortium. This Consortium has prepared a proposal for a Core Programme of highly motivated observations. The programme, encompassing approximately 40% of the available observing time over the first ten years of CTA operation, is made up of individual Key Science Projects (KSPs), which are presented in this document.",2017.0,"The Cherenkov Telescope Array Consortium B.S. Acharya, I. Agudo, I. A. Samarai, R. Alfaro, J. Alfaro, C. Alispach, R. A. Batista, J. Amans, E. Amato, G. Ambrosi, E. Antolini, L. A. Antonelli, C. Aramo, M. Araya, T. Armstrong, F. Arqueros, L. Arrabito, K. Asano, M. Ashley, Michael Backes, C. Balázs, M. Balbo, O. Ballester, J. Ballet, A. Bamba, M. Barkov, U. D. Almeida, J. Barrio, D. Bastieri, Y. Becherini, A. Belfiore, W. Benbow, D. Berge, E. Bernardini, M. G. Bernardini, M. Bernardos, K. Bernlohr, B. Bertucci, B. Biasuzzi, C. Bigongiari, A. Biland, E. Bissaldi, J. Biteau, O. Blanch, J. Blazek, C. Boisson, J. Bolmont, G. Bonanno, A. Bonardi, C. Bonavolontà, G. Bonnoli, Ž. Bošnjak, M. Bottcher, C. Braiding, J. Bregeon, A. Brill, A. Brown, P. Brun, G. Brunetti, T. Buanes, J. Buckley, V. Bugaev, R. Buhler, A. Bulgarelli, T. Bulik, M. Burton, A. Burtovoi, G. Busetto, R. Canestrari, M. Capalbi, F. Capitanio, A. Caproni, P. Caraveo, V. C'ardenas, C. Carlile, R. Carosi, E. Carquin, J. Carr, S. Casanova, E. Cascone, F. Catalani, O. Catalano, D. Cauz, M. Cerruti, P. Chadwick, S. Chaty, R. Chaves, A. Chen, X. Chen, M. Chernyakova, M. Chikawa, A. Christov, J. Chudoba, M. Cie'slar, V. Coco, S. Colafrancesco, P. Colin, V. Conforti, V. Connaughton, J. Conrad, J. Contreras, J. Cortina, A. Costa, H. Costantini, G. Cotter, S. Covino, R. Crocker, J. Cuadra, O. Cuevas, P. Cumani, A. D’Aì, F. D’Ammando, P. D’Avanzo, D. D’Urso, M. Daniel, I. Davids, B. Dawson, F. Dazzi, A. Angelis, R. Anjos, G. D. Cesare, A. D. Franco, E. M. G. D. Pino, I. Calle, R. López, B. Lotto, A. Luca, M. D. Lucia, M. Naurois, E. O. Wilhelmi, F. Palma, F. D. Persio, V. Souza, C. Deil, M. Santo, C. Delgado, D. Volpe, T. D. Girolamo, F. D. Pierro, L. Venere, C. D'iaz, C. Dib, S. Diebold, A. Djannati-Atai, A. Domínguez, D. Prester, D. Dorner, M. Doro, H. Drass, D. Dravins, G. Dubus, V. Dwarkadas, J. Ebr, C. Eckner, K. Egberts, S. Einecke, T. Ekoume, D. Elsasser, J. Ernenwein, C. Espinoza, Carmelo Evoli, M. Fairbairn, D. Falceta-Gonçalves, A. Falcone, C. Farnier, G. Fasola, E. Fedorova, S. Fegan, M. Fernández-Alonso, A. Fern'andez-Barral, G. Ferrand, M. Fesquet, M. Filipović, V. Fioretti, G. Fontaine, M. Fornasa, L. Fortson, L. Coromina, C. Fruck, Y. Fujita, Y. Fukazawa, S. Funk, M. Fussling, S. Gabici, A. Gadola, Y. Gallant, B. Garcia, R. L'opez, M. Garczarczyk, J. Gaskins, T. Gasparetto, M. Gaug, L. Gérard, G. Giavitto, N. Giglietto, P. Giommi, F. Giordano, E. Giro, M. Giroletti, A. Giuliani, J. Glicenstein, R. Gnatyk, N. Godinovic, P. Goldoni, G. G'omez-Vargas, M. Gonz'alez, J. M. Gonz'alez, D. Gotz, J. Graham, P. Grandi, J. Granot, A. Green, T. Greenshaw, S. Griffiths, S. Gunji, D. Hadasch, S. Hara, M. Hardcastle, T. Hassan, K. Hayashi, M. Hayashida, M. Heller, J. Helo, G. Hermann, J. Hinton, B. Hnatyk, W. Hofmann, J. Holder, D. Horan, J. Horandel, D. Horns, P. Horváth, T. Hovatta, M. Hrabovsky, D. Hrupec, T. Humensky, M. Hutten, M. Iarlori, T. Inada, Y. Inome, S. Inoue, T. Inoue, Y. Inoue, F. Iocco, K. Ioka, M. Iori, K. Ishio, Y. Iwamura, M. Jamrozy, P. Janeček, D. Jankowsky, P. Jean, I. Jung-Richardt, J. Juryšek, P. Kaaret, S. Karkar, H. Katagiri, U. Katz, N. Kawanaka, D. Kazanas, B. Kh'elifi, D. Kieda, S. Kimeswenger, S. Kimura, S. Kisaka, J. Knapp, J. Knodlseder, B. Koch, K. Kohri, N. Komin, K. Kosack, M. Kraus, M. Krause, F. Krauß, H. Kubo, G. K. Mezek, H. Kuroda, J. Kushida, N. Palombara, G. Lamanna, R. Lang, J. Lapington, O. L. Blanc, S. Leach, J. Lees, J. Lefaucheur, M. Oliveira, J. Lenain, R. Lico, M. Limon, E. Lindfors, T. Lohse, S. Lombardi, F. Longo, M. L'opez, R. L'opez-Coto, C.-C. Lu, F. Lucarelli, P. Luque-Escamilla, E. Lyard, M. Maccarone, G. Maier, P. Majumdar, G. Malaguti, D. Mandát, G. Maneva, M. Manganaro, S. Mangano, A. Marcowith, J. Mar'in, S. Markoff, J. Mart'i, P. Martin, M. Mart'inez, G. Mart'inez, N. Masetti, S. Masuda, G. Maurin, N. Maxted, D. Mazin, C. Medina, A. Melandri, S. Mereghetti, M. Meyer, I. Minaya, N. Mirabal, R. Mirzoyan, A. Mitchell, T. Mizuno, R. Moderski, M. Mohammed, L. Mohrmann, T. Montaruli, A. Moralejo, D. Morcuende-Parrilla, K. Mori, G. Morlino, P. Morris, A. Morselli, E. Moulin, R. Mukherjee, C. Mundell, T. Murach, H. Muraishi, K. Murase, A. Nagai, S. Nagataki, T. Nagayoshi, T. Naito, T. Nakamori, Y. Nakamura, J. Niemiec, D. Nieto, M. Nikołajuk, K. Nishijima, K. Noda, D. Nosek, B. Novosyadlyj, S. Nozaki, P. O’Brien, L. Oakes, Y. Ohira, M. Ohishi, S. Ohm, N. Okazaki, A. Okumura, R. Ong, M. Orienti, R. Orito, J. Osborne, M. Ostrowski, N. Otte, I. Oya, M. Padovani, A. Paizis, M. Palatiello, M. Palatka, R. Paoletti, J. Paredes, G. Pareschi, R. Parsons, A. Pe’er, M. Pech, G. Pedaletti, M. Perri, M. Persic, A. Petrashyk, P. Petrucci, O. Petruk, B. Peyaud, M. Pfeifer, G. Piano, A. Pisarski, S. Pita, M. Pohl, M. Polo, D. Pozo, E. Prandini, J. Prast, G. Principe, D. Prokhorov, H. Prokoph, M. Prouza, G. Puhlhofer, M. Punch, S. Purckhauer, F. Queiroz, A. Quirrenbach, S. Rainò, S. Razzaque, O. Reimer, A. Reimer, A. Reisenegger, M. Renaud, A. Rezaeian, W. Rhode, D. Ribeiro, M. Rib'o, T. Richtler, J. Rico, F. Rieger, M. Riquelme, S. Rivoire, V. Rizi, J. Rodriguez, G. R. Fernandez, J. R. V'azquez, G. Rojas, P. Romano, G. Romeo, J. Rosado, A. Rovero, G. Rowell, B. Rudak, A. Rugliancich, C. Rulten, I. Sadeh, S. Safi-Harb, T. Saito, N. Sakaki, S. Sakurai, G. Salina, M. S'anchez-Conde, H. Sandaker, A. Sandoval, P. Sangiorgi, M. Sanguillon, H. Sano, M. Santander, S. Sarkar, K. Satalecka, F. Saturni, E. Schioppa, S. Schlenstedt, M. Schneider, H. Schoorlemmer, P. Schovánek, A. Schulz, F. Schussler, U. Schwanke, E. Sciacca, S. Scuderi, I. Seitenzahl, D. Semikoz, O. Sergijenko, M. Servillat, A. Shalchi, R. Shellard, L. Sidoli, H. Siejkowski, A. Sillanpaa, G. Sironi, J. Sitarek, V. Sliusar, A. Słowikowska, H. Sol, A. Stamerra, S. Stanivc, R. Starling, L. Stawarz, S. Stefanik, M. Stephan, T. Stolarczyk, G. Stratta, U. Straumann, T. Suomijarvi, A. Supanitsky, G. Tagliaferri, H. Tajima, M. Tavani, F. Tavecchio, J. Tavernet, K. Tayabaly, L. A. Tejedor, P. Temnikov, Y. Terada, R. Terrier, T. Terzić, M. Teshima, V. Testa"
ee09b6f83d9b169f534e2acb5a0ab19face6b33e,https://www.semanticscholar.org/paper/ee09b6f83d9b169f534e2acb5a0ab19face6b33e,Data Reduction and Error Analysis for the Physical Sciences,Uncertainties in measurements probability distributions error analysis estimates of means and errors Monte Carlo techniques dependent and independent variables least-squares fit to a polynomial least-squares fit to an arbitrary function fitting composite peaks direct application of the maximum likelihood. Appendices: numerical methods matrices graphs and tables histograms and graphs computer routines in Pascal.,1969.0,P. R. Bevington
c115d436572afbe2a1f528a581bb3d7b0aa031dc,https://www.semanticscholar.org/paper/c115d436572afbe2a1f528a581bb3d7b0aa031dc,Handbook of photovoltaic science and engineering,"About the Editors. List of Contributors. Preface to the 2nd Edition. 1 Achievements and Challenges of Solar Electricity from Photovoltaics (Steven Hegedus and Antonio Luque). 1.1 The Big Picture. 1.2 What is Photovoltaics? 1.3 Photovoltaics Today. 1.4 The Great Challenge. 1.5 Trends in Technology. 1.6 Conclusions. 2 The Role of Policy in PV Industry Growth: Past, Present and Future (John Byrne and Lado Kurdgelashvili). 2.1 Introduction. 2.2 Policy Review of Selected Countries. 2.3 Policy Impact on PV Market Development. 2.4 Future PV Market Growth Scenarios. 2.5 Toward a Sustainable Future. 3 The Physics of the Solar Cell (Jeffery L. Gray). 3.1 Introduction. 3.2 Fundamental Properties of Semiconductors. 3.3 Solar Cell Fundamentals. 3.4 Additional Topics. 3.5 Summary. 4 Theoretical Limits of Photovoltaic Conversion and New-generation Solar Cells (Antonio Luque and Antonio Marti). 4.1 Introduction. 4.2 Thermodynamic Background. 4.3 Photovoltaic Converters. 4.4 The Technical Efficiency Limit for Solar Converters. 4.5 Very-high-efficiency Concepts. 4.6 Conclusions. 5 Solar Grade Silicon Feedstock (Bruno Ceccaroli and Otto Lohne). 5.1 Introduction. 5.2 Silicon. 5.3 Production of Silicon Metal/Metallurgical Grade Silicon. 5.4 Production of Polysilicon/Silicon of Electronic and Photovoltaic Grade. 5.5 Current Silicon Feedstock to Solar Cells. 5.6 Requirements of Silicon for Crystalline Solar Cells. 5.7 Routes to Solar Grade Silicon. 5.8 Conclusions. 6 Bulk Crystal Growth and Wafering for PV (Hugo Rodriguez, Ismael Guerrero, Wolfgang Koch, Arthur L. Endros, Dieter Franke, Christian Hassler, Juris P. Kalejs and H. J. Moller). 6.1 Introduction. 6.2 Bulk Monocrystalline Material. 6.3 Bulk Multicrystalline Silicon. 6.4 Wafering. 6.5 Silicon Ribbon and Foil Production. 6.6 Numerical Simulations of Crystal Growth Techniques. 6.7 Conclusions. 7 Crystalline Silicon Solar Cells and Modules (Ignacio Tobias, Carlos del Ca""nizo and Jesus Alonso). 7.1 Introduction. 7.2 Crystalline Silicon as a Photovoltaic Material. 7.3 Crystalline Silicon Solar Cells. 7.4 Manufacturing Process. 7.5 Variations to the Basic Process. 7.6 Other Industrial Approaches. 7.7 Crystalline Silicon Photovoltaic Modules. 7.8 Electrical and Optical Performance of Modules. 7.9 Field Performance of Modules. 7.10 Conclusions. 8 High-efficiency III-V Multijunction Solar Cells (D. J. Friedman, J. M. Olson and Sarah Kurtz). 8.1 Introduction. 8.2 Applications. 8.3 Physics of III-V Multijunction and Single-junction Solar Cells. 8.4 Cell Configuration. 8.5 Computation of Series-connected Device Performance. 8.6 Materials Issues Related to GaInP/GaAs/Ge Solar Cells. 8.7 Epilayer Characterization and Other Diagnostic Techniques. 8.8 Reliability and Degradation. 8.9 Future-generation Solar Cells. 8.10 Summary. 9 Space Solar Cells and Arrays (Sheila Bailey and Ryne Raffaelle). 9.1 The History of Space Solar Cells. 9.2 The Challenge for Space Solar Cells. 9.3 Silicon Solar Cells. 9.4 III-V Solar Cells. 9.5 Space Solar Arrays. 9.6 Future Cell and Array Possibilities. 9.7 Power System Figures of Merit. 9.8 Summary. 10 Photovoltaic Concentrators (Gabriel Sala and Ignacio Anton). 10.1 What is the Aim of Photovoltaic Concentration and What Does it Do? 10.2 Objectives, Limitations and Opportunities. 10.3 Typical Concentrators: an Attempt at Classification. 10.4 Concentration Optics: Thermodynamic Limits. 10.5 Factors of Merit for Concentrators in Relation to the Optics. 10.6 Photovoltaic Concentration Modules and Assemblies. 10.7 Tracking for Concentrator Systems. 10.8 Measurements of Cells, Modules and Photovoltaic Systems in Concentration. 10.9 Summary. 11 Crystalline Silicon Thin-Film Solar Cells via High-temperature and Intermediate-temperature Approaches (Armin G. Aberle and Per I. Widenborg). 11.1 Introduction. 11.2 Modelling. 11.4 Crystalline Silicon Thin-Film Solar Cells on Intermediate-T Foreign Supporting Materials. 11.5 Conclusions. 12 Amorphous Silicon-based Solar Cells (Eric A. Schiff, Steven Hegedus and Xunming Deng). 12.1 Overview. 12.2 Atomic and Electronic Structure of Hydrogenated Amorphous Silicon. 12.3 Depositing Amorphous Silicon. 12.4 Understanding a-Si pin Cells. 12.5 Multijunction Solar Cells. 12.6 Module Manufacturing. 12.7 Conclusions and Future Projections. 13 Cu(InGa)Se2 Solar Cells (William N. Shafarman, Susanne Siebentritt and Lars Stolt). 13.1 Introduction. 13.2 Material Properties. 13.3 Deposition Methods. 13.4 Junction and Device Formation. 13.5 Device Operation. 13.6 Manufacturing Issues. 13.7 The Cu(InGa)Se2 Outlook. 14 Cadmium Telluride Solar Cells (Brian E. McCandless and James R. Sites). 14.1 Introduction. 14.2 Historical Development. 14.3 CdTe Properties. 14.4 CdTe Film Deposition. 14.5 CdTe Thin Film Solar Cells. 14.6 CdTe Modules. 14.7 Future of CdTe-based Solar Cells. 15 Dye-sensitized Solar Cells (Kohjiro Hara and Shogo Mori). 15.1 Introduction. 15.2 Operating Mechanism of DSSC. 15.3 Materials. 15.4 Performance of Highly Efficient DSSCs. 15.5 Electron-transfer Processes. 15.6 New Materials. 15.7 Stability. 15.8 Approach to Commercialization. 15.9 Summary and Prospects. 16 Sunlight Energy Conversion Via Organics (Sam-Shajing Sun and Hugh O'Neill). 16.1 Principles of Organic and Polymeric Photovoltaics. 16.2 Evolution and Types of Organic and Polymeric Solar Cells. 16.3 Organic and Polymeric Solar Cell Fabrication and Characterization. 16.4 Natural Photosynthetic Sunlight Energy Conversion Systems. 16.5 Artificial Photosynthetic Systems. 16.6 Artificial Reaction Centers. 16.7 Towards Device Architectures. 16.8 Summary and Future Perspectives. 17 Transparent Conducting Oxides for Photovoltaics (Alan E. Delahoy and Sheyu Guo). 17.1 Introduction. 17.2 Survey of Materials. 17.3 Deposition Methods. 17.4 TCO Theory and Modeling: Electrical and Optical Properties and their Impact on Module Performance. 17.5 Principal Materials and Issues for Thin Film and Wafer-based PV. 17.6 Textured Films. 17.7 Measurements and Characterization Methods. 17.8 TCO Stability. 17.9 Recent Developments and Prospects. 18 Measurement and Characterization of Solar Cells and Modules (Keith Emery). 18.1 Introduction. 18.2 Rating PV Performance. 18.3 Current-Voltage Measurements. 18.4 Spectral Responsivity Measurements. 18.5 Module Qualification and Certification. 18.6 Summary. 19 PV Systems (Charles M. Whitaker, Timothy U. Townsend, Anat Razon, Raymond M. Hudson and Xavier Vallve). 19.1 Introduction: There is gold at the end of the rainbow. 19.2 System Types. 19.3 Exemplary PV Systems. 19.4 Ratings. 19.5 Key System Components. 19.6 System Design Considerations. 19.7 System Design. 19.8 Installation. 19.9 Operation and Maintenance/Monitoring. 19.10 Removal, Recycling and Remediation. 19.11 Examples. 20 Electrochemical Storage for Photovoltaics (Dirk Uwe Sauer). 20.1 Introduction. 20.2 General Concept of Electrochemical Batteries. 20.3 Typical Operation Conditions of Batteries in PV Applications. 20.4 Secondary Electrochemical Accumulators with Internal Storage. 20.5 Secondary Electrochemical Battery Systems with External Storage. 20.6 Investment and Lifetime Cost Considerations. 20.7 Conclusion. 21 Power Conditioning for Photovoltaic Power Systems (Heribert Schmidt, Bruno Burger and Jurgen Schmid). 21.1 Charge Controllers and Monitoring Systems for Batteries in PV Power Systems. 21.2 Inverters. 22 Energy Collected and Delivered by PV Modules (Eduardo Lorenzo). 22.1 Introduction. 22.2 Movement between Sun and Earth. 22.3 Solar Radiation Components. 22.4 Solar Radiation Data and Uncertainty. 22.5 Radiation on Inclined Surfaces. 22.6 Diurnal Variations of the Ambient Temperature. 22.7 Effects of the Angle of Incidence and of Dirt. 22.8 Some Calculation Tools. 22.9 Irradiation on Most Widely Studied Surfaces. 22.10 PV Generator Behaviour Under Real Operation Conditions. 22.11 Reliability and Sizing of Stand-alone PV Systems. 22.12 The Case of Solar Home Systems. 22.13 Energy Yield of Grid-connected PV Systems. 22.14 Conclusions. 23 PV in Architecture (Tjerk H. Reijenga and Henk F. Kaan). 23.1 Introduction. 23.2 PV in Architecture. 23.3 BIPV Basics. 23.4 Steps in the Design Process with PV. 23.5 Concluding Remarks. 24 Photovoltaics and Development (Jorge M. Huacuz, Jaime Agredano and Lalith Gunaratne). 24.1 Electricity and Development. 24.2 Breaking the Chains of Underdevelopment. 24.3 The PV Alternative. 24.4 Examples of PV Rural Electrification. 24.5 Toward a New Paradigm for Rural Electrification. References. Index.",2011.0,"A. Luque, S. Hegedus"
a04ce672e8838e3c73a10559b2068b93810c3f24,https://www.semanticscholar.org/paper/a04ce672e8838e3c73a10559b2068b93810c3f24,Regression Analysis of Count Data,"Students in both social and natural sciences often seek regression methods to explain the frequency of events, such as visits to a doctor, auto accidents, or new patents awarded. This book, now in its second edition, provides the most comprehensive and up-to-date account of models and methods to interpret such data. The authors combine theory and practice to make sophisticated methods of analysis accessible to researchers and practitioners working with widely different types of data and software in areas such as applied statistics, econometrics, marketing, operations research, actuarial studies, demography, biostatistics and quantitative social sciences. The new material includes new theoretical topics, an updated and expanded treatment of cross-section models, coverage of bootstrap-based and simulation-based inference, expanded treatment of time series, multivariate and panel data, expanded treatment of endogenous regressors, coverage of quantile count regression, and a new chapter on Bayesian methods.",1998.0,P. Trivedi
76b0d691be19c0796990685f31cbcde46f864858,https://www.semanticscholar.org/paper/76b0d691be19c0796990685f31cbcde46f864858,Qualitative Data Analysis with NVivo,"This straightforward, jargon-free book provides an invaluable introduction to planning and conducting qualitative data analysis with NVivo. Written by leading authorities, with over 40 years combined experience in computer-assisted analysis of qualitative and mixed-mode data, the new edition of this best selling textbook is an ideal mix of practical instruction, methodology and real world examples. Practical, clear and focused the book effectively shows how NVivo software can accommodate and assist analysis across a wide range of research questions, data types, perspectives and methodologies. It sets out: - The power and flexibility of the NVivo software - How best to use NVivo at each stage in your research project - Examples from the authors' own research and the sample data that accompanies the software, supplemented with vignettes drawn from across the social sciences - Annotated screen shots - A website with links to data, sample projects, supplementary/updated instructions, and SAGE journal content This second edition contains new chapters on handling a literature review, visualizing data, working in mixed methods and social media datasets, and approaching NVivo as a team. An insightful step-by-step guide to the messy reality of doing computer-assisted analysis, this successful book is essential reading for anyone considering using NVivo software.",2007.0,"P. Bazeley, Kristi Jackson"
85e7d96dfd5efcbb0711bcc89039ba0d1dbda64e,https://www.semanticscholar.org/paper/85e7d96dfd5efcbb0711bcc89039ba0d1dbda64e,Toward discovery science of human brain function,"Although it is being successfully implemented for exploration of the genome, discovery science has eluded the functional neuroimaging community. The core challenge remains the development of common paradigms for interrogating the myriad functional systems in the brain without the constraints of a priori hypotheses. Resting-state functional MRI (R-fMRI) constitutes a candidate approach capable of addressing this challenge. Imaging the brain during rest reveals large-amplitude spontaneous low-frequency (<0.1 Hz) fluctuations in the fMRI signal that are temporally correlated across functionally related areas. Referred to as functional connectivity, these correlations yield detailed maps of complex neural systems, collectively constituting an individual's “functional connectome.” Reproducibility across datasets and individuals suggests the functional connectome has a common architecture, yet each individual's functional connectome exhibits unique features, with stable, meaningful interindividual differences in connectivity patterns and strengths. Comprehensive mapping of the functional connectome, and its subsequent exploitation to discern genetic influences and brain–behavior relationships, will require multicenter collaborative datasets. Here we initiate this endeavor by gathering R-fMRI data from 1,414 volunteers collected independently at 35 international centers. We demonstrate a universal architecture of positive and negative functional connections, as well as consistent loci of inter-individual variability. Age and sex emerged as significant determinants. These results demonstrate that independent R-fMRI datasets can be aggregated and shared. High-throughput R-fMRI can provide quantitative phenotypes for molecular genetic studies and biomarkers of developmental and pathological processes in the brain. To initiate discovery science of brain function, the 1000 Functional Connectomes Project dataset is freely accessible at www.nitrc.org/projects/fcon_1000/.",2010.0,"B. Biswal, M. Mennes, X. Zuo, S. Gohel, C. Kelly, Steve M. Smith, C. Beckmann, Jonathan S. Adelstein, R. Buckner, S. Colcombe, A. Dogonowski, M. Ernst, Damien Fair, M. Hampson, M. Hoptman, J. Hyde, V. Kiviniemi, R. Kötter, Shi-Jiang Li, Ching-Po Lin, M. Lowe, C. Mackay, D. Madden, Kristoffer Hougaard Madsen, D. Margulies, H. Mayberg, K. Mcmahon, Christopher S. Monk, S. Mostofsky, B. Nagel, J. Pekar, S. Peltier, S. Petersen, V. Riedl, S. Rombouts, B. Rypma, B. Schlaggar, S. Schmidt, R. Seidler, G. Siegle, C. Sorg, G. Teng, J. Veijola, A. Villringer, M. Walter, Lihong Wang, X. Weng, S. Whitfield-Gabrieli, Peter Williamson, C. Windischberger, Y. Zang, Hong-Ying Zhang, F. Castellanos, M. Milham"
d8f16b07147e2cbedd8227ea5a8ef2cd7f8ae413,https://www.semanticscholar.org/paper/d8f16b07147e2cbedd8227ea5a8ef2cd7f8ae413,GeoDa: An Introduction to Spatial Data Analysis,"The development of specialized software for spatial data analysis has seen rapid growth since the lack of such tools was lamented in the late 1980s by Haining (1989) and cited as a major impediment to the adoption and use of spatial statistics by GIS researchers. Initially, attention tended to focus on conceptual issues, such as how to integrate spatial statistical methods and a GIS environment (loosely vs. tightly coupled, embedded vs. modular, etc.), and which techniques would be most fruitfully included in such a framework. Familiar reviews of these issues are represented in, among others, Anselin and Getis (1992), Goodchild et al. (1992), Fischer and Nijkamp (1993), Fotheringham and Rogerson (1993, 1994), Fischer et al. (1996), and Fischer and Getis (1997). Today, the situation is quite different, and a fairly substantial collection of spatial data analysis software is readily available, ranging from niche programs, customized scripts and extensions for commercial statistical and GIS packages, to a burgeoning open source effort using software environments such as R, Java and Python. This is exemplified by the growing contents of the software tools clearing house maintained by the U.S.- based Center for Spatially Integrated Social Science [CSISS] (see http://www.csiss.org/clearinghouse/).",2006.0,"L. Anselin, I. Syabri, Youngihn Kho"
1d4c7199e5165011175ff1e83aa2a51188834795,https://www.semanticscholar.org/paper/1d4c7199e5165011175ff1e83aa2a51188834795,Analysing Qualitative Data in Psychology,"Analysing Qualitative Data in Psychology equips students and researchers in psychology and the social sciences to carry out qualitative data analysis, focusing on four major methods (grounded theory, interpretative phenomenological analysis, discourse analysis and narrative analysis). Assuming no prior knowledge of qualitative research, chapters on the nature, assumptions and practicalities of each method are written by acknowledged experts. To help students and researchers make informed methodological choices about their own research the book addresses data collection and the writing up of research using each method, while providing a sustained comparison of the four methods, backed up with authoritative analyses using the different methods.",2016.0,"E. Lyons, A. Coyle"
75ffcc4d8fde3834b3433f0e67d5916ebfead74c,https://www.semanticscholar.org/paper/75ffcc4d8fde3834b3433f0e67d5916ebfead74c,Statistical Methods for Survival Data Analysis,"Praise for the Third Edition. . . an easy-to read introduction to survival analysis which covers the major concepts and techniques of the subject. Statistics in Medical ResearchUpdated and expanded to reflect the latest developments, Statistical Methods for Survival Data Analysis, Fourth Edition continues to deliver a comprehensive introduction to the most commonly-used methods for analyzing survival data. Authored by a uniquely well-qualified author team, the Fourth Edition is a critically acclaimed guide to statistical methods with applications in clinical trials, epidemiology, areas of business, and the social sciences. The book features many real-world examples to illustrate applications within these various fields, although special consideration is given to the study of survival data in biomedical sciences.Emphasizing the latest research and providing the most up-to-date information regarding software applications in the field, Statistical Methods for Survival Data Analysis, Fourth Edition also includes:Marginal and random effect models for analyzing correlated censored or uncensored dataMultiple types of two-sample and K-sample comparison analysisUpdated treatment of parametric methods for regression model fitting with a new focus on accelerated failure time modelsExpanded coverage of the Cox proportional hazards modelExercises at the end of each chapter to deepen knowledge of the presented materialStatistical Methods for Survival Data Analysis is an ideal text for upper-undergraduate and graduate-level courses on survival data analysis. The book is also an excellent resource for biomedical investigators, statisticians, and epidemiologists, as well as researchers in every field in which the analysis of survival data plays a role.",1994.0,Elisa T. Lee
ba8b6322ceb07449edc660cab1b019daac35b5dc,https://www.semanticscholar.org/paper/ba8b6322ceb07449edc660cab1b019daac35b5dc,Analysis of Longitudinal Data,"chemists. Commenting on the new material in the second edition (2E), which was published in 1991, Blackwood (1994) noted the predominance of citations from the chemometrics literature and commented that “references from other statistical sources are sparse.” Chemometrics had certainly arrived in a big way by 1991, and there had not been much impact from the statistical community. If the methodology has not developed greatly through the 1990s, then the applications certainly have blossomed. (See the Journal of Chemometrics or Chemometrics and Intelligent Laboratory Systems, which have a lot of papers that would work perfectly well in the pages of Technometrics.) Blackwood (1994) noted in his review of the 2E that “The mathematical and statistical theory behind factor analysis is generally well presented, but it is in the practice and application areas that the book does best” (p. 115). In the Preface, the author notes that “the introductory chapters, 1 through 5, remain unchanged” (p. ix). Why mess with a proven product? Blackwood (1994) did comment that “the book is not an easy read,” and “it requires a good deal of mathematical understanding to get through.” See the review for a complete summary of the 2E. The remainder of the book has been revised considerably. Chapter 6, formerly “Spectral Methods of Factor Analysis,” has been reorganized and retitled as “Evolutionary Methods.” It focuses on self-modeling methods and rank-annihilation factor analysis. This chapter is followed by additional material from the former Chapter 6 that has been expanded into two new chapters, “Multimode Analysis” and “Partial Least-Squares Regression.” All statisticians are quite familiar with the latter methodology, if not with some of the advanced realizations in chemistry, such as multiblock PLS, serial PLS, and multilinear PLS, that are described in this chapter. The chapter on multimode analysis carries the factor analysis tools into the arena of multiway arrays. This chapter deals with three-dimensional rank-annihilation factor analysis, simultaneous analysis, three-mode factor analysis, and PARAFAC (parallel factor analysis). The four application chapters that conclude the book continue to bear the same titles as before, but they have all been updated to incorporate the latest advances in a wide variety of disciplines where various factor analysis methodologies have been applied. Three of the chapters are focused strictly within the realm of chemistry, whereas the  nal chapter broadens the spectrum to incorporate examples from related sciences, speci cally biomedical, fuels, environmental, and food science applications. Despite the vast computational complexity of many of the methods in the book, the author continues to make no attempt to integrate statistical software into the text. There is an appendix that discusses the Toolbox for Chemical Factor Analysis, a suite of Matlab which the author apparently wrote. However, no CD-ROM is included. These programs need to be purchased. Another appendix provides the actual Matlab code for three of the programs, which are like subroutines in FORTRAN in that they need to be strung together to create a factor analysis program. There is no reference to software packages from Umetrics or other companies whose software can carry out many of the analyses in the book.",2003.0,Patrick J. Heagerty
6b5206e860f258dc3d228eb498005371c19e809c,https://www.semanticscholar.org/paper/6b5206e860f258dc3d228eb498005371c19e809c,Advancing the Science of Collaborative Problem Solving,"Collaborative problem solving (CPS) has been receiving increasing international attention because much of the complex work in the modern world is performed by teams. However, systematic education and training on CPS is lacking for those entering and participating in the workforce. In 2015, the Programme for International Student Assessment (PISA), a global test of educational progress, documented the low levels of proficiency in CPS. This result not only underscores a significant societal need but also presents an important opportunity for psychological scientists to develop, adopt, and implement theory and empirical research on CPS and to work with educators and policy experts to improve training in CPS. This article offers some directions for psychological science to participate in the growing attention to CPS throughout the world. First, it identifies the existing theoretical frameworks and empirical research that focus on CPS. Second, it provides examples of how recent technologies can automate analyses of CPS processes and assessments so that substantially larger data sets can be analyzed and so students can receive immediate feedback on their CPS performance. Third, it identifies some challenges, debates, and uncertainties in creating an infrastructure for research, education, and training in CPS. CPS education and assessment are expected to improve when supported by larger data sets and theoretical frameworks that are informed by psychological science. This will require interdisciplinary efforts that include expertise in psychological science, education, assessment, intelligent digital technologies, and policy.",2018.0,"A. Graesser, S. Fiore, Samuel Greiff, Jessica Andrews-Todd, P. Foltz, F. Hesse"
b14c291d99f32580fe987c20e52a248c62838e68,https://www.semanticscholar.org/paper/b14c291d99f32580fe987c20e52a248c62838e68,"The data revolution : big data, open data, data infrastructures & their consequences","Chapter 1: Conceptualising Data What are data? Kinds of data Data, information, knowledge, wisdom Framing data Thinking critically about databases and data infrastructures Data assemblages and the data revolution Chapter 2: Small Data, Data Infrastructures and Data Brokers Data holdings, data archives and data infrastructures Rationale for research data infrastructures The challenges of building data infrastructures The challenges of building data infrastructuresData brokers and markets Chapter 3: Open and Linked Data Open data Linked data The case for open data The economics of open data Concerns with respect to opening data Chapter 4: Big Data Volume Exhaustive Resolution and indexicality Relationality Velocity Variety Flexibility Chapter 5: Enablers and Sources of Big Data The enablers of big data Sources of big data Directed Data Automated data Volunteered data Chapter 6: Data Analytics Pre-analytics Machine learning Data mining and pattern recognition Data visualisation and visual analytics Statistical analysis Prediction, simulation and optimization Chapter 7: The Governmental and Business Rationale for Big Data Governing people Managing organisations Leveraging value and producing capital Creating better places Chapter 8: The Reframing of Science, Social Science and Humanities Research The fourth paradigm in science? The re-emergence of empiricism The fallacies of empiricism Data-driven science Computational social sciences and digital humanities Chapter 9: Technical and Organisational Issues Deserts and deluges Access Data quality, veracity and lineage Data integration and interoperability Poor analysis and ecological fallacies Skills and human resourcing Chapter 10: Ethical, Political, Social and Legal Concerns Data shadows and dataveillance Privacy Data security Profiling, social sorting and redlining Secondary uses, control creep and anticipatory governance Modes of governance and technological lock-ins Chapter 11: Making Sense of the Data Revolution Understanding data and the data revolution Researching data assemblages Final thoughts",2014.0,Rob Kitchin
d8d139a250b7d30ac917dda842096b65819f1a8e,https://www.semanticscholar.org/paper/d8d139a250b7d30ac917dda842096b65819f1a8e,Regression Models for Count Data in R,"The classical Poisson, geometric and negative binomial regression models for count data belong to the family of generalized linear models and are available at the core of the statistics toolbox in the R system for statistical computing. After reviewing the conceptual and computational features of these methods, a new implementation of zero-inflated and hurdle regression models in the functions zeroinfl() and hurdle() from the package pscl is introduced. It re-uses design and functionality of the basic R functions just as the underlying conceptual tools extend the classical models. Both model classes are able to incorporate over-dispersion and excess zeros - two problems that typically occur in count data sets in economics and the social and political sciences - better than their classical counterparts. Using cross-section data on the demand for medical care, it is illustrated how the classical as well as the zero-augmented models can be fitted, inspected and tested in practice.",2008.0,"A. Zeileis, Christian Kleiber, S. Jackman"
0140a0f2bc07b3768b2cdee91c455f9680cd45a2,https://www.semanticscholar.org/paper/0140a0f2bc07b3768b2cdee91c455f9680cd45a2,Longitudinal And Panel Data Analysis And Applications In The Social Sciences,"longitudinal and panel data: analysis and applications for longitudinal and panel data library of congress longitudinal and panel data analysis and applications in longitudinal and panel data analysis and applications in longitudinal and panel data analysis and applications in longitudinal and panel data analysis and applications in longitudinal analysis michael l. berbaum institute for ps2701 advanced methodology: longitudinal analysis course august, 2003 longitudinal and panel data: analysis and an overview of methods for the analysis of panel pol 574: quantitative analysis iv q-apsinceton longitudinal data analysis icpsr data analysis an introduction quantitative applications in home syllabus university of iowa department of 22s:162 applied generalized regression, spring 2010 poli 803: longitudinal data analysis multivariate longitudinal data analysis for actuarial growth mixture models in longitudinal research statmodel browse the table of contents, or select an option from by robert a. yaffee september 2003 portland state university biostatistics 537: longitudinal data analysis econometrics institute of information theory and automation sociology 7140—longitudinal data analysis (fall semester data analysis an introduction quantitative applications in by robert a. yaffee september 2003 sites.google analyzing panel data quantitative applications in the time series 10 panel data [read-only] thus spake vm mixture models for longitudinal analysis: applications of quantitative approaches to longitudinal research vijayamohanan pillai n thusspakevmles.wordpress an experience rating approach to insurer projected loss ratios g93.2314 longitudinal statistics course description/syllabus using panel data techniques for social science unirioja missing data quantitative applications in the social sciences download applied longitudinal data analysis: modeling sociology 7140—longitudinal data analysis (spring semester quantitative longitudinal research: references / resources",2016.0,S. Lowe
5289463e90a350b57a5f84190c456d8c38d86368,https://www.semanticscholar.org/paper/5289463e90a350b57a5f84190c456d8c38d86368,MassBank: a public repository for sharing mass spectral data for life sciences.,"MassBank is the first public repository of mass spectra of small chemical compounds for life sciences (<3000 Da). The database contains 605 electron-ionization mass spectrometry (EI-MS), 137 fast atom bombardment MS and 9276 electrospray ionization (ESI)-MS(n) data of 2337 authentic compounds of metabolites, 11 545 EI-MS and 834 other-MS data of 10,286 volatile natural and synthetic compounds, and 3045 ESI-MS(2) data of 679 synthetic drugs contributed by 16 research groups (January 2010). ESI-MS(2) data were analyzed under nonstandardized, independent experimental conditions. MassBank is a distributed database. Each research group provides data from its own MassBank data servers distributed on the Internet. MassBank users can access either all of the MassBank data or a subset of the data by specifying one or more experimental conditions. In a spectral search to retrieve mass spectra similar to a query mass spectrum, the similarity score is calculated by a weighted cosine correlation in which weighting exponents on peak intensity and the mass-to-charge ratio are optimized to the ESI-MS(2) data. MassBank also provides a merged spectrum for each compound prepared by merging the analyzed ESI-MS(2) data on an identical compound under different collision-induced dissociation conditions. Data merging has significantly improved the precision of the identification of a chemical compound by 21-23% at a similarity score of 0.6. Thus, MassBank is useful for the identification of chemical compounds and the publication of experimental data.",2010.0,"Hisayuki Horai, Masanori Arita, S. Kanaya, Yoshito Nihei, Tasuku Ikeda, Kazuhiro Suwa, Y. Ojima, Kenichi Tanaka, Satoshi Tanaka, K. Aoshima, Y. Oda, Yuji Kakazu, M. Kusano, Takayuki Tohge, Fumio Matsuda, Y. Sawada, M. Hirai, H. Nakanishi, Kazutaka Ikeda, N. Akimoto, T. Maoka, Hiroki Takahashi, T. Ara, N. Sakurai, H. Suzuki, D. Shibata, S. Neumann, T. Iida, Kenichi Tanaka, K. Funatsu, Fumito Matsuura, T. Soga, R. Taguchi, K. Saito, T. Nishioka"
ef1a0e88c2474162222a68f9eb13c996925d31ba,https://www.semanticscholar.org/paper/ef1a0e88c2474162222a68f9eb13c996925d31ba,The Danish National Hospital Register. A valuable source of data for modern health sciences.,"The Danish National Hospital Register (LPR) has collected nationwide data on all somatic hospital admissions since 1977, and since 1995 data on outpatients and emergency patients have been included as well. Numerous research projects have been undertaken in the national Danish context as well as in collaboration with international teams, and the LPR is truly a valuable source of data for health sciences, especially in epidemiology, health services research and clinical research. Nearly complete registration of somatic hospital events in Denmark is combined with ideal conditions for longterm follow-up due to the existence of a national system of unique person identification in a population of relative demographic stability. Examples of studies are provided for illustration within three main areas: I: Using LPR for surveillance of the occurrence of diseases and of surgical procedures, II: Using the Register as a sampling frame for longitudinal population based and clinical research, and III: Using the Register as a data source for monitoring outcomes. Data available from the Register as well as studies of the validity of the data are mentioned, and it is described how researchers may get access to the Register. The Danish National Hospital Register is well suited to contribute to international comparative studies with relevance for evidence-based medicine.",1999.0,"T. Andersen, M. Madsen, J. Jørgensen, L. Mellemkjoer, Jørn Olsen"
65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,https://www.semanticscholar.org/paper/65d61afd9c35b0a75d9de77c2a4a2428af0f7f7b,Big Data Analysis with Signal Processing on Graphs: Representation and processing of massive data sets with irregular structure,"Analysis and processing of very large data sets, or big data, poses a significant challenge. Massive data sets are collected and studied in numerous domains, from engineering sciences to social networks, biomolecular research, commerce, and security. Extracting valuable information from big data requires innovative approaches that efficiently process large amounts of data as well as handle and, moreover, utilize their structure. This article discusses a paradigm for large-scale data analysis based on the discrete signal processing (DSP) on graphs (DSPG). DSPG extends signal processing concepts and methodologies from the classical signal processing theory to data indexed by general graphs. Big data analysis presents several challenges to DSPG, in particular, in filtering and frequency analysis of very large data sets. We review fundamental concepts of DSPG, including graph signals and graph filters, graph Fourier transform, graph frequency, and spectrum ordering, and compare them with their counterparts from the classical signal processing theory. We then consider product graphs as a graph model that helps extend the application of DSPG methods to large data sets through efficient implementation based on parallelization and vectorization. We relate the presented framework to existing methods for large-scale data processing and illustrate it with an application to data compression.",2014.0,"A. Sandryhaila, José M. F. Moura"
f7d3f3a23c1b284a17adc93a922c56be38d221df,https://www.semanticscholar.org/paper/f7d3f3a23c1b284a17adc93a922c56be38d221df,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.",2016.0,"Kush R. Varshney, H. Alemzadeh"
8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c,https://www.semanticscholar.org/paper/8dbd57469bb32e6d57f23f5e765bf1c9ac8e080c,Sparks of Artificial General Intelligence: Early experiments with GPT-4,"Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.",2023.0,"Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, J. Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Y. Lee, Yuan-Fang Li, Scott M. Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, Yi Zhang"
21dff47a4142445f83016da0819ffe6dd2947f66,https://www.semanticscholar.org/paper/21dff47a4142445f83016da0819ffe6dd2947f66,Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI),"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.",2018.0,"Amina Adadi, M. Berrada"
4b4279db68b16e20fbc56f9d41980a950191d30a,https://www.semanticscholar.org/paper/4b4279db68b16e20fbc56f9d41980a950191d30a,"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence","From the Publisher: 
Genetic algorithms are playing an increasingly important role in studies of complex adaptive systems, ranging from adaptive agents in economic theory to the use of machine learning techniques in the design of complex devices such as aircraft turbines and integrated circuits. Adaptation in Natural and Artificial Systems is the book that initiated this field of study, presenting the theoretical foundations and exploring applications. 
In its most familiar form, adaptation is a biological process, whereby organisms evolve by rearranging genetic material to survive in environments confronting them. In this now classic work, Holland presents a mathematical model that allows for the nonlinearity of such complex interactions. He demonstrates the model's universality by applying it to economics, physiological psychology, game theory, and artificial intelligence and then outlines the way in which this approach modifies the traditional views of mathematical genetics. 
Initially applying his concepts to simply defined artificial systems with limited numbers of parameters, Holland goes on to explore their use in the study of a wide range of complex, naturally occuring processes, concentrating on systems having multiple factors that interact in nonlinear ways. Along the way he accounts for major effects of coadaptation and coevolution: the emergence of building blocks, or schemata, that are recombined and passed on to succeeding generations to provide, innovations and improvements. 
John H. Holland is Professor of Psychology and Professor of Electrical Engineering and Computer Science at the University of Michigan. He is also Maxwell Professor at the Santa Fe Institute and isDirector of the University of Michigan/Santa Fe Institute Advanced Research Program.",1992.0,J. Holland
f92922a9fe4e6bb603291249796d80d09d1fd9f3,https://www.semanticscholar.org/paper/f92922a9fe4e6bb603291249796d80d09d1fd9f3,Impact of Artificial Intelligence in Customer Journey,"The entire gamut of Customer journey is undergoing a massive transformation due to the rapid advancement of Artificial Intelligence (AI). Leveraging the power of AI , CRM & systems have refined the aspect of how businesses manage and optimize the customer journey. AI-powered systems have significant impact across various stages of the customer lifecycle by use of techniques such as machine learning to empower businesses to use systems that can analyse vast amounts of customer dataset in real-time, enabling them to gain deeper insights in customer behaviours, preferences, & sentiment. The AI-driven techniques help businesses to drive more personalized & targeted marketing campaigns, tailored recommendations, and extend efficient customer service leading ultimately to enhancing customer satisfaction and loyalty. Moreover, AI-powered systems have capabilities of offering predictive analytics which empower businesses to forecast customer behaviours and anticipate their needs. The capabilities help businesses in effective resource optimization and improve efficiency. For customer service AI-powered chatbots and virtual assistants are used to enhance engagement by providing instant responses and ability to handle resolving issues promptly.",2024.0,"Murali Krishna Pendyala, Vishnu Varma Lakkamraju"
e1d2f2a717aa03280126f87c8e5fad695f52bf7c,https://www.semanticscholar.org/paper/e1d2f2a717aa03280126f87c8e5fad695f52bf7c,Explainable Artificial Intelligence (XAI),"Explainable Artificial Intelligence (XAI) has emerged as a critical facet in the realm of machine learning and artificial intelligence, responding to the increasing complexity of models, particularly deep neural networks, and the subsequent need for transparent decision making processes. This research paper delves into the essence of XAI, unraveling its significance across diverse domains such as healthcare, finance, and criminal justice. As a countermeasure to the opacity of intricate models, the paper explores various XAI methods and techniques, including LIME and SHAP, weighing their interpretability against computational efficiency and accuracy. Through an examination of real-world applications, the research elucidates how XAI not only enhances decision-making processes but also influences user trust and acceptance in AI systems. However, the paper also scrutinizes the delicate balance between interpretability and performance, shedding light on instances where the pursuit of accuracy may compromise explain-ability. Additionally, it navigates through the current challenges and limitations in XAI, the regulatory landscape surrounding AI explain-ability, and offers insights into future trends and directions, fostering a comprehensive understanding of XAI's present state and future potential.",2023.0,"Ranu Sewada, Ashwani Jangid, Piyush Kumar, Neha Mishra"
9dafa6c5c609348b46734fc8997b93b3587fec6e,https://www.semanticscholar.org/paper/9dafa6c5c609348b46734fc8997b93b3587fec6e,Collaborating With ChatGPT: Considering the Implications of Generative Artificial Intelligence for Journalism and Media Education,Generative artificial intelligence (AI) is ushering in an era of potential transformation of journalism and media content. This essay considers one notable generative AI platform called ChatGPT made available to the public in 2022 for free use. ChatGPT allows users to enter text prompts and rapidly generates text responses drawn from its knowledge acquired via machine learning in engagement with the internet. This essay is coauthored by a human journalism and media professor in collaboration with ChatGPT. The essay demonstrates the capacity and limitations of ChatGPT and offers reflections on the implications of generative AI for journalism and media education.,2023.0,J. Pavlik
8d020275181c69e5e768c6ffc40e09710a6f54f1,https://www.semanticscholar.org/paper/8d020275181c69e5e768c6ffc40e09710a6f54f1,Experimental evidence on the productivity effects of generative artificial intelligence,"We examined the productivity effects of a generative artificial intelligence (AI) technology, the assistive chatbot ChatGPT, in the context of midlevel professional writing tasks. In a preregistered online experiment, we assigned occupation-specific, incentivized writing tasks to 453 college-educated professionals and randomly exposed half of them to ChatGPT. Our results show that ChatGPT substantially raised productivity: The average time taken decreased by 40% and output quality rose by 18%. Inequality between workers decreased, and concern and excitement about AI temporarily rose. Workers exposed to ChatGPT during the experiment were 2 times as likely to report using it in their real job 2 weeks after the experiment and 1.6 times as likely 2 months after the experiment. Description Editor’s summary Automation has historically displaced human workers in factories (e.g., automotive manufacturing) or in performing routine computational tasks. Will generative artificial intelligence (AI) tools such as ChatGPT disrupt the labor market by making educated professionals obsolete, or will these tools complement their skills and enhance productivity? Noy and Zhang examined this issue in an experiment that recruited college-educated professionals to complete incentivized writing tasks. Participants assigned to use ChatGPT were more productive, efficient, and enjoyed the tasks more. Participants with weaker skills benefited the most from ChatGPT, which carries policy implications for efforts to reduce productivity inequality through AI. —EEU The assistive chatbot ChatGPT raises productivity in professional writing tasks and reduces productivity inequality.",2023.0,"Shakked Noy, Whitney Zhang"
eac11727ef9c7c29711cb1ba82ef6f011e8ad78d,https://www.semanticscholar.org/paper/eac11727ef9c7c29711cb1ba82ef6f011e8ad78d,New Era of Artificial Intelligence in Education: Towards a Sustainable Multifaceted Revolution,"The recent high performance of ChatGPT on several standardized academic tests has thrust the topic of artificial intelligence (AI) into the mainstream conversation about the future of education. As deep learning is poised to shift the teaching paradigm, it is essential to have a clear understanding of its effects on the current education system to ensure sustainable development and deployment of AI-driven technologies at schools and universities. This research aims to investigate the potential impact of AI on education through review and analysis of the existing literature across three major axes: applications, advantages, and challenges. Our review focuses on the use of artificial intelligence in collaborative teacher–student learning, intelligent tutoring systems, automated assessment, and personalized learning. We also report on the potential negative aspects, ethical issues, and possible future routes for AI implementation in education. Ultimately, we find that the only way forward is to embrace the new technology, while implementing guardrails to prevent its abuse.",2023.0,"Firuz Kamalov, David Santandreu Calonge, Ikhlaas Gurrib"
438ea4f6becaadca82c9f9904208a423a0cfeba0,https://www.semanticscholar.org/paper/438ea4f6becaadca82c9f9904208a423a0cfeba0,Artificial Intelligence in Pharmaceutical Technology and Drug Delivery Design,"Artificial intelligence (AI) has emerged as a powerful tool that harnesses anthropomorphic knowledge and provides expedited solutions to complex challenges. Remarkable advancements in AI technology and machine learning present a transformative opportunity in the drug discovery, formulation, and testing of pharmaceutical dosage forms. By utilizing AI algorithms that analyze extensive biological data, including genomics and proteomics, researchers can identify disease-associated targets and predict their interactions with potential drug candidates. This enables a more efficient and targeted approach to drug discovery, thereby increasing the likelihood of successful drug approvals. Furthermore, AI can contribute to reducing development costs by optimizing research and development processes. Machine learning algorithms assist in experimental design and can predict the pharmacokinetics and toxicity of drug candidates. This capability enables the prioritization and optimization of lead compounds, reducing the need for extensive and costly animal testing. Personalized medicine approaches can be facilitated through AI algorithms that analyze real-world patient data, leading to more effective treatment outcomes and improved patient adherence. This comprehensive review explores the wide-ranging applications of AI in drug discovery, drug delivery dosage form designs, process optimization, testing, and pharmacokinetics/pharmacodynamics (PK/PD) studies. This review provides an overview of various AI-based approaches utilized in pharmaceutical technology, highlighting their benefits and drawbacks. Nevertheless, the continued investment in and exploration of AI in the pharmaceutical industry offer exciting prospects for enhancing drug development processes and patient care.",2023.0,"Lalitkumar K. Vora, A. Gholap, Keshava Jetha, R. Thakur, Hetvi K. Solanki, Vivek P. Chavda"
64e750a466549aa39b91799a740ea293b3d21bb5,https://www.semanticscholar.org/paper/64e750a466549aa39b91799a740ea293b3d21bb5,Artificial Intelligence in Medicine,"Artificial Intelligence in Medicine is looking for novelty in the methodological and/or theoretical content of submitted papers. Such kind of novelty has to be mainly acknowledged in the area of AI and Computer Science. Methodological papers deal with the proposal of some strategy and related methods to solve some scientific issues in specific domains. They must show, usually through an experimental evaluation, how the proposed methodology can be applied to medicine, medicallyoriented human biology, and health care, respectively. They have also to provide a comparison with other proposals, and explicitly discuss elements of novelty. Theoretical papers focus on more fundamental, general and formal topics of AI and must show the novel expected effects of the proposed solution in some medical or healthcare field.",2023.0,Andrew Cupples
712fde4644bc9c7e3a59695396a2614bbde1fcb3,https://www.semanticscholar.org/paper/712fde4644bc9c7e3a59695396a2614bbde1fcb3,CS 188 Introduction to Artificial Intelligence Fall 2023,"In order to create a rational planning agent, we need a way to mathematically express the given environment in which the agent will exist. To do this, we must formally express a search problem given our agent’s current state (its configuration within its environment), how can we arrive at a new state that satisfies its goals in the best possible way? A search problem consists of the following elements:",2023.0,"Nikhil Sharma, Regina Wang"
22ff1f6f4df0497323ac03f446cbc49463128486,https://www.semanticscholar.org/paper/22ff1f6f4df0497323ac03f446cbc49463128486,Appropriateness of Cardiovascular Disease Prevention Recommendations Obtained From a Popular Online Chat-Based Artificial Intelligence Model.,"
 This study examines the appropriateness of artificial intelligence model responses to fundamental cardiovascular disease prevention questions.
",2023.0,"Ashish Sarraju, Dennis Bruemmer, E. V. Van Iterson, L. Cho, Fatima Rodriguez, Luke Laffin"
10c64e5aaff9f70dffc8c29a577376d085e9340b,https://www.semanticscholar.org/paper/10c64e5aaff9f70dffc8c29a577376d085e9340b,A Review of the Role of Artificial Intelligence in Healthcare,"Artificial intelligence (AI) applications have transformed healthcare. This study is based on a general literature review uncovering the role of AI in healthcare and focuses on the following key aspects: (i) medical imaging and diagnostics, (ii) virtual patient care, (iii) medical research and drug discovery, (iv) patient engagement and compliance, (v) rehabilitation, and (vi) other administrative applications. The impact of AI is observed in detecting clinical conditions in medical imaging and diagnostic services, controlling the outbreak of coronavirus disease 2019 (COVID-19) with early diagnosis, providing virtual patient care using AI-powered tools, managing electronic health records, augmenting patient engagement and compliance with the treatment plan, reducing the administrative workload of healthcare professionals (HCPs), discovering new drugs and vaccines, spotting medical prescription errors, extensive data storage and analysis, and technology-assisted rehabilitation. Nevertheless, this science pitch meets several technical, ethical, and social challenges, including privacy, safety, the right to decide and try, costs, information and consent, access, and efficacy, while integrating AI into healthcare. The governance of AI applications is crucial for patient safety and accountability and for raising HCPs’ belief in enhancing acceptance and boosting significant health consequences. Effective governance is a prerequisite to precisely address regulatory, ethical, and trust issues while advancing the acceptance and implementation of AI. Since COVID-19 hit the global health system, the concept of AI has created a revolution in healthcare, and such an uprising could be another step forward to meet future healthcare needs.",2023.0,"Ahmed Al Kuwaiti, Khalid Nazer, Abdullah Al-Reedy, Shaher Z. Al-Shehri, A. Al-Muhanna, A. Subbarayalu, Dhoha Al Muhanna, Fahad A Al-Muhanna"
12c6be503e4e5b7c9cb1810152d4364f26628a8d,https://www.semanticscholar.org/paper/12c6be503e4e5b7c9cb1810152d4364f26628a8d,Data-centric Artificial Intelligence: A Survey,"
 Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of
 data-centric AI
 . The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI
",2023.0,"D. Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan Yang, Zhimeng Jiang, Shaochen Zhong, Xia Hu"
2eeff0f534d303581bc1199671600fbd04a2d01c,https://www.semanticscholar.org/paper/2eeff0f534d303581bc1199671600fbd04a2d01c,Empowering Education with Generative Artificial Intelligence Tools: Approach with an Instructional Design Matrix,"This study focuses on the potential of generative artificial intelligence tools in education, particularly through the practical application of the 4PADAFE instructional design matrix. The objective was to evaluate how these tools, in combination with the matrix, can enhance education and improve the teaching–learning process. Through surveys conducted with teachers from the University of ESPE Armed Forces who participated in the MOOC course “Generative Artificial Intelligence Tools for Education: GPT Chat Techniques”, the study explores the impact of these tools on education. The findings reveal that generative artificial intelligence tools are crucial in developing massive MOOC virtual classrooms when integrated with an instructional design matrix. The results demonstrate the potential of generative artificial intelligence tools in university education. By utilizing these tools in conjunction with an instructional design matrix, educators can design and deliver personalized and enriching educational experiences. The devices offer opportunities to enhance the teaching–learning process and tailor educational materials to individual needs, ultimately preparing students for the demands of the 21st century. The study concludes that generative artificial intelligence tools have significant potential in education. They provide innovative ways to engage students, adapt content, and promote personalized learning. Implementing the 4PADAFE instructional design matrix further enhances the effectiveness and coherence of educational activities. By embracing these technological advancements, education can stay relevant and effectively meet the digital world’s challenges.",2023.0,"Lena Ivannova Ruiz-Rojas, Patricia Acosta-Vargas, Javier De-Moreta-Llovet, Mario González-Rodríguez"
5a5e03c3c8bf5052a99f4631e874b1e608e59319,https://www.semanticscholar.org/paper/5a5e03c3c8bf5052a99f4631e874b1e608e59319,Artificial intelligence in developing countries: The impact of generative artificial intelligence (AI) technologies for development,"This paper explores the potential impact of Generative Artificial Intelligence (Generative AI) on developing countries, considering both positive and negative effects across various domains of information, culture, and industry. Generative Artificial Intelligence refers to artificial intelligence (AI) systems that generate content, such as text, audio, or video, aiming to produce novel and creative outputs based on training data. Compared to conversational artificial intelligence, generative artificial intelligence systems have the unique capability of not only providing replies but also generating the content of those responses. Recent advancements in Artificial Intelligence during the Fourth Industrial Revolution, exemplified by tools like ChatGPT, have gained popularity and reshaped content production and creation. However, the benefits of generative artificial intelligence are not equally accessible to all, especially in developing countries, where limited access to cutting-edge technologies and inadequate infrastructure pose challenges. This paper seeks to understand the potential impact of generative AI technologies on developing countries, considering economic growth, access to technology, and the potential paradigm shift in education, healthcare, and the environment. The findings emphasize the importance of providing the necessary support and infrastructure to ensure that generative AI contributes to inclusive development rather than deepening existing inequalities. The study highlights the significance of integrating Generative AI into the context of the Fourth Industrial Revolution in developing countries, where technological change is a crucial determinant of progress and equitable growth.",2023.0,"Nishith Reddy Mannuru, Sakib Shahriar, Z. A. Teel, Ting Wang, Brady D. Lund, Solomon Tijani, Chalermchai Oak Pohboon, Daniel A. Agbaji, Joy Alhassan, JaKLyn Galley, Raana Kousari, Lydia Ogbadu-Oladapo, Shubham Kumar Saurav, Aishwarya Srivastava, Sai Priya Tummuru, Sravya Uppala, Praveenkumar Vaidya"
378236591fc05e79204fd904e9f864efa31cdc74,https://www.semanticscholar.org/paper/378236591fc05e79204fd904e9f864efa31cdc74,An Overview of Artificial Intelligence Ethics,"Artificial intelligence (AI) has profoundly changed and will continue to change our lives. AI is being applied in more and more fields and scenarios such as autonomous driving, medical care, media, finance, industrial robots, and internet services. The widespread application of AI and its deep integration with the economy and society have improved efficiency and produced benefits. At the same time, it will inevitably impact the existing social order and raise ethical concerns. Ethical issues, such as privacy leakage, discrimination, unemployment, and security risks, brought about by AI systems have caused great trouble to people. Therefore, AI ethics, which is a field related to the study of ethical issues in AI, has become not only an important research topic in academia, but also an important topic of common concern for individuals, organizations, countries, and society. This article will give a comprehensive overview of this field by summarizing and analyzing the ethical risks and issues raised by AI, ethical guidelines and principles issued by different organizations, approaches for addressing ethical issues in AI, and methods for evaluating the ethics of AI. Additionally, challenges in implementing ethics in AI and some future perspectives are pointed out. We hope our work will provide a systematic and comprehensive overview of AI ethics for researchers and practitioners in this field, especially the beginners of this research discipline.",2023.0,"Changwu Huang, Zeqi Zhang, Bifei Mao, X. Yao"
7c1933359a6860fe49d15c6353a241763879e81f,https://www.semanticscholar.org/paper/7c1933359a6860fe49d15c6353a241763879e81f,"From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where","Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.",2022.0,"Imran Ahmed, Gwanggil Jeon, F. Piccialli"
dfd1d219c7e1993bef152f79b81204a828b77d21,https://www.semanticscholar.org/paper/dfd1d219c7e1993bef152f79b81204a828b77d21,"Generative artificial intelligence empowers educational reform: current status, issues, and prospects","The emergence of Chat GPT has once again sparked a wave of information revolution in generative artificial intelligence. This article provides a detailed overview of the development and technical support of generative artificial intelligence. It conducts an in-depth analysis of the current application of generative artificial intelligence in the field of education, and identifies problems in four aspects: opacity and unexplainability, data privacy and security, personalization and fairness, and effectiveness and reliability. Corresponding solutions are proposed, such as developing explainable and fair algorithms, upgrading encryption technology, and formulating relevant laws and regulations to protect data, as well as improving the quality and quantity of datasets. The article also looks ahead to the future development trends of generative artificial intelligence in education from four perspectives: personalized education, intelligent teaching, collaborative education, and virtual teaching. The aim of the study is to provide important reference value for research and practice in this field.",2023.0,"Haotian Yu, Yunyun Guo"
8ebd4ae177fb1a62298d19891fd6e45e2a5f7685,https://www.semanticscholar.org/paper/8ebd4ae177fb1a62298d19891fd6e45e2a5f7685,Artificial Intelligence A Modern Approach 3rd,"Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach",2022.0,
10f919b1a5161b560504c225cfb2d1b3a4768f80,https://www.semanticscholar.org/paper/10f919b1a5161b560504c225cfb2d1b3a4768f80,"Artificial intelligence in healthcare: past, present and future","Artificial intelligence (AI) aims to mimic human cognitive functions. It is bringing a paradigm shift to healthcare, powered by increasing availability of healthcare data and rapid progress of analytics techniques. We survey the current status of AI applications in healthcare and discuss its future. AI can be applied to various types of healthcare data (structured and unstructured). Popular AI techniques include machine learning methods for structured data, such as the classical support vector machine and neural network, and the modern deep learning, as well as natural language processing for unstructured data. Major disease areas that use AI tools include cancer, neurology and cardiology. We then review in more details the AI applications in stroke, in the three major areas of early detection and diagnosis, treatment, as well as outcome prediction and prognosis evaluation. We conclude with discussion about pioneer AI systems, such as IBM Watson, and hurdles for real-life deployment of AI.",2017.0,"F. Jiang, Yong Jiang, Hui Zhi, Yi Dong, Hao Li, Sufeng Ma, Yilong Wang, Q. Dong, Haipeng Shen, Yongjun Wang"
e8441a9d8c22f333b4092d3a95d3fbb64a36d428,https://www.semanticscholar.org/paper/e8441a9d8c22f333b4092d3a95d3fbb64a36d428,Legal and Ethical Consideration in Artificial Intelligence in Healthcare: Who Takes Responsibility?,"The legal and ethical issues that confront society due to Artificial Intelligence (AI) include privacy and surveillance, bias or discrimination, and potentially the philosophical challenge is the role of human judgment. Concerns about newer digital technologies becoming a new source of inaccuracy and data breaches have arisen as a result of its use. Mistakes in the procedure or protocol in the field of healthcare can have devastating consequences for the patient who is the victim of the error. Because patients come into contact with physicians at moments in their lives when they are most vulnerable, it is crucial to remember this. Currently, there are no well-defined regulations in place to address the legal and ethical issues that may arise due to the use of artificial intelligence in healthcare settings. This review attempts to address these pertinent issues highlighting the need for algorithmic transparency, privacy, and protection of all the beneficiaries involved and cybersecurity of associated vulnerabilities.",2022.0,"Nithesh Naik, B. Hameed, Dasharathraj K. Shetty, Dishant Swain, M. Shah, R. Paul, Kaivalya Aggarwal, Sufyan Ibrahim, Vathsala Patil, Komal Smriti, Suyog Shetty, Bhavan Prasad Rai, P. Chłosta, B. Somani"
2d93d27fb07fc43bb1e430c37f802586bc9aaf00,https://www.semanticscholar.org/paper/2d93d27fb07fc43bb1e430c37f802586bc9aaf00,Trustworthy Artificial Intelligence: A Review,"Artificial intelligence (AI) and algorithmic decision making are having a profound impact on our daily lives. These systems are vastly used in different high-stakes applications like healthcare, business, government, education, and justice, moving us toward a more algorithmic society. However, despite so many advantages of these systems, they sometimes directly or indirectly cause harm to the users and society. Therefore, it has become essential to make these systems safe, reliable, and trustworthy. Several requirements, such as fairness, explainability, accountability, reliability, and acceptance, have been proposed in this direction to make these systems trustworthy. This survey analyzes all of these different requirements through the lens of the literature. It provides an overview of different approaches that can help mitigate AI risks and increase trust and acceptance of the systems by utilizing the users and society. It also discusses existing strategies for validating and verifying these systems and the current standardization efforts for trustworthy AI. Finally, we present a holistic view of the recent advancements in trustworthy AI to help the interested researchers grasp the crucial facets of the topic efficiently and offer possible future research directions.",2022.0,"Davinder Kaur, Suleyman Uslu, Kaley J. Rittichier, A. Durresi"
a7a407968c13ced804a063259d72315a43b84f29,https://www.semanticscholar.org/paper/a7a407968c13ced804a063259d72315a43b84f29,Artificial Intelligence in Education: A Review,"The purpose of this study was to assess the impact of Artificial Intelligence (AI) on education. Premised on a narrative and framework for assessing AI identified from a preliminary analysis, the scope of the study was limited to the application and effects of AI in administration, instruction, and learning. A qualitative research approach, leveraging the use of literature review as a research design and approach was used and effectively facilitated the realization of the study purpose. Artificial intelligence is a field of study and the resulting innovations and developments that have culminated in computers, machines, and other artifacts having human-like intelligence characterized by cognitive abilities, learning, adaptability, and decision-making capabilities. The study ascertained that AI has extensively been adopted and used in education, particularly by education institutions, in different forms. AI initially took the form of computer and computer related technologies, transitioning to web-based and online intelligent education systems, and ultimately with the use of embedded computer systems, together with other technologies, the use of humanoid robots and web-based chatbots to perform instructors’ duties and functions independently or with instructors. Using these platforms, instructors have been able to perform different administrative functions, such as reviewing and grading students’ assignments more effectively and efficiently, and achieve higher quality in their teaching activities. On the other hand, because the systems leverage machine learning and adaptability, curriculum and content has been customized and personalized in line with students’ needs, which has fostered uptake and retention, thereby improving learners experience and overall quality of learning.",2020.0,"Lijia Chen, Pingping Chen, Zhijian Lin"
df5f3ffe15207eb6ae2f00f3ccc818625b9bfbe7,https://www.semanticscholar.org/paper/df5f3ffe15207eb6ae2f00f3ccc818625b9bfbe7,Artificial Intelligence and Jobs: Evidence from Online Vacancies,"We study the impact of artificial intelligence (AI) on labor markets using establishment-level data on the near universe of online vacancies in the United States from 2010 onward. There is rapid growth in AI-related vacancies over 2010–18 that is driven by establishments whose workers engage in tasks compatible with AI’s current capabilities. As these AI-exposed establishments adopt AI, they simultaneously reduce hiring in non-AI positions and change the skill requirements of remaining postings. While visible at the establishment level, the aggregate impacts of AI-labor substitution on employment and wage growth in more exposed occupations and industries is currently too small to be detectable.",2022.0,"D. Acemoglu, David Autor, J. Hazell, P. Restrepo"
ceceaabccaf61edfd1c924d419328f0c2bfe9f81,https://www.semanticscholar.org/paper/ceceaabccaf61edfd1c924d419328f0c2bfe9f81,A Proposal For The Dartmouth Summer Research Project On Artificial Intelligence,"We propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans",2022.0,"J. McCarthy, Dartmouth College, M. Minsky, Harvard University, N. Rochester, I.B.M. Corporation, C. E. Shannon"
8603193192a64f0c9943989d209e7492689045c1,https://www.semanticscholar.org/paper/8603193192a64f0c9943989d209e7492689045c1,Artificial Intelligence A Modern Approach 3rd Edition,"Artificial IntelligenceArtificial Intelligence: A Modern Approach 2Nd Ed.Introduction to Machine LearningArtificial IntelligenceArtificial Intelligence: A Modern Approach, eBook, Global EditionIntroduction to Artificial IntelligenceModern Approaches in Machine Learning and Cognitive Science: A WalkthroughArtificial Intelligence: Pearson New International EditionArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachFundamentals of the New Artificial IntelligenceMultiagent SystemsArtificial IntelligenceArtificial IntelligenceThe Hundred-page Machine Learning BookArtificial IntelligenceArtificial IntelligenceArtificial IntelligenceDistributed Artificial IntelligenceArtificial Intelligence For BeginnersParadigms of Artificial Intelligence ProgrammingHuman CompatibleHuman CompatibleARTIFICIAL INTELLIGENCEArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachDo the Right ThingArtificial IntelligenceArtificial Intelligence : a Modern ApproachArtificial IntelligenceIntelligent Help Systems for UNIXArtificial IntelligenceArtificial IntelligenceArtificial Intelligence a Modern ApproachArtificial IntelligenceArtificial IntelligenceArtificial Intelligence for Human Computer Interaction: A Modern Approach",2020.0,
ef0c62ff070a476f216fe478cc190c773f12a1f6,https://www.semanticscholar.org/paper/ef0c62ff070a476f216fe478cc190c773f12a1f6,Human Trust in Artificial Intelligence: Review of Empirical Research,Artificial intelligence (AI) characterizes a new generation of technologies capable of interacting with the environment and aiming to simulate human intelligence. The success of integrating AI into...,2020.0,"Ella Glikson, A. Woolley"
ddf4172cad889f178c2db9b1b6302b3c7d5c0147,https://www.semanticscholar.org/paper/ddf4172cad889f178c2db9b1b6302b3c7d5c0147,The potential for artificial intelligence in healthcare,"ABSTRACT The complexity and rise of data in healthcare means that artificial intelligence (AI) will increasingly be applied within the field. Several types of AI are already being employed by payers and providers of care, and life sciences companies. The key categories of applications involve diagnosis and treatment recommendations, patient engagement and adherence, and administrative activities. Although there are many instances in which AI can perform healthcare tasks as well or better than humans, implementation factors will prevent large-scale automation of healthcare professional jobs for a considerable period. Ethical issues in the application of AI to healthcare are also discussed.",2019.0,"T. Davenport, R. Kalakota"
928cd808aba140ec298508df87c5579811ff2f41,https://www.semanticscholar.org/paper/928cd808aba140ec298508df87c5579811ff2f41,Edge Intelligence: Paving the Last Mile of Artificial Intelligence With Edge Computing,"With the breakthroughs in deep learning, the recent years have witnessed a booming of artificial intelligence (AI) applications and services, spanning from personal assistant to recommendation systems to video/audio surveillance. More recently, with the proliferation of mobile computing and Internet of Things (IoT), billions of mobile and IoT devices are connected to the Internet, generating zillions bytes of data at the network edge. Driving by this trend, there is an urgent need to push the AI frontiers to the network edge so as to fully unleash the potential of the edge big data. To meet this demand, edge computing, an emerging paradigm that pushes computing tasks and services from the network core to the network edge, has been widely recognized as a promising solution. The resulted new interdiscipline, edge AI or edge intelligence (EI), is beginning to receive a tremendous amount of interest. However, research on EI is still in its infancy stage, and a dedicated venue for exchanging the recent advances of EI is highly desired by both the computer system and AI communities. To this end, we conduct a comprehensive survey of the recent research efforts on EI. Specifically, we first review the background and motivation for AI running at the network edge. We then provide an overview of the overarching architectures, frameworks, and emerging key technologies for deep learning model toward training/inference at the network edge. Finally, we discuss future research opportunities on EI. We believe that this survey will elicit escalating attentions, stimulate fruitful discussions, and inspire further research ideas on EI.",2019.0,"Zhi Zhou, Xu Chen, En Li, Liekang Zeng, Ke Luo, Junshan Zhang"
573e6814c16178186daf537b1e1a5d3c840eef2f,https://www.semanticscholar.org/paper/573e6814c16178186daf537b1e1a5d3c840eef2f,Artificial Intelligence in Service,"Artificial intelligence (AI) is increasingly reshaping service by performing various tasks, constituting a major source of innovation, yet threatening human jobs. We develop a theory of AI job replacement to address this double-edged impact. The theory specifies four intelligences required for service tasks—mechanical, analytical, intuitive, and empathetic—and lays out the way firms should decide between humans and machines for accomplishing those tasks. AI is developing in a predictable order, with mechanical mostly preceding analytical, analytical mostly preceding intuitive, and intuitive mostly preceding empathetic intelligence. The theory asserts that AI job replacement occurs fundamentally at the task level, rather than the job level, and for “lower” (easier for AI) intelligence tasks first. AI first replaces some of a service job’s tasks, a transition stage seen as augmentation, and then progresses to replace human labor entirely when it has the ability to take over all of a job’s tasks. The progression of AI task replacement from lower to higher intelligences results in predictable shifts over time in the relative importance of the intelligences for service employees. An important implication from our theory is that analytical skills will become less important, as AI takes over more analytical tasks, giving the “softer” intuitive and empathetic skills even more importance for service employees. Eventually, AI will be capable of performing even the intuitive and empathetic tasks, which enables innovative ways of human–machine integration for providing service but also results in a fundamental threat for human employment.",2018.0,"Ming-Hui Huang, R. Rust"
b4916c497d996ad21433a8fda701b6306b0854cd,https://www.semanticscholar.org/paper/b4916c497d996ad21433a8fda701b6306b0854cd,Artificial Intelligence and Life in 2030: The One Hundred Year Study on Artificial Intelligence,"In September 2016, Stanford's""One Hundred Year Study on Artificial Intelligence""project (AI100) issued the first report of its planned long-term periodic assessment of artificial intelligence (AI) and its impact on society. It was written by a panel of 17 study authors, each of whom is deeply rooted in AI research, chaired by Peter Stone of the University of Texas at Austin. The report, entitled""Artificial Intelligence and Life in 2030,""examines eight domains of typical urban settings on which AI is likely to have impact over the coming years: transportation, home and service robots, healthcare, education, public safety and security, low-resource communities, employment and workplace, and entertainment. It aims to provide the general public with a scientifically and technologically accurate portrayal of the current state of AI and its potential and to help guide decisions in industry and governments, as well as to inform research and development in the field. The charge for this report was given to the panel by the AI100 Standing Committee, chaired by Barbara Grosz of Harvard University.",2022.0,"P. Stone, R. Brooks, Erik Brynjolfsson, Ryan Calo, Oren Etzioni, G. Hager, Julia Hirschberg, Shivaram Kalyanakrishnan, Ece Kamar, Sarit Kraus, Kevin Leyton-Brown, D. Parkes, W. Press, A. Saxenian, J. Shah, Milind Tambe, Astro Teller"
e5ab21314e9c7b226dfa3c7d6f4d85d8205f878f,https://www.semanticscholar.org/paper/e5ab21314e9c7b226dfa3c7d6f4d85d8205f878f,Artificial Intelligence in Education: AIEd for Personalised Learning Pathways,"Artificial intelligence is the driving force of change focusing on the needs and demands of the student. The research explores Artificial Intelligence in Education (AIEd) for building personalised learning systems for students. The research investigates and proposes a framework for AIEd: social networking sites and chatbots, expert systems for education, intelligent mentors and agents, machine learning, personalised educational systems and virtual educational environments. These technologies help educators to develop and introduce personalised approaches to master new knowledge and develop professional competencies. The research presents a case study of AIEd implementation in education. The scholars conducted the experiment in educational establishments using artificial intelligence in the curriculum. The scholars surveyed 184 second-year students of the Institute of Pedagogy and Psychology at the Abay Kazakh National Pedagogical University and the Kuban State Technological University to collect the data. The scholars considered the collective group discussions regarding the application of artificial intelligence in education to improve the effectiveness of learning. The research identified key advantages to creating personalised learning pathways such as access to training in 24/7 mode, training in virtual contexts, adaptation of educational content to personal needs of students, real-time and regular feedback, improvements in the educational process and mental stimulations. The proposed education paradigm reflects the increasing role of artificial intelligence in socio-economic life, the social and ethical concerns artificial intelligence may pose to humanity and its role in the digitalisation of education. The current article may be used as a theoretical framework for many educational institutions planning to exploit the capabilities of artificial intelligence in their adaptation to personalized learning.",2022.0,"Olga Tapalova, N. Zhiyenbayeva, Dmitry Gura"
6fb5ca0ff6821a92b080d0654d245d2407484701,https://www.semanticscholar.org/paper/6fb5ca0ff6821a92b080d0654d245d2407484701,"The Roles of Personality Traits, AI Anxiety, and Demographic Factors in Attitudes toward Artificial Intelligence","Abstract The present study adapted the General Attitudes toward Artificial Intelligence Scale (GAAIS) to Turkish and investigated the impact of personality traits, artificial intelligence anxiety, and demographics on attitudes toward artificial intelligence. The sample consisted of 259 female (74%) and 91 male (26%) individuals aged between 18 and 51 (Mean = 24.23). Measures taken were demographics, the Ten-Item Personality Inventory, the Artificial Intelligence Anxiety Scale, and the General Attitudes toward Artificial Intelligence Scale. The Turkish GAAIS had good validity and reliability. Hierarchical Multiple Linear Regression Analyses showed that positive attitudes toward artificial intelligence were significantly predicted by the level of computer use (β = 0.139, p = 0.013), level of knowledge about artificial intelligence (β = 0.119, p = 0.029), and AI learning anxiety (β = −0.172, p = 0.004). Negative attitudes toward artificial intelligence were significantly predicted by agreeableness (β = 0.120, p = 0.019), AI configuration anxiety (β = −0.379, p < 0.001), and AI learning anxiety (β = −0.211, p < 0.001). Personality traits, AI anxiety, and demographics play important roles in attitudes toward AI. Results are discussed in light of the previous research and theoretical explanations.",2022.0,"Feridun Kaya, F. Aydın, A. Schepman, P. Rodway, Okan Yeti̇şensoy, Meva Demir Kaya"
5e2ef6abd77e9d3512e4f9ba694d7c6ad35c8db5,https://www.semanticscholar.org/paper/5e2ef6abd77e9d3512e4f9ba694d7c6ad35c8db5,Explainable Artificial Intelligence Applications in Cyber Security: State-of-the-Art in Research,"This survey presents a comprehensive review of current literature on Explainable Artificial Intelligence (XAI) methods for cyber security applications. Due to the rapid development of Internet-connected systems and Artificial Intelligence in recent years, Artificial Intelligence including Machine Learning (ML) and Deep Learning (DL) has been widely utilized in the fields of cyber security including intrusion detection, malware detection, and spam filtering. However, although Artificial Intelligence-based approaches for the detection and defense of cyber attacks and threats are more advanced and efficient compared to the conventional signature-based and rule-based cyber security strategies, most ML-based techniques and DL-based techniques are deployed in the “black-box” manner, meaning that security experts and customers are unable to explain how such procedures reach particular conclusions. The deficiencies of transparencies and interpretability of existing Artificial Intelligence techniques would decrease human users’ confidence in the models utilized for the defense against cyber attacks, especially in current situations where cyber attacks become increasingly diverse and complicated. Therefore, it is essential to apply XAI in the establishment of cyber security models to create more explainable models while maintaining high accuracy and allowing human users to comprehend, trust, and manage the next generation of cyber defense mechanisms. Although there are papers reviewing Artificial Intelligence applications in cyber security areas and the vast literature on applying XAI in many fields including healthcare, financial services, and criminal justice, the surprising fact is that there are currently no survey research articles that concentrate on XAI applications in cyber security. Therefore, the motivation behind the survey is to bridge the research gap by presenting a detailed and up-to-date survey of XAI approaches applicable to issues in the cyber security field. Our work is the first to propose a clear roadmap for navigating the XAI literature in the context of applications in cyber security.",2022.0,"Zhibo Zhang, H. A. Hamadi, E. Damiani, C. Yeun, Fatma Taher"
5d5829723fb240543ff15ffeda1f63fff47f628d,https://www.semanticscholar.org/paper/5d5829723fb240543ff15ffeda1f63fff47f628d,"Has the Future Started? The Current Growth of Artificial Intelligence, Machine Learning, and Deep Learning","In the modern era, many terms related to artificial intelligence, machine learning, and deep learning are widely used in domains such as business, healthcare, industries, and military. In these fields, the accurate prediction and analysis of data are crucial, regardless of how large the data are. However, using big data is confusing due to the rapid growth and massive development in public life, which requires a tremendous human effort in order to deal with such type of data and extract worthy information from it. Thus, the role of artificial intelligence begins in analyzing big data based on scientific techniques, especially in machine learning, whereby it can identify patterns of decision-making and reduce human intervention. In this regard, the significance role of artificial intelligence, machine learning and deep learning is growing rapidly. In this article, the authors decide to highlight these sciences by discussing how to develop and apply them in many decision-making domains. In addition, the influence of artificial intelligence in healthcare and the gains this science provides in the face of the COVID-19 pandemic are highlighted. This article concludes that these sciences have a significant impact, especially in healthcare, as well as the ability to grow and improve their methodology in decision-making. Additionally, artificial intelligence is a vital science, especially in the face of COVID-19.",2022.0,Maad M. Mijwil
36e2d3fd64ac4b9d1de81efd7db13c394f9dc95d,https://www.semanticscholar.org/paper/36e2d3fd64ac4b9d1de81efd7db13c394f9dc95d,Artificial Intelligence in Meta-optics,"Recent years have witnessed promising artificial intelligence (AI) applications in many disciplines, including optics, engineering, medicine, economics, and education. In particular, the synergy of AI and meta-optics has greatly benefited both fields. Meta-optics are advanced flat optics with novel functions and light-manipulation abilities. The optical properties can be engineered with a unique design to meet various optical demands. This review offers comprehensive coverage of meta-optics and artificial intelligence in synergy. After providing an overview of AI and meta-optics, we categorize and discuss the recent developments integrated by these two topics, namely AI for meta-optics and meta-optics for AI. The former describes how to apply AI to the research of meta-optics for design, simulation, optical information analysis, and application. The latter reports the development of the optical Al system and computation via meta-optics. This review will also provide an in-depth discussion of the challenges of this interdisciplinary field and indicate future directions. We expect that this review will inspire researchers in these fields and benefit the next generation of intelligent optical device design.",2022.0,"M. Chen, Xiaoyuan Liu, Yanni Sun, D. Tsai"
6094988c9ebe2621e6cccc7bc6f52248117e19d0,https://www.semanticscholar.org/paper/6094988c9ebe2621e6cccc7bc6f52248117e19d0,Frontiers in Artificial Intelligence and Applications,"The industrial revolution has been the main cause ever since tremendous technological advancement was observed. The ubiquitous deployment of recent information and communication technologies (ICT), namely Artificial Intelligence (AI), Internet of Things (IoT), and Blockchain technology, is hastening the world’s industrial and technological transformation. This technical aggrandizement enhances the working culture and has a favorable impact on the workplace, as per the progressivist perspective. The breakneck pace of technological advancement, as well as AI, has enabled humans to replace manual labor in various industries. As being a domain of science and technology, AI develops machines and programs for computers that are intelligent and can accomplish tasks that would normally require human intelligence abilities. This paper mainly explores the frontiers of artificial intelligence and its applications in various fields. The AI Frontiers promulgate methodical concepts that are peer-reviewed cutting-edge research on the disruptive technological revolution of Artificial Intelligence. Additionally, some key viewpoints in the field of AI have been listed along with the main frontiers, including Machine Learning (ML), Deep Learning (DL), Fuzzy Logic (FL), Natural Language Processor (NLP), and Genetic Algorithm (GA). Furthermore, this paper discussed some common AI applications and a briefing about the current scenario in the worldwide market for artificial intelligence.",2022.0,"Jaya Yadav, S. Shukla, Kanika Sharma, Nupur Soni, S. Agarwal, Prabhash Chandra Pathak"
0ca9a5ef7695fdaa65325761164c70e56739a902,https://www.semanticscholar.org/paper/0ca9a5ef7695fdaa65325761164c70e56739a902,Explainable artificial intelligence: an analytical review,"This paper provides a brief analytical review of the current state‐of‐the‐art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested.",2021.0,"P. Angelov, E. Soares, Richard Jiang, Nicholas I. Arnold, Peter M. Atkinson"
83311744b174550032cfe09cb2940703dc9c9245,https://www.semanticscholar.org/paper/83311744b174550032cfe09cb2940703dc9c9245,A Review of Artificial Intelligence (AI) in Education from 2010 to 2020,"This study provided a content analysis of studies aiming to disclose how artificial intelligence (AI) has been applied to the education sector and explore the potential research trends and challenges of AI in education. A total of 100 papers including 63 empirical papers (74 studies) and 37 analytic papers were selected from the education and educational research category of Social Sciences Citation Index database from 2010 to 2020. The content analysis showed that the research questions could be classified into development layer (classification, matching, recommendation, and deep learning), application layer (feedback, reasoning, and adaptive learning), and integration layer (affection computing, role-playing, immersive learning, and gamification). Moreover, four research trends, including Internet of Things, swarm intelligence, deep learning, and neuroscience, as well as an assessment of AI in education, were suggested for further investigation. However, we also proposed the challenges in education may be caused by AI with regard to inappropriate use of AI techniques, changing roles of teachers and students, as well as social and ethical issues. The results provide insights into an overview of the AI used for education domain, which helps to strengthen the theoretical foundation of AI in education and provides a promising channel for educators and AI engineers to carry out further collaborative research.",2021.0,"Xuesong Zhai, Xiaoyan Chu, C. Chai, M. Jong, Andreja Istenic, Michael Spector, Jia-bao Liu, Jing Yuan, Yan Li"
bd3d0238549555bd07fd25ff61b3d7e01eb02296,https://www.semanticscholar.org/paper/bd3d0238549555bd07fd25ff61b3d7e01eb02296,"Artificial intelligence, robotics, advanced technologies and human resource management: a systematic review","Abstract Although academic production in intelligent automation (e.g. artificial intelligence, robotics) has grown rapidly, we still lack a comprehensive understanding of the impacts of the utilization of these technologies in human resource management (HRM) at an organizational (firms) and individual (employees) level. This study therefore aims to systematize the academic inputs on intelligent automation so far and to clarify what are its main contributions to and challenges for HRM. In a systematic search of 13,136 potentially relevant studies published in the top HRM, international business (IB), general management (GM) and information management (IM) journals, we found 45 articles studying artificial intelligence, robotics and other advanced technologies within HRM settings. Results show that intelligent automation technologies constitute a new approach to managing employees and enhancing firm performance, thus offering several opportunities for HRM but also considerable challenges at a technological and ethical level. The impact of these technologies has been identified to concentrate on HRM strategies, namely, job replacement, human-robot/AI collaboration, decision-making and learning opportunities, and HRM activities, namely, recruiting, training and job performance. This study discusses these shifts in detail, along with the main contributions to theory and practice and directions for future research.",2021.0,"D. Vrontis, M. Christofi, V. Pereira, S. Tarba, Anna Makrides, Eleni Trichina"
be0bbf06977c4dadbf702287733187884a531b8a,https://www.semanticscholar.org/paper/be0bbf06977c4dadbf702287733187884a531b8a,"Edge Artificial Intelligence for 6G: Vision, Enabling Technologies, and Applications","The thriving of artificial intelligence (AI) applications is driving the further evolution of wireless networks. It has been envisioned that 6G will be transformative and will revolutionize the evolution of wireless from “connected things” to “connected intelligence”. However, state-of-the-art deep learning and big data analytics based AI systems require tremendous computation and communication resources, causing significant latency, energy consumption, network congestion, and privacy leakage in both of the training and inference processes. By embedding model training and inference capabilities into the network edge, edge AI stands out as a disruptive technology for 6G to seamlessly integrate sensing, communication, computation, and intelligence, thereby improving the efficiency, effectiveness, privacy, and security of 6G networks. In this paper, we shall provide our vision for scalable and trustworthy edge AI systems with integrated design of wireless communication strategies and decentralized machine learning models. New design principles of wireless networks, service-driven resource allocation optimization methods, as well as a holistic end-to-end system architecture to support edge AI will be described. Standardization, software and hardware platforms, and application scenarios are also discussed to facilitate the industrialization and commercialization of edge AI systems.",2021.0,"K. Letaief, Yuanming Shi, Jianmin Lu, Jianhua Lu"
2b6d375d8abea91d46894ebfa7051077253834d5,https://www.semanticscholar.org/paper/2b6d375d8abea91d46894ebfa7051077253834d5,Artificial intelligence in healthcare: transforming the practice of medicine,"ABSTRACT Artificial intelligence (AI) is a powerful and disruptive area of computer science, with the potential to fundamentally transform the practice of medicine and the delivery of healthcare. In this review article, we outline recent breakthroughs in the application of AI in healthcare, describe a roadmap to building effective, reliable and safe AI systems, and discuss the possible future direction of AI augmented healthcare systems.",2021.0,"Junaid Bajwa, Usman Munir, A. Nori, Bryan Williams"
3221bd430f90af7ebc033b8a10dab08fc40c8eba,https://www.semanticscholar.org/paper/3221bd430f90af7ebc033b8a10dab08fc40c8eba,Attitudes and perception of artificial intelligence in healthcare: A cross-sectional survey among patients,"Objective The attitudes about the usage of artificial intelligence in healthcare are controversial. Unlike the perception of healthcare professionals, the attitudes of patients and their companions have been of less interest so far. In this study, we aimed to investigate the perception of artificial intelligence in healthcare among this highly relevant group along with the influence of digital affinity and sociodemographic factors. Methods We conducted a cross-sectional study using a paper-based questionnaire with patients and their companions at a German tertiary referral hospital from December 2019 to February 2020. The questionnaire consisted of three sections examining (a) the respondents’ technical affinity, (b) their perception of different aspects of artificial intelligence in healthcare and (c) sociodemographic characteristics. Results From a total of 452 participants, more than 90% already read or heard about artificial intelligence, but only 24% reported good or expert knowledge. Asked on their general perception, 53.18% of the respondents rated the use of artificial intelligence in medicine as positive or very positive, but only 4.77% negative or very negative. The respondents denied concerns about artificial intelligence, but strongly agreed that artificial intelligence must be controlled by a physician. Older patients, women, persons with lower education and technical affinity were more cautious on the healthcare-related artificial intelligence usage. Conclusions German patients and their companions are open towards the usage of artificial intelligence in healthcare. Although showing only a mediocre knowledge about artificial intelligence, a majority rated artificial intelligence in healthcare as positive. Particularly, patients insist that a physician supervises the artificial intelligence and keeps ultimate responsibility for diagnosis and therapy.",2022.0,"S. Fritsch, Andrea Blankenheim, Alina Wahl, Petra Hetfeld, Oliver Maassen, Saskia Deffge, J. Kunze, R. Rossaint, Morris Riedel, G. Marx, J. Bickenbach"
38f23fe236b152cd4983c8f30d305a568afd0d3e,https://www.semanticscholar.org/paper/38f23fe236b152cd4983c8f30d305a568afd0d3e,A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.",2019.0,"Erico Tjoa, Cuntai Guan"
129c66d240883c735dbb08c8f025a6573328827b,https://www.semanticscholar.org/paper/129c66d240883c735dbb08c8f025a6573328827b,The Clinician and Dataset Shift in Artificial Intelligence.,"To the Editor: Artificial intelligence (AI) systems are now regularly being used in medical settings,1 although regulatory oversight is inconsistent and undeveloped.2,3 Safe deployment of clinical AI requires informed clinician-users, who are generally responsible for identifying and reporting emerging problems. Clinicians may also serve as administrators in governing the use of clinical AI. A natural question follows: are clinicians adequately prepared to identify circumstances in which AI systems fail to perform their intended function reliably? A major driver of AI system malfunction is known as “dataset shift.”4,5 Most clinical AI systems today use machine learning, algorithms that leverage statistical methods to learn key patterns from clinical data. Dataset shift occurs when a machine-learning system underperforms because of a mismatch between the data set with which it was developed and the data on which it is deployed.4 For example, the University of Michigan Hospital implemented the widely used sepsis-alerting model developed by Epic Systems; in April 2020, the model had to be deactivated because of spurious alerting owing to changes in patients’ demographic characteristics associated with the coronavirus disease 2019 pandemic. This was a case in which dataset shift fundamentally altered the relationship between fevers and bacterial sepsis, leading the hospital’s clinical AI governing committee (which one of the authors of this letter chairs) to decommission its use. This is an extreme example; many causes of dataset shift are more subtle. In Table 1, we present common causes of dataset shift, which we group into changes in technology (e.g., software vendors), changes in population and setting (e.g., new demographics), and changes in behavior (e.g., new reimbursement incentives); the list is not meant to be exhaustive. Successful recognition and mitigation of dataset shift require both vigilant clinicians and sound technical oversight through AI governance teams.4,5 When using an AI system, clinicians should note misalignment between the predictions of the model and their own clinical judgment, as in the sepsis example above. Clinicians who use AI systems must frequently consider whether relevant aspects of their own clinical practice are atypical or have recently changed. For their part, AI governance teams must be sure that it is easy for clinicians to report concerns about the function of AI systems and provide feedback so that the clinician who is reporting will understand that the registered concern has been noted and, if appropriate, actions to mitigate the concern have been taken. Teams must also establish AI monitoring and updating protocols that integrate technical solu-",2021.0,"S. G. Finlayson, Adarsh Subbaswamy, Karandeep Singh, John Bowers, Annabel Kupke, Jonathan Zittrain, I. Kohane, S. Saria"
9c145390e6073c96e89cf03d3df3b559f0bb0496,https://www.semanticscholar.org/paper/9c145390e6073c96e89cf03d3df3b559f0bb0496,Artificial Intelligence and Management: The Automation–Augmentation Paradox,"Taking three recent business books on artificial intelligence (AI) as a starting point, we explore the automation and augmentation concepts in the management domain. Whereas automation implies that...",2020.0,"Sebastian Raisch, Sebastian Krakowski"
e49f67fa5c946ad24afcf59699a9cacf1ca53924,https://www.semanticscholar.org/paper/e49f67fa5c946ad24afcf59699a9cacf1ca53924,"Human-Centered Artificial Intelligence: Reliable, Safe & Trustworthy","ABSTRACT Well-designed technologies that offer high levels of human control and high levels of computer automation can increase human performance, leading to wider adoption. The Human-Centered Artificial Intelligence (HCAI) framework clarifies how to (1) design for high levels of human control and high levels of computer automation so as to increase human performance, (2) understand the situations in which full human control or full computer control are necessary, and (3) avoid the dangers of excessive human control or excessive computer control. The methods of HCAI are more likely to produce designs that are Reliable, Safe & Trustworthy (RST). Achieving these goals will dramatically increase human performance, while supporting human self-efficacy, mastery, creativity, and responsibility.",2020.0,B. Shneiderman
c483beec0afae8d08f011182460095049025b8d1,https://www.semanticscholar.org/paper/c483beec0afae8d08f011182460095049025b8d1,Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey,"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.",2020.0,"Arun Das, P. Rad"
3b750e38452b3a6a555e3534178984973e69ddf9,https://www.semanticscholar.org/paper/3b750e38452b3a6a555e3534178984973e69ddf9,Artificial intelligence and machine learning in design of mechanical materials.,"Artificial intelligence, especially machine learning (ML) and deep learning (DL) algorithms, is becoming an important tool in the fields of materials and mechanical engineering, attributed to its power to predict materials properties, design de novo materials and discover new mechanisms beyond intuitions. As the structural complexity of novel materials soars, the material design problem to optimize mechanical behaviors can involve massive design spaces that are intractable for conventional methods. Addressing this challenge, ML models trained from large material datasets that relate structure, properties and function at multiple hierarchical levels have offered new avenues for fast exploration of the design spaces. The performance of a ML-based materials design approach relies on the collection or generation of a large dataset that is properly preprocessed using the domain knowledge of materials science underlying chemical and physical concepts, and a suitable selection of the applied ML model. Recent breakthroughs in ML techniques have created vast opportunities for not only overcoming long-standing mechanics problems but also for developing unprecedented materials design strategies. In this review, we first present a brief introduction of state-of-the-art ML models, algorithms and structures. Then, we discuss the importance of data collection, generation and preprocessing. The applications in mechanical property prediction, materials design and computational methods using ML-based approaches are summarized, followed by perspectives on opportunities and open challenges in this emerging and exciting field.",2021.0,"Kai Guo, Zhenze Yang, Chi-Hua Yu, M. Buehler"
cce1f4f0ffce089ce623f6d132b75fb139302b0f,https://www.semanticscholar.org/paper/cce1f4f0ffce089ce623f6d132b75fb139302b0f,Artificial Intelligence in Cancer Research and Precision Medicine.,"Artificial intelligence (AI) is rapidly reshaping cancer research and personalized clinical care. Availability of high-dimensionality datasets coupled with advances in high-performance computing, as well as innovative deep learning architectures, has led to an explosion of AI use in various aspects of oncology research. These applications range from detection and classification of cancer, to molecular characterization of tumors and their microenvironment, to drug discovery and repurposing, to predicting treatment outcomes for patients. As these advances start penetrating the clinic, we foresee a shifting paradigm in cancer care becoming strongly driven by AI. SIGNIFICANCE: AI has the potential to dramatically affect nearly all aspects of oncology-from enhancing diagnosis to personalizing treatment and discovering novel anticancer drugs. Here, we review the recent enormous progress in the application of AI to oncology, highlight limitations and pitfalls, and chart a path for adoption of AI in the cancer clinic.",2021.0,"B. Bhinder, Coryandar Gilvary, Neel S. Madhukar, O. Elemento"
6256380fca2b6039df2449a1d35727f17933316b,https://www.semanticscholar.org/paper/6256380fca2b6039df2449a1d35727f17933316b,On evaluation metrics for medical applications of artificial intelligence,"Clinicians and software developers need to understand how proposed machine learning (ML) models could improve patient care. No single metric captures all the desirable properties of a model, which is why several metrics are typically reported to summarize a model’s performance. Unfortunately, these measures are not easily understandable by many clinicians. Moreover, comparison of models across studies in an objective manner is challenging, and no tool exists to compare models using the same performance metrics. This paper looks at previous ML studies done in gastroenterology, provides an explanation of what different metrics mean in the context of binary classification in the presented studies, and gives a thorough explanation of how different metrics should be interpreted. We also release an open source web-based tool that may be used to aid in calculating the most relevant metrics presented in this paper so that other researchers and clinicians may easily incorporate them into their research.",2021.0,"S. Hicks, I. StruÌke, Vajira Lasantha Thambawita, M. Hammou, P. Halvorsen, M. Riegler, S. Parasa"
f2f2026f76ebada185531ab95808515923ea5952,https://www.semanticscholar.org/paper/f2f2026f76ebada185531ab95808515923ea5952,An Overview of Artificial Intelligence Applications for Power Electronics,"This article gives an overview of the artificial intelligence (AI) applications for power electronic systems. The three distinctive life-cycle phases, design, control, and maintenance are correlated with one or more tasks to be addressed by AI, including optimization, classification, regression, and data structure exploration. The applications of four categories of AI are discussed, which are expert system, fuzzy logic, metaheuristic method, and machine learning. More than 500 publications have been reviewed to identify the common understandings, practical implementation challenges, and research opportunities in the application of AI for power electronics. This article is accompanied by an Excel file listing the relevant publications for statistical analytics.",2020.0,"Shuai Zhao, F. Blaabjerg, Huai Wang"
7807218eba0e71259f8cdb7861c0466d0d490366,https://www.semanticscholar.org/paper/7807218eba0e71259f8cdb7861c0466d0d490366,Artificial Intelligence In,"It goes without saying that coronavirus (COVID-19) is an infectious disease and many countries are coping with its different variants. Owing to the limited medical facilities, vaccine and medical experts, need of the hour is to intelligently tackle its spread by making artificial intelligence (AI) based smart decisions for COVID-19 suspects who develop different symptoms and they are kept under observation and monitored to see the severity of the symptoms. The target of this study is to analyze COVID-19 suspects data and detect whether a suspect is a COVID-19 patient or not, and if yes, then to what extent, so that a suitable decision can be made. The decision can be categorized such that an infected person can be isolated or quarantined at home or at a facilitation center or the person can be sent to the hospital for the treatment. This target is achieved by designing a mathematical model of COVID-19 suspects in the form of a multi-criteria decision making (MCDM) model and a novel AI based technique is devised and implemented with the help of newly developed plithogenic distance and similarity measures in fuzzy environment. All findings are depicted graphically for a clear understanding and to provide an insight of the necessity and effectiveness of the proposed method. The concept and results of the proposed technique make it suitable for implementation in machine learning, deep learning, pattern recognition etc.",,"Muhammad Rayees, Usman Ahmad, Afzal"
107169ebaa4f979572bebfe56452120440bacb7a,https://www.semanticscholar.org/paper/107169ebaa4f979572bebfe56452120440bacb7a,Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension,"The SPIRIT 2013 statement aims to improve the completeness of clinical trial protocol reporting by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI (Standard Protocol Items: Recommendations for Interventional Trials–Artificial Intelligence) extension is a new reporting guideline for clinical trial protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI (Consolidated Standards of Reporting Trials–Artificial Intelligence). Both guidelines were developed through a staged consensus process involving literature review and expert consultation to generate 26 candidate items, which were consulted upon by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed upon in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items that were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations for the handling of input and output data, the human–AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer reviewers, as well as the general readership, to understand, interpret and critically appraise the design and risk of bias for a planned clinical trial. The CONSORT-AI and SPIRIT-AI extensions improve the transparency of clinical trial design and trial protocol reporting for artificial intelligence interventions.",2020.0,"S. Cruz Rivera, Xiaoxuan Liu, A. Chan, A. Denniston, M. Calvert, Ara Christopher Christopher David Hutan Jonathan J. La Darzi Holmes Yau Moher Ashrafian Deeks Ferrante di, A. Darzi, Christopher Holmes, Christopher Yau, D. Moher, H. Ashrafian, J. J. Deeks, Lavinia Ferrante di Ruffano, Livia Faes, P. Keane, Sebastian J. Vollmer, Aaron Y. Adrian Andre Andrew L. Maria Beatrice Cecilia S Lee Jonas Esteva Beam Panico Lee Haug Kelly Yau Mu, Aaron Y. Lee, Adrian Jonas, Andre Esteva, A. L. Beam, M. Panico, Cecilia S Lee, Charlotte Haug, Christopher J. Kelly, C. Mulrow, Cyrus Espinoza, John Fletcher, Dina Paltoo, Elaine Manna, G. Price, Gary S Collins, Hugh Harvey, James Matcham, João Monteiro, M. Elzarrad, Luke Oakden-Rayner, Melissa McCradden, Richard Savage, R. Golub, Rupa Sarkar, Samuel Rowley"
5ab9776bf67a6470951a932d5f9a1beaf1cec184,https://www.semanticscholar.org/paper/5ab9776bf67a6470951a932d5f9a1beaf1cec184,Consumers and Artificial Intelligence: An Experiential Perspective,"Artificial intelligence (AI) helps companies offer important benefits to consumers, such as health monitoring with wearable devices, advice with recommender systems, peace of mind with smart household products, and convenience with voice-activated virtual assistants. However, although AI can be seen as a neutral tool to be evaluated on efficiency and accuracy, this approach does not consider the social and individual challenges that can occur when AI is deployed. This research aims to bridge these two perspectives: on one side, the authors acknowledge the value that embedding AI technology into products and services can provide to consumers. On the other side, the authors build on and integrate sociological and psychological scholarship to examine some of the costs consumers experience in their interactions with AI. In doing so, the authors identify four types of consumer experiences with AI: (1) data capture, (2) classification, (3) delegation, and (4) social. This approach allows the authors to discuss policy and managerial avenues to address the ways in which consumers may fail to experience value in organizations’ investments into AI and to lay out an agenda for future research.",2020.0,"S. Puntoni, R. W. Reczek, M. Giesler, Simona Botti"
17f423a5e542a4bd4de0243548e127038dea6ab5,https://www.semanticscholar.org/paper/17f423a5e542a4bd4de0243548e127038dea6ab5,Bias in data‐driven artificial intelligence systems—An introductory survey,"Artificial Intelligence (AI)‐based systems are widely employed nowadays to make decisions that have far‐reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well‐grounded in a legal frame. In this survey, we focus on data‐driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth.",2020.0,"Eirini Ntoutsi, P. Fafalios, U. Gadiraju, Vasileios Iosifidis, W. Nejdl, Maria-Esther Vidal, S. Ruggieri, F. Turini, S. Papadopoulos, Emmanouil Krasanakis, I. Kompatsiaris, K. Kinder-Kurlanda, Claudia Wagner, F. Karimi, Miriam Fernández, Harith Alani, Bettina Berendt, Tina Kruegel, C. Heinze, Klaus Broelemann, Gjergji Kasneci, T. Tiropanis, Steffen Staab"
152ef7762980f60ad23aa6fad59aaf7df9b82df0,https://www.semanticscholar.org/paper/152ef7762980f60ad23aa6fad59aaf7df9b82df0,Ethical and legal challenges of artificial intelligence-driven healthcare,"
 Abstract
 
 This chapter will map the ethical and legal challenges posed by artificial intelligence (AI) in healthcare and suggest directions for resolving them. Section 1 will briefly clarify what AI is and Section 2 will give an idea of the trends and strategies in the United States (US) and Europe, thereby tailoring the discussion to the ethical and legal debate of AI-driven healthcare. This will be followed in Section 3 by a discussion of four primary ethical challenges, namely, (1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy. Section 4 will then analyze five legal challenges in the US and Europe: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and (5) intellectual property law. Finally, Section 5 will summarize the major conclusions and especially emphasize the importance of building an AI-driven healthcare system that is successful and promotes trust and the motto Health AIs for All of Us.
 
",2020.0,"S. Gerke, T. Minssen, Glenn Cohen"
04c902a91806288af4c7646e95cc2c94d9f15d97,https://www.semanticscholar.org/paper/04c902a91806288af4c7646e95cc2c94d9f15d97,Recommendation of the Council on Artificial Intelligence (OECD),"On May 22, 2019, the Organisation for Economic Co-operation and Development (OECD) Ministerial Council Meeting adopted the Recommendation on Artificial Intelligence, signed by all 36 OECD member countries and non-member countries Argentina, Brazil, Columbia, Costa Rica, Peru, and Romania. Its aim is to foster innovation and trust in artificial intelligence (AI) by promoting the “responsible stewardship of trustworthy AI.”",2020.0,K. Yeung
76b1768c4185b4b6e525e797be137964ffd46cd5,https://www.semanticscholar.org/paper/76b1768c4185b4b6e525e797be137964ffd46cd5,Artificial Intelligence in Dentistry: Chances and Challenges,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.",2020.0,"F. Schwendicke, W. Samek, J. Krois"
a21be1e1774ad4a9eb2bca66de16fab23daa6e04,https://www.semanticscholar.org/paper/a21be1e1774ad4a9eb2bca66de16fab23daa6e04,Artificial Intelligence in Medicine: Today and Tomorrow,"Artificial intelligence-powered medical technologies are rapidly evolving into applicable solutions for clinical practice. Deep learning algorithms can deal with increasing amounts of data provided by wearables, smartphones, and other mobile monitoring sensors in different areas of medicine. Currently, only very specific settings in clinical practice benefit from the application of artificial intelligence, such as the detection of atrial fibrillation, epilepsy seizures, and hypoglycemia, or the diagnosis of disease based on histopathological examination or medical imaging. The implementation of augmented medicine is long-awaited by patients because it allows for a greater autonomy and a more personalized treatment, however, it is met with resistance from physicians which were not prepared for such an evolution of clinical practice. This phenomenon also creates the need to validate these modern tools with traditional clinical trials, debate the educational upgrade of the medical curriculum in light of digital medicine as well as ethical consideration of the ongoing connected monitoring. The aim of this paper is to discuss recent scientific literature and provide a perspective on the benefits, future opportunities and risks of established artificial intelligence applications in clinical practice on physicians, healthcare institutions, medical education, and bioethics.",2020.0,"G. Briganti, O. le Moine"
9f6034d02bf0766c021489bfe498488b0fd5eff5,https://www.semanticscholar.org/paper/9f6034d02bf0766c021489bfe498488b0fd5eff5,Mapping the Landscape of Artificial Intelligence Applications against COVID-19,"
 
 
COVID-19, the disease caused by the SARS-CoV-2 virus, has been declared a pandemic by the World Health Organization, which has reported over 18 million confirmed cases as of August 5, 2020. In this review, we present an overview of recent studies using Machine Learning and, more broadly, Artificial Intelligence, to tackle many aspects of the COVID19 crisis. We have identified applications that address challenges posed by COVID-19 at different scales, including: molecular, by identifying new or existing drugs for treatment; clinical, by supporting diagnosis and evaluating prognosis based on medical imaging and non-invasive measures; and societal, by tracking both the epidemic and the accompanying infodemic using multiple data sources. We also review datasets, tools, and resources needed to facilitate Artificial Intelligence research, and discuss strategic considerations related to the operational implementation of multidisciplinary partnerships and open science. We highlight the need for international cooperation to maximize the potential of AI in this and future pandemics. 
 
 
",2020.0,"Joseph Aylett-Bullock, A. Luccioni, K. H. Pham, C. Lam, M. Luengo-Oroz"
3e3df220673388402b6b114eab68a9c5396210b1,https://www.semanticscholar.org/paper/3e3df220673388402b6b114eab68a9c5396210b1,"Empowering Things With Intelligence: A Survey of the Progress, Challenges, and Opportunities in Artificial Intelligence of Things","In the Internet-of-Things (IoT) era, billions of sensors and devices collect and process data from the environment, transmit them to cloud centers, and receive feedback via the Internet for connectivity and perception. However, transmitting massive amounts of heterogeneous data, perceiving complex environments from these data, and then making smart decisions in a timely manner are difficult. Artificial intelligence (AI), especially deep learning, is now a proven success in various areas, including computer vision, speech recognition, and natural language processing. AI introduced into the IoT heralds the era of AI of things (AIoT). This article presents a comprehensive survey on AIoT to show how AI can empower the IoT to make it faster, smarter, greener, and safer. Specifically, we briefly present the AIoT architecture in the context of cloud computing, fog computing, and edge computing. Then, we present progress in AI research for IoT from four perspectives: 1) perceiving; 2) learning; 3) reasoning; and 4) behaving. Next, we summarize some promising applications of AIoT that are likely to profoundly reshape our world. Finally, we highlight the challenges facing AIoT and some potential research opportunities.",2020.0,"Jing Zhang, D. Tao"
5b34da942633f3a3afe01386341b8839a985ca0d,https://www.semanticscholar.org/paper/5b34da942633f3a3afe01386341b8839a985ca0d,Checklist for Artificial Intelligence in Medical Imaging (CLAIM): A Guide for Authors and Reviewers.,"2 Structured summary of study design, methods, results, and conclusions",2020.0,"John T Mongan, L. Moy, C. E. Kahn"
1a59ca238c64e4bae304240d7df787cf0345d50e,https://www.semanticscholar.org/paper/1a59ca238c64e4bae304240d7df787cf0345d50e,Artificial Intelligence Forecasting of Covid-19 in China,"BACKGROUND An alternative to epidemiological models for transmission dynamics of Covid-19 in China, we propose the artificial intelligence (AI)-inspired methods for real-time forecasting of Covid-19 to estimate the size, lengths and ending time of Covid-19 across China. METHODS We developed a modified stacked auto-encoder for modeling the transmission dynamics of the epidemics. We applied this model to real-time forecasting the confirmed cases of Covid-19 across China. The data were collected from January 11 to February 27, 2020 by WHO. We used the latent variables in the auto-encoder and clustering algorithms to group the provinces/cities for investigating the transmission structure. RESULTS We forecasted curves of cumulative confirmed cases of Covid-19 across China from Jan 20, 2020 to April 20, 2020. Using the multiple-step forecasting, the estimated average errors of 6-step, 7-step, 8-step, 9-step and 10-step forecasting were 1.64%, 2.27%, 2.14%, 2.08%, 0.73%, respectively. We predicted that the time points of the provinces/cities entering the plateau of the forecasted transmission dynamic curves varied, ranging from Jan 21 to April 19, 2020. The 34 provinces/cities were grouped into 9 clusters. CONCLUSIONS The accuracy of the AI-based methods for forecasting the trajectory of Covid-19 was high. We predicted that the epidemics of Covid-19 will be over by the middle of April. If the data are reliable and there are no second transmissions, we can accurately forecast the transmission dynamics of the Covid-19 across the provinces/cities in China. The AI-inspired methods are a powerful tool for helping public health planning and policymaking.",2020.0,"Zixin Hu, Qiyang Ge, Shudi Li, Li Jin, M. Xiong"
97d69e7e8c04714bf58dcbe5ae7454db69b657a7,https://www.semanticscholar.org/paper/97d69e7e8c04714bf58dcbe5ae7454db69b657a7,The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.",2020.0,G. Marcus
77a2f5515b8957163bc64d408af5683f650c3573,https://www.semanticscholar.org/paper/77a2f5515b8957163bc64d408af5683f650c3573,Innovation and Design in the Age of Artificial Intelligence,"At the heart of any innovation process lies a fundamental practice: the way people create ideas and solve problems. This “decision making” side of innovation is what scholars and practitioners refer to as “design.” Decisions in innovation processes have so far been taken by humans. What happens when they can be substituted by machines? Artificial Intelligence (AI) brings data and algorithms to the core of the innovation processes. What are the implications of this diffusion of AI for our understanding of design and innovation? Is AI just another digital technology that, akin to many others, will not significantly question what we know about design? Or will it create transformations in design that current theoretical frameworks cannot capture? This paper proposes a framework for understanding the design and innovation in the age of AI. We discuss the implications for design and innovation theory. Specifically, we observe that, as creative problem-solving is significantly conducted by algorithms, human design increasingly becomes an activity of sensemaking, that is, understanding which problems should or could be addressed. This shift in focus calls for the new theories and brings design closer to leadership, which is, inherently, an activity of sensemaking. Our insights are derived from and illustrated with two cases at the frontier of AI—Netflix and Airbnb (com-plemented with analyses of Microsoft and Tesla)—which point to two directions for the evolution of design and innovation in firms. First, AI enables an organization to overcome many past limitations of human-intensive design processes, by improving the scalability of the process, broadening its scope across traditional boundaries, and en-hancing its ability to learn and adapt on the fly. Second, and maybe more surprising, while removing these limitations, AI also appears to deeply enact several popular design principles. AI thus reinforces the principles of Design Thinking, namely: being people-centered, abductive, and iterative. In fact, AI enables the creation of solutions that are more highly user centered than human-based approaches (i.e., to an extreme level of granularity, designed for every single person); that are potentially more creative; and that are continuously updated through learning iterations across the entire product life cycle. In sum, while AI does not undermine the basic principles of design, it profoundly changes the practice of design. Problem-solving tasks, traditionally carried out by designers, are now automated into learning loops that operate without limitations of volume and speed. The algorithms embedded in these loops think in a radically different way than a designer who handles the complex problems holistically with a systemic perspective. Algorithms instead han-dle complexity through very simple tasks, which are iterated continuously. This paper discusses the implications of these insights for design and innovation management scholars and practitioners.",2020.0,"R. Verganti, Luca Vendraminelli, M. Iansiti"
f7774f83d0dca26b0403fc76912af3484eb6c4b7,https://www.semanticscholar.org/paper/f7774f83d0dca26b0403fc76912af3484eb6c4b7,XAI—Explainable artificial intelligence,"Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications. Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.",2019.0,"David Gunning, M. Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, Guang-Zhong Yang"
4a8692d75d9bb1bbc2b84cddc52521d6a03461d4,https://www.semanticscholar.org/paper/4a8692d75d9bb1bbc2b84cddc52521d6a03461d4,On the Interpretability of Artificial Intelligence in Radiology: Challenges and Opportunities.,"As artificial intelligence (AI) systems begin to make their way into clinical radiology practice, it is crucial to assure that they function correctly and that they gain the trust of experts. Toward this goal, approaches to make AI ""interpretable"" have gained attention to enhance the understanding of a machine learning algorithm, despite its complexity. This article aims to provide insights into the current state of the art of interpretability methods for radiology AI. This review discusses radiologists' opinions on the topic and suggests trends and challenges that need to be addressed to effectively streamline interpretability methods in clinical practice. Supplemental material is available for this article. © RSNA, 2020 See also the commentary by Gastounioti and Kontos in this issue.",2020.0,"M. Reyes, Raphael Meier, Sérgio Pereira, Carlos A. Silva, F. Dahlweid, H. von Tengg-Kobligk, R. Summers, R. Wiest"
2bd20336dd0b024eff47fd4b1bfd8d57b3553794,https://www.semanticscholar.org/paper/2bd20336dd0b024eff47fd4b1bfd8d57b3553794,Artificial intelligence applications in the development of autonomous vehicles: a survey,"The advancement of artificial intelligence ( AI ) has truly stimulated the development and deployment of autonomous vehicles ( AVs ) in the transportation industry. Fueled by big data from various sensing devices and advanced computing resources, AI has become an essential component of AVs for perceiving the surrounding environment and making appropriate decision in motion. To achieve goal of full automation ( i.e., self-driving ( , it is important to know how AI works in AV systems. Existing research have made great efforts in investigating different aspects of applying AI in AV development. However, few studies have offered the research community a thorough examination of current practices in implementing AI in AVs. Thus, this paper aims to shorten the gap by providing a comprehensive survey of key studies in this research avenue. Specifically, it intends to analyze their use of AIs in supporting the primary applications in AVs: 1) perception; 2) localization and mapping; and 3) decision making. It investigates the current practices to understand how AI can be used and what are the challenges and issues associated with their implementation. Based on the exploration of current practices and technology advances, this paper further provides insights into potential opportunities regarding the use of AI in conjunction with other emerging technologies: 1) high definition maps, big data, and high performance computing; 2) augmented reality( AR ) / virtual reality ( VR ) enhanced simulation platform; and 3) 5G communication for connected AVs. This paper is expected to offer a quick reference for researchers interested in understanding the use of AI in AV research.",2020.0,"Yifang Ma, Zhenyu Wang, Hong Yang, Lin Yang"
34602875dc6c40f8060d517669b80bbed5538da5,https://www.semanticscholar.org/paper/34602875dc6c40f8060d517669b80bbed5538da5,The promise of artificial intelligence: a review of the opportunities and challenges of artificial intelligence in healthcare.,"INTRODUCTION
Artificial intelligence (AI) and machine learning (ML) are rapidly evolving fields in various sectors, including healthcare. This article reviews AI's present applications in healthcare, including its benefits, limitations and future scope.


SOURCES OF DATA
A review of the English literature was conducted with search terms 'AI' or 'ML' or 'deep learning' and 'healthcare' or 'medicine' using PubMED and Google Scholar from 2000-2021.


AREAS OF AGREEMENT
AI could transform physician workflow and patient care through its applications, from assisting physicians and replacing administrative tasks to augmenting medical knowledge.


AREAS OF CONTROVERSY
From challenges training ML systems to unclear accountability, AI's implementation is difficult and incremental at best. Physicians also lack understanding of what AI implementation could represent.


GROWING POINTS
AI can ultimately prove beneficial in healthcare, but requires meticulous governance similar to the governance of physician conduct.


AREAS TIMELY FOR DEVELOPING RESEARCH
Regulatory guidelines are needed on how to safely implement and assess AI technology, alongside further research into the specific capabilities and limitations of its medical use.",2021.0,"Y. Y. Aung, D. C. Wong, D. Ting"
40cae128f445f6124cabc2fc5ce5702bea1dcabb,https://www.semanticscholar.org/paper/40cae128f445f6124cabc2fc5ce5702bea1dcabb,"Artificial Intelligence in the Industry 4.0, and Its Impact on Poverty, Innovation, Infrastructure Development, and the Sustainable Development Goals: Lessons from Emerging Economies?","Artificial intelligence in the fourth industrial revolution is beginning to live up to its promises of delivering real value necessitated by the availability of relevant data, computational ability, and algorithms. Therefore, this study sought to investigate the influence of artificial intelligence on the attainment of Sustainable Development Goals with a direct focus on poverty reduction, goal one, industry, innovation, and infrastructure development goal 9, in emerging economies. Using content analysis, the result pointed to the fact that artificial intelligence has a strong influence on the attainment of Sustainable Development Goals particularly on poverty reduction, improvement of the certainty and reliability of infrastructure like transport making economic growth and development possible in emerging economies. The results revealed that Artificial intelligence is making poverty reduction possible through improving the collection of poverty-related data through poverty maps, revolutionizing agriculture education and the finance sector through financial inclusion. The study also discovered that AI is also assisting a lot in education, and the financial sector allowing the previously excluded individuals to be able to participate in the mainstream economy. Therefore, it is important that governments in emerging economies need to invest more in the use of AI and increase the research related to it so that the Sustainable Development Goals (SDGs) related to innovation, infrastructure development, poverty reduction are attained.",2021.0,David Mhlanga
363165e6781dc79829d9e775a4bece3d9639ced6,https://www.semanticscholar.org/paper/363165e6781dc79829d9e775a4bece3d9639ced6,Human- versus Artificial Intelligence,"AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and “collaborate” with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI “partners” with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying ‘psychological’ mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.",2021.0,"J. Korteling, G. V. D. Boer-Visschedijk, R. Blankendaal, R. Boonekamp, A. Eikelboom"
4b89b8238b695b5163ad8a54a2fdf1b429df1cba,https://www.semanticscholar.org/paper/4b89b8238b695b5163ad8a54a2fdf1b429df1cba,Artificial Intelligence Applied to Battery Research: Hype or Reality?,"This is a critical review of artificial intelligence/machine learning (AI/ML) methods applied to battery research. It aims at providing a comprehensive, authoritative, and critical, yet easily understandable, review of general interest to the battery community. It addresses the concepts, approaches, tools, outcomes, and challenges of using AI/ML as an accelerator for the design and optimization of the next generation of batteries—a current hot topic. It intends to create both accessibility of these tools to the chemistry and electrochemical energy sciences communities and completeness in terms of the different battery R&D aspects covered.",2021.0,"Teo Lombardo, M. Duquesnoy, Hassna El-Bouysidy, Fabian Årén, Alfonso Gallo-Bueno, P. B. Jørgensen, Arghya Bhowmik, Arnaud Demortière, E. Ayerbe, F. Alcaide, M. Reynaud, J. Carrasco, A. Grimaud, Chao Zhang, T. Vegge, P. Johansson, A. Franco"
7d89b05c7e8d374b39d363075540d3a3fddd5e34,https://www.semanticscholar.org/paper/7d89b05c7e8d374b39d363075540d3a3fddd5e34,"Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness","Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit.",2020.0,"Sebastian J. Vollmer, Bilal A. Mateen, Gergo Bohner, Franz J. Király, Rayid Ghani, Páll Jónsson, Sarah Cumbers, Adrian Jonas, Katherine S L McAllister, Puja Myles, David Grainger, Mark Birse, Richard Branson, K. Moons, Gary S. Collins, J. Ioannidis, Chris Holmes, Harry Hemingway"
8c479e81ddaf55aba9044449b5be7b7bf2046b7e,https://www.semanticscholar.org/paper/8c479e81ddaf55aba9044449b5be7b7bf2046b7e,Abstraction and analogy‐making in artificial intelligence,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.",2021.0,M. Mitchell
9b21374e3dc537e87f7047cddaf6555a4399119a,https://www.semanticscholar.org/paper/9b21374e3dc537e87f7047cddaf6555a4399119a,RESEARCH ON ARTIFICIAL INTELLIGENCE FOR CITIZEN SERVICES AND GOVERNMENT,"— The main aim of this paper was to review how artificial intelligence works in improving Citizen Services and Government. Several government agencies throughout the world are experimenting with artificial intelligence applications (AI). The most common use cases for citizen services are inquiries and information [1]. This article examined the many forms of artificial intelligence applications, as well as the existing and prospective software solutions of AI in the delivery of citizen services by the government, with a particular emphasis on citizen queries and information. It also provides solutions for governments considering the use of artificial intelligence. Most artificial intelligence (AI) published studies in citizen services now fall into the following categories: responding to inquiries, filling out and finding papers, routing requests, translating documents, and generating paperwork, among others [1]. These technologies have the potential to increase the efficiency of government operations while also providing personnel with more time to create stronger ties with residents. Artificial intelligence (AI) may be one solution to bridge the gap between the dissatisfaction of citizens with digital government products and enhancing citizen participation and service delivery [2].",2021.0,Dhaya Sindhu Battina
9d9cd469279655f8efc2c0c95e55295036c8ab12,https://www.semanticscholar.org/paper/9d9cd469279655f8efc2c0c95e55295036c8ab12,Governance of artificial intelligence,"ABSTRACT The rapid developments in Artificial Intelligence (AI) and the intensification in the adoption of AI in domains such as autonomous vehicles, lethal weapon systems, robotics and alike pose serious challenges to governments as they must manage the scale and speed of socio-technical transitions occurring. While there is considerable literature emerging on various aspects of AI, governance of AI is a significantly underdeveloped area. The new applications of AI offer opportunities for increasing economic efficiency and quality of life, but they also generate unexpected and unintended consequences and pose new forms of risks that need to be addressed. To enhance the benefits from AI while minimising the adverse risks, governments worldwide need to understand better the scope and depth of the risks posed and develop regulatory and governance processes and structures to address these challenges. This introductory article unpacks AI and describes why the Governance of AI should be gaining far more attention given the myriad of challenges it presents. It then summarises the special issue articles and highlights their key contributions. This special issue introduces the multifaceted challenges of governance of AI, including emerging governance approaches to AI, policy capacity building, exploring legal and regulatory challenges of AI and Robotics, and outstanding issues and gaps that need attention. The special issue showcases the state-of-the-art in the governance of AI, aiming to enable researchers and practitioners to appreciate the challenges and complexities of AI governance and highlight future avenues for exploration.",2021.0,Araz Taeihagh
9bac3c2ccf4b65f5346ed843adb92bd294c092b0,https://www.semanticscholar.org/paper/9bac3c2ccf4b65f5346ed843adb92bd294c092b0,A Review on Artificial Intelligence in Education,"The emergence of innovative technologies has an impact on the methods of teaching and learning. With the rapid development of artificial intelligence (AI) technology in recent years, using AI in education has become more and more apparent. This article first outlines the application of AI in the field of education, such as adaptive learning, teaching evaluation, virtual classroom, etc. And then analyzes its impact on teaching and learning, which has a positive meaning for improving teachers' teaching level and students' learning quality. Finally, it puts forward the challenges that AI applications may face in education in the future and provides references for AI to promote education reform. 
  
Received: 16 January 2021 / Accepted: 24 March 2021 / Published: 10 May 2021",2021.0,"Jiahui Huang, S. Saleh, Yufei Liu"
d142835b642711b18669227ff0d554b1c8a0d481,https://www.semanticscholar.org/paper/d142835b642711b18669227ff0d554b1c8a0d481,Application of artificial intelligence to the electrocardiogram,"Abstract Artificial intelligence (AI) has given the electrocardiogram (ECG) and clinicians reading them super-human diagnostic abilities. Trained without hard-coded rules by finding often subclinical patterns in huge datasets, AI transforms the ECG, a ubiquitous, non-invasive cardiac test that is integrated into practice workflows, into a screening tool and predictor of cardiac and non-cardiac diseases, often in asymptomatic individuals. This review describes the mathematical background behind supervised AI algorithms, and discusses selected AI ECG cardiac screening algorithms including those for the detection of left ventricular dysfunction, episodic atrial fibrillation from a tracing recorded during normal sinus rhythm, and other structural and valvular diseases. The ability to learn from big data sets, without the need to understand the biological mechanism, has created opportunities for detecting non-cardiac diseases as COVID-19 and introduced challenges with regards to data privacy. Like all medical tests, the AI ECG must be carefully vetted and validated in real-world clinical environments. Finally, with mobile form factors that allow acquisition of medical-grade ECGs from smartphones and wearables, the use of AI may enable massive scalability to democratize healthcare.",2021.0,"Z. Attia, D. Harmon, E. Behr, P. Friedman"
26370f71cbd16a35b43526a96b3f04b9adbe9e38,https://www.semanticscholar.org/paper/26370f71cbd16a35b43526a96b3f04b9adbe9e38,Artificial intelligence act,The European Commission unveiled a new proposal for an EU regulatory framework on artificial intelligence (AI) in April 2021. The draft AI act is the first ever attempt to enact a horizontal regulation of AI. The proposed legal framework focuses on the specific utilisation of AI systems and associated risks. The Commission proposes to establish a technology-neutral definition of AI systems in EU law and to lay down a classification for AI systems with different requirements and obligations tailored on a 'risk-based approach'. Some AI systems presenting 'unacceptable' risks would be prohibited. A wide range of 'high-risk' AI systems would be authorised,2021.0,Samy Chahri
f743b1dccd53ac18bd028cd92d95549357becedd,https://www.semanticscholar.org/paper/f743b1dccd53ac18bd028cd92d95549357becedd,Possibilities and Apprehensions in the Landscape of Artificial Intelligence in Education,"Artificial intelligence (AI) may be utilized outside of traditional computer settings and is also readily available in low-cost smart devices, making AI easily accessible to general population. Built-in capabilities for conducting complicated computer operations (edge computing), cloud-based services for collaboratively addressing difficult issues, access to huge quantities of open and closed data resources, and conciliatory accession for agile network connections are all available on these low-cost devices. Education is helped by AI in at least two ways: (1) the educational process – assistance and modifications to pedagogy and educator's routine function; and (2) the educational ambit and content – what kind of education is needed. Author, in this article, explores the challenges and potentialities that AI offers in the field of education. While the focus is on AI's participation, it may be difficult to differentiate it from other technological advancements, especially when it comes to work life. Author conclude that AI (and associated technological advancements) will substitute some professions (didactics will not be required), that other professions will transform impressively (didactic materials will need to be updated), and that a significant number of novel vocations will be created (new-fangled didactics must be constituted). In educational operations – the task itself – AI will be a reformer as well as a facilitator, altering the characteristics and division of labor.",2021.0,Ashraf Alam
f06a7d6abd4eea02007a961a304c0c7b69fd9268,https://www.semanticscholar.org/paper/f06a7d6abd4eea02007a961a304c0c7b69fd9268,Multiagent systems: a modern approach to distributed artificial intelligence,"From the Publisher: 
This is the first comprehensive introduction to multiagent systems and contemporary distributed artificial intelligence. The book provides detailed coverage of basic topics as well as several closely related ones and is suitable as a textbook. The book can be used for teaching as well as self-study, and it is designed to meet the needs of both researchers and practitioners. In view of the interdisciplinary nature of the field, it will be a useful reference not only for computer scientists and engineers, but for social scientists and management and organization scientists as well.",1999.0,Gerhard Weiss
e3e5a78b4161a6eeea863c156bee7cf5c67e72ea,https://www.semanticscholar.org/paper/e3e5a78b4161a6eeea863c156bee7cf5c67e72ea,Application of Artificial Intelligence in Healthcare: Chances and Challenges,"Use of Artificial intelligence (AI) has increased in the healthcare in many sectors. Organizations from health care of different sizes, types and different specialties are now a days more interested in how artificial intelligence has evolved and is helping patient needs and their care, also reducing costs, and increasing efficiency. This study explores the implications of AI on healthcare management, and challenges involved with using AI in healthcare along with the review of several research papers that used AI models in different sectors of healthcare like Dermatology, Radiology, Drug design etc.",2021.0,"Ravi Manne, S. Kantheti"
06645d735b59b14479ae1d0392136bbf44227d0f,https://www.semanticscholar.org/paper/06645d735b59b14479ae1d0392136bbf44227d0f,DARPA's Explainable Artificial Intelligence (XAI) Program,"Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.",2019.0,"David Gunning, D. Aha"
e3b6d42ab070a148086478bb980a6a30c4d16176,https://www.semanticscholar.org/paper/e3b6d42ab070a148086478bb980a6a30c4d16176,Power to the Teachers: An Exploratory Review on Artificial Intelligence in Education,"This exploratory review attempted to gather evidence from the literature by shedding light on the emerging phenomenon of conceptualising the impact of artificial intelligence in education. The review utilised the PRISMA framework to review the analysis and synthesis process encompassing the search, screening, coding, and data analysis strategy of 141 items included in the corpus. Key findings extracted from the review incorporate a taxonomy of artificial intelligence applications with associated teaching and learning practice and a framework for helping teachers to develop and self-reflect on the skills and capabilities envisioned for employing artificial intelligence in education. Implications for ethical use and a set of propositions for enacting teaching and learning using artificial intelligence are demarcated. The findings of this review contribute to developing a better understanding of how artificial intelligence may enhance teachers’ roles as catalysts in designing, visualising, and orchestrating AI-enabled teaching and learning, and this will, in turn, help to proliferate AI-systems that render computational representations based on meaningful data-driven inferences of the pedagogy, domain, and learner models.",2021.0,"Petros Lameras, S. Arnab"
8b835a6dedd55e57c2d5328b94b839faa25faca8,https://www.semanticscholar.org/paper/8b835a6dedd55e57c2d5328b94b839faa25faca8,"A Brief History of Artificial Intelligence: On the Past, Present, and Future of Artificial Intelligence","This introduction to this special issue discusses artificial intelligence (AI), commonly defined as “a system’s ability to interpret external data correctly, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.” It summarizes seven articles published in this special issue that present a wide variety of perspectives on AI, authored by several of the world’s leading experts and specialists in AI. It concludes by offering a comprehensive outlook on the future of AI, drawing on micro-, meso-, and macro-perspectives.",2019.0,"M. Haenlein, A. Kaplan"
9b529fe170823f95509585d5aa39fa01a43558fd,https://www.semanticscholar.org/paper/9b529fe170823f95509585d5aa39fa01a43558fd,How Does NLP Benefit Legal System: A Summary of Legal Artificial Intelligence,"Legal Artificial Intelligence (LegalAI) focuses on applying the technology of artificial intelligence, especially natural language processing, to benefit tasks in the legal domain. In recent years, LegalAI has drawn increasing attention rapidly from both AI researchers and legal professionals, as LegalAI is beneficial to the legal system for liberating legal professionals from a maze of paperwork. Legal professionals often think about how to solve tasks from rule-based and symbol-based methods, while NLP researchers concentrate more on data-driven and embedding methods. In this paper, we introduce the history, the current state, and the future directions of research in LegalAI. We illustrate the tasks from the perspectives of legal professionals and NLP researchers and show several representative applications in LegalAI. We conduct experiments and provide an in-depth analysis of the advantages and disadvantages of existing works to explore possible future directions. You can find the implementation of our work from https://github.com/thunlp/CLAIM.",2020.0,"Haoxiang Zhong, Chaojun Xiao, Cunchao Tu, T. Zhang, Zhiyuan Liu, Maosong Sun"
3fb5d4ca683f714fe89a565b6ad108f8dcc4bf20,https://www.semanticscholar.org/paper/3fb5d4ca683f714fe89a565b6ad108f8dcc4bf20,Application of artificial intelligence in physical education,The development of artificial intelligence is accompanied by the progress of human society. Purpose of the article is to using artificial intelligence in coordination of physical education teaching...,2021.0,Songkun Yu
ffa0c918fba3010ccb635c03b4a740bd938a0493,https://www.semanticscholar.org/paper/ffa0c918fba3010ccb635c03b4a740bd938a0493,Evaluation of Combined Artificial Intelligence and Radiologist Assessment to Interpret Screening Mammograms,"This diagnostic accuracy study evaluates whether artificial intelligence can overcome human mammography interpretation limits with a rigorous, unbiased evaluation of machine learning algorithms.",2020.0,"T. Schaffter, D. Buist, Christoph I. Lee, Yaroslav Nikulin, D. Ribli, Y. Guan, William Lotter, Zequn Jie, Hao Du, Sijia Wang, Jiashi Feng, Mengling Feng, Hyo-Eun Kim, F. Albiol, A. Albiol, Stephen Morrell, Z. Wojna, M. Ahsen, U. Asif, Antonio Jose Jimeno Yepes, Shivanthan A. C. Yohanandan, S. Rabinovici-Cohen, Darvin Yi, B. Hoff, Thomas Yu, E. Chaibub Neto, D. Rubin, Peter Lindholm, L. Margolies, R. McBride, J. Rothstein, W. Sieh, Rami Ben-Ari, S. Harrer, A. Trister, S. Friend, Thea C. Norman, B. Sahiner, Fredrik Strand, J. Guinney, G. Stolovitzky, Lester W. Mackey, Joyce Cahoon, Li Shen, J. Sohn, H. Trivedi, Yiqiu Shen, L. Buturovic, José Costa Pereira, Jaime S. Cardoso, Eduardo Castro, K. T. Kalleberg, Obioma Pelka, Imane Nedjar, Krzysztof J. Geras, F. Nensa, Ethan Goan, S. Koitka, Luis Caballero, David D. Cox, Pavitra Krishnaswamy, G. Pandey, C. Friedrich, Dimitri Perrin, C. Fookes, Bibo Shi, Gerard Cardoso Negrie, M. Kawczynski, Kyunghyun Cho, Can Son Khoo, Joseph Y. Lo, A. Sorensen, Hwejin Jung"
b1e9c068f9df7c32b58d50dccd426f458b6df021,https://www.semanticscholar.org/paper/b1e9c068f9df7c32b58d50dccd426f458b6df021,A Literature Review of Artificial Intelligence,"Artificial Intelligence has ameliorated in prominence during the last decade. In practically every area, Artificial Intelligence has had a consequential contribution. It has grown into a tremendous technology that has revolutionized the way human beings communicate and may transform the way human beings look to the future. Nowadays, discoveries in artificial intelligence (AI) that outperform humans in some tasks generate headlines. I exhibit a spiffing updated literature-review for Artificial Intelligence. Other works offered domain-specific plus non-comprehensive, as well as shortcomings on their introduction, background information, related work, and discussion and future directions. This research intends to provide diverse AI techniques, which can be implement to preclude cyber-assaults; the Artificial Intelligence and its uses in a variety of fields. This literature review will definitely assist scientists and readers in comprehending the technologies, fields, uses, and applications of AI. Furthermore, in terms of state of knowledge, introduction, background information, related work, discussion, and future directions, this literature review outperformed previous literature review publications.",2021.0,"Agha Wafa Abbas Wafa, Muzammil Hussain Muzammil Hussain"
2bbc7e46425d8dabb2c2ebbf28dbbb0462d3b5e3,https://www.semanticscholar.org/paper/2bbc7e46425d8dabb2c2ebbf28dbbb0462d3b5e3,Artificial Intelligence in Health Care: Bibliometric Analysis,"Background As a critical driving power to promote health care, the health care–related artificial intelligence (AI) literature is growing rapidly. Objective The purpose of this analysis is to provide a dynamic and longitudinal bibliometric analysis of health care–related AI publications. Methods The Web of Science (Clarivate PLC) was searched to retrieve all existing and highly cited AI-related health care research papers published in English up to December 2019. Based on bibliometric indicators, a search strategy was developed to screen the title for eligibility, using the abstract and full text where needed. The growth rate of publications, characteristics of research activities, publication patterns, and research hotspot tendencies were computed using the HistCite software. Results The search identified 5235 hits, of which 1473 publications were included in the analyses. Publication output increased an average of 17.02% per year since 1995, but the growth rate of research papers significantly increased to 45.15% from 2014 to 2019. The major health problems studied in AI research are cancer, depression, Alzheimer disease, heart failure, and diabetes. Artificial neural networks, support vector machines, and convolutional neural networks have the highest impact on health care. Nucleosides, convolutional neural networks, and tumor markers have remained research hotspots through 2019. Conclusions This analysis provides a comprehensive overview of the AI-related research conducted in the field of health care, which helps researchers, policy makers, and practitioners better understand the development of health care–related AI research and possible practice implications. Future AI research should be dedicated to filling in the gaps between AI health care research and clinical applications.",2020.0,"Yuqi Guo, Zhichao Hao, Shichong Zhao, Jiaqi Gong, Fan Yang"
7c663204f4b1f5c0a112bce0d84b21e1c1b7f798,https://www.semanticscholar.org/paper/7c663204f4b1f5c0a112bce0d84b21e1c1b7f798,Artificial intelligence for education: Knowledge and its assessment in AI-enabled learning ecologies,"Abstract Over the past ten years, we have worked in a collaboration between educators and computer scientists at the University of Illinois to imagine futures for education in the context of what is loosely called “artificial intelligence.” Unhappy with the first generation of digital learning environments, our agenda has been to design alternatives and research their implementation. Our starting point has been to ask, what is the nature of machine intelligence, and what are its limits and potentials in education? This paper offers some tentative answers, first conceptually, and then practically in an overview of the results of a number of experimental implementations documented in greater detail elsewhere. Our key finding is that artificial intelligence—in the context of the practices of electronic computing developing over the past three quarters of a century—will never in any sense “take over” the role of teacher, because how it works and what it does are so profoundly different from human intelligence. However, within the limits that we describe in this paper, it offers the potential to transform education in ways that—counterintuitively perhaps—make education more human, not less.",2020.0,"B. Cope, M. Kalantzis, Duane Searsmith"
1fb5cd122affeb0385e2a087d14be4bac103460e,https://www.semanticscholar.org/paper/1fb5cd122affeb0385e2a087d14be4bac103460e,Explainable Artificial Intelligence: a Systematic Review,"Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.",2020.0,"Giulia Vilone, Luca Longo"
06b5090c00326183f7b3fe6e891586449e14650e,https://www.semanticscholar.org/paper/06b5090c00326183f7b3fe6e891586449e14650e,Ethics of artificial intelligence and robotics,"Artificial intelligence (AI) and robotics are digital technologies that will have significant impact on the development of humanity in the near future. They have raised fundamental questions about what we should do with these systems, what the systems themselves should do, what risks they involve, and how we can control these. - After the Introduction to the field (§1), the main themes (§2) of this article are: Ethical issues that arise with AI systems as objects, i.e., tools made and used by humans. This includes issues of privacy (§2.1) and manipulation (§2.2), opacity (§2.3) and bias (§2.4), human-robot interaction (§2.5), employment (§2.6), and the effects of autonomy (§2.7). Then AI systems as subjects, i.e., ethics for the AI systems themselves in machine ethics (§2.8) and artificial moral agency (§2.9). Finally, the problem of a possible future AI superintelligence leading to a “singularity” (§2.10). We close with a remark on the vision of AI (§3). - For each section within these themes, we provide a general explanation of the ethical issues, outline existing positions and arguments, then analyse how these play out with current technologies and finally, what policy consequences may be drawn.",2020.0,V. C. Müller
753d9bb1ffb825cb014568764a16fdaef70ca6e0,https://www.semanticscholar.org/paper/753d9bb1ffb825cb014568764a16fdaef70ca6e0,Application of Artificial Intelligence in Dentistry,"Artificial intelligence (AI) is a technology that utilizes machines to mimic intelligent human behavior. To appreciate human-technology interaction in the clinical setting, augmented intelligence has been proposed as a cognitive extension of AI in health care, emphasizing its assistive and supplementary role to medical professionals. While truly autonomous medical robotic systems are still beyond reach, the virtual component of AI, known as software-type algorithms, is the main component used in dentistry. Because of their powerful capabilities in data analysis, these virtual algorithms are expected to improve the accuracy and efficacy of dental diagnosis, provide visualized anatomic guidance for treatment, simulate and evaluate prospective results, and project the occurrence and prognosis of oral diseases. Potential obstacles in contemporary algorithms that prevent routine implementation of AI include the lack of data curation, sharing, and readability; the inability to illustrate the inner decision-making process; the insufficient power of classical computing; and the neglect of ethical principles in the design of AI frameworks. It is necessary to maintain a proactive attitude toward AI to ensure its affirmative development and promote human-technology rapport to revolutionize dental practice. The present review outlines the progress and potential dental applications of AI in medical-aided diagnosis, treatment, and disease prediction and discusses their data limitations, interpretability, computing power, and ethical considerations, as well as their impact on dentists, with the objective of creating a backdrop for future research in this rapidly expanding arena.",2020.0,"T. Shan, F. R. Tay, Lisha Gu"
967ae25c457d4b7069916b17b46e1f800a3bd662,https://www.semanticscholar.org/paper/967ae25c457d4b7069916b17b46e1f800a3bd662,A historical perspective of explainable Artificial Intelligence,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.",2020.0,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold"
4955335096c038f2cf39fdb4cc2bc79edaf363f9,https://www.semanticscholar.org/paper/4955335096c038f2cf39fdb4cc2bc79edaf363f9,Artificial intelligence in cancer imaging: Clinical challenges and applications,"Judgement, as one of the core tenets of medicine, relies upon the integration of multilayered data with nuanced decision making. Cancer offers a unique context for medical decisions given not only its variegated forms with evolution of disease but also the need to take into account the individual condition of patients, their ability to receive treatment, and their responses to treatment. Challenges remain in the accurate detection, characterization, and monitoring of cancers despite improved technologies. Radiographic assessment of disease most commonly relies upon visual evaluations, the interpretations of which may be augmented by advanced computational analyses. In particular, artificial intelligence (AI) promises to make great strides in the qualitative interpretation of cancer imaging by expert clinicians, including volumetric delineation of tumors over time, extrapolation of the tumor genotype and biological course from its radiographic phenotype, prediction of clinical outcome, and assessment of the impact of disease and treatment on adjacent organs. AI may automate processes in the initial interpretation of images and shift the clinical workflow of radiographic detection, management decisions on whether or not to administer an intervention, and subsequent observation to a yet to be envisioned paradigm. Here, the authors review the current state of AI as applied to medical imaging of cancer and describe advances in 4 tumor types (lung, brain, breast, and prostate) to illustrate how common clinical problems are being addressed. Although most studies evaluating AI applications in oncology to date have not been vigorously validated for reproducibility and generalizability, the results do highlight increasingly concerted efforts in pushing AI technology to clinical use and to impact future directions in cancer care.",2019.0,"W. Bi, A. Hosny, M. Schabath, M. Giger, Nicolai J. Birkbak, Alireza Mehrtash, Tavis Allison, O. Arnaout, C. Abbosh, I. Dunn, R. Mak, R. Tamimi, C. Tempany, C. Swanton, U. Hoffmann, L. Schwartz, R. Gillies, Raymond Y Huang, H. Aerts"
6e80e4b2f2a09bdbc20c63af5d85e6fc333746a0,https://www.semanticscholar.org/paper/6e80e4b2f2a09bdbc20c63af5d85e6fc333746a0,Causability and explainability of artificial intelligence in medicine,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system",2019.0,"Andreas Holzinger, G. Langs, H. Denk, K. Zatloukal, Heimo Müller"
4e48fe66f895c070aee0e023602cea9975575a05,https://www.semanticscholar.org/paper/4e48fe66f895c070aee0e023602cea9975575a05,Artificial intelligence in oncology,"Artificial intelligence (AI) has contributed substantially to the resolution of a variety of biomedical problems, including cancer, over the past decade. Deep learning, a subfield of AI that is highly flexible and supports automatic feature extraction, is increasingly being applied in various areas of both basic and clinical cancer research. In this review, we describe numerous recent examples of the application of AI in oncology, including cases in which deep learning has efficiently solved problems that were previously thought to be unsolvable, and we address obstacles that must be overcome before such application can become more widespread. We also highlight resources and datasets that can help harness the power of AI for cancer research. The development of innovative approaches to and applications of AI will yield important insights in oncology in the coming decade.",2020.0,"Hideyuki Shimizu, K. Nakayama"
da64e0b776f61c4b59be22f3fe9766d0b42d2475,https://www.semanticscholar.org/paper/da64e0b776f61c4b59be22f3fe9766d0b42d2475,Artificial Intelligence & Accounting Artificial Intelligence & Accounting,"Over the years, the accounting profession has evolved greatly since its creation thousands of years prior. Arguably the most monumental reason for the amount of change and growth the industry has experienced is due to various technological advancements seen over the past few decades. The question posed in this paper relates to the analyzation of whether or not artificial intelligence is beneficial for the accounting industry or not. In this paper, I will present the potential negative and positive benefits associated with technology in general. I will also introduce the field of accounting and describe the diversity of positions offered within accounting and the evolution of the industry itself. Furthermore, I will relate the two topics of technology and accounting together and demonstrate how they are interrelated and have transformed into a coexistent state. Additionally, we will take a look at what the accounting industry is like in today(cid:182)s present age with the technology available to the industry and what this means for the profession moving forward. Finally, we will answer the question of whether or not this technological growth is vital or fatal for the profession of accounting.",2022.0,R. J. Kline
028d026be997ebf1f1c762407d216ff35a81ca73,https://www.semanticscholar.org/paper/028d026be997ebf1f1c762407d216ff35a81ca73,Resistance To Medical Artificial Intelligence,"
 Artificial intelligence (AI) is revolutionizing healthcare, but little is known about consumer receptivity to AI in medicine. Consumers are reluctant to utilize healthcare provided by AI in real and hypothetical choices, separate and joint evaluations. Consumers are less likely to utilize healthcare (study 1), exhibit lower reservation prices for healthcare (study 2), are less sensitive to differences in provider performance (studies 3A–3C), and derive negative utility if a provider is automated rather than human (study 4). Uniqueness neglect, a concern that AI providers are less able than human providers to account for consumers’ unique characteristics and circumstances, drives consumer resistance to medical AI. Indeed, resistance to medical AI is stronger for consumers who perceive themselves to be more unique (study 5). Uniqueness neglect mediates resistance to medical AI (study 6), and is eliminated when AI provides care (a) that is framed as personalized (study 7), (b) to consumers other than the self (study 8), or (c) that only supports, rather than replaces, a decision made by a human healthcare provider (study 9). These findings make contributions to the psychology of automation and medical decision making, and suggest interventions to increase consumer acceptance of AI in medicine.",2019.0,"Chiara Longoni, Andrea Bonezzi, Carey K. Morewedge"
2f858284b3f3e05155cf33c0badc37f3ac90ca85,https://www.semanticscholar.org/paper/2f858284b3f3e05155cf33c0badc37f3ac90ca85,"A Review of Further Directions for Artificial Intelligence, Machine Learning, and Deep Learning in Smart Logistics","Industry 4.0 concepts and technologies ensure the ongoing development of micro- and macro-economic entities by focusing on the principles of interconnectivity, digitalization, and automation. In this context, artificial intelligence is seen as one of the major enablers for Smart Logistics and Smart Production initiatives. This paper systematically analyzes the scientific literature on artificial intelligence, machine learning, and deep learning in the context of Smart Logistics management in industrial enterprises. Furthermore, based on the results of the systematic literature review, the authors present a conceptual framework, which provides fruitful implications based on recent research findings and insights to be used for directing and starting future research initiatives in the field of artificial intelligence (AI), machine learning (ML), and deep learning (DL) in Smart Logistics.",2020.0,"M. Woschank, E. Rauch, Helmut E. Zsifkovits"
82870bc488b57cdf5ea62877109a7278af2926b3,https://www.semanticscholar.org/paper/82870bc488b57cdf5ea62877109a7278af2926b3,Big Data and Artificial Intelligence Modeling for Drug Discovery.,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",2020.0,Hao Zhu
632db2f25b13638609c8e8d1aa313bc7bbdf366d,https://www.semanticscholar.org/paper/632db2f25b13638609c8e8d1aa313bc7bbdf366d,Design of online intelligent English teaching platform based on artificial intelligence techniques,"Artificial intelligence education (AIEd) is defined in the field of education as the utilization of artificial intelligence. There are currently many AIEd‐driven applications in schools and universities. This paper applies an artificial intelligence module combined with the knowledge recommendation to the system and develops an online English teaching system in comparison with the common teaching auxiliary system. The method of English teaching is useful in investigating the potential internal connections between evaluation outcomes and various factors. This article develops deep learning‐assisted online intelligent English teaching system that utilizes to create a modern tool platform to help students improve their English language teaching efficiency in line with their mastery of knowledge and personality. The decision tree algorithm and neural networks have been used and to generate an English teaching assessment implementation model based on decision tree technologies. It provides valuable data from extensive information, summarizes rules and data, and helps teachers to improve their education and the English scores of students. This system reflects the thinking of the artificial intelligence expert system. Test application demonstrates that the system can help students improve their learning efficiency and will make learning content more relevant. Besides, the system provides an example model with similar methods and has a referential definition.",2020.0,"Zhuomin Sun, M. Anbarasan, D. P. Kumar, Praveen Kumar"
8d2650784b6491fd2b9e73bb212aae229cea01ea,https://www.semanticscholar.org/paper/8d2650784b6491fd2b9e73bb212aae229cea01ea,"Artificial Intelligence and Machine Learning Applications in Smart Production: Progress, Trends, and Directions","Adaptation and innovation are extremely important to the manufacturing industry. This development should lead to sustainable manufacturing using new technologies. To promote sustainability, smart production requires global perspectives of smart production application technology. In this regard, thanks to intensive research efforts in the field of artificial intelligence (AI), a number of AI-based techniques, such as machine learning, have already been established in the industry to achieve sustainable manufacturing. Thus, the aim of the present research was to analyze, systematically, the scientific literature relating to the application of artificial intelligence and machine learning (ML) in industry. In fact, with the introduction of the Industry 4.0, artificial intelligence and machine learning are considered the driving force of smart factory revolution. The purpose of this review was to classify the literature, including publication year, authors, scientific sector, country, institution, and keywords. The analysis was done using the Web of Science and SCOPUS database. Furthermore, UCINET and NVivo 12 software were used to complete them. A literature review on ML and AI empirical studies published in the last century was carried out to highlight the evolution of the topic before and after Industry 4.0 introduction, from 1999 to now. Eighty-two articles were reviewed and classified. A first interesting result is the greater number of works published by the USA and the increasing interest after the birth of Industry 4.0.",2020.0,"R. Cioffi, Marta Travaglioni, G. Piscitelli, A. Petrillo, F. De Felice"
d1615484d8af03aef212b2764bca93d70ac9707c,https://www.semanticscholar.org/paper/d1615484d8af03aef212b2764bca93d70ac9707c,Four Principles of Explainable Artificial Intelligence,"We introduce four principles for explainable artiﬁcial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly reﬂect the system’s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches sufﬁcient conﬁdence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through signiﬁcant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the ﬁelds of computer science, engineering, and psychology. Because one-size-ﬁts-all explanations do not exist, different users will require different types of explanations. We present ﬁve categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the ﬁeld that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.",2020.0,"P. Phillips, Carina A. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki"
2d18bb251f056b24f95eed242d5cd4b50c055f4f,https://www.semanticscholar.org/paper/2d18bb251f056b24f95eed242d5cd4b50c055f4f,Urban Artificial Intelligence: From Automation to Autonomy in the Smart City,"Technological innovation is constantly reshaping the materiality and mechanics of smart-city initiatives. Recently, innovation in artificial intelligence (AI) in the shape of self-driving cars, robots and city brains, has been pushing the so-called smart city to morph into an autonomous urban creature which is largely unknown. In this emerging strand of smart urbanism, artificially intelligent entities are taking the management of urban services as well as urban governance out of the hands of humans, operating the city in an autonomous manner. This paper explores, in theory and practice, how the development of AI intersects with the development of the city. The contribution of the paper is threefold. First, the paper advances a theoretical framework to understand AI specifically in urban contexts. It develops the concept of urban artificial intelligence, capturing the main manifestations of AI in cities. Second, the paper examines the case of Masdar City, an Emirati urban experiment, to show how the genesis of urban artificial intelligences is part of a long-standing process of technological development and a politico-economic agenda which together are enabling the transition from automation to autonomy. Third, it proposes a research agenda to investigate what the paper terms the autonomous city.",2020.0,Federico Cugurullo
80c5d486c73bc43fcbbfcfbbb1971c1a72a8f27b,https://www.semanticscholar.org/paper/80c5d486c73bc43fcbbfcfbbb1971c1a72a8f27b,Artificial Intelligence in Human Resources Management: Challenges and a Path Forward,"There is a substantial gap between the promise and reality of artificial intelligence in human resource (HR) management. This article identifies four challenges in using data science techniques for HR tasks: complexity of HR phenomena, constraints imposed by small data sets, accountability questions associated with fairness and other ethical and legal constraints, and possible adverse employee reactions to management decisions via data-based algorithms. It then proposes practical responses to these challenges based on three overlapping principles—causal reasoning, randomization and experiments, and employee contribution—that would be both economically efficient and socially appropriate for using data science in the management of employees.",2019.0,"Prasanna Tambe, P. Cappelli, V. Yakubovich"
c97f7d8cc3f8473779c9d65173cc1d4baa79344a,https://www.semanticscholar.org/paper/c97f7d8cc3f8473779c9d65173cc1d4baa79344a,Role of artificial intelligence in operations environment: a review and bibliometric analysis,"Purpose“Technological intelligence” is the capacity to appreciate and adapt technological advancements, and “artificial intelligence” is the key to achieve persuasive operational transformations in majority of contemporary organizational set-ups. Implicitly, artificial intelligence (the philosophies of machines to think, behave and perform either same or similar to humans) has knocked the doors of business organizations as an imperative activity. Artificial intelligence, as a discipline, initiated by scientist John McCarthy and formally publicized at Dartmouth Conference in 1956, now occupies a central stage for many organizations. Implementation of artificial intelligence provides competitive edge to an organization with a definite augmentation in its social and corporate status. Mere application of a concept will not furnish real output until and unless its performance is reviewed systematically. Technological changes are dynamic and advancing at a rapid rate. Subsequently, it becomes highly crucial to understand that where have the people reached with respect to artificial intelligence research. The present article aims to review significant work by eminent researchers towards artificial intelligence in the form of top contributing universities, authors, keywords, funding sources, journals and citation statistics.Design/methodology/approachAs rightly remarked by past researchers that reviewing is learning from experience, research team has reviewed (by applying systematic literature review through bibliometric analysis) the concept of artificial intelligence in this article. A sum of 1,854 articles extracted from Scopus database for the year 2018–2019 (31st of May) with selected keywords (artificial intelligence, genetic algorithms, agent-based systems, expert systems, big data analytics and operations management) along with certain filters (subject–business, management and accounting; language-English; document–article, article in press, review articles and source-journals).FindingsResults obtained from cluster analysis focus on predominant themes for present as well as future researchers in the area of artificial intelligence. Emerged clusters include Cluster 1: Artificial Intelligence and Optimization; Cluster 2: Industrial Engineering/Research and Automation; Cluster 3: Operational Performance and Machine Learning; Cluster 4: Sustainable Supply Chains and Sustainable Development; Cluster 5: Technology Adoption and Green Supply Chain Management and Cluster 6: Internet of Things and Reverse Logistics.Originality/valueThe result of review of selected studies is in itself a unique contribution and a food for thought for operations managers and policy makers.",2020.0,"Pavitra Dhamija, Surajit Bag"
19ca6924d782ac57216d254dcc31861cb0b0a89a,https://www.semanticscholar.org/paper/19ca6924d782ac57216d254dcc31861cb0b0a89a,Health Care Employees’ Perceptions of the Use of Artificial Intelligence Applications: Survey Study,"Background The advancement of health care information technology and the emergence of artificial intelligence has yielded tools to improve the quality of various health care processes. Few studies have investigated employee perceptions of artificial intelligence implementation in Saudi Arabia and the Arabian world. In addition, limited studies investigated the effect of employee knowledge and job title on the perception of artificial intelligence implementation in the workplace. Objective The aim of this study was to explore health care employee perceptions and attitudes toward the implementation of artificial intelligence technologies in health care institutions in Saudi Arabia. Methods An online questionnaire was published, and responses were collected from 250 employees, including doctors, nurses, and technicians at 4 of the largest hospitals in Riyadh, Saudi Arabia. Results The results of this study showed that 3.11 of 4 respondents feared artificial intelligence would replace employees and had a general lack of knowledge regarding artificial intelligence. In addition, most respondents were unaware of the advantages and most common challenges to artificial intelligence applications in the health sector, indicating a need for training. The results also showed that technicians were the most frequently impacted by artificial intelligence applications due to the nature of their jobs, which do not require much direct human interaction. Conclusions The Saudi health care sector presents an advantageous market potential that should be attractive to researchers and developers of artificial intelligence solutions.",2020.0,"R. Abdullah, Bahjat Fakieh"
e8f834463b7540a3db591a19c1025f89d1a1be1d,https://www.semanticscholar.org/paper/e8f834463b7540a3db591a19c1025f89d1a1be1d,Deep Medicine: How Artificial Intelligence Can Make Healthcare Human Again,"As Rob Atkinson writse in a book review for the New York Journal of Books, anyone who has experience with the U.S. healthcare system will find a lot to agree with in Dr. Eric Topol’s ""Deep Medicine"". Topol, a cardiologist, medical researcher, and author, argues that medicine as it is practiced today is “shallow” rather than deep, and that accounts for many of its problems. By deep, Topol is referring to more meaningful encounters between clinician and patient, rather than a quick visit with a doctor. To get to a world of “deep medicine,” Topol argues that artificial intelligence (AI) will play a key role.",2019.0,R. Atkinson
7a19f30e02c34c4eb7b197d3bcd4fbcb8a4e1602,https://www.semanticscholar.org/paper/7a19f30e02c34c4eb7b197d3bcd4fbcb8a4e1602,Transparency and trust in artificial intelligence systems,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.",2020.0,"Philipp Schmidt, F. Biessmann, Timm Teubner"
2eaa830db7e6a98f1eb84df9be313debc8332ea8,https://www.semanticscholar.org/paper/2eaa830db7e6a98f1eb84df9be313debc8332ea8,State-of-the-Art Artificial Intelligence Techniques for Distributed Smart Grids: A Review,"The power system worldwide is going through a revolutionary transformation due to the integration with various distributed components, including advanced metering infrastructure, communication infrastructure, distributed energy resources, and electric vehicles, to improve the reliability, energy efficiency, management, and security of the future power system. These components are becoming more tightly integrated with IoT. They are expected to generate a vast amount of data to support various applications in the smart grid, such as distributed energy management, generation forecasting, grid health monitoring, fault detection, home energy management, etc. With these new components and information, artificial intelligence techniques can be applied to automate and further improve the performance of the smart grid. In this paper, we provide a comprehensive review of the state-of-the-art artificial intelligence techniques to support various applications in a distributed smart grid. In particular, we discuss how artificial techniques are applied to support the integration of renewable energy resources, the integration of energy storage systems, demand response, management of the grid and home energy, and security. As the smart grid involves various actors, such as energy produces, markets, and consumers, we also discuss how artificial intelligence and market liberalization can potentially help to increase the overall social welfare of the grid. Finally, we provide further research challenges for large-scale integration and orchestration of automated distributed devices to realize a truly smart grid.",2020.0,"S. Ali, B. Choi"
5e307e074f5a54185834aaf83c5fb4a9bac6f7e6,https://www.semanticscholar.org/paper/5e307e074f5a54185834aaf83c5fb4a9bac6f7e6,Artificial intelligence and communication: A Human–Machine Communication research agenda,"Artificial intelligence (AI) and people’s interactions with it—through virtual agents, socialbots, and language-generation software—do not fit neatly into paradigms of communication theory that have long focused on human–human communication. To address this disconnect between communication theory and emerging technology, this article provides a starting point for articulating the differences between communicative AI and previous technologies and introduces a theoretical basis for navigating these conditions in the form of scholarship within human–machine communication (HMC). Drawing on an HMC framework, we outline a research agenda built around three key aspects of communicative AI technologies: (1) the functional dimensions through which people make sense of these devices and applications as communicators, (2) the relational dynamics through which people associate with these technologies and, in turn, relate to themselves and others, and (3) the metaphysical implications called up by blurring ontological boundaries surrounding what constitutes human, machine, and communication.",2019.0,"Andrea L. Guzman, S. Lewis"
40fb2468c3a77c68fe703a6e614f4ad25bd4e3dd,https://www.semanticscholar.org/paper/40fb2468c3a77c68fe703a6e614f4ad25bd4e3dd,Edge Intelligence: The Confluence of Edge Computing and Artificial Intelligence,"Along with the rapid developments in communication technologies and the surge in the use of mobile devices, a brand-new computation paradigm, edge computing, is surging in popularity. Meanwhile, the artificial intelligence (AI) applications are thriving with the breakthroughs in deep learning and the many improvements in hardware architectures. Billions of data bytes, generated at the network edge, put massive demands on data processing and structural optimization. Thus, there exists a strong demand to integrate edge computing and AI, which gives birth to edge intelligence. In this article, we divide edge intelligence into AI for edge (intelligence-enabled edge computing) and AI on edge (artificial intelligence on edge). The former focuses on providing more optimal solutions to key problems in edge computing with the help of popular and effective AI technologies while the latter studies how to carry out the entire process of building AI models, i.e., model training and inference, on the edge. This article provides insights into this new interdisciplinary field from a broader perspective. It discusses the core concepts and the research roadmap, which should provide the necessary background for potential future research initiatives in edge intelligence.",2019.0,"Shuiguang Deng, Hailiang Zhao, Jianwei Yin, S. Dustdar, Albert Y. Zomaya"
6e23ae3969078ecd6e59260a895c96c360b4921a,https://www.semanticscholar.org/paper/6e23ae3969078ecd6e59260a895c96c360b4921a,"Artificial intelligence, bias and clinical safety","In medicine, artificial intelligence (AI) research is becoming increasingly focused on applying machine learning (ML) techniques to complex problems, and so allowing computers to make predictions from large amounts of patient data, by learning their own associations.1 Estimates of the impact of AI on the wider economy globally vary wildly, with a recent report suggesting a 14% effect on global gross domestic product by 2030, half of which coming from productivity improvements.2 These predictions create political appetite for the rapid development of the AI industry,3 and healthcare is a priority area where this technology has yet to be exploited.2 3 The digital health revolution described by Duggal et al 4 is already in full swing with the potential to ‘disrupt’ healthcare. Health AI research has demonstrated some impressive results,5–10 but its clinical value has not yet been realised, hindered partly by a lack of a clear understanding of how to quantify benefit or ensure patient safety, and increasing concerns about the ethical and medico-legal impact.11 

This analysis is written with the dual aim of helping clinical safety professionals to critically appraise current medical AI research from a quality and safety perspective, and supporting research and development in AI by highlighting some of the clinical safety questions that must be considered if medical application of these exciting technologies is to be successful.

Clinical decision support systems (DSS) are in widespread use in medicine and have had most impact providing guidance on the safe prescription of medicines,12 guideline adherence, simple risk screening13 or prognostic scoring.14 These systems use predefined rules, which have predictable behaviour and are usually shown to reduce clinical error,12 although sometimes inadvertently introduce safety issues themselves.15 16 Rules-based systems have also been developed to address diagnostic uncertainty17–19 …",2019.0,"R. Challen, J. Denny, M. Pitt, Luke Gompels, Tom Edwards, K. Tsaneva-Atanasova"
efcfb79f4cd92db09dbb21e9e8cd25dbcccd5460,https://www.semanticscholar.org/paper/efcfb79f4cd92db09dbb21e9e8cd25dbcccd5460,Overview of artificial intelligence in medicine,"Background: Artificial intelligence (AI) is the term used to describe the use of computers and technology to simulate intelligent behavior and critical thinking comparable to a human being. John McCarthy first described the term AI in 1956 as the science and engineering of making intelligent machines. Objective: This descriptive article gives a broad overview of AI in medicine, dealing with the terms and concepts as well as the current and future applications of AI. It aims to develop knowledge and familiarity of AI among primary care physicians. Materials and Methods: PubMed and Google searches were performed using the key words 'artificial intelligence'. Further references were obtained by cross-referencing the key articles. Results: Recent advances in AI technology and its current applications in the field of medicine have been discussed in detail. Conclusions: AI promises to change the practice of medicine in hitherto unknown ways, but many of its practical applications are still in their infancy and need to be explored and developed better. Medical professionals also need to understand and acclimatize themselves with these advances for better healthcare delivery to the masses.",2019.0,"Amisha, Paras Malik, Monika Pathania, V. Rathaur"
77f6a75bba74f36699edceb0edb107dcfb1aaaa1,https://www.semanticscholar.org/paper/77f6a75bba74f36699edceb0edb107dcfb1aaaa1,Artificial Intelligence and Human Trust in Healthcare: Focus on Clinicians,"Artificial intelligence (AI) can transform health care practices with its increasing ability to translate the uncertainty and complexity in data into actionable—though imperfect—clinical decisions or suggestions. In the evolving relationship between humans and AI, trust is the one mechanism that shapes clinicians’ use and adoption of AI. Trust is a psychological mechanism to deal with the uncertainty between what is known and unknown. Several research studies have highlighted the need for improving AI-based systems and enhancing their capabilities to help clinicians. However, assessing the magnitude and impact of human trust on AI technology demands substantial attention. Will a clinician trust an AI-based system? What are the factors that influence human trust in AI? Can trust in AI be optimized to improve decision-making processes? In this paper, we focus on clinicians as the primary users of AI systems in health care and present factors shaping trust between clinicians and AI. We highlight critical challenges related to trust that should be considered during the development of any AI system for clinical use.",2019.0,"Onur Asan, A. E. Bayrak, Avishek Choudhury"
16a7b6234e070025a3b6a1e07bd9794d4cdbc2f2,https://www.semanticscholar.org/paper/16a7b6234e070025a3b6a1e07bd9794d4cdbc2f2,Toward understanding the impact of artificial intelligence on labor,"Rapid advances in artificial intelligence (AI) and automation technologies have the potential to significantly disrupt labor markets. While AI and automation can augment the productivity of some workers, they can replace the work done by others and will likely transform almost all occupations at least to some degree. Rising automation is happening in a period of growing economic inequality, raising fears of mass technological unemployment and a renewed call for policy efforts to address the consequences of technological change. In this paper we discuss the barriers that inhibit scientists from measuring the effects of AI and automation on the future of work. These barriers include the lack of high-quality data about the nature of work (e.g., the dynamic requirements of occupations), lack of empirically informed models of key microlevel processes (e.g., skill substitution and human–machine complementarity), and insufficient understanding of how cognitive technologies interact with broader economic dynamics and institutional mechanisms (e.g., urban migration and international trade policy). Overcoming these barriers requires improvements in the longitudinal and spatial resolution of data, as well as refinements to data on workplace skills. These improvements will enable multidisciplinary research to quantitatively monitor and predict the complex evolution of work in tandem with technological progress. Finally, given the fundamental uncertainty in predicting technological change, we recommend developing a decision framework that focuses on resilience to unexpected scenarios in addition to general equilibrium behavior.",2019.0,"M. Frank, David Autor, James E. Bessen, Erik Brynjolfsson, M. Cebrián, D. Deming, M. Feldman, Matthew Groh, J. Lobo, E. Moro, Dashun Wang, Hyejin Youn, Iyad Rahwan"
205383e5929027989c01f33652b1ed6f344fe993,https://www.semanticscholar.org/paper/205383e5929027989c01f33652b1ed6f344fe993,The impact of artificial intelligence in medicine on the future role of the physician,"The practice of medicine is changing with the development of new Artificial Intelligence (AI) methods of machine learning. Coupled with rapid improvements in computer processing, these AI-based systems are already improving the accuracy and efficiency of diagnosis and treatment across various specializations. The increasing focus of AI in radiology has led to some experts suggesting that someday AI may even replace radiologists. These suggestions raise the question of whether AI-based systems will eventually replace physicians in some specializations or will augment the role of physicians without actually replacing them. To assess the impact on physicians this research seeks to better understand this technology and how it is transforming medicine. To that end this paper researches the role of AI-based systems in performing medical work in specializations including radiology, pathology, ophthalmology, and cardiology. It concludes that AI-based systems will augment physicians and are unlikely to replace the traditional physician–patient relationship.",2019.0,A. Ahuja
6df2126301ab415aed034b0bcd9589b1897fe983,https://www.semanticscholar.org/paper/6df2126301ab415aed034b0bcd9589b1897fe983,Human Compatible: Artificial Intelligence and the Problem of Control,"""The most important book I have read in quite some time"" (Daniel Kahneman); ""A must-read"" (Max Tegmark); ""The book we've all been waiting for"" (Sam Harris) LONGLISTED FOR THE 2019 FINANCIAL TIMES AND MCKINSEY BUSINESS BOOK OF THE YEAR; A FINANCIAL TIMES BEST BOOK OF THE YEAR 2019 Humans dream of super-intelligent machines. But what happens if we actually succeed? Creating superior intelligence would be the biggest event in human history. Unfortunately, according to the world's pre-eminent AI expert, it could also be the last. In this groundbreaking book on the biggest question facing humanity, Stuart Russell explains why he has come to consider his own discipline an existential threat to our species, and lays out how we can change course before it's too late. There is no one better placed to assess the promise and perils of the dominant technology of the future than Russell, who has spent decades at the forefront of AI research. Through brilliant analogies and crisp, lucid prose, he explains how AI actually works, how it has an enormous capacity to improve our lives - but why we must ensure that we never lose control of machines more powerful than we are. Here Russell shows how we can avert the worst threats by reshaping the foundations of AI to guarantee that machines pursue our objectives, not theirs. Profound, urgent and visionary, Human Compatible is the one book everyone needs to read to understand a future that is coming sooner than we think.",2019.0,Stuart Russell
0980d299a05499639af63e530aa8b5e133022c8e,https://www.semanticscholar.org/paper/0980d299a05499639af63e530aa8b5e133022c8e,Artificial Intelligence and Acute Stroke Imaging,"SUMMARY: Artificial intelligence technology is a rapidly expanding field with many applications in acute stroke imaging, including ischemic and hemorrhage subtypes. Early identification of acute stroke is critical for initiating prompt intervention to reduce morbidity and mortality. Artificial intelligence can help with various aspects of the stroke treatment paradigm, including infarct or hemorrhage detection, segmentation, classification, large vessel occlusion detection, Alberta Stroke Program Early CT Score grading, and prognostication. In particular, emerging artificial intelligence techniques such as convolutional neural networks show promise in performing these imaging-based tasks efficiently and accurately. The purpose of this review is twofold: first, to describe AI methods and available public and commercial platforms in stroke imaging, and second, to summarize the literature of current artificial intelligence–driven applications for acute stroke triage, surveillance, and prediction.",2020.0,"J. Soun, D. Chow, M. Nagamine, R. Takhtawala, C. Filippi, Wengui Yu, P. Chang"
340712391e245e245a76cd74523c1d6443035b62,https://www.semanticscholar.org/paper/340712391e245e245a76cd74523c1d6443035b62,The unreasonable effectiveness of deep learning in artificial intelligence,"Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.",2020.0,T. Sejnowski
c65a2b18be114d29b824ac74dbf30f79cc3a16e6,https://www.semanticscholar.org/paper/c65a2b18be114d29b824ac74dbf30f79cc3a16e6,Artificial Intelligence (AI) or Intelligence Augmentation (IA): What Is the Future?,"Artificial intelligence (AI) is a rapidly growing technological phenomenon that all industries wish to exploit to benefit from efficiency gains and cost reductions. At the macrolevel, AI appears to be capable of replacing humans by undertaking intelligent tasks that were once limited to the human mind. However, another school of thought suggests that instead of being a replacement for the human mind, AI can be used for intelligence augmentation (IA). Accordingly, our research seeks to address these different views, their implications, and potential risks in an age of increased artificial awareness. We show that the ultimate goal of humankind is to achieve IA through the exploitation of AI. Moreover, we articulate the urgent need for ethical frameworks that define how AI should be used to trigger the next level of IA.",2020.0,"Hossein Hassani, E. Silva, Stephane Unger, Maedeh TajMazinani, Stephen Mac Feely"
54adcba53a639e988d4555b5f22b28ad382164b8,https://www.semanticscholar.org/paper/54adcba53a639e988d4555b5f22b28ad382164b8,"The trainer, the verifier, the imitator: Three ways in which human platform workers support artificial intelligence","This paper sheds light on the role of digital platform labour in the development of today’s artificial intelligence, predicated on data-intensive machine learning algorithms. Focus is on the specific ways in which outsourcing of data tasks to myriad ‘micro-workers’, recruited and managed through specialized platforms, powers virtual assistants, self-driving vehicles and connected objects. Using qualitative data from multiple sources, we show that micro-work performs a variety of functions, between three poles that we label, respectively, ‘artificial intelligence preparation’, ‘artificial intelligence verification’ and ‘artificial intelligence impersonation’. Because of the wide scope of application of micro-work, it is a structural component of contemporary artificial intelligence production processes – not an ephemeral form of support that may vanish once the technology reaches maturity stage. Through the lens of micro-work, we prefigure the policy implications of a future in which data technologies do not replace human workforce but imply its marginalization and precariousness.",2020.0,"Paola Tubaro, Antonio A. Casilli, Marion Coville"
94ae261ee01bd6be58c48cce83c1f4d927432c3a,https://www.semanticscholar.org/paper/94ae261ee01bd6be58c48cce83c1f4d927432c3a,Artificial Intelligence in Anesthesiology,"Artificial intelligence has been advancing in fields including anesthesiology. This scoping review of the intersection of artificial intelligence and anesthesia research identified and summarized six themes of applications of artificial intelligence in anesthesiology: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event and risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Based on papers identified in the review, several topics within artificial intelligence were described and summarized: (1) machine learning (including supervised, unsupervised, and reinforcement learning), (2) techniques in artificial intelligence (e.g., classical machine learning, neural networks and deep learning, Bayesian methods), and (3) major applied fields in artificial intelligence. The implications of artificial intelligence for the practicing anesthesiologist are discussed as are its limitations and the role of clinicians in further developing artificial intelligence for use in clinical care. Artificial intelligence has the potential to impact the practice of anesthesiology in aspects ranging from perioperative support to critical care delivery to outpatient pain management. This scoping review of artificial intelligence in anesthesiology summarizes six areas of research: (1) depth of anesthesia monitoring, (2) control of anesthesia, (3) event/risk prediction, (4) ultrasound guidance, (5) pain management, and (6) operating room logistics. Supplemental Digital Content is available in the text.",2020.0,"D. Hashimoto, E. Witkowski, Lei Gao, O. Meireles, G. Rosman"
73395c0a04a86f2c504fc9d02076266620a54e7a,https://www.semanticscholar.org/paper/73395c0a04a86f2c504fc9d02076266620a54e7a,Transparency in artificial intelligence,"This conceptual paper addresses the issues of transparency as linked to artificial intelligence (AI) from socio-legal and computer scientific perspectives. Firstly, we discuss the conceptual distinction between transparency in AI and algorithmic transparency, and argue for the wider concept ‘in AI’, as a partly contested albeit useful notion in relation to transparency. Secondly, we show that transparency as a general concept is multifaceted, and of widespread theoretical use in multiple disciplines over time, particularly since the 1990s. Still, it has had a resurgence in contemporary notions of AI governance, such as in the multitude of recently published ethics guidelines on AI. Thirdly, we discuss and show the relevance of the fact that transparency expresses a conceptual metaphor of more general significance, linked to knowing, bringing positive connotations that may have normative effects to regulatory debates. Finally, we draw a possible categorisation of aspects related to transparency in AI, or what we interchangeably call AI transparency, and argue for the need of developing a multidisciplinary understanding, in order to contribute to the governance of AI as applied on markets and in society. (Less)",2020.0,"S. Larsson, F. Heintz"
a56067135463c276fddb6183dde0b109ed92b400,https://www.semanticscholar.org/paper/a56067135463c276fddb6183dde0b109ed92b400,Artificial Intelligence in Cyber Security,"Cybersecurity is the process of protecting computer networks from cyber attacks or unintended unauthorized access.  It is the need of the hour. Organizations, businesses, and governments need cybersecurity solutions because cyber criminals pose a threat to everyone. Artificial intelligence promises to be a great solution for this. By combining the strength of artificial intelligence with cybersecurity, security experts are more capable to defend vulnerable networks and data from cyber attackers. This paper provides an introduction to the use of artificial intelligence in cybersecurity.",2020.0,"M. Sadiku, Omobayode Fagbohungbe, S. Musa"
8641738a3fec953bc22263be4dbdb923b39213a9,https://www.semanticscholar.org/paper/8641738a3fec953bc22263be4dbdb923b39213a9,"Improving public services using artificial intelligence: possibilities, pitfalls, governance","Artificial intelligence arising from the use of machine learning is rapidly being developed and deployed by governments to enhance operations, public services, and compliance and security activities. This article reviews how artificial intelligence is being used in public sector for automated decision making, for chatbots to provide information and advice, and for public safety and security. It then outlines four public administration challenges to deploying artificial intelligence in public administration: accuracy, bias and discrimination; legality, due process and administrative justice; responsibility, accountability, transparency and explainability; and power, compliance and control. The article outlines technological and governance innovations that are being developed to address these challenges.",2020.0,Paul Henman
f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10,https://www.semanticscholar.org/paper/f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10,Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.,"Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.",2019.0,"Xin Yang, Yifei Wang, Ryan Byrne, G. Schneider, Sheng-yong Yang"
1886314f85a9458b1090d59a12c402652d838b21,https://www.semanticscholar.org/paper/1886314f85a9458b1090d59a12c402652d838b21,Human Judges in the Era of Artificial Intelligence: Challenges and Opportunities,"ABSTRACT In recent years, artificial intelligence technology has been widely used in the field of justice. Compared with human judges, judicial artificial intelligence is more efficient, experience and objective. But artificial intelligence has its limits. Artificial intelligence is still essentially machine intelligence based on big data, algorithms and computing power, not organic intelligence. Subject to the difference between judicial artificial intelligence and human judges in knowledge structure, application scenario and potential ability, judicial artificial intelligence can not completely replace human judges. Therefore, it is important to make it clear that judicial artificial intelligence is only a helper of human judges, not a stand-in. Firstly, it should give full play to the role of judicial artificial intelligence in dealing with simple cases and transactional work. Secondly, the roles and functions of judges should be actively transformed to make them more professional, rational and warm.",2021.0,Zichun Xu
6a6e113725480438194bc3fa70ef7bfbfd6af8e8,https://www.semanticscholar.org/paper/6a6e113725480438194bc3fa70ef7bfbfd6af8e8,Artificial intelligence and education in China,"ABSTRACT This paper examines the political economy of artificial intelligence (AI) and education in China, through an analysis of government policy and private sector enterprise. While media and policy discourse often portray China’s AI development in terms of a unified national strategy, and a burgeoning geopolitical contestation for future global dominance, this analysis will suggest a more nuanced internal complexity, involving differing regional networks and international corporate activity. The first section considers two key policy documents published by the central Chinese government, which are shown to implicate educational institutions as influential actors in national and regional strategies for AI development, with a significant role in plans to train domestic expertise. The second section outlines three prominent private education companies: New Oriental Group, Tomorrow Advancing Life (TAL), and Squirrel AI. These companies are selected to represent important aspects of China’s development of educational AI applications, including the influence of a well-established private education sector, and a growing interest in international corporate activity. The paper concludes with the suggestion that while central government policy reserves a significant role for education in the national AI strategy, the private sector is utilising favourable political conditions to rapidly develop educational applications and markets.",2020.0,Jeremy Knox
2d9597a69e58b27ffb8531abb885da62432666d3,https://www.semanticscholar.org/paper/2d9597a69e58b27ffb8531abb885da62432666d3,Artificial intelligence and deep learning in ophthalmology,"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI ‘black-box’ algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.",2018.0,"D. Ting, L. Pasquale, L. Peng, J. P. Campbell, Aaron Y. Lee, R. Raman, G. Tan, L. Schmetterer, P. Keane, T. Wong"
3b420fdab1480a5e8ece3fa76a741538997efa50,https://www.semanticscholar.org/paper/3b420fdab1480a5e8ece3fa76a741538997efa50,Artificial Intelligence in Healthcare,"Abstract not available 
KYAMC Journal Vol. 11, No.-4, January 2021, Page 164-165",2021.0,"Ajay Kumar Yadav, Rajesh Mamilla"
0f1c31aa40a7d0b3ba68a6d1ef2bfc92e7f8ae0d,https://www.semanticscholar.org/paper/0f1c31aa40a7d0b3ba68a6d1ef2bfc92e7f8ae0d,Introduction to artificial intelligence in medicine,"Abstract The term Artificial Intelligence (AI) was coined by John McCarthy in 1956 during a conference held on this subject. However, the possibility of machines being able to simulate human behavior and actually think was raised earlier by Alan Turing who developed the Turing test in order to differentiate humans from machines. Since then, computational power has grown to the point of instant calculations and the ability evaluate new data, according to previously assessed data, in real time. Today, AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients’ electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians’ capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends.",2019.0,"Y. Mintz, Ronit Brodie"
a4da7f5ceeb34615707af2e4f29e27c152590dfc,https://www.semanticscholar.org/paper/a4da7f5ceeb34615707af2e4f29e27c152590dfc,"Artificial intelligence: a survey on evolution, models, applications and future trends","Artificial intelligence (AI) is one of the core drivers of industrial development and a critical factor in promoting the integration of emerging technologies, such as graphic processing unit, Inter...",2019.0,Yang Lu
5314b05697d58b27a9a16bc2049e9b98999a81f7,https://www.semanticscholar.org/paper/5314b05697d58b27a9a16bc2049e9b98999a81f7,Artificial Intelligence: The Ambiguous Labor Market Impact of Automating Prediction,"Recent advances in artificial intelligence are primarily driven by machine learning, a prediction technology. Prediction is useful because it is an input into decision-making. In order to appreciate the impact of artificial intelligence on jobs, it is important to understand the relative roles of prediction and decision tasks. We describe and provide examples of how artificial intelligence will affect labor, emphasizing differences between when the automation of prediction leads to automating decisions versus enhancing decision-making by humans.",2019.0,"A. Agrawal, J. Gans, Avi Goldfarb"
563daeb9d787f63af90e2f6d26721dcd1048aeee,https://www.semanticscholar.org/paper/563daeb9d787f63af90e2f6d26721dcd1048aeee,From Statistical Relational to Neuro-Symbolic Artificial Intelligence,Neuro-symbolic and statistical relational artificial intelligence both integrate frameworks for learning with logical reasoning. This survey identifies several parallels across seven different dimensions between these two fields. These cannot only be used to characterize and position neuro-symbolic artificial intelligence approaches but also to identify a number of directions for further research.,2020.0,"L. D. Raedt, Sebastijan Dumancic, Robin Manhaeve, Giuseppe Marra"
442e88a8532dec2e14b6de62df06abc30add32e0,https://www.semanticscholar.org/paper/442e88a8532dec2e14b6de62df06abc30add32e0,Understanding the Role of Artificial Intelligence in Personalized Engagement Marketing,"This article explores the role of artificial intelligence (AI) in aiding personalized engagement marketing—an approach to create, communicate, and deliver personalized offerings to customers. It proposes that consumers are ready for a new journey in which AI is a tool for endless options and information that are narrowed and curated in a personalized way. It also provides predictions for managers regarding the AI-driven environment on branding and customer management practices in both developed and developing countries.",2019.0,"Vinay Kumar, Bharath Rajan, R. Venkatesan, Jim Lecinski"
494e89a013256934470c53145c2ad84b3d752280,https://www.semanticscholar.org/paper/494e89a013256934470c53145c2ad84b3d752280,Robustness and Explainability of Artificial Intelligence,"In the light of the recent advances in artificial intelligence (AI), the serious negative consequences of its use for EU citizens and organisations have led to multiple initiatives from the European Commission to set up the principles of a trustworthy and secure AI. Among the identified requirements, the concepts of robustness and explainability of AI systems have emerged as key elements for a future regulation of this technology. This Technical Report by the European Commission Joint Research Centre (JRC) aims to contribute to this movement for the establishment of a sound regulatory framework for AI, by making the connection between the principles embodied in current regulations regarding to the cybersecurity of digital systems and the protection of data, the policy activities concerning AI, and the technical discussions within the scientific community of AI, in particular in the field of machine learning, that is largely at the origin of the recent advancements of this technology. The individual objectives of this report are to provide a policy-oriented description of the current perspectives of AI and its implications in society, an objective view on the current landscape of AI, focusing of the aspects of robustness and explainability. This also include a technical discussion of the current risks associated with AI in terms of security, safety, and data protection, and a presentation of the scientific solutions that are currently under active development in the AI community to mitigate these risks. This report puts forward several policy-related considerations for the attention of policy makers to establish a set of standardisation and certification tools for AI. First, the development of methodologies to evaluate the impacts of AI on society, built on the model of the Data Protection Impact Assessments (DPIA) introduced in the General Data Protection Regulation (GDPR), is discussed. Secondly, a focus is made on the establishment of methodologies to assess the robustness of systems that would be adapted to the context of use. This would come along with the identification of known vulnerabilities of AI systems, and the technical solutions that have been proposed in the scientific community to address them. Finally, the aspects of transparency and explainability of AI are discussed, including the explainability-by-design approaches for AI models.",2020.0,"Hamon Ronan, Junklewitz Henrik, S. Ignacio"
bc89a2e9c6541a770c231dca751189f597ac7a97,https://www.semanticscholar.org/paper/bc89a2e9c6541a770c231dca751189f597ac7a97,Artificial intelligence in medical diagnosis,"The use of artificial intelligence (AI) in medicine made its beginning five decades ago in 1972 when researchers at Stanford University in the USA developed an expert system MYCIN for treating blood infections. Since then, AI-based research in medicine is evolving every day to bring efficiency into medical diagnosis, early prediction of outcome, better clinical treatment and for rapid prognosis and recovery of patients. With the availability of an increasingly growing patients' associated information such as texts, audios, images and videos in structured and unstructured formats, it has become a challenge for computer scientists to process such information and to understand its insights into meaningful conclusions. With the development of powerful algorithms, AI has proven its ability in understanding the insights into data and taking health-related decisions in real time and improving clinical practice. This article provides an overview of various AI techniques in healthcare, especially deep learning used in medical diagnosis. The article also highlights the challenges and concerns of AI implementation.",2020.0,"Peter Szolovits, R. Patil, W. Schwartz"
be357b6fe7c4fd5f07948947f4bc83bdfbf1d34f,https://www.semanticscholar.org/paper/be357b6fe7c4fd5f07948947f4bc83bdfbf1d34f,The Use of Artificial Intelligence in Tribology—A Perspective,"Artificial intelligence and, in particular, machine learning methods have gained notable attention in the tribological community due to their ability to predict tribologically relevant parameters such as, for instance, the coefficient of friction or the oil film thickness. This perspective aims at highlighting some of the recent advances achieved by implementing artificial intelligence, specifically artificial neutral networks, towards tribological research. The presentation and discussion of successful case studies using these approaches in a tribological context clearly demonstrates their ability to accurately and efficiently predict these tribological characteristics. Regarding future research directions and trends, we emphasis on the extended use of artificial intelligence and machine learning concepts in the field of tribology including the characterization of the resulting surface topography and the design of lubricated systems.",2020.0,"A. Rosenkranz, Max Marian, F. Profito, Nathan Aragon, Raj Shah"
a132e66655f9d4f8edba54df053c6656c4c617be,https://www.semanticscholar.org/paper/a132e66655f9d4f8edba54df053c6656c4c617be,Artificial intelligence in health care: accountability and safety,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.",2020.0,"Ibrahim Habli, Tom Lawton, Zoe Porter"
ad34a4b66a24f4607577da0994ecb7250957a599,https://www.semanticscholar.org/paper/ad34a4b66a24f4607577da0994ecb7250957a599,Artificial intelligence as the next step towards precision pathology,"Pathology is the cornerstone of cancer care. The need for accuracy in histopathologic diagnosis of cancer is increasing as personalized cancer therapy requires accurate biomarker assessment. The appearance of digital image analysis holds promise to improve both the volume and precision of histomorphological evaluation. Recently, machine learning, and particularly deep learning, has enabled rapid advances in computational pathology. The integration of machine learning into routine care will be a milestone for the healthcare sector in the next decade, and histopathology is right at the centre of this revolution. Examples of potential high‐value machine learning applications include both model‐based assessment of routine diagnostic features in pathology, and the ability to extract and identify novel features that provide insights into a disease. Recent groundbreaking results have demonstrated that applications of machine learning methods in pathology significantly improves metastases detection in lymph nodes, Ki67 scoring in breast cancer, Gleason grading in prostate cancer and tumour‐infiltrating lymphocyte (TIL) scoring in melanoma. Furthermore, deep learning models have also been demonstrated to be able to predict status of some molecular markers in lung, prostate, gastric and colorectal cancer based on standard HE slides. Moreover, prognostic (survival outcomes) deep neural network models based on digitized HE slides have been demonstrated in several diseases, including lung cancer, melanoma and glioma. In this review, we aim to present and summarize the latest developments in digital image analysis and in the application of artificial intelligence in diagnostic pathology.",2020.0,"B. Ács, M. Rantalainen, J. Hartman"
6a1a3dc022c30454574b9c530393735bbd1345ea,https://www.semanticscholar.org/paper/6a1a3dc022c30454574b9c530393735bbd1345ea,Artificial intelligence in oral and maxillofacial radiology: what is currently possible?,"Artificial intelligence, which has been actively applied in a broad range of industries in recent years, is an active area of interest for many researchers. Dentistry is no exception to this trend, and the applications of artificial intelligence are particularly promising in the field of oral and maxillofacial (OMF) radiology. Recent researches on artificial intelligence in OMF radiology have mainly used convolutional neural networks, which can perform image classification, detection, segmentation, registration, generation, and refinement. Artificial intelligence systems in this field have been developed for the purposes of radiographic diagnosis, image analysis, forensic dentistry, and image quality improvement. Tremendous amounts of data are needed to achieve good results, and involvement of OMF radiologist is essential for making accurate and consistent data sets, which is a time-consuming task. In order to widely use artificial intelligence in actual clinical practice in the future, there are lots of problems to be solved, such as building up a huge amount of fine-labeled open data set, understanding of the judgment criteria of artificial intelligence, and DICOM hacking threats using artificial intelligence. If solutions to these problems are presented with the development of artificial intelligence, artificial intelligence will develop further in the future and is expected to play an important role in the development of automatic diagnosis systems, the establishment of treatment plans, and the fabrication of treatment tools. OMF radiologists, as professionals who thoroughly understand the characteristics of radiographic images, will play a very important role in the development of artificial intelligence applications in this field.",2020.0,"M. Heo, Jo-Eun Kim, Jaejoon Hwang, Sang-Sun Han, Jin-Soo Kim, W. Yi, I. Park"
2333016ded3dd7ff4f06ad0d7b0139e34559c4b0,https://www.semanticscholar.org/paper/2333016ded3dd7ff4f06ad0d7b0139e34559c4b0,Artificial intelligence in cancer therapy,"Artificial intelligence can optimize cancer drug discovery, development, and administration Artificial intelligence (AI) approaches have the potential to affect several facets of cancer therapy. These include drug discovery and development and how these drugs are clinically validated and ultimately administered at the point of care, among others. Currently, these processes are expensive and time-consuming. Moreover, therapies often result in variable treatment outcomes between patients. The convergence of AI and cancer therapy has resulted in multiple solutions to address these challenges. AI platforms ranging from machine learning to neural networks can accelerate drug discovery, harness biomarkers to accurately match patients to clinical trials, and truly personalize cancer therapy using only a patient's own data. These advances are indicators that practice-changing cancer therapy empowered by AI may be on the horizon.",2020.0,D. Ho
8991b622e0b9fd8f8a4cb219b3aa8bad0a6346cf,https://www.semanticscholar.org/paper/8991b622e0b9fd8f8a4cb219b3aa8bad0a6346cf,Swarm intelligence: from natural to artificial systems,"1. Introduction 2. Ant Foraging Behavior, Combinatorial Optimization, and Routing in Communications Networks 3. Division of Labor and Task Allocation 4. Cemetery Organization, Brood Sorting, Data Analysis, and Graph Partitioning 5. Self-Organization and Templates: Application to Data Analysis and Graph Partitioning 6. Nest Building and Self-Assembling 7. Cooperative Transport by Insects and Robots 8. Epilogue",1999.0,"E. Bonabeau, M. Dorigo, G. Theraulaz"
d2d79513f32c4d09b6255b18514d7ad07ebf43fe,https://www.semanticscholar.org/paper/d2d79513f32c4d09b6255b18514d7ad07ebf43fe,Explainable artificial intelligence: A survey,"In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.",2018.0,"Filip Karlo Dosilovic, Mario Brčič, N. Hlupic"
f61f531d856232aae17da5121b89281f2f01be62,https://www.semanticscholar.org/paper/f61f531d856232aae17da5121b89281f2f01be62,ARTIFICIAL INTELLIGENCE IN BUSINESS AND ECONOMICS RESEARCH: TRENDS AND FUTURE,"Artificial Intelligence is a disruptive technology developed during the 20th century, which has undergone an accelerated evolution, underpinning solutions to complex problems in the business world. Neural Networks, Machine Learning, or Deep Learning are concepts currently associated with terms such as digital marketing, decision making, industry 4.0 and business digital transformation. Interest in this technology will increase as the competitive advantages of the use of Artificial Intelligence by economic entities is realised. The aim of this research is to analyse the state-of-the-art research of Artificial Intelligence in business. To this end, a bibliometric analysis has been implement using the Web of Science and Scopus online databases. By using a fractional counting method, this paper identifies 11 clusters and the most frequent terms used in Artificial Intelligence research. The present study identifies the main trends in research on Artificial Intelligence in business and proposes future lines of inquiry.",2020.0,"J. Ruiz-Real, Juan Uribe-Toril, J. A. Torres, Jaime De Pablo"
522c3f62cb117bb2928b8bcfc95d7430eb749115,https://www.semanticscholar.org/paper/522c3f62cb117bb2928b8bcfc95d7430eb749115,Artificial Intelligence and the Public Sector—Applications and Challenges,"ABSTRACT Advances in artificial intelligence (AI) have attracted great attention from researchers and practitioners and have opened up a broad range of beneficial opportunities for AI usage in the public sector. Against this background, there is an emerging need for a holistic understanding of the range and impact of AI-based applications and associated challenges. However, previous research considers AI applications and challenges only in isolation and fragmentarily. Given the lack of a comprehensive overview of AI-based applications and challenges for the public sector, our conceptual approach analyzes and compiles relevant insights from scientific literature to provide an integrative overview of AI applications and related challenges. Our results suggest 10 AI application areas, describing their value creation and functioning as well as specific public use cases. In addition, we identify four major dimensions of AI challenges. We finally discuss our findings, deriving implications for theory and practice and providing suggestions for future research.",2018.0,"B. Wirtz, Jan C. Weyerer, Carolin Geyer"
4daeaf6a5a4d578d25275e779cf9257fb30fbcfe,https://www.semanticscholar.org/paper/4daeaf6a5a4d578d25275e779cf9257fb30fbcfe,Application of Artificial Intelligence to Gastroenterology and Hepatology.,"Since 2010, substantial progress has been made in artificial intelligence (AI) and its application to medicine. AI is explored in gastroenterology for endoscopic analysis of lesions, in detection of cancer, and to facilitate the analysis of inflammatory lesions or gastrointestinal bleeding during wireless capsule endoscopy. AI is also tested to assess liver fibrosis and to differentiate patients with pancreatic cancer from those with pancreatitis. AI might also be used to establish prognoses of patients or predict their response to treatments, based on multiple factors. We review the ways in which AI may help physicians make a diagnosis or establish a prognosis and discuss its limitations, knowing that further randomized controlled studies will be required before the approval of AI techniques by the health authorities.",2019.0,"C. Le Berre, W. Sandborn, Sabeur Aridhi, M. Devignes, L. Fournier, Malika Smaïl-Tabbone, S. Danese, L. Peyrin-Biroulet"
34e22341d14ae680aae1da06f9ce6bf937058097,https://www.semanticscholar.org/paper/34e22341d14ae680aae1da06f9ce6bf937058097,Methodologic Guide for Evaluating Clinical Performance and Effect of Artificial Intelligence Technology for Medical Diagnosis and Prediction.,"The use of artificial intelligence in medicine is currently an issue of great interest, especially with regard to the diagnostic or predictive analysis of medical images. Adoption of an artificial intelligence tool in clinical practice requires careful confirmation of its clinical utility. Herein, the authors explain key methodology points involved in a clinical evaluation of artificial intelligence technology for use in medicine, especially high-dimensional or overparameterized diagnostic or predictive models in which artificial deep neural networks are used, mainly from the standpoints of clinical epidemiology and biostatistics. First, statistical methods for assessing the discrimination and calibration performances of a diagnostic or predictive model are summarized. Next, the effects of disease manifestation spectrum and disease prevalence on the performance results are explained, followed by a discussion of the difference between evaluating the performance with use of internal and external datasets, the importance of using an adequate external dataset obtained from a well-defined clinical cohort to avoid overestimating the clinical performance as a result of overfitting in high-dimensional or overparameterized classification model and spectrum bias, and the essentials for achieving a more robust clinical evaluation. Finally, the authors review the role of clinical trials and observational outcome studies for ultimate clinical verification of diagnostic or predictive artificial intelligence tools through patient outcomes, beyond performance metrics, and how to design such studies. © RSNA, 2018.",2018.0,"S. Park, Kyunghwa Han"
257d8f6a89b682f63a63a1f6b7e46e1803dfcc94,https://www.semanticscholar.org/paper/257d8f6a89b682f63a63a1f6b7e46e1803dfcc94,"Artificial Intelligence, Automation and Work","We summarize a framework for the study of the implications of automation and AI on the demand for labor, wages, and employment. Our task-based framework emphasizes the displacement effect that automation creates as machines and AI replace labor in tasks that it used to perform. This displacement effect tends to reduce the demand for labor and wages. But it is counteracted by a productivity effect, resulting from the cost savings generated by automation, which increase the demand for labor in non-automated tasks. The productivity effect is complemented by additional capital accumulation and the deepening of automation (improvements of existing machinery), both of which further increase the demand for labor. These countervailing effects are incomplete. Even when they are strong, automation in- creases output per worker more than wages and reduce the share of labor in national income. The more powerful countervailing force against automation is the creation of new labor-intensive tasks, which reinstates labor in new activities and tends to in- crease the labor share to counterbalance the impact of automation. Our framework also highlights the constraints and imperfections that slow down the adjustment of the economy and the labor market to automation and weaken the resulting produc- tivity gains from this transformation: a mismatch between the skill requirements of new technologies, and the possibility that automation is being introduced at an excessive rate, possibly at the expense of other productivity-enhancing technologies.",2018.0,"D. Acemoglu, P. Restrepo"
d92c390fcb0ea1cb9da8ee4a2e85bebdfc65c96f,https://www.semanticscholar.org/paper/d92c390fcb0ea1cb9da8ee4a2e85bebdfc65c96f,Artificial Intelligence in Surgery: Promises and Perils,"Objective: The aim of this review was to summarize major topics in artificial intelligence (AI), including their applications and limitations in surgery. This paper reviews the key capabilities of AI to help surgeons understand and critically evaluate new AI applications and to contribute to new developments. Summary Background Data: AI is composed of various subfields that each provide potential solutions to clinical problems. Each of the core subfields of AI reviewed in this piece has also been used in other industries such as the autonomous car, social networks, and deep learning computers. Methods: A review of AI papers across computer science, statistics, and medical sources was conducted to identify key concepts and techniques within AI that are driving innovation across industries, including surgery. Limitations and challenges of working with AI were also reviewed. Results: Four main subfields of AI were defined: (1) machine learning, (2) artificial neural networks, (3) natural language processing, and (4) computer vision. Their current and future applications to surgical practice were introduced, including big data analytics and clinical decision support systems. The implications of AI for surgeons and the role of surgeons in advancing the technology to optimize clinical effectiveness were discussed. Conclusions: Surgeons are well positioned to help integrate AI into modern practice. Surgeons should partner with data scientists to capture data across phases of care and to provide clinical context, for AI has the potential to revolutionize the way surgery is taught and practiced with the promise of a future optimized for the highest quality patient care.",2018.0,"D. Hashimoto, G. Rosman, D. Rus, O. Meireles"
d5a8e412f80dee5d23b2d838445b20d575c90de4,https://www.semanticscholar.org/paper/d5a8e412f80dee5d23b2d838445b20d575c90de4,Supporting Artificial Social Intelligence With Theory of Mind,"In this paper, we discuss the development of artificial theory of mind as foundational to an agent's ability to collaborate with human team members. Agents imbued with artificial social intelligence will require various capabilities to gather the social data needed to inform an artificial theory of mind of their human counterparts. We draw from social signals theorizing and discuss a framework to guide consideration of core features of artificial social intelligence. We discuss how human social intelligence, and the development of theory of mind, can contribute to the development of artificial social intelligence by forming a foundation on which to help agents model, interpret and predict the behaviors and mental states of humans to support human-agent interaction. Artificial social intelligence will need the processing capabilities to perceive, interpret, and generate combinations of social cues to operate within a human-agent team. Artificial Theory of Mind affords a structure by which a socially intelligent agent could be imbued with the ability to model their human counterparts and engage in effective human-agent interaction. Further, modeling Artificial Theory of Mind can be used by an ASI to support transparent communication with humans, improving trust in agents, so that they may better predict future system behavior based on their understanding of and support trust in artificial socially intelligent agents.",2022.0,"Jessica Williams, S. Fiore, F. Jentsch"
770058895898d098f149dafc86af9caff92f8427,https://www.semanticscholar.org/paper/770058895898d098f149dafc86af9caff92f8427,Artificial intelligence faces reproducibility crisis.,"The booming field of artificial intelligence (AI) is grappling with a replication crisis, much like the ones that have afflicted psychology, medicine, and other fields over the past decade. Just because algorithms are based on code doesn9t mean experiments are easily replicated. Far from it. Unpublished codes and a sensitivity to training conditions have made it difficult for AI researchers to reproduce many key results. That is leading to a new conscientiousness about research methods and publication protocols. Last week, at a meeting of the Association for the Advancement of Artificial Intelligence in New Orleans, Louisiana, reproducibility was on the agenda, with some teams diagnosing the problem—and one laying out tools to mitigate it.",2018.0,M. Hutson
a2155552ca5afb784a3c1d67a5bcbd4e688b6e05,https://www.semanticscholar.org/paper/a2155552ca5afb784a3c1d67a5bcbd4e688b6e05,DeepStack: Expert-level artificial intelligence in heads-up no-limit poker,"Computer code based on continual problem re-solving beats human professional poker players at a two-player variant of poker. Artificial intelligence masters poker Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents' cards. Moravčík et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold'em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry. Science, this issue p. 508 Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold’em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.",2017.0,"Matej Moravcík, Martin Schmid, Neil Burch, V. Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, K. Waugh, Michael Bradley Johanson, Michael H. Bowling"
b32f7a3d40eb9e95590d69b065b6458fdac0f703,https://www.semanticscholar.org/paper/b32f7a3d40eb9e95590d69b065b6458fdac0f703,The Impact of Artificial Intelligence on the Labor Market,"I develop a new method to predict the impacts of a technology on occupations. I use the overlap between the text of job task descriptions and the text of patents to construct a measure of the exposure of tasks to automation. I first apply the method to historical cases such as software and industrial robots. I establish that occupations I measure as highly exposed to previous automation technologies saw declines in employment and wages over the relevant periods. I use the fitted parameters from the case studies to predict the impacts of artificial intelligence. I find that, in contrast to software and robots, AI is directed at high-skilled tasks. Under the assumption that the historical pattern of long-run substitution will continue, I estimate that AI will reduce 90:10 wage inequality, but will not affect the top 1%.",2019.0,Michael Webb
9e58e541b8549cd438c9d3e8cfb91dd12760589f,https://www.semanticscholar.org/paper/9e58e541b8549cd438c9d3e8cfb91dd12760589f,Organizational Decision-Making Structures in the Age of Artificial Intelligence,"How does organizational decision-making change with the advent of artificial intelligence (AI)-based decision-making algorithms? This article identifies the idiosyncrasies of human and AI-based decision making along five key contingency factors: specificity of the decision search space, interpretability of the decision-making process and outcome, size of the alternative set, decision-making speed, and replicability. Based on a comparison of human and AI-based decision making along these dimensions, the article builds a novel framework outlining how both modes of decision making may be combined to optimally benefit the quality of organizational decision making. The framework presents three structural categories in which decisions of organizational members can be combined with AI-based decisions: full human to AI delegation; hybrid—human-to-AI and AI-to-human—sequential decision making; and aggregated human–AI decision making.",2019.0,"Y. Shrestha, Shiko M. Ben-Menahem, G. von Krogh"
c63f428911e9ea11a7d582e5f3f817197a6cbfe8,https://www.semanticscholar.org/paper/c63f428911e9ea11a7d582e5f3f817197a6cbfe8,Artiﬁcial Intelligence,"Distributional semantics based on neural approaches is a cornerstone of Natural Language Processing, with surprising connections to human meaning representation as well. Recent Transformer-based Language Models have proven capable of producing contextual word representations that reliably convey sense-speciﬁc information, simply as a product of self-supervision. Prior work has shown that these contextual representations can be used to accurately represent large sense inventories as sense embeddings, to the extent that a distance-based solution to Word Sense Disambiguation (WSD) tasks outperforms models trained speciﬁcally for the task. Still, there remains much to understand on how to use these Neural Language Models (NLMs) to produce sense embeddings that can better harness each NLM’s meaning representation abilities. In this work we introduce a more principled approach to leverage information from all layers of NLMs, informed by a probing analysis on 14 NLM variants. We also emphasize the versatility of these sense embeddings in contrast to task-speciﬁc models, applying them on several sense-related tasks, besides WSD, while demonstrating improved performance using our proposed approach over prior work focused on sense embeddings. Finally, we discuss unexpected ﬁndings regarding layer and model performance variations, and potential applications for downstream tasks.",2019.0,"Daniel Loureiro, Alípio Mário, José Camacho-Collados"
7c4a9643c701c0c91ea50fd587038f79187a0a5e,https://www.semanticscholar.org/paper/7c4a9643c701c0c91ea50fd587038f79187a0a5e,Artificial Intelligence: A Guide to Intelligent Systems,"From the Publisher: 
Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author. 
 
The book provides an introduction to the field of computer intelligence, covering 
 
rule-based expert systems, 
fuzzy expert systems, 
frame-based expert systems, 
artificail neural networks, 
evolutionary computation, 
hybrid intelligent systems, 
knowledge engineering, 
data mining. 
 
 
In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit",2001.0,M. Negnevitsky
4bfdc3910684396e22f6675470002ef162bebdbf,https://www.semanticscholar.org/paper/4bfdc3910684396e22f6675470002ef162bebdbf,Artificial-Intelligence-Enabled Intelligent 6G Networks,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.",2019.0,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu"
71ab4f9c2d36b4b05e58263a06e074a321505016,https://www.semanticscholar.org/paper/71ab4f9c2d36b4b05e58263a06e074a321505016,The Artificial Intelligence of the Ethics of Artificial Intelligence,"Artificial intelligence (AI) is a technical term often referring to artifacts used to detect contexts for human actions, or sometimes also for machines able to effect actions in response to detected contexts. Our capacity to build such artifacts has been increasing, and with it the impact they have on our society. This does not alter the fundamental roots or motivations of law, regulation, or diplomacy, which rest on persuading humans to behave in a way that provides sustainable security for humans. It does however alter nearly every other aspect of human social behaviour, including making accountability and responsibility potentially easier to trace. This chapter reviews the nature and implications of AI with particular attention to how they impinge on possible applications to and of law.",2020.0,J. Bryson
bb0e6bc1b5d9a1f93b5f7e8975db72bde253710f,https://www.semanticscholar.org/paper/bb0e6bc1b5d9a1f93b5f7e8975db72bde253710f,Machine Learning and Artificial Intelligence,"This chapter proposes a cost-effective and scalable approach to obtain information on the current living standards and development in rural areas across India. The model utilizes a CNN to analyze satellite images of an area and predict its land type and level of development. A decision tree classifies a region as rural or urban based on the analysis. A summary describing the area is generated from inferences made on the recorded statistics. The CNN is able to predict the land and development distribution with an accuracy of 95.1%. The decision tree predicts rural areas with a precision of 99.6% and recall of 88.9%. The statistics obtained for a dataset of more than 1000 villages in India are cross-validated against the Census of India 2011 data. The proposed technique is in contrast to traditional door-to-door surveying methods as the information retrieved is relevant and obtained without human intervention. Hence, it can aid efforts in tracking poverty at a finer level and provide insight on improving the economic livelihood in rural areas.",2020.0,"Anupama Hoskoppa Sundaramurthy, Nitya Raviprakash, Divija Devarla, Asmitha Rathis"
c4e255a80cb86de26003930a16a4c16b7270e8c5,https://www.semanticscholar.org/paper/c4e255a80cb86de26003930a16a4c16b7270e8c5,Introducing Artificial Intelligence Training in Medical Education,"Health care is evolving and with it the need to reform medical education. As the practice of medicine enters the age of artificial intelligence (AI), the use of data to improve clinical decision making will grow, pushing the need for skillful medicine-machine interaction. As the rate of medical knowledge grows, technologies such as AI are needed to enable health care professionals to effectively use this knowledge to practice medicine. Medical professionals need to be adequately trained in this new technology, its advantages to improve cost, quality, and access to health care, and its shortfalls such as transparency and liability. AI needs to be seamlessly integrated across different aspects of the curriculum. In this paper, we have addressed the state of medical education at present and have recommended a framework on how to evolve the medical education curriculum to include AI.",2019.0,"K. Paranjape, M. Schinkel, R. N. Nannan Panday, J. Car, P. Nanayakkara"
d385c3ed552661751b436e4cff3e72f7e77ef59f,https://www.semanticscholar.org/paper/d385c3ed552661751b436e4cff3e72f7e77ef59f,Arming the public with artificial intelligence to counter social bots,"The increased relevance of social media in our daily life has been accompanied by efforts to manipulate online conversations and opinions. Deceptive social bots -- automated or semi-automated accounts designed to impersonate humans -- have been successfully exploited for these kinds of abuse. Researchers have responded by developing AI tools to arm the public in the fight against social bots. Here we review the literature on different types of bots, their impact, and detection methods. We use the case study of Botometer, a popular bot detection tool developed at Indiana University, to illustrate how people interact with AI countermeasures. A user experience survey suggests that bot detection has become an integral part of the social media experience for many users. However, barriers in interpreting the output of AI tools can lead to fundamental misunderstandings. The arms race between machine learning methods to develop sophisticated bots and effective countermeasures makes it necessary to update the training data and features of detection tools. We again use the Botometer case to illustrate both algorithmic and interpretability improvements of bot scores, designed to meet user expectations. We conclude by discussing how future AI developments may affect the fight between malicious bots and the public.",2019.0,"Kai-Cheng Yang, Onur Varol, Clayton A. Davis, Emilio Ferrara, A. Flammini, F. Menczer"
8c5b1865d0b439630cb0c75a3f2c634fb2b7989b,https://www.semanticscholar.org/paper/8c5b1865d0b439630cb0c75a3f2c634fb2b7989b,Multi-agent systems: An introduction to distributed artificial intelligence,"From the Publisher: 
What are multi-agent systems? How do they work? What do they do? If you are looking for the answers to these questions, read on; for Jacques Ferber's authoritative book is the first to provide a single, coherent overview of multi-agent systems. Introduces and defines key concepts throughout the text; provides numerous examples to illustrate core principles; draws on contributions from different disciplines to present a holistic, comprehensive picture of state-of-the art agent technology; and describes all the latest developments in the field and encourages the reader to reflect on possibilities for the future.",1999.0,J. Ferber
cccd2cc52b088b930a2dc5829c59c9ae6124cb20,https://www.semanticscholar.org/paper/cccd2cc52b088b930a2dc5829c59c9ae6124cb20,Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning,"Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.",2019.0,R. Byrne
a4d513cfc9d4902ef1a80198582f29b8ba46ac28,https://www.semanticscholar.org/paper/a4d513cfc9d4902ef1a80198582f29b8ba46ac28,"The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation","The following organisations are named on the report: Future of Humanity Institute, University of Oxford, Centre for the Study of Existential Risk, University of Cambridge, Center for a New American Security, Electronic Frontier Foundation, OpenAI. The Future of Life Institute is acknowledged as a funder.",2018.0,"Miles Brundage, S. Avin, Jack Clark, H. Toner, P. Eckersley, Ben Garfinkel, A. Dafoe, P. Scharre, Thomas Zeitzoff, Bobby Filar, H. Anderson, H. Roff, Gregory C. Allen, J. Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, S. Beard, Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael Page, Joanna J. Bryson, Roman V. Yampolskiy, Dario Amodei"
35daaf19a893c1450dc2774fffef0c968dfc1616,https://www.semanticscholar.org/paper/35daaf19a893c1450dc2774fffef0c968dfc1616,Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns,"Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.",2019.0,"Heike Felzmann, E. F. Villaronga, C. Lutz, Aurelia Tamó-Larrieux"
49c58ea9d9a71913aef89d38c1994ff62c570634,https://www.semanticscholar.org/paper/49c58ea9d9a71913aef89d38c1994ff62c570634,Artificial Intelligence: American Attitudes and Trends,"This report presents a broad look at the American public’s attitudes toward artificial intelligence (AI) and AI governance, based on findings from a nationally representative survey of 2,000 American adults. As the study of the public opinion toward AI is relatively new, we aimed for breadth over depth, with our questions touching on: workplace automation; attitudes regarding international cooperation; the public’s trust in various actors to develop and regulate AI; views about the importance and likely impact of different AI governance challenges; and historical and cross-national trends in public opinion regarding AI. Our results provide preliminary insights into the character of US public opinion regarding AI.",2019.0,"Baobao Zhang, A. Dafoe"
cf4efa86481a961596e0d04050d6ff96e30e45e4,https://www.semanticscholar.org/paper/cf4efa86481a961596e0d04050d6ff96e30e45e4,Artificial intelligence: Implications for the future of work.,"Artificial intelligence (AI) is a broad transdisciplinary field with roots in logic, statistics, cognitive psychology, decision theory, neuroscience, linguistics, cybernetics, and computer engineering. The modern field of AI began at a small summer workshop at Dartmouth College in 1956. Since then, AI applications made possible by machine learning (ML), an AI subdiscipline, include Internet searches, e-commerce sites, goods and services recommender systems, image and speech recognition, sensor technologies, robotic devices, and cognitive decision support systems (DSSs). As more applications are integrated into everyday life, AI is predicted to have a globally transformative influence on economic and social structures similar to the effect that other general-purpose technologies, such as steam engines, railroads, electricity, electronics, and the Internet, have had. Novel AI applications in the workplace of the future raise important issues for occupational safety and health. This commentary reviews the origins of AI, use of ML methods, and emerging AI applications embedded in physical objects like sensor technologies, robotic devices, or operationalized in intelligent DSSs. Selected implications on the future of work arising from the use of AI applications, including job displacement from automation and management of human-machine interactions, are also reviewed. Engaging in strategic foresight about AI workplace applications will shift occupational research and practice from a reactive posture to a proactive one. Understanding the possibilities and challenges of AI for the future of work will help mitigate the unfavorable effects of AI on worker safety, health, and well-being.",2019.0,J. Howard
a00ee7ae4642b986b622e0f48c845e79707b707b,https://www.semanticscholar.org/paper/a00ee7ae4642b986b622e0f48c845e79707b707b,The EU Approach to Ethics Guidelines for Trustworthy Artificial Intelligence,"A continuous journey towards an appropriate governance framework for AI As part of its European strategy for Artificial Intelligence (AI), and as a response to the increasing ethical questions raised by this technology, the European Commission established an independent High-Level Expert Group on Artificial Intelligence (AI HLEG) in June 2018. The group was tasked to draft two deliver-ables: AI Ethics Guidelines and Policy and Investment Recommendations. Nine months later, its first deliverable was published, putting forward a comprehensive framework to achieve “ Trustworthy AI ” by offering ethical guidance to AI practitioners. This paper dives into the work carried out by the group, focusing in particular on its AI Ethics Guidelines. First, this paper clarifies the context that led to the creation of the AI HLEG and its mandate (I.). Subsequently, it elaborates on the Guidelines ’ aim and purpose (II.), and analyses the Guidelines ’ drafting process (III.). Particular focus is given to the questions surrounding the respective role played by ethics and law in the AI governance landscape (IV.), as well as some of the challenges that had to be overcome throughout the process (V.). Finally, this paper places the Guidelines in an international context, and sets out the next steps (VI.) ahead on the journey towards an appropriate governance framework for AI (VII.).",2019.0,Nathalie A. Smuha
b944b71f789393902e161bf5946b71036caa4863,https://www.semanticscholar.org/paper/b944b71f789393902e161bf5946b71036caa4863,Artificial Intelligence and the Implementation Challenge,"Background Applications of artificial intelligence (AI) in health care have garnered much attention in recent years, but the implementation issues posed by AI have not been substantially addressed. Objective In this paper, we have focused on machine learning (ML) as a form of AI and have provided a framework for thinking about use cases of ML in health care. We have structured our discussion of challenges in the implementation of ML in comparison with other technologies using the framework of Nonadoption, Abandonment, and Challenges to the Scale-Up, Spread, and Sustainability of Health and Care Technologies (NASSS). Methods After providing an overview of AI technology, we describe use cases of ML as falling into the categories of decision support and automation. We suggest these use cases apply to clinical, operational, and epidemiological tasks and that the primary function of ML in health care in the near term will be decision support. We then outline unique implementation issues posed by ML initiatives in the categories addressed by the NASSS framework, specifically including meaningful decision support, explainability, privacy, consent, algorithmic bias, security, scalability, the role of corporations, and the changing nature of health care work. Results Ultimately, we suggest that the future of ML in health care remains positive but uncertain, as support from patients, the public, and a wide range of health care stakeholders is necessary to enable its meaningful implementation. Conclusions If the implementation science community is to facilitate the adoption of ML in ways that stand to generate widespread benefits, the issues raised in this paper will require substantial attention in the coming years.",2019.0,"James Shaw, Frank Rudzicz, T. Jamieson, Avi Goldfarb"
6e087108aa8048da8cfc82cdecb7071a55bab488,https://www.semanticscholar.org/paper/6e087108aa8048da8cfc82cdecb7071a55bab488,Applications of Artificial Intelligence in Agriculture: A Review,"The application of Artificial Intelligence (AI) has been evident in the agricultural sector recently. The sector faces numerous challenges in order to maximize its yield including improper soil treatment, disease and pest infestation, big data requirements, low output, and knowledge gap between farmers and technology. The main concept of AI in agriculture is its flexibility, high performance, accuracy, and cost-effectiveness. This paper presents a review of the applications of AI in soil management, crop management, weed management and disease management. A special focus is laid on the strength and limitations of the application and the way in utilizing expert systems for higher productivity.",2019.0,N. C. Eli-Chukwu
4881e926632f7a5110b802e618ba5bab3e8956c1,https://www.semanticscholar.org/paper/4881e926632f7a5110b802e618ba5bab3e8956c1,Swarm Intelligence: From Natural to Artificial Systems,"‘Swarm intelligence’ is defined in the preface of this book to be ‘the emergent collective intelligence of groups of simple agents’. As such, there is a huge range of work that the book could cover – from distributed software agents to cellular automata. A more accurate gloss of the actual content of the book comes in the introductory chapter – ‘designing problem solving devices inspired by social insects’ (p.23). Within this more circumscribed eld, the book provides fairly comprehensive coverage of recent research developments. It thus constitutes an excellent resource for researchers in the eld or for those wishing to familiarize themselves with current approaches e.g. it would be an ideal introduction for a doctoral student wanting to enter this area. But why and how should social insects inspire methods for problem solving? The introduction (Chapter 1) claims the critical concepts are self-organization and stigmergy. Self-organization is more or less self-explanatory – the idea that some kind of global coherence in structure or behaviour can emerge from purely local interactions. As such it is not a new concept (as the authors recognize, much of the theoretical work in this area comes from physics) nor is it evident that it applies to all social insect behaviour. For example, it could be argued that the idea (discussed in Chapter 5) of a template – a structure in the environment that organises the local activity towards a constrained result – is a global mechanism in much the same way as a centralized plan. However it is true that much of the success and exibility of social insect behaviour does seem to be the emergent result of local actions. Insect swarms provide an existence proof of the potential power of self-organizing systems. They can also provide specic ideas as to the kinds of local interactions to implement. If termites can build a huge, well-structured nest, and ants alternate their activities dynamically to increase colony efciency, then it should be possible to copy their local behaviours to achieve comparable global results. Stigmergy may be a less familiar term but refers to one important mechanism for successful self-organization: how agents’ effects on their environment can feed back to modify each others’ and their own behaviour. A good example discussed at length in Chapter 4 is distributed clustering. A simple agent behaviour of picking up isolated objects and dropping them when another object is encountered leads to the formation of initial small clusters that can effectively guide the further development of larger ones. These ideas have become an attractive (some might say seductive) alternative to more conventional ‘centralized’ approaches to problem solving. By providing an overview, this book allows some assessment of how successful this approach has been, as discussed further below. The presentation is nicely structured, with each chapter taking a different example of insect social behaviour – foraging, task allocation, brood sorting, self-organization and templates, nest building and co-operative transport – and presenting rst the biological data, then models proposed to explain it, then the generalization of such models to solve analogous problems such as routing in Connection Science, Vol. 14, No. 2, 2002, 163–164",2002.0,B. Webb
7da6a9935f454769f0a091efb7a7c366a9736340,https://www.semanticscholar.org/paper/7da6a9935f454769f0a091efb7a7c366a9736340,Global Evolution of Research in Artificial Intelligence in Health and Medicine: A Bibliometric Study,"The increasing application of Artificial Intelligence (AI) in health and medicine has attracted a great deal of research interest in recent decades. This study aims to provide a global and historical picture of research concerning AI in health and medicine. A total of 27,451 papers that were published between 1977 and 2018 (84.6% were dated 2008–2018) were retrieved from the Web of Science platform. The descriptive analysis examined the publication volume, and authors and countries collaboration. A global network of authors’ keywords and content analysis of related scientific literature highlighted major techniques, including Robotic, Machine learning, Artificial neural network, Artificial intelligence, Natural language process, and their most frequent applications in Clinical Prediction and Treatment. The number of cancer-related publications was the highest, followed by Heart Diseases and Stroke, Vision impairment, Alzheimer’s, and Depression. Moreover, the shortage in the research of AI application to some high burden diseases suggests future directions in AI research. This study offers a first and comprehensive picture of the global efforts directed towards this increasingly important and prolific field of research and suggests the development of global and national protocols and regulations on the justification and adaptation of medical AI products.",2019.0,"B. Tran, G. Vu, G. H. Ha, Q. Vuong, Manh-Tung Ho, Thu-Trang Vuong, Viet-Phuong La, Manh-Toan Ho, Kien-Cuong P. Nghiem, Huong Lan Thi Nguyen, C. Latkin, W. Tam, Ngai-Man Cheung, H. Nguyen, Cyrus S. H. Ho, R. Ho"
163fd9fac485940724e14fcd45d1fd6a4fce3932,https://www.semanticscholar.org/paper/163fd9fac485940724e14fcd45d1fd6a4fce3932,GeoAI: spatially explicit artificial intelligence techniques for geographic knowledge discovery and beyond,"Recent progress in Artificial Intelligence (AI) techniques, the large-scale availability of high-quality data, as well as advances in both hardware and software to efficiently process these data, a...",2019.0,"K. Janowicz, Song Gao, Grant McKenzie, Yingjie Hu, B. Bhaduri"
383dc22d1d6ca1310626ec56a63cd578273c2f93,https://www.semanticscholar.org/paper/383dc22d1d6ca1310626ec56a63cd578273c2f93,Artificial Intelligence and Surgical Decision-Making.,"Importance
Surgeons make complex, high-stakes decisions under time constraints and uncertainty, with significant effect on patient outcomes. This review describes the weaknesses of traditional clinical decision-support systems and proposes that artificial intelligence should be used to augment surgical decision-making.


Observations
Surgical decision-making is dominated by hypothetical-deductive reasoning, individual judgment, and heuristics. These factors can lead to bias, error, and preventable harm. Traditional predictive analytics and clinical decision-support systems are intended to augment surgical decision-making, but their clinical utility is compromised by time-consuming manual data management and suboptimal accuracy. These challenges can be overcome by automated artificial intelligence models fed by livestreaming electronic health record data with mobile device outputs. This approach would require data standardization, advances in model interpretability, careful implementation and monitoring, attention to ethical challenges involving algorithm bias and accountability for errors, and preservation of bedside assessment and human intuition in the decision-making process.


Conclusions and Relevance
Integration of artificial intelligence with surgical decision-making has the potential to transform care by augmenting the decision to operate, informed consent process, identification and mitigation of modifiable risk factors, decisions regarding postoperative management, and shared decisions regarding resource use.",2019.0,"T. Loftus, P. Tighe, A. Filiberto, P. Efron, S. Brakenridge, A. Mohr, Parisa Rashidi, G. Upchurch, A. Bihorac"
405087814efd38f2a8d852e9d523b8a9c10e841b,https://www.semanticscholar.org/paper/405087814efd38f2a8d852e9d523b8a9c10e841b,Radiomics with artificial intelligence: a practical guide for beginners.,"Radiomics is a relatively new word for the field of radiology, meaning the extraction of a high number of quantitative features from medical images. Artificial intelligence (AI) is broadly a set of advanced computational algorithms that basically learn the patterns in the data provided to make predictions on unseen data sets. Radiomics can be coupled with AI because of its better capability of handling a massive amount of data compared with the traditional statistical methods. Together, the primary purpose of these fields is to extract and analyze as much and meaningful hidden quantitative data as possible to be used in decision support. Nowadays, both radiomics and AI have been getting attention for their remarkable success in various radiological tasks, which has been met with anxiety by most of the radiologists due to the fear of replacement by intelligent machines. Considering ever-developing advances in computational power and availability of large data sets, the marriage of humans and machines in future clinical practice seems inevitable. Therefore, regardless of their feelings, the radiologists should be familiar with these concepts. Our goal in this paper was three-fold: first, to familiarize radiologists with the radiomics and AI; second, to encourage the radiologists to get involved in these ever-developing fields; and, third, to provide a set of recommendations for good practice in design and assessment of future works.",2019.0,"B. Koçak, E. S. Durmaz, Ece Ateş, Ö. Kılıçkesmez"
48b44b31b4d378d73bbdcf3c0a38346fea578177,https://www.semanticscholar.org/paper/48b44b31b4d378d73bbdcf3c0a38346fea578177,Artificial intelligence in healthcare: An essential guide for health leaders,"Artificial Intelligence (AI) is evolving rapidly in healthcare, and various AI applications have been developed to solve some of the most pressing problems that health organizations currently face. It is crucial for health leaders to understand the state of AI technologies and the ways that such technologies can be used to improve the efficiency, safety, and access of health services, achieving value-based care. This article provides a guide to understand the fundamentals of AI technologies (ie, machine learning, natural language processing, and AI voice assistants) as well as their proper use in healthcare. It also provides practical recommendations to help decision-makers develop an AI strategy that can support their digital healthcare transformation.",2019.0,"Mei Chen, Michel Décary"
ac1b26081b65d65888c96fa4d2e67a1930662db2,https://www.semanticscholar.org/paper/ac1b26081b65d65888c96fa4d2e67a1930662db2,"Artificial Intelligence, Algorithmic Pricing and Collusion","Increasingly, algorithms are supplanting human decision-makers in pricing goods and services. To analyze the possible consequences, we study experimentally the behavior of algorithms powered by Artificial Intelligence (Q-learning) in a workhorse oligopoly model of repeated price competition. We find that the algorithms consistently learn to charge supracompetitive prices, without communicating with one another. The high prices are sustained by collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand, changes in the number of players, and various forms of uncertainty. (JEL D21, D43, D83, L12, L13)",2018.0,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello"
d02f37dfd387ac4682d90bb0130a7d8fdad4c6c7,https://www.semanticscholar.org/paper/d02f37dfd387ac4682d90bb0130a7d8fdad4c6c7,Artificial Intelligence—The Revolution Hasn’t Happened Yet,"We praise Jordan for bringing much needed clarity about the current status of Artificial Intelligence (AI)—what it currently is and what it is not—as well as explaining the current challenges lying ahead and outlining what is missing and remains to be done. Jordan makes several claims supported by a list of talking points that we hope will reach a wide audience; ideally, that audience will include academic, university, and governmental leaders, at a time where significant resources are being allocated to AI for research and education.",2019.0,M. I. Jordan
1106db8568b404bfa6692f4fe41da56d1a68453e,https://www.semanticscholar.org/paper/1106db8568b404bfa6692f4fe41da56d1a68453e,Questions for Artificial Intelligence in Health Care.,"Artificial intelligence (AI) is gaining high visibility in the realm of health care innovation. Broadly defined, AI is a field of computer science that aims to mimic human intelligence with computer systems.1 This mimicry is accomplished through iterative, complex pattern matching, generally at a speed and scale that exceed human capability. Proponents suggest, often enthusiastically, that AI will revolutionize health care for patients and populations. However, key questions must be answered to translate its promise into action.",2019.0,"T. Maddox, J. Rumsfeld, Philip R. O. Payne"
b099ca77c4b9acbcdc9011e8f3a12c09f9b402fb,https://www.semanticscholar.org/paper/b099ca77c4b9acbcdc9011e8f3a12c09f9b402fb,On Defining Artificial Intelligence,"Abstract This article systematically analyzes the problem of defining “artificial intelligence.” It starts by pointing out that a definition influences the path of the research, then establishes four criteria of a good working definition of a notion: being similar to its common usage, drawing a sharp boundary, leading to fruitful research, and as simple as possible. According to these criteria, the representative definitions in the field are analyzed. A new definition is proposed, according to it intelligence means “adaptation with insufficient knowledge and resources.” The implications of this definition are discussed, and it is compared with the other definitions. It is claimed that this definition sheds light on the solution of many existing problems and sets a sound foundation for the field.",2019.0,Pei Wang
4af6c0c61bbaaca04d33deea73b69b8494e0c77e,https://www.semanticscholar.org/paper/4af6c0c61bbaaca04d33deea73b69b8494e0c77e,Review of Artificial Intelligence Adversarial Attack and Defense Technologies,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.",2019.0,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu"
836eedb1f4f3f4bd6d9c7b77045ca74166417b29,https://www.semanticscholar.org/paper/836eedb1f4f3f4bd6d9c7b77045ca74166417b29,The NOMAD laboratory: from data sharing to artificial intelligence,"The Novel Materials Discovery (NOMAD) Laboratory is a user-driven platform for sharing and exploiting computational materials science data. It accounts for the various aspects of data being a crucial raw material and most relevant to accelerate materials research and engineering. NOMAD, with the NOMAD Repository, and its code-independent and normalized form, the NOMAD Archive, comprises the worldwide largest data collection of this field. Based on its findable accessible, interoperable, reusable data infrastructure, various services are offered, comprising advanced visualization, the NOMAD Encyclopedia, and artificial-intelligence tools. The latter are realized in the NOMAD Analytics Toolkit. Prerequisite for all this is the NOMAD metadata, a unique and thorough description of the data, that are produced by all important computer codes of the community. Uploaded data are tagged by a persistent identifier, and users can also request a digital object identifier to make data citable. Developments and advancements of parsers and metadata are organized jointly with users and code developers. In this work, we review the NOMAD concept and implementation, highlight its orthogonality to and synergistic interplay with other data collections, and provide an outlook regarding ongoing and future developments.",2019.0,"C. Draxl, M. Scheffler"
78a9bab542e9fdb8f0b7d8e4cbaf4c6a9a0677b3,https://www.semanticscholar.org/paper/78a9bab542e9fdb8f0b7d8e4cbaf4c6a9a0677b3,Artificial intelligence (AI) and its implications for market knowledge in B2B marketing,"
Purpose
The purpose of this paper is to explain the technological phenomenon artificial intelligence (AI) and how it can contribute to knowledge-based marketing in B2B. Specifically, this paper describes the foundational building blocks of any artificial intelligence system and their interrelationships. This paper also discusses the implications of the different building blocks with respect to market knowledge in B2B marketing and outlines avenues for future research.


Design/methodology/approach
The paper is conceptual and proposes a framework to explicate the phenomenon AI and its building blocks. It further provides a structured discussion of how AI can contribute to different types of market knowledge critical for B2B marketing: customer knowledge, user knowledge and external market knowledge.


Findings
The paper explains AI from an input–processes–output lens and explicates the six foundational building blocks of any AI system. It also discussed how the combination of the building blocks transforms data into information and knowledge.


Practical implications
Aimed at general marketing executives, rather than AI specialists, this paper explains the phenomenon artificial intelligence, how it works and its relevance for the knowledge-based marketing in B2B firms. The paper highlights illustrative use cases to show how AI can impact B2B marketing functions.


Originality/value
The study conceptualizes the technological phenomenon artificial intelligence from a knowledge management perspective and contributes to the literature on knowledge management in the era of big data. It addresses calls for more scholarly research on AI and B2B marketing.
",2019.0,"Jeannette Paschen, Jan H. Kietzmann, Tim C Kietzmann"
1fccba11583dc9e1030713d61bd65e9e9990e39f,https://www.semanticscholar.org/paper/1fccba11583dc9e1030713d61bd65e9e9990e39f,Human-Centered Artificial Intelligence and Machine Learning,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.",2019.0,Mark O. Riedl
9e2bb62eec93c81aa57a0b0e7f9c8f59a4af9464,https://www.semanticscholar.org/paper/9e2bb62eec93c81aa57a0b0e7f9c8f59a4af9464,Integrating Artificial Intelligence and Nanotechnology for Precision Cancer Medicine,"Artificial intelligence (AI) and nanotechnology are two fields that are instrumental in realizing the goal of precision medicine—tailoring the best treatment for each cancer patient. Recent conversion between these two fields is enabling better patient data acquisition and improved design of nanomaterials for precision cancer medicine. Diagnostic nanomaterials are used to assemble a patient‐specific disease profile, which is then leveraged, through a set of therapeutic nanotechnologies, to improve the treatment outcome. However, high intratumor and interpatient heterogeneities make the rational design of diagnostic and therapeutic platforms, and analysis of their output, extremely difficult. Integration of AI approaches can bridge this gap, using pattern analysis and classification algorithms for improved diagnostic and therapeutic accuracy. Nanomedicine design also benefits from the application of AI, by optimizing material properties according to predicted interactions with the target drug, biological fluids, immune system, vasculature, and cell membranes, all affecting therapeutic efficacy. Here, fundamental concepts in AI are described and the contributions and promise of nanotechnology coupled with AI to the future of precision cancer medicine are reviewed.",2019.0,"O. Adir, M. Poley, Gal Chen, Sahar Froim, N. Krinsky, J. Shklover, Janna Shainsky‐Roitman, T. Lammers, Avi Schroeder"
789f699d80b00db53d58787b5ce7b146b386c330,https://www.semanticscholar.org/paper/789f699d80b00db53d58787b5ce7b146b386c330,Artificial Intelligence in Healthcare,"Artificial intelligence is to reduce human cognitive functions. It is bringing an approach to healthcare, powdered by increasing the availability of healthcare data and rapid progress of analyst techniques. We can survey the current status of Artificial intelligence applications in healthcare and discuss its future uses. It is the most transformative technology of the 21th century. Healthcare has been identified as an early candidate to be revolutized by artificial intelligence technologies. This article aims for providing an early stage contribution with the decision making capacities of artificial intelligence technologies. The possible ethical and legally complex backdrop of the existing framework. I will conclude the present structures are largely fit to deal with the challenge of artificial intelligence are present will discuss clearly about the artificial intelligence contribution to the present health care. Artificial intelligence, machine learning, deep learning can assist with proactive patient care, reduced future risk and streamlined work processes. 
Keywords: Artificial intelligence, machine learning, clinical decision support.",2019.0,"G.V.K.S. Abhinav, S. N. Subrahmanyam"
b735f2bbc005247cdb3fc4d4ce6a547163f63d81,https://www.semanticscholar.org/paper/b735f2bbc005247cdb3fc4d4ce6a547163f63d81,"Artificial Intelligence, Discretion, and Bureaucracy","This essay highlights the increasing use of artificial intelligence (AI) in governance and society and explores the relationship between AI, discretion, and bureaucracy. AI is an advanced information communication technology tool (ICT) that changes both the nature of human discretion within a bureaucracy and the structure of bureaucracies. To better understand this relationship, AI, discretion, and bureaucracy are explored in some detail. It is argued that discretion and decision-making are strongly influenced by intelligence, and that improvements in intelligence, such as those that can be found within the field of AI, can help improve the overall quality of administration. Furthermore, the characteristics, strengths, and weaknesses of both human discretion and AI are explored. Once these characteristics are laid out, a further exploration of the role AI may play in bureaucracies and bureaucratic structure is presented, followed by a specific focus on systems-level bureaucracies. In addition, it is argued that task distribution and task characteristics play a large role, along with the organizational and legal context, in whether a task favors human discretion or the use of AI. Complexity and uncertainty are presented as the major defining characteristics for categorizing tasks. Finally, a discussion is provided about the important cautions and concerns of utilizing AI in governance, in particular, with respect to existential risk and administrative evil.",2019.0,Justin B. Bullock
a9ef68be93c896d91c1f87ce622e28da540f5fba,https://www.semanticscholar.org/paper/a9ef68be93c896d91c1f87ce622e28da540f5fba,Using neuroscience to develop artificial intelligence,"Combining deep learning with brain-like innate structures may guide network models toward human-like learning When the mathematician Alan Turing posed the question “Can machines think?” in the first line of his seminal 1950 paper that ushered in the quest for artificial intelligence (AI) (1), the only known systems carrying out complex computations were biological nervous systems. It is not surprising, therefore, that scientists in the nascent field of AI turned to brain circuits as a source for guidance. One path that was taken since the early attempts to perform intelligent computation by brain-like circuits (2), and which led recently to remarkable successes, can be described as a highly reductionist approach to model cortical circuitry. In its basic current form, known as a “deep network” (or deep net) architecture, this brain-inspired model is built from successive layers of neuron-like elements, connected by adjustable weights, called “synapses” after their biological counterparts (3). The application of deep nets and related methods to AI systems has been transformative. They proved superior to previously known methods in central areas of AI research, including computer vision, speech recognition and production, and playing complex games. Practical applications are already in broad use, in areas such as computer vision and speech and text translation, and large-scale efforts are under way in many other areas. Here, I discuss how additional aspects of brain circuitry could supply cues for guiding network models toward broader aspects of cognition and general AI.",2019.0,S. Ullman
8ac05e528e5951184db25a8dd92971c786599627,https://www.semanticscholar.org/paper/8ac05e528e5951184db25a8dd92971c786599627,A Critical Take on the Policy Recommendations of the EU High-Level Expert Group on Artificial Intelligence,"The European Commission recently published the policy recommendations of its “High-Level Expert Group on Artificial Intelligence”: a heavily anticipated document, particularly in the context of the stated ambition of the new Commission President to regulate in that area. This article argues that these recommendations have significant deficits in a range of areas. It analyses a selection of the Group’s proposals in context of the governance of artificial intelligence more broadly, focusing on issues of framing, representation and expertise, and on the lack of acknowledgement of key issues of power and infrastructure underpinning modern information economies and practices of optimisation.",2019.0,Michael Veale
5f0625c30014c12f333eb518268647673d18f9f1,https://www.semanticscholar.org/paper/5f0625c30014c12f333eb518268647673d18f9f1,What can the brain teach us about building artificial intelligence?,"Abstract Lake et al. offer a timely critique on the recent accomplishments in artificial intelligence from the vantage point of human intelligence and provide insightful suggestions about research directions for building more human-like intelligence. Because we agree with most of the points they raised, here we offer a few points that are complementary.",2016.0,"B. Lake, T. Ullman, J. Tenenbaum, S. Gershman"
4a333658f27d620f6f37dc54868cea8dabb574e7,https://www.semanticscholar.org/paper/4a333658f27d620f6f37dc54868cea8dabb574e7,Artificial Intelligence for Vehicle-to-Everything: A Survey,"Recently, the advancement in communications, intelligent transportation systems, and computational systems has opened up new opportunities for intelligent traffic safety, comfort, and efficiency solutions. Artificial intelligence (AI) has been widely used to optimize traditional data-driven approaches in different areas of the scientific research. Vehicle-to-everything (V2X) system together with AI can acquire the information from diverse sources, can expand the driver’s perception, and can predict to avoid potential accidents, thus enhancing the comfort, safety, and efficiency of the driving. This paper presents a comprehensive survey of the research works that have utilized AI to address various research challenges in V2X systems. We have summarized the contribution of these research works and categorized them according to the application domains. Finally, we present open problems and research challenges that need to be addressed for realizing the full potential of AI to advance V2X systems.",2019.0,"Wang Tong, Azhar Hussain, Wang Bo, Sabita Maharjan"
4f50f2df1858f2427ec5e3586bdc4e4e5b8357ce,https://www.semanticscholar.org/paper/4f50f2df1858f2427ec5e3586bdc4e4e5b8357ce,The Economics of Artificial Intelligence,"With so many perspectives on the impact of artificial intelligence (AI) flooding the business press, it’s becoming increasingly rare to find one that’s truly original. So when strategy professor Ajay Agrawal shared his brilliantly simple view on AI, we stood up and took notice. Agrawal, who teaches at the University of Toronto’s Rotman School of Management and works with AI start-ups at the Creative Destruction Lab (which he founded), posits that AI serves a single, but potentially transformative, economic purpose: it significantly lowers the cost of prediction.",2019.0,"A. Agrawal, J. Gans, Avi Goldfarb"
7870593045c9f7ed926c4422c1b52d95a044bec2,https://www.semanticscholar.org/paper/7870593045c9f7ed926c4422c1b52d95a044bec2,Artificial Intelligence in Lung Cancer Pathology Image Analysis,"Objective: Accurate diagnosis and prognosis are essential in lung cancer treatment selection and planning. With the rapid advance of medical imaging technology, whole slide imaging (WSI) in pathology is becoming a routine clinical procedure. An interplay of needs and challenges exists for computer-aided diagnosis based on accurate and efficient analysis of pathology images. Recently, artificial intelligence, especially deep learning, has shown great potential in pathology image analysis tasks such as tumor region identification, prognosis prediction, tumor microenvironment characterization, and metastasis detection. Materials and Methods: In this review, we aim to provide an overview of current and potential applications for AI methods in pathology image analysis, with an emphasis on lung cancer. Results: We outlined the current challenges and opportunities in lung cancer pathology image analysis, discussed the recent deep learning developments that could potentially impact digital pathology in lung cancer, and summarized the existing applications of deep learning algorithms in lung cancer diagnosis and prognosis. Discussion and Conclusion: With the advance of technology, digital pathology could have great potential impacts in lung cancer patient care. We point out some promising future directions for lung cancer pathology image analysis, including multi-task learning, transfer learning, and model interpretation.",2019.0,"Shidan Wang, Donghan M. Yang, Ruichen Rong, Xiaowei Zhan, J. Fujimoto, Hongyu Liu, J. Minna, I. Wistuba, Yang Xie, Guanghua Xiao"
c066aad4fbdd01c3926f2861bb95af16ece26056,https://www.semanticscholar.org/paper/c066aad4fbdd01c3926f2861bb95af16ece26056,Emotional intelligence or artificial intelligence– an employee perspective,ABSTRACT Emotional intelligence as personal intelligence and artificial intelligence as a machine intelligence have been popular in the relevant literature over the last two decades. The current study integrates these two concepts and explores how emotional and artificial intelligence influences employee retention and performance with a focus on service employees in the hotel industry. Employee performance is operationalised into internal and external dimensions that captures employees’ task efficiency over both internal and external service encounters with co-workers and customers respectively. The data were collected from a variety of different ranking hotels. The results show that emotional intelligence has a significant effect on employee retention and performance; whereas artificial intelligence plays a significant moderating role in employee performance. A discussion of the findings and implications concludes this paper.,2019.0,"C. Prentice, Sergio Dominique Lopes, Xuequn Wang"
e4082122602b47d986de04cdbbc533a83291e77e,https://www.semanticscholar.org/paper/e4082122602b47d986de04cdbbc533a83291e77e,Artificial Intelligence and Digital Pathology: Challenges and Opportunities,"In light of the recent success of artificial intelligence (AI) in computer vision applications, many researchers and physicians expect that AI would be able to assist in many tasks in digital pathology. Although opportunities are both manifest and tangible, there are clearly many challenges that need to be overcome in order to exploit the AI potentials in computational pathology. In this paper, we strive to provide a realistic account of all challenges and opportunities of adopting AI algorithms in digital pathology from both engineering and pathology perspectives.",2018.0,"H. Tizhoosh, L. Pantanowitz"
8c1a26267b0d3cac6ac987cbaebd7cade50029eb,https://www.semanticscholar.org/paper/8c1a26267b0d3cac6ac987cbaebd7cade50029eb,The Impact of Artificial Intelligence on Innovation,"The current economy's efficiency may be greatly improved by artificial intelligence. We distinguish between automation-oriented applications like robotics and the potential for recent developments in ""deep learning"" to serve as a general-purpose method of invention, finding strong evidence of a ""shift"" in the importance of application-oriented learning research since 2009. However, it may have an even larger impact by serving as a new general-purpose ""method of invention"" that can reshape the nature of the innovation process and the organization of R&D. We suggest that this will likely result in a significant shift away from routine, labor-intensive research and toward research that makes use of the interaction between improved prediction algorithms and passively generated large datasets. In addition, strong incentives for specific businesses to acquire and control crucial large datasets and application-specific algorithms will likely usher in a period of racing as a result of the potential commercial rewards of mastering this method of research. We suggest that in the future, policies that promote transparency and the sharing of core datasets between public and private actors may be essential for boosting research productivity and encouraging innovation-oriented competition.",2018.0,"I. Cockburn, R. Henderson, Scott Stern"
426ed641ac50d6aadc2c748bb435af3da9b31d13,https://www.semanticscholar.org/paper/426ed641ac50d6aadc2c748bb435af3da9b31d13,Application of artificial intelligence in gastroenterology,"Artificial intelligence (AI) using deep-learning (DL) has emerged as a breakthrough computer technology. By the era of big data, the accumulation of an enormous number of digital images and medical records drove the need for the utilization of AI to efficiently deal with these data, which have become fundamental resources for a machine to learn by itself. Among several DL models, the convolutional neural network showed outstanding performance in image analysis. In the field of gastroenterology, physicians handle large amounts of clinical data and various kinds of image devices such as endoscopy and ultrasound. AI has been applied in gastroenterology in terms of diagnosis, prognosis, and image analysis. However, potential inherent selection bias cannot be excluded in the form of retrospective study. Because overfitting and spectrum bias (class imbalance) have the possibility of overestimating the accuracy, external validation using unused datasets for model development, collected in a way that minimizes the spectrum bias, is mandatory. For robust verification, prospective studies with adequate inclusion/exclusion criteria, which represent the target populations, are needed. DL has its own lack of interpretability. Because interpretability is important in that it can provide safety measures, help to detect bias, and create social acceptance, further investigations should be performed.",2019.0,"Y. Yang, C. S. Bang"
b80081576b761925150b69935b971ce6790197af,https://www.semanticscholar.org/paper/b80081576b761925150b69935b971ce6790197af,Artificial Intelligence in Higher Education: A Bibliometric Study on its Impact in the Scientific Literature,"Artificial intelligence has experienced major developments in recent years and represents an emerging technology that will revolutionize the ways in which human beings live. This technology is already being introduced in the field of higher education, although many teachers are unaware of its scope and, above all, of what it consists of. Therefore, the purpose of this paper was to analyse the scientific production on artificial intelligence in higher education indexed in Web of Science and Scopus databases during 2007–2017. A bespoke methodology of bibliometric studies was used in the most relevant databases in social science. The sample was composed of 132 papers in total. From the results obtained, it was observed that there is a worldwide interest in the topic and that the literature on this subject is just at an incipient stage. We conclude that, although artificial intelligence is a reality, the scientific production about its application in higher education has not been consolidated.",2019.0,"Francisco-Javier Hinojo-Lucena, I. Aznar-Díaz, María-Pilar Cáceres-Reche, José-María Romero-Rodríguez"
da167ea53dcbe93866727ea0da1f3210dcda3cbf,https://www.semanticscholar.org/paper/da167ea53dcbe93866727ea0da1f3210dcda3cbf,Should Health Care Demand Interpretable Artificial Intelligence or Accept “Black Box” Medicine?,"Health care applications of artificial intelligence (AI) have recently emerged. Artificial intelligence approaches, such as deep learning, rely on vast amounts of data and complex model structures ...",2019.0,"Fei Wang, R. Kaushal, D. Khullar"
346b0a7d642b4d86f17388d5a4965520b818bb1b,https://www.semanticscholar.org/paper/346b0a7d642b4d86f17388d5a4965520b818bb1b,"Evolutionary Fuzzy Systems for Explainable Artificial Intelligence: Why, When, What for, and Where to?","Evolutionary fuzzy systems are one of the greatest advances within the area of computational intelligence. They consist of evolutionary algorithms applied to the design of fuzzy systems. Thanks to this hybridization, superb abilities are provided to fuzzy modeling in many different data science scenarios. This contribution is intended to comprise a position paper developing a comprehensive analysis of the evolutionary fuzzy systems research field. To this end, the ""4 W"" questions are posed and addressed with the aim of understanding the current context of this topic and its significance. Specifically, it will be pointed out why evolutionary fuzzy systems are important from an explainable point of view, when they began, what they are used for, and where the attention of researchers should be directed to in the near future in this area. They must play an important role for the emerging area of eXplainable Artificial Intelligence (XAI) learning from data.",2019.0,"Alberto Fernández, F. Herrera, O. Cordón, M. J. Jesús, F. Marcelloni"
97b446e2ec3b402ea104421dab5eb4b99a21e42a,https://www.semanticscholar.org/paper/97b446e2ec3b402ea104421dab5eb4b99a21e42a,Understanding artificial intelligence ethics and safety,"A remarkable time of human promise has been ushered in by the convergence of the ever-expanding availability of big data, the soaring speed and stretch of cloud computing platforms, and the advancement of increasingly sophisticated machine learning algorithms. Innovations in AI are already leaving a mark on government by improving the provision of essential social goods and services from healthcare, education, and transportation to food supply, energy, and environmental management. These bounties are likely just the start. The prospect that progress in AI will help government to confront some of its most urgent challenges is exciting, but legitimate worries abound. As with any new and rapidly evolving technology, a steep learning curve means that mistakes and miscalculations will be made and that both unanticipated and harmful impacts will occur. 
This guide, written for department and delivery leads in the UK public sector and adopted by the British Government in its publication, 'Using AI in the Public Sector,' identifies the potential harms caused by AI systems and proposes concrete, operationalisable measures to counteract them. It stresses that public sector organisations can anticipate and prevent these potential harms by stewarding a culture of responsible innovation and by putting in place governance processes that support the design and implementation of ethical, fair, and safe AI systems. It also highlights the need for algorithmically supported outcomes to be interpretable by their users and made understandable to decision subjects in clear, non-technical, and accessible ways. Finally, it builds out a vision of human-centred and context-sensitive implementation that gives a central role to communication, evidence-based reasoning, situational awareness, and moral justifiability.",2019.0,David Leslie
3d0d63b6517efa8f35e4207ca907b21103433941,https://www.semanticscholar.org/paper/3d0d63b6517efa8f35e4207ca907b21103433941,Assessment of Accuracy of an Artificial Intelligence Algorithm to Detect Melanoma in Images of Skin Lesions,"Key Points Question How accurate is an artificial intelligence–based melanoma detection algorithm, which analyzes dermoscopic images taken by smartphone and digital single-lens reflex cameras, compared with clinical assessment and histopathological diagnosis? Findings In this diagnostic study, 1550 images of suspicious and benign skin lesions were analyzed by an artificial intelligence algorithm. When compared with histopathological diagnosis, the algorithm achieved an area under the receiver operator characteristic curve of 91.8%. At 100% sensitivity, the algorithm achieved a specificity of 64.8%, while clinicians achieved a specificity of 69.9%. Meaning As the burden of skin cancer increases, artificial intelligence technology could play a role in identifying lesions with a high likelihood of melanoma.",2019.0,"M. Phillips, Helen Marsden, W. Jaffe, R. Matin, G. Wali, J. Greenhalgh, E. McGrath, Rob James, E. Ladoyanni, Anthony Bewley, G. Argenziano, I. Palamaras"
4b9955402487fce0a69a8dcf4df177b2169d15ab,https://www.semanticscholar.org/paper/4b9955402487fce0a69a8dcf4df177b2169d15ab,Artificial intelligence in medical imaging of the liver,"Artificial intelligence (AI), particularly deep learning algorithms, is gaining extensive attention for its excellent performance in image-recognition tasks. They can automatically make a quantitative assessment of complex medical image characteristics and achieve an increased accuracy for diagnosis with higher efficiency. AI is widely used and getting increasingly popular in the medical imaging of the liver, including radiology, ultrasound, and nuclear medicine. AI can assist physicians to make more accurate and reproductive imaging diagnosis and also reduce the physicians’ workload. This article illustrates basic technical knowledge about AI, including traditional machine learning and deep learning algorithms, especially convolutional neural networks, and their clinical application in the medical imaging of liver diseases, such as detecting and evaluating focal liver lesions, facilitating treatment, and predicting liver treatment response. We conclude that machine-assisted medical services will be a promising solution for future liver medical care. Lastly, we discuss the challenges and future directions of clinical application of deep learning techniques.",2019.0,"Li-Qiang Zhou, Jia-Yu Wang, Song-Yuan Yu, Ge-Ge Wu, Qi Wei, Youbin Deng, Xingxing Wu, X. Cui, C. Dietrich"
71844f56cc7478d8d558ca9f7188fa10e09bf7df,https://www.semanticscholar.org/paper/71844f56cc7478d8d558ca9f7188fa10e09bf7df,Artificial intelligence in medical education,"Abstract Artificial intelligence (AI) is a growing phenomenon, and will soon facilitate wide-scale changes in many professions, including medical education. In order for medical educators to be properly prepared for AI, they will need to have at least a fundamental knowledge of AI in relation to learning and teaching, and the extent to which it will impact on medical education. This Guide begins by introducing the broad concepts of AI by using fairly well-known examples to illustrate AI’s implications within the context of education. It then considers the impact of AI on medicine and the implications of this impact for educators trying to educate future doctors. Drawing on these strands, it then identifies AI’s direct impact on the methodology and content of medical education, in an attempt to prepare medical educators for the changing demands and opportunities that are about to face them because of AI.",2019.0,K. Masters
0adfae854ebf47d6f5a5c0f9c1033279793e62e0,https://www.semanticscholar.org/paper/0adfae854ebf47d6f5a5c0f9c1033279793e62e0,Artificial Intelligence for Diabetes Management and Decision Support: Literature Review,"Background Artificial intelligence methods in combination with the latest technologies, including medical devices, mobile computing, and sensor technologies, have the potential to enable the creation and delivery of better management services to deal with chronic diseases. One of the most lethal and prevalent chronic diseases is diabetes mellitus, which is characterized by dysfunction of glucose homeostasis. Objective The objective of this paper is to review recent efforts to use artificial intelligence techniques to assist in the management of diabetes, along with the associated challenges. Methods A review of the literature was conducted using PubMed and related bibliographic resources. Analyses of the literature from 2010 to 2018 yielded 1849 pertinent articles, of which we selected 141 for detailed review. Results We propose a functional taxonomy for diabetes management and artificial intelligence. Additionally, a detailed analysis of each subject category was performed using related key outcomes. This approach revealed that the experiments and studies reviewed yielded encouraging results. Conclusions We obtained evidence of an acceleration of research activity aimed at developing artificial intelligence-powered tools for prediction and prevention of complications associated with diabetes. Our results indicate that artificial intelligence methods are being progressively established as suitable for use in clinical daily practice, as well as for the self-management of diabetes. Consequently, these methods provide powerful tools for improving patients’ quality of life.",2018.0,"Iván Contreras, J. Vehí"
30d3715f9ed3f240e5d208db4f6c8ee8792edf91,https://www.semanticscholar.org/paper/30d3715f9ed3f240e5d208db4f6c8ee8792edf91,Artificial Intelligence and the Modern Productivity Paradox: A Clash of Expectations and Statistics,"We live in an age of paradox. Systems using artificial intelligence match or surpass human level performance in more and more domains, leveraging rapid advances in other technologies and driving soaring stock prices. Yet measured productivity growth has declined by half over the past decade, and real income has stagnated since the late 1990s for a majority of Americans. We describe four potential explanations for this clash of expectations and statistics: false hopes, mismeasurement, redistribution, and implementation lags. While a case can be made for each, we argue that lags have likely been the biggest contributor to the paradox. The most impressive capabilities of AI, particularly those based on machine learning, have not yet diffused widely. More importantly, like other general purpose technologies, their full effects won’t be realized until waves of complementary innovations are developed and implemented. The required adjustment costs, organizational changes, and new skills can be modeled as a kind of intangible capital. A portion of the value of this intangible capital is already reflected in the market value of firms. However, going forward, national statistics could fail to measure the full benefits of the new technologies and some may even have the wrong sign.",2017.0,"Erik Brynjolfsson, Daniel Rock, C. Syverson"
151c7cb45c75ac5573a6c03b2ea089b34512898c,https://www.semanticscholar.org/paper/151c7cb45c75ac5573a6c03b2ea089b34512898c,State of the Art: Reproducibility in Artificial Intelligence,"
 
 Background: Research results in artificial intelligence (AI) are criticized for not being reproducible. Objective: To quantify the state of reproducibility of empirical AI research using six reproducibility metrics measuring three different degrees of reproducibility. Hypotheses: 1) AI research is not documented well enough to reproduce the reported results. 2) Documentation practices have improved over time. Method: The literature is reviewed and a set of variables that should be documented to enable reproducibility are grouped into three factors: Experiment, Data and Method. The metrics describe how well the factors have been documented for a paper. A total of 400 research papers from the conference series IJCAI and AAAI have been surveyed using the metrics. Findings: None of the papers document all of the variables. The metrics show that between 20% and 30% of the variables for each factor are documented. One of the metrics show statistically significant increase over time while the others show no change. Interpretation: The reproducibility scores decrease with in- creased documentation requirements. Improvement over time is found. Conclusion: Both hypotheses are supported.
 
",2018.0,"Odd Erik Gundersen, Sigbjørn Kjensmo"
e95efbe0fbe496e61da17cedacbe4357cf9db57c,https://www.semanticscholar.org/paper/e95efbe0fbe496e61da17cedacbe4357cf9db57c,"Governing artificial intelligence: ethical, legal and technical opportunities and challenges","This paper is the introduction to the special issue entitled: ‘Governing artificial intelligence: ethical, legal and technical opportunities and challenges'. Artificial intelligence (AI) increasingly permeates every aspect of our society, from the critical, like urban infrastructure, law enforcement, banking, healthcare and humanitarian aid, to the mundane like dating. AI, including embodied AI in robotics and techniques like machine learning, can improve economic, social welfare and the exercise of human rights. Owing to the proliferation of AI in high-risk areas, the pressure is mounting to design and govern AI to be accountable, fair and transparent. How can this be achieved and through which frameworks? This is one of the central questions addressed in this special issue, in which eight authors present in-depth analyses of the ethical, legal-regulatory and technical challenges posed by developing governance regimes for AI systems. It also gives a brief overview of recent developments in AI governance, how much of the agenda for defining AI regulation, ethical frameworks and technical approaches is set, as well as providing some concrete suggestions to further the debate on AI governance. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.",2018.0,Corinne Cath
ca9689ead45a7ad1cfcd764d547ea03fd21fd464,https://www.semanticscholar.org/paper/ca9689ead45a7ad1cfcd764d547ea03fd21fd464,Marketing and Artificial Intelligence,"In recent years, artificial intelligence (AI) has become an emerging trend in different fields: science, business, medicine, automotive and education. AI has also reached marketing. The aim of the paper is to research how deeply AI is applied in marketing and what implications there are for marketing practitioners. Authors stated two research questions - which areas of AI are used in marketing and what implications AI delivers for marketing managers. To answer those questions, the authors conducted research on secondary data with AI examples used for marketing purpose. The analysis of gathered examples shows that AI is widely introduced into the marketing field, though the applications are at the operational level. This may be the effect of careful implementation of the new technology, still at the level of experimenting with it. The uncertainty of the outcome of AI implementation may affect the caution in putting these innovations into practice as well. Gathered examples proved that AI influences all aspects of marketing mix impacting both consumer value delivery as well as the marketing organization and management. The paper delivers implications for business, especially ideas about implementing AI into marketing, designing innovations and the ideas on how to incorporate new skills into marketing team required by the new technology.",2019.0,"Krystyna Jarek, G. Mazurek"
ad680d13cd06ea8a8722d1da1121f105277adb73,https://www.semanticscholar.org/paper/ad680d13cd06ea8a8722d1da1121f105277adb73,Artificial intelligence-enabled healthcare delivery,"Summary In recent years, there has been massive progress in artificial intelligence (AI) with the development of deep neural networks, natural language processing, computer vision and robotics. These techniques are now actively being applied in healthcare with many of the health service activities currently being delivered by clinicians and administrators predicted to be taken over by AI in the coming years. However, there has also been exceptional hype about the abilities of AI with a mistaken notion that AI will replace human clinicians altogether. These perspectives are inaccurate, and if a balanced perspective of the limitations and promise of AI is taken, one can gauge which parts of the health system AI can be integrated to make a meaningful impact. The four main areas where AI would have the most influence would be: patient administration, clinical decision support, patient monitoring and healthcare interventions. This health system where AI plays a central role could be termed an AI-enabled or AI-augmented health system. In this article, we discuss how this system can be developed based on a realistic assessment of current AI technologies and predicted developments.",2018.0,"S. Reddy, John Fox, M. Purohit"
9a44a1770114adfa50d64ba98ba8c954db3cb527,https://www.semanticscholar.org/paper/9a44a1770114adfa50d64ba98ba8c954db3cb527,Ethical governance is essential to building trust in robotics and artificial intelligence systems,"This paper explores the question of ethical governance for robotics and artificial intelligence (AI) systems. We outline a roadmap—which links a number of elements, including ethics, standards, regulation, responsible research and innovation, and public engagement—as a framework to guide ethical governance in robotics and AI. We argue that ethical governance is essential to building public trust in robotics and AI, and conclude by proposing five pillars of good ethical governance. This article is part of the theme issue ‘Governing artificial intelligence: ethical, legal, and technical opportunities and challenges’.",2018.0,"A. Winfield, M. Jirotka"
173d97b2a940775cbe65e639264845fc909f7405,https://www.semanticscholar.org/paper/173d97b2a940775cbe65e639264845fc909f7405,Role of Artificial Intelligence within the Telehealth Domain,"Summary Objectives: This paper provides a discussion about the potential scope of applicability of Artificial Intelligence methods within the telehealth domain. These methods are focussed on clinical needs and provide some insight to current directions, based on reports of recent advances. Methods: Examples of telehealth innovations involving Artificial Intelligence to support or supplement remote health care delivery were identified from recent literature by the authors, on the basis of expert knowledge. Observations from the examples were synthesized to yield an overview of contemporary directions for the perceived role of Artificial Intelligence in telehealth. Results: Two major focus areas for related contemporary directions were established. These were first, quality improvement for existing clinical practice and service delivery, and second, the development and support of new models of care. Case studies from each focus area have been chosen for illustration purposes. Conclusion: Examples of the role of Artificial Intelligence in delivery of health care remotely include use of tele-assessment, tele-diagnosis, tele-interactions, and tele-monitoring. Further developments of underlying algorithms and validation of methods will be required for wider adoption. Certain key social and ethical considerations also need consideration more generally in the health system, as Artificial-Intelligence-enabled-telehealth becomes more commonplace.",2019.0,"C. Kuziemsky, A. Maeder, Oommen John, S. Gogia, Arindam Basu, S. Meher, Márcia Ito"
561b15685fb900593f43ac8f7e615a73bb4ff965,https://www.semanticscholar.org/paper/561b15685fb900593f43ac8f7e615a73bb4ff965,"Artificial intelligence, machine learning and health systems","Artificial Intelligence and machine learning have the potential to be the catalyst for transformation of health systems to improve efficiency and effectiveness, create headroom for universal health coverage and improve outcomes",2018.0,"T. Panch, Peter Szolovits, R. Atun"
41cb576653d218f8017d33ab9b4ceec971bebbe0,https://www.semanticscholar.org/paper/41cb576653d218f8017d33ab9b4ceec971bebbe0,Towards Intelligent Regulation of Artificial Intelligence,"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.",2019.0,Miriam C. Buiten
dce7838d3ae89946ff656ea8dc60c889d053fcb7,https://www.semanticscholar.org/paper/dce7838d3ae89946ff656ea8dc60c889d053fcb7,A is for Artificial Intelligence: The Impact of Artificial Intelligence Activities on Young Children's Perceptions of Robots,"We developed a novel early childhood artificial intelligence (AI) platform, PopBots, where preschool children train and interact with social robots to learn three AI concepts: knowledge-based systems, supervised machine learning, and generative AI. We evaluated how much children learned by using AI assessments we developed for each activity. The median score on the cumulative assessment was 70% and children understood knowledge-based systems the best. Then, we analyzed the impact of the activities on children's perceptions of robots. Younger children came to see robots as toys that were smarter than them, but their older counterparts saw them more as people that were not as smart as them. Children who performed worse on the AI assessments believed that robots were like toys that were not as smart as them, however children who did better on the assessments saw robots as people who were smarter than them. We believe early AI education can empower children to understand the AI devices that are increasingly in their lives.",2019.0,"Randi Williams, Hae Won Park, C. Breazeal"
0dff4441dca73fbefffd9444553fc640d62a9c77,https://www.semanticscholar.org/paper/0dff4441dca73fbefffd9444553fc640d62a9c77,Applications of artificial intelligence in intelligent manufacturing: a review,"Based on research into the applications of artificial intelligence (AI) technology in the manufacturing industry in recent years, we analyze the rapid development of core technologies in the new era of ‘Internet plus AI’, which is triggering a great change in the models, means, and ecosystems of the manufacturing industry, as well as in the development of AI. We then propose new models, means, and forms of intelligent manufacturing, intelligent manufacturing system architecture, and intelligent manufacturing technology system, based on the integration of AI technology with information communications, manufacturing, and related product technology. Moreover, from the perspectives of intelligent manufacturing application technology, industry, and application demonstration, the current development in intelligent manufacturing is discussed. Finally, suggestions for the application of AI in intelligent manufacturing in China are presented.",2017.0,"Bo-Hu Li, Baocun Hou, Wentao Yu, Xiaobing Lu, Chun-Wei Yang"
0f76dba2e13a4ac2448cb4df978a95aec615aae1,https://www.semanticscholar.org/paper/0f76dba2e13a4ac2448cb4df978a95aec615aae1,Artificial intelligence in reproductive medicine,"Artificial intelligence (AI) has experienced rapid growth over the past few years, moving from the experimental to the implementation phase in various fields, including medicine. Advances in learning algorithms and theories, the availability of large datasets and improvements in computing power have contributed to breakthroughs in current AI applications. Machine learning (ML), a subset of AI, allows computers to detect patterns from large complex datasets automatically and uses these patterns to make predictions. AI is proving to be increasingly applicable to healthcare, and multiple machine learning techniques have been used to improve the performance of assisted reproductive technology (ART). Despite various challenges, the integration of AI and reproductive medicine is bound to give an essential direction to medical development in the future. In this review, we discuss the basic aspects of AI and machine learning, and we address the applications, potential limitations and challenges of AI. We also highlight the prospects and future directions in the context of reproductive medicine.",2019.0,"Renjie Wang, Wei Pan, L. Jin, Yuehan Li, Yudi Geng, Chun Gao, Gang Chen, Hui Wang, D. Ma, Shujie Liao"
7a942329e2e9d96628e7714b6b47f98d6b347f97,https://www.semanticscholar.org/paper/7a942329e2e9d96628e7714b6b47f98d6b347f97,Physician Confidence in Artificial Intelligence: An Online Mobile Survey,"Background It is expected that artificial intelligence (AI) will be used extensively in the medical field in the future. Objective The purpose of this study is to investigate the awareness of AI among Korean doctors and to assess physicians’ attitudes toward the medical application of AI. Methods We conducted an online survey composed of 11 closed-ended questions using Google Forms. The survey consisted of questions regarding the recognition of and attitudes toward AI, the development direction of AI in medicine, and the possible risks of using AI in the medical field. Results A total of 669 participants completed the survey. Only 40 (5.9%) answered that they had good familiarity with AI. However, most participants considered AI useful in the medical field (558/669, 83.4% agreement). The advantage of using AI was seen as the ability to analyze vast amounts of high-quality, clinically relevant data in real time. Respondents agreed that the area of medicine in which AI would be most useful is disease diagnosis (558/669, 83.4% agreement). One possible problem cited by the participants was that AI would not be able to assist in unexpected situations owing to inadequate information (196/669, 29.3%). Less than half of the participants(294/669, 43.9%) agreed that AI is diagnostically superior to human doctors. Only 237 (35.4%) answered that they agreed that AI could replace them in their jobs. Conclusions This study suggests that Korean doctors and medical students have favorable attitudes toward AI in the medical field. The majority of physicians surveyed believed that AI will not replace their roles in the future.",2018.0,"Songhee Oh, J. Kim, Sung-Woo Choi, Hee Jeong Lee, Jungrak Hong, S. Kwon"
b7f2f024a1498b2c47f3fce569efbc3154ee1e8f,https://www.semanticscholar.org/paper/b7f2f024a1498b2c47f3fce569efbc3154ee1e8f,Artificial Intelligence and Machine Learning in Anesthesiology.,"Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery.",2019.0,C. Connor
f6940e511324c7e1580d6cafcd131545ebec7b41,https://www.semanticscholar.org/paper/f6940e511324c7e1580d6cafcd131545ebec7b41,Artificial Intelligence and Law: An Overview,"Much has been written recently about artificial intelligence (AI) and law. But what is AI, and what is its relation to the practice and administration of law? This article addresses those questions by providing a high-level overview of AI and its use within law. The discussion aims to be nuanced but also understandable to those without a technical background. To that end, I first discuss AI generally. I then turn to AI and how it is being used by lawyers in the practice of law, people and companies who are governed by the law, and government officials who administer the law. A key motivation in writing this article is to provide a realistic, demystified view of AI that is rooted in the actual capabilities of the technology. This is meant to contrast with discussions about AI and law that are decidedly futurist in nature. That body of work speculates about the effects of AI developments that do not currently exist and which may, or may not, ever come about. Although those futurist conversations have their place, it is important to acknowledge that they involve significant, sometimes unsupported, assumptions about where the technology is headed. That speculative discussion often distracts from the important, but perhaps less exotic, law and policy issues actually raised by AI technology today.",2019.0,Harry Surden
8d68dd845c6dadf7c8109a1e87358aab0e934b1b,https://www.semanticscholar.org/paper/8d68dd845c6dadf7c8109a1e87358aab0e934b1b,Sustainable development of organizations based on the combinatorial model of artificial intelligence,"The article specifies the organizational capabilities of application of artificial intelligence technologies in the model of sustainable development of the organization. Also, the article provided the theoretical and methodological background of the organizational changes and development, as well as determined the possibilities of application of artificial intelligence technologies in the functionality of the organization. It was proposed to use the methodological approach to application of neural networks in maintaining of the intelligent management of organizational development. There was developed the combinatorial model of artificial intelligence for decision making about the organizational development.",2019.0,"Aleksandra Kuzior, A. Kwiliński, V. Tkachenko"
84ebbe441e974eaf1e7bb0b53969f851fa6c210f,https://www.semanticscholar.org/paper/84ebbe441e974eaf1e7bb0b53969f851fa6c210f,"Artificial Intelligence in Nephrology: Core Concepts, Clinical Applications, and Perspectives.","Artificial intelligence is playing an increasingly important role in many fields of medicine, assisting physicians in most steps of patient management. In nephrology, artificial intelligence can already be used to improve clinical care, hemodialysis prescriptions, and follow-up of transplant recipients. However, many nephrologists are still unfamiliar with the basic principles of medical artificial intelligence. This review seeks to provide an overview of medical artificial intelligence relevant to the practicing nephrologist, in all fields of nephrology. We define the core concepts of artificial intelligence and machine learning and cover the basics of the functioning of neural networks and deep learning. We also discuss the most recent clinical applications of artificial intelligence in nephrology and medicine; as an example, we describe how artificial intelligence can predict the occurrence of progressive immunoglobulin A nephropathy. Finally, we consider the future of artificial intelligence in clinical nephrology and its impact on medical practice, and conclude with a discussion of the ethical issues that the use of artificial intelligence raises in terms of clinical decision making, physician-patient relationship, patient privacy, and data collection.",2019.0,"O. Niel, P. Bastard"
e9775a99ecd12f1c5accebd2da83e94708c6ab3f,https://www.semanticscholar.org/paper/e9775a99ecd12f1c5accebd2da83e94708c6ab3f,Artificial intelligence—the third revolution in pathology,"Histopathology has undergone major changes firstly with the introduction of Immunohistochemistry, and latterly with Genomic Medicine. We argue that a third revolution is underway: Artificial Intelligence (AI). Coming on the back of Digital Pathology (DP), the introduction of AI has the potential to both challenge traditional practice and provide a totally new realm for pathology diagnostics. Hereby we stress the importance of certified pathologists having learned from the experience of previous revolutions and be willing to accept such disruptive technologies, ready to innovate and actively engage in the creation, application and validation of technologies and oversee the safe introduction of AI into diagnostic practice. This article is protected by copyright. All rights reserved.",2019.0,"M. Salto‐Tellez, P. Maxwell, P. Hamilton"
ad1a9e389d738635ffaf35e74bcf6bccb6c79b68,https://www.semanticscholar.org/paper/ad1a9e389d738635ffaf35e74bcf6bccb6c79b68,"Art, Creativity, and the Potential of Artificial Intelligence","Our essay discusses an AI process developed for making art (AICAN), and the issues AI creativity raises for understanding art and artists in the 21st century. Backed by our training in computer science (Elgammal) and art history (Mazzone), we argue for the consideration of AICAN’s works as art, relate AICAN works to the contemporary art context, and urge a reconsideration of how we might define human and machine creativity. Our work in developing AI processes for art making, style analysis, and detecting large-scale style patterns in art history has led us to carefully consider the history and dynamics of human art-making and to examine how those patterns can be modeled and taught to the machine. We advocate for a connection between machine creativity and art broadly defined as parallel to but not in conflict with human artists and their emotional and social intentions of art making. Rather, we urge a partnership between human and machine creativity when called for, seeing in this collaboration a means to maximize both partners’ creative strengths.",2019.0,"Marian Mazzone, A. Elgammal"
4435d6ba578fc061dfa66f14e163dc94cc0f7056,https://www.semanticscholar.org/paper/4435d6ba578fc061dfa66f14e163dc94cc0f7056,Artificial Intelligence in Nuclear Medicine,"Despite the great media attention for artificial intelligence (AI), for many health care professionals the term and the functioning of AI remain a “black box,” leading to exaggerated expectations on the one hand and unfounded fears on the other. In this review, we provide a conceptual classification and a brief summary of the technical fundamentals of AI. Possible applications are discussed on the basis of a typical work flow in medical imaging, grouped by planning, scanning, interpretation, and reporting. The main limitations of current AI techniques, such as issues with interpretability or the need for large amounts of annotated data, are briefly addressed. Finally, we highlight the possible impact of AI on the nuclear medicine profession, the associated challenges and, last but not least, the opportunities.",2019.0,"F. Nensa, A. Demircioğlu, C. Rischpler"
974cc2ac3babb39116314f88bb1903e36638d0d1,https://www.semanticscholar.org/paper/974cc2ac3babb39116314f88bb1903e36638d0d1,The Promise of Artificial Intelligence.,"""Prepare Yourselves, Robots Will Soon Replace Doctors in Healthcare,"" screamed the headline in a 2017 Forbes magazine article. Media coverage like that makes it easy to see why artificial intelligence (AI) sounds like scary science fiction to some physicians.",2019.0,S. Price
4e3cf1f761b8749afbac46ab949ed30896d3f44a,https://www.semanticscholar.org/paper/4e3cf1f761b8749afbac46ab949ed30896d3f44a,Artificial Intelligence in Drug Discovery and Development,"Artificial Intelligence (AI) has recently been developed into a sizzling topic in the area of medical care industry. The biopharmaceutical industries are making efforts to approach AI to enhance drug discovery process, reduce research and development expenses, diminish failure rates in clinical trials and ultimately generate superior medicines. The accessibility of immense statistics in life sciences and a speedy development in machine learning algorithms led to an evolution of AI-based start-up companies focused on drug discovery over the recent years [1]. Numerous remarkable AIbiopharmaceutical alliance were declared in 2016-2017 that include Pfizer and IBM Watson, Sanofi Genzyme and Recursion Pharmaceuticals, AstraZeneca, Abbvie, Merck, Novartis, GSK and Exscientia, etc.",2018.0,Prashansa Agrawal
bb82053d4229357925b28ffe647377f56d93b06e,https://www.semanticscholar.org/paper/bb82053d4229357925b28ffe647377f56d93b06e,Intelligent nanophotonics: merging photonics and artificial intelligence at the nanoscale,"Abstract Nanophotonics has been an active research field over the past two decades, triggered by the rising interests in exploring new physics and technologies with light at the nanoscale. As the demands of performance and integration level keep increasing, the design and optimization of nanophotonic devices become computationally expensive and time-inefficient. Advanced computational methods and artificial intelligence, especially its subfield of machine learning, have led to revolutionary development in many applications, such as web searches, computer vision, and speech/image recognition. The complex models and algorithms help to exploit the enormous parameter space in a highly efficient way. In this review, we summarize the recent advances on the emerging field where nanophotonics and machine learning blend. We provide an overview of different computational methods, with the focus on deep learning, for the nanophotonic inverse design. The implementation of deep neural networks with photonic platforms is also discussed. This review aims at sketching an illustration of the nanophotonic design with machine learning and giving a perspective on the future tasks.",2018.0,"Kan Yao, Rohit Unni, Yuebing Zheng"
4079337b2e2f1e85dc3fea0ec3df73f711bcfc4c,https://www.semanticscholar.org/paper/4079337b2e2f1e85dc3fea0ec3df73f711bcfc4c,Review on the Application of Artificial Intelligence in Smart Homes,"Smart home and artificial intelligence technologies are developing rapidly, and various smart home products associated with artificial intelligence (AI) improved the quality of living for occupants. Although some studies discussed the application of artificial intelligence in smart homes, few publications fully considered the integration of literature and products. In this paper, we aim to answer the research questions of “what is the trend of smart home technology and products” and “what is the relationship between literature and products in smart homes with AI”. Literature reviews and product reviews are given to define the functions and roles of artificial intelligence in smart homes. We determined the application status of artificial intelligence in smart home products and how it is utilized in our house so that we could understand how artificial intelligence is used to make smart homes. Furthermore, our results revealed that there is a delay between literature and products, and smart home intelligent interactions will become more and more popular.",2019.0,"Xiao Guo, Zhenjiang Shen, Yajing Zhang, Teng Wu"
7e9b75b60914c36cf94bc98078aee8bc01a193ff,https://www.semanticscholar.org/paper/7e9b75b60914c36cf94bc98078aee8bc01a193ff,"Trust Me, I’m a Chatbot: How Artificial Intelligence in Health Care Fails the Turing Test","Over the next decade, one issue which will dominate sociotechnical studies in health informatics is the extent to which the promise of artificial intelligence in health care will be realized, along with the social and ethical issues which accompany it. A useful thought experiment is the application of the Turing test to user-facing artificial intelligence systems in health care (such as chatbots or conversational agents). In this paper I argue that many medical decisions require value judgements and the doctor-patient relationship requires empathy and understanding to arrive at a shared decision, often handling large areas of uncertainty and balancing competing risks. Arguably, medicine requires wisdom more than intelligence, artificial or otherwise. Artificial intelligence therefore needs to supplement rather than replace medical professionals, and identifying the complementary positioning of artificial intelligence in medical consultation is a key challenge for the future. In health care, artificial intelligence needs to pass the implementation game, not the imitation game.",2019.0,J. Powell
fe68fdd7af4e27d42fc336833fced0e217dca255,https://www.semanticscholar.org/paper/fe68fdd7af4e27d42fc336833fced0e217dca255,THE ETHICS OF ARTIFICIAL INTELLIGENCE,"Summary There are many ethical questions relating the issue of developing an intelligent system. There is strong and increasing pressure to raise capabilities of the artificial intelligence at least to the human levelled intelligence as the ultimate goal. This essay describes possible paths of development of the artificial intelligence. It is discussed how this changes will affect our society and challenges that humanity will have to face. Principles, guideways and modern viewpoints are presented and confirmed with the statements of the renowned scientists and experts in the field of the artificial intelligence ethics.",2016.0,J. Horvat
bdb3516a0d661e67243c1d162d0a71e9ddbccbda,https://www.semanticscholar.org/paper/bdb3516a0d661e67243c1d162d0a71e9ddbccbda,Artificial intelligence and ambient intelligence,"Ambient intelligence (AmI) is intrinsically and thoroughly connected with artificial intelligence (AI). Some even say that it is, in essence, AI in the environment. AI, on the other hand, owes its success to the phenomenal development of the information and communication technologies (ICTs), based on principles such as Moore’s law. In this paper we give an overview of the progress in AI and AmI interconnected with ICT through information-society laws, superintelligence, and several related disciplines, such as multi-agent systems and the Semantic Web, ambient assisted living and e-healthcare, AmI for assisting medical diagnosis, ambient intelligence for e-learning and ambient intelligence for smart cities. Besides a short history and a description of the current state, the frontiers and the future of AmI and AI are also considered in the paper.",2019.0,"M. Gams, I. Gu, Aki Härmä, A. Muñoz, V. W. L. Tam"
ea5bf3ead829ec8599ceab436884d2b62e436f11,https://www.semanticscholar.org/paper/ea5bf3ead829ec8599ceab436884d2b62e436f11,Artificial intelligence in medicine: current trends and future possibilities.,"Artificial intelligence (AI) research within medicine is growing rapidly. In 2016, healthcare AI projects attracted more investment than AI projects within any other sector of the global economy.1 However, among the excitement, there is equal scepticism, with some urging caution at inflated expectations.2 This article takes a close look at current trends in medical AI and the future possibilities for general practice.

Informing clinical decision making through insights from past data is the essence of evidence-based medicine. Traditionally, statistical methods have approached this task by characterising patterns within data as mathematical equations, for example, linear regression suggests a ‘line of best fit’. Through ‘machine learning’ (ML), AI provides techniques that uncover complex associations which cannot easily be reduced to an equation. For example, neural networks represent data through vast numbers of interconnected neurones in a similar fashion to the human brain. This allows ML systems to approach complex problem solving just as a clinician might — by carefully weighing evidence to reach reasoned conclusions. However, unlike a single clinician, these systems can simultaneously observe and rapidly process an almost limitless number of inputs. For example, an AI-driven smartphone app now capably handles the task of triaging 1.2 million people in North London to Accident & Emergency (A&E).3 Furthermore, these systems are able to learn from each incremental case and can be exposed, within minutes, to more cases than a clinician could see in many lifetimes. This is why an AI-driven application is able to …",2018.0,"V. Buch, Irfan Ahmed, M. Maruthappu"
b186405889f0ad97eeed10c077c9257c8b2d4353,https://www.semanticscholar.org/paper/b186405889f0ad97eeed10c077c9257c8b2d4353,Artificial Intelligence Review,"In this chapter, the authors present a profound literature review of artificial intelligence (AI). After defining it, they briefly cover its history and enumerate its principal fields of application. They name, for example, information system, commerce, image processing, human-computer interaction, data compression, robotics, route planning, etc. Moreover, the test that defines an artificially intelligent system, called the Turing test, is also defined and detailed. Afterwards, the authors describe some AI tools such as fuzzy logic, genetic algorithms, and swarm intelligence. Special attention will be given to neural networks and fuzzy logic. The authors also present the future research directions and ethics.",2019.0,"A. Kilani, A. Hamida, H. Hamam"
339513f9a02ddc2d5e096085865d2fc6ae3c6a6c,https://www.semanticscholar.org/paper/339513f9a02ddc2d5e096085865d2fc6ae3c6a6c,Economic Policy for Artificial Intelligence,"Recent progress in artificial intelligence (AI)—a general purpose technology affecting many industries—has been focused on advances in machine learning, which we recast as a quality-adjusted drop in the price of prediction. How will this sharp drop in price impact society? Policy will influence the impact on two key dimensions: diffusion and consequences. First, in addition to subsidies and intellectual property (IP) policy that will influence the diffusion of AI in ways similar to their effect on other technologies, three policy categories—privacy, trade, and liability—may be uniquely salient in their influence on the diffusion patterns of AI. Second, labor and antitrust policies will influence the consequences of AI in terms of employment, inequality, and competition.",2018.0,"A. Agrawal, J. Gans, Avi Goldfarb"
cff7f7f5aa2393a343444f469b2419c8770fd65d,https://www.semanticscholar.org/paper/cff7f7f5aa2393a343444f469b2419c8770fd65d,Artificial Intelligence and Big Data in Public Health,"Artificial intelligence and automation are topics dominating global discussions on the future of professional employment, societal change, and economic performance. In this paper, we describe fundamental concepts underlying AI and Big Data and their significance to public health. We highlight issues involved and describe the potential impacts and challenges to medical professionals and diagnosticians. The possible benefits of advanced data analytics and machine learning are described in the context of recently reported research. Problems are identified and discussed with respect to ethical issues and the future roles of professionals and specialists in the age of artificial intelligence.",2018.0,"K. Benke, G. Benke"
26cad5ab89eeffc2341b7bec105efeb3c764a52e,https://www.semanticscholar.org/paper/26cad5ab89eeffc2341b7bec105efeb3c764a52e,From analytics to artificial intelligence,"ABSTRACT Analytics have been employed by companies for several decades, but now many firms are interested in building their capabilities for artificial intelligence (AI). Many AI systems, however, are based on statistics and other forms of analytics. Companies can get a “running start” on AI by building upon their analytical competencies. The focus of this article is how to transition from analytics to AI. Three eras of analytical focus are detailed, with AI portrayed as a fourth era. The types of AI methods that are and are not based on analytics are described. AI applications that build on analytical strengths are discussed. Approaches to assessing analytical capabilities that relate to AI, and the development of an organizational plan and strategy for AI, are also described in brief.",2018.0,T. Davenport
6c7634f8717df6199f994e8945046b7d227d0e20,https://www.semanticscholar.org/paper/6c7634f8717df6199f994e8945046b7d227d0e20,Applying artificial intelligence: implications for recruitment,"
Purpose
This paper aims to review the applications of artificial intelligence (AI) in the hiring process and its practical implications. This paper highlights the strategic shift in recruitment industry caused due to the adoption of AI in the recruitment process.


Design/methodology/approach
This paper is prepared by independent academicians who have synthesized their views by a review of the latest reports, articles, research papers and other relevant literature.


Findings
This paper describes the impact of developments in the field of AI on the hiring process and the recruitment industry. The application of AI for managing the recruitment process is leading to efficiency as well as qualitative gains for both clients and candidates.


Practical implications
This paper offers strategic insights into automation of the recruitment process and presents practical ideas for implementation of AI in the recruitment industry. It also discusses the strategic implications of the usage of AI in the recruitment industry.


Originality/value
This article describes the role of technological advancements in AI and its application for creating value for the recruitment industry as well as the clients. It saves the valuable reading time of practitioners and researchers by highlighting the AI applications in the recruitment industry in a concise and simple format.
",2018.0,"A. Upadhyay, Komal Khandelwal"
a511a986af07804261d3b35badf764c622e3f502,https://www.semanticscholar.org/paper/a511a986af07804261d3b35badf764c622e3f502,Artificial Intelligence: A European Perspective,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.",2018.0,"A. Annoni, P. Benczúr, P. Bertoldi, Blagoj Delipetrev, G. Prato, C. Feijóo, Enrique Fernández-Macías, E. Gutiérrez, M. Portela, H. Junklewitz, M. L. Cobo, B. Martens, Susana Nascimento, S. Nativi, Alexandre Pólvora, Jose Ignacio Sanchez Martin, Songuel Tolan, I. Tuomi, Lucia Vesnić Alujević"
0945b1ea6338545bd9fb47626ca90abbb69a8820,https://www.semanticscholar.org/paper/0945b1ea6338545bd9fb47626ca90abbb69a8820,Artificial intelligence in Internet of things,"Functioning of the Internet is persistently transforming from the Internet of computers (IoC) to the ‘Internet of things (IoT)’. Furthermore, massively interconnected systems, also known as cyber-physical systems (CPSs), are emerging from the assimilation of many facets like infrastructure, embedded devices, smart objects, humans, and physical environments. What the authors are heading to is a huge ‘Internet of Everything in a Smart Cyber Physical Earth’. IoT and CPS conjugated with ‘data science’ may emerge as the next ‘smart revolution’. The concern that arises then is to handle the huge data generated with the much weaker existing computation power. The research in data science and artificial intelligence (AI) has been striving to give an answer to this problem. Thus, IoT with AI can become a huge breakthrough. This is not just about saving money, smart things, reducing human effort, or any trending hype. This is much more than that – easing human life. There are, however, some serious issues like the security concerns and ethical issues which will go on plaguing IoT. The big picture is not how fascinating IoT with AI seems, but how the common people perceive it – a boon, a burden, or a threat.",2018.0,"Ashish Ghosh, Debasrita Chakraborty, Anwesha Law"
2ac4db657ee8e9b556f68fb58d82c2f2f4351651,https://www.semanticscholar.org/paper/2ac4db657ee8e9b556f68fb58d82c2f2f4351651,Artificial Intelligence in the 21st Century,"The field of artificial intelligence (AI) has shown an upward trend of growth in the 21st century (from 2000 to 2015). The evolution in AI has advanced the development of human society in our own time, with dramatic revolutions shaped by both theories and techniques. However, the multidisciplinary and fast-growing features make AI a field in which it is difficult to be well understood. In this paper, we study the evolution of AI at the beginning of the 21st century using publication metadata extracted from 9 top-tier journals and 12 top-tier conferences of this discipline. We find that the area is in the sustainable development and its impact continues to grow. From the perspective of reference behavior, the decrease in self-references indicates that the AI is becoming more and more open-minded. The influential papers/researchers/institutions we identified outline landmarks in the development of this field. Last but not least, we explore the inner structure in terms of topics’ evolution over time. We have quantified the temporal trends at the topic level and discovered the inner connection among these topics. These findings provide deep insights into the current scientific innovations, as well as shedding light on funding policies.",2018.0,"Jiaying Liu, Xiangjie Kong, Feng Xia, Xiaomei Bai, Lei Wang, Qing Qing, Ivan Lee"
f6a13db36687d61eb3751e6d769f19a52c49cf8d,https://www.semanticscholar.org/paper/f6a13db36687d61eb3751e6d769f19a52c49cf8d,Artificial Intelligence and Journalism,"heart, journalism is about telling stories about the human condition. How can we, as scholars and practitioners, do better at centering humans in our sociotechnical discourse about AI?",2019.0,"Meredith Broussard, N. Diakopoulos, Andrea L. Guzman, Rediet Abebe, Michel Dupagne, C. Chuan"
80179a17eab0f9fb6e21840f3fed96c4d75c3442,https://www.semanticscholar.org/paper/80179a17eab0f9fb6e21840f3fed96c4d75c3442,Building Ethics into Artificial Intelligence,"As artificial intelligence (AI) systems become increasingly ubiquitous, the topic of AI governance for ethical decision-making by AI has captured public imagination. Within the AI research community, this topic remains less familiar to many researchers. In this paper, we complement existing surveys, which largely focused on the psychological, social and legal discussions of the topic, with an analysis of recent advances in technical solutions for AI governance. By reviewing publications in leading AI conferences including AAAI, AAMAS, ECAI and IJCAI, we propose a taxonomy which divides the field into four areas: 1) exploring ethical dilemmas; 2) individual ethical decision frameworks; 3) collective ethical decision frameworks; and 4) ethics in human-AI interactions. We highlight the intuitions and key techniques used in each approach, and discuss promising future research directions towards successful integration of ethical AI systems into human societies.",2018.0,"Han Yu, Zhiqi Shen, C. Miao, Cyril Leung, V. Lesser, Qiang Yang"
d094866f223e4ade23e4f3e455b511f98d8a5773,https://www.semanticscholar.org/paper/d094866f223e4ade23e4f3e455b511f98d8a5773,Artificial Intelligence in Pathology,"As in other domains, artificial intelligence is becoming increasingly important in medicine. In particular, deep learning-based pattern recognition methods can advance the field of pathology by incorporating clinical, radiologic, and genomic data to accurately diagnose diseases and predict patient prognoses. In this review, we present an overview of artificial intelligence, the brief history of artificial intelligence in the medical domain, recent advances in artificial intelligence applied to pathology, and future prospects of pathology driven by artificial intelligence.",2018.0,"H. Chang, C. Jung, Junwoo Woo, Sanghun Lee, Joonyoung Cho, Sun Woo Kim, Tae-Yeong Kwak"
e2a240cc67c2909128f2154db2facba5810a94b0,https://www.semanticscholar.org/paper/e2a240cc67c2909128f2154db2facba5810a94b0,"Artificial Intelligence, Economics, and Industrial Organization","Machine learning (ML) and artificial intelligence (AI) have been around for many years. However, in the last 5 years, remarkable progress has been made using multilayered neural networks in diverse areas such as image recognition, speech recognition, and machine translation. AI is a general purpose technology that is likely to impact many industries. In this chapter I consider how machine learning availability might affect the industrial organization of both firms that provide AI services and industries that adopt AI technology. My intent is not to provide an extensive overview of this rapidly-evolving area, but instead to provide a short summary of some of the forces at work and to describe some possible areas for future research.",2018.0,H. Varian
48dbd3090e306dd7e36651777ece31270229eb90,https://www.semanticscholar.org/paper/48dbd3090e306dd7e36651777ece31270229eb90,Adapting to Artificial Intelligence Radiologists and Pathologists as Information Specialists,"Artificial intelligence—the mimicking of human cognition by computers—was once a fable in science fiction but is becoming reality in medicine. The combination of big data and artificial intelligence, referred to by some as the fourth industrial revolution,1 will change radiology and pathology along with other medical specialties. Although reports of radiologists and pathologists being replaced by computers seem exaggerated,2 these specialties must plan strategically for a future in which artificial intelligence is part of the health care workforce. Radiologists have always revered machines and technology. In 1960, Lusted predicted “an electronic scannercomputer to examine chest photofluorograms, to separate the clearly normal chest films from the abnormal chest films.”3 Lusted further suggested that “the abnormal chest films would be marked for later study by the radiologists.”3 Lusted’s intuitions were prescient: interpreting radiographs is pattern recognition; computers can recognize patterns and may be helpful because some roentgenographic analyses can be automated. Nearly 60 years after Lusted’s prediction, Enlitic, a technology company in Silicon Valley, inputted images of normal radiographs and radiographs with fractures into a computerized database.4 Using deep learning, a refined version of artificial neural networks, the",2016.0,
9de90994a2bdde3d50926d966e5bac0bac690037,https://www.semanticscholar.org/paper/9de90994a2bdde3d50926d966e5bac0bac690037,Artificial Intelligence in Drug Design,Artificial Intelligence (AI) plays a pivotal role in drug discovery. In particular artificial neural networks such as deep neural networks or recurrent networks drive this area. Numerous applications in property or activity predictions like physicochemical and ADMET properties have recently appeared and underpin the strength of this technology in quantitative structure-property relationships (QSPR) or quantitative structure-activity relationships (QSAR). Artificial intelligence in de novo design drives the generation of meaningful new biologically active molecules towards desired properties. Several examples establish the strength of artificial intelligence in this field. Combination with synthesis planning and ease of synthesis is feasible and more and more automated drug discovery by computers is expected in the near future.,2018.0,"G. Hessler, K. Baringhaus"
8d3eb4174ed35698654d98232dd378e68f383d12,https://www.semanticscholar.org/paper/8d3eb4174ed35698654d98232dd378e68f383d12,Artificial intelligence and echocardiography,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.",2018.0,"M. Alsharqi, W. Woodward, J. Mumith, D. C. Markham, R. Upton, P. Leeson"
b5d688d8a03bda383fca82835bbfce13941670cf,https://www.semanticscholar.org/paper/b5d688d8a03bda383fca82835bbfce13941670cf,[Artificial intelligence and machine learning].,"Machine learning (ML) is the ability of computers to learn from data without being programmed explicitly for that purpose, and to apply the acquired knowledge to unknown cases. The application of ML in medicine will increase exponentially in the years to come. Doctors should have some basic knowledge of ML. Only then will they be able to use ML optimally and to recognise the limits and difficulties of ML.",2019.0,Peter Hahn
5dcdb410fc2bebac52552594287971f8a35a0d40,https://www.semanticscholar.org/paper/5dcdb410fc2bebac52552594287971f8a35a0d40,Framing the challenges of artificial intelligence in medicine,"On a clear January morning in Florida, a Tesla enthusiast and network entrepreneur was driving his new Tesla Model S on US Highway 27A, returning from a family trip. He had posted dozens of widely circulated YouTube tutorial videos on his vehicle and clearly understood many of the technical details of his car. That day, he let the vehicle run autonomously on Autopilot mode for 37 min, before it crashed into the trailer of a truck turning left. The Autopilot did not identify the white side of the trailer as a potential hazard, and the driver was killed, leaving his family and his high-tech business behind.1 This tragedy is not a metaphor for artificial intelligence (AI) applications but an example of a long-recognised challenge in AI: the Frame Problem.2 Although rarely appreciated in the scholarly and lay descriptions of the stunning recent successes of AI in medical applications, the Frame Problem and related AI challenges will have unintended harmful effects to the care of patients if not directly addressed.

With the recent advancement in machine learning algorithms, many medical tasks previously thought to require human expertise have been replicated by AI systems at or above the level of accuracy in human experts. These important demonstrations range from evaluating fundus retinography3 and histopathology4 to reading chest radiographs5 and assessment of skin lesions.6 These studies have encompassed very large numbers of patient cases and have been extensively benchmarked against clinicians. However, all these studies are retrospective in that they involve a collection of labelled cases against which the AI systems are trained and another collection against which they are tested or validated. So far, they have not entered into routine prospective use in the clinic where the Frame Problem will manifest itself most pathologically.

The Frame …",2018.0,"Kun‐Hsing Yu, I. Kohane"
7b459e728c6fab51e57ef3461b14c71e5703a3cd,https://www.semanticscholar.org/paper/7b459e728c6fab51e57ef3461b14c71e5703a3cd,Artificial Intelligence: A Very Short Introduction,"The applications of Artificial Intelligence lie all around us and affect all aspects of our lives. The results of Artificial Intelligence have been invaluable to biologists, psychologists, and linguists in helping to understand the processes of memory, learning, and language from a fresh angle. As a concept, Artificial Intelligence has fuelled and sharpened the philosophical debates concerning the nature of the mind, intelligence, and the uniqueness of human beings. Artificial Intelligence: A Very Short Introduction considers the history of Artificial Intelligence, its successes, its limitations, and its future goals. It also reviews the philosophical and technological challenges raised by Artificial Intelligence, considering whether programs could ever be really intelligent, creative, or even conscious.",2018.0,M. Boden
b93d9995f9ce3b15f4c4855ae62f0bf6f9bc041f,https://www.semanticscholar.org/paper/b93d9995f9ce3b15f4c4855ae62f0bf6f9bc041f,Artificial Intelligence and its Role in Near Future,"AI technology has a long history which is actively and constantly changing and growing. It focuses on intelligent agents, which contain devices that perceive the environment and based on which takes actions in order to maximize goal success chances. In this paper, we will explain the modern AI basics and various representative applications of AI. In the context of the modern digitalized world, AI is the property of machines, computer programs, and systems to perform the intellectual and creative functions of a person, independently find ways to solve problems, be able to draw conclusions and make decisions. Most artificial intelligence systems have the ability to learn, which allows people to improve their performance over time. The recent research on AI tools, including machine learning, deep learning and predictive analysis intended toward increasing the planning, learning, reasoning, thinking and action taking ability. Based on which, the proposed research intends towards exploring on how the human intelligence differs from the artificial intelligence. Moreover, we critically analyze what AI of today is capable of doing, why it still cannot reach human intelligence and what are the open challenges existing in front of AI to reach and outperform human level of intelligence. Furthermore, it will explore the future predictions for artificial intelligence and based on which potential solution will be recommended to solve it within next decades.",2018.0,"Jahanzaib Shabbir, T. Anwer"
ea294e42049fd79c31abad5fa31b307b64523c3e,https://www.semanticscholar.org/paper/ea294e42049fd79c31abad5fa31b307b64523c3e,Programming Expert Systems In Ops5 An Introduction To Rule Based Programming The Addison Wesley Series In Artificial Intelligence,Recognizing the mannerism ways to get this book Programming Expert Systems In Ops5 An Introduction To Rule Based Programming The Addison Wesley Series In Artificial Intelligence is additionally useful. You have remained in right site to begin getting this info. get the Programming Expert Systems In Ops5 An Introduction To Rule Based Programming The Addison Wesley Series In Artificial Intelligence colleague that we manage to pay for here and check out the link.,2017.0,
5a12074dda3b12d686dda1969d23fee9fed9a87f,https://www.semanticscholar.org/paper/5a12074dda3b12d686dda1969d23fee9fed9a87f,A Review on Application of Artificial Intelligence in Teaching and Learning in Educational Contexts,"Innovative educational technologies have revolutionized the methods of teaching and learning. Recently, with advancements of artificial intelligence, higher education has begun to adopt new technologies. This conceptual review paper aims to investigate the emergence of using artificial intelligence in teaching and learning in education. It examines the educational consequences of emergent technologies on how institutions teach and the way students learn. This study intends to predict the role of artificial intelligence in the future nature of education in a world. The effective application of artificial intelligence methods is considered as a means of improving the quality of teaching and learning. However, the challenges of integrating artificial intelligence in educational institutions is addressed. Moreover, the challenges faced by students in adopting artificial intelligence in terms of students’ support, teaching, learning, and administration are discussed.  This paper presents a concise overview of the most recent studies to showcase the application of artificial intelligence in educational contexts. The implications and directions for further research are suggested. ",2018.0,"Mehrnaz Fahimirad, Sedigheh Shakib Kotamjani"
d525e86997c65e634f5df450c5ef93536db036cf,https://www.semanticscholar.org/paper/d525e86997c65e634f5df450c5ef93536db036cf,Artificial Intelligence and Economic Growth,"This paper examines the potential impact of artificial intelligence (A.I.) on economic growth. We model A.I. as the latest form of automation, a broader process dating back more than 200 years. Electricity, internal combustion engines, and semiconductors facilitated automation in the last century, but A.I. now seems poised to automate many tasks once thought to be out of reach, from driving cars to making medical recommendations and beyond. How will this affect economic growth and the division of income between labor and capital? What about the potential emergence of “singularities” and “superintelligence,” concepts that animate many discussions in the machine intelligence community? How will the linkages between A.I. and growth be mediated by firm-level considerations, including organization and market structure? The goal throughout is to refine a set of critical questions about A.I. and economic growth and to contribute to shaping an agenda for the field. One theme that emerges is based on Baumol’s “cost disease” insight: growth may be constrained not by what we are good at but rather by what is essential and yet hard to improve.",2017.0,"Philippe Aghion, Benjamin F. Jones, Charles I. Jones"
5fd9091800f81f8acf0806f26ddb7dc9008db8ef,https://www.semanticscholar.org/paper/5fd9091800f81f8acf0806f26ddb7dc9008db8ef,The Emergence of Artificial Intelligence: How Automation is Changing Auditing,"ABSTRACT: This paper provides an overview of the emergence of artificial intelligence in accounting and auditing. We discuss the current capabilities of cognitive technologies and the implications these technologies will have on human auditors and the audit process itself. We also provide industry examples of artificial intelligence implementation by Big 4 accounting firms. Finally, we address some potential biases associated with the creation and use of artificial intelligence and discuss implications for future research.",2017.0,"Julia Kokina, T. Davenport"
a1994d7eb6207aaae93eadb850ad0eafab64077b,https://www.semanticscholar.org/paper/a1994d7eb6207aaae93eadb850ad0eafab64077b,The Technological Elements of Artificial Intelligence,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.",2018.0,Matt Taddy
e126e3dda4f5ea25614ab957f681246b542434d9,https://www.semanticscholar.org/paper/e126e3dda4f5ea25614ab957f681246b542434d9,Analysis of the Impact of Artificial Intelligence Application on the Development of Accounting Industry,"With the rapid development of information technology and the needs of economic society, artificial intelligence has ushered in the golden age. The application of artificial intelligence technology in the accounting field is an inevitable trend, which will bring tremendous changes and development to the accounting industry. This paper takes the application of artificial intelligence in the accounting industry as the research object, analyzes the impact of artificial intelligence on the development of accounting industry, and puts forward relevant suggestions for its existing problems.",2018.0,"Jiaxin Luo, Q. Meng, Yan-Song Cai"
3596037df42e75efc31352f31103a512de7e9f61,https://www.semanticscholar.org/paper/3596037df42e75efc31352f31103a512de7e9f61,Artificial intelligence for analyzing orthopedic trauma radiographs,"Background and purpose — Recent advances in artificial intelligence (deep learning) have shown remarkable performance in classifying non-medical images, and the technology is believed to be the next technological revolution. So far it has never been applied in an orthopedic setting, and in this study we sought to determine the feasibility of using deep learning for skeletal radiographs. Methods — We extracted 256,000 wrist, hand, and ankle radiographs from Danderyd’s Hospital and identified 4 classes: fracture, laterality, body part, and exam view. We then selected 5 openly available deep learning networks that were adapted for these images. The most accurate network was benchmarked against a gold standard for fractures. We furthermore compared the network’s performance with 2 senior orthopedic surgeons who reviewed images at the same resolution as the network. Results — All networks exhibited an accuracy of at least 90% when identifying laterality, body part, and exam view. The final accuracy for fractures was estimated at 83% for the best performing network. The network performed similarly to senior orthopedic surgeons when presented with images at the same resolution as the network. The 2 reviewer Cohen’s kappa under these conditions was 0.76. Interpretation — This study supports the use for orthopedic radiographs of artificial intelligence, which can perform at a human level. While current implementation lacks important features that surgeons require, e.g. risk of dislocation, classifications, measurements, and combining multiple exam views, these problems have technical solutions that are waiting to be implemented for orthopedics.",2017.0,"Jakub Olczak, Niklas Fahlberg, A. Maki, A. Razavian, Anthony Jilert, A. Stark, O. Sköldenberg, M. Gordon"
d395659b96efe66f612d635f837a853caab55a75,https://www.semanticscholar.org/paper/d395659b96efe66f612d635f837a853caab55a75,Artificial Intelligence and Behavioral Economics,"This paper describes 2-1/2 highly speculative ideas about how artificial intelligence (AI) and behavioral economics may interact, particular in future developments in the economy and in research frontiers. First note that I’ll use the terms AI and machine learning (ML) interchangeably (although AI is broader) because the examples I have in mind all involve ML and prediction. A good introduction to ML for economists is Mullainathan and Spiess (2017).",2018.0,Colin Camerer
bd6b6291c3c14551cf9f2aa0e04e2e33c86b800e,https://www.semanticscholar.org/paper/bd6b6291c3c14551cf9f2aa0e04e2e33c86b800e,The Malmo Platform for Artificial Intelligence Experimentation,"We present Project Malmo - an AI experimentation platform built on top of the popular computer game Minecraft, and designed to support fundamental research in artificial intelligence. As the AI research community pushes for artificial general intelligence (AGI), experimentation platforms are needed that support the development of flexible agents that learn to solve diverse tasks in complex environments. Minecraft is an ideal foundation for such a platform, as it exposes agents to complex 3D worlds, coupled with infinitely varied game-play. 
 
Project Malmo provides a sophisticated abstraction layer on top of Minecraft that supports a wide range of experimentation scenarios, ranging from navigation and survival to collaboration and problem solving tasks. In this demo we present the Malmo platform and its capabilities. The platform is publicly released as open source software at IJCAI, to support openness and collaboration in AI research.",2016.0,"Matthew Johnson, Katja Hofmann, Tim Hutton, David Bignell"
8b351583c9985265ee8880f53c9a2a64e2d2b76a,https://www.semanticscholar.org/paper/8b351583c9985265ee8880f53c9a2a64e2d2b76a,Artificial Intelligence and its Implications for Income Distribution and Unemployment,"Inequality is one of the main challenges posed by the proliferation of artificial intelligence (AI) and other forms of worker-replacing technological progress. This paper provides a taxonomy of the associated economic issues: First, we discuss the general conditions under which new technologies such as AI may lead to a Pareto improvement. Secondly, we delineate the two main channels through which inequality is affected – the surplus arising to innovators and redistributions arising from factor price changes. Third, we provide several simple economic models to describe how policy can counter these effects, even in the case of a “singularity” where machines come to dominate human labor. Under plausible conditions, non-distortionary taxation can be levied to compensate those who otherwise might lose. Fourth, we describe the two main channels through which technological progress may lead to technological unemployment – via efficiency wage effects and as a transitional phenomenon. Lastly, we speculate on how technologies to create super-human levels of intelligence may affect inequality and on how to save humanity from the Malthusian destiny that may ensue.",2017.0,"Anton Korinek, J. Stiglitz"
c482327b0c810317ef074c1c7a1f2421c14b1d53,https://www.semanticscholar.org/paper/c482327b0c810317ef074c1c7a1f2421c14b1d53,"Chatbots, Humbots, and the Quest for Artificial General Intelligence","What began as a quest for artificial general intelligence branched into several pursuits, including intelligent assistants developed by tech companies and task-oriented chatbots that deliver more information or services in specific domains. Progress quickened with the spread of low-latency networking, then accelerated dramatically a few years ago. In 2016, task-focused chatbots became a centerpiece of machine intelligence, promising interfaces that are more engaging than robotic answering systems and that can accommodate our increasingly phone-based information needs. Hundreds of thousands were built. Creating successful non-trivial chatbots proved more difficult than anticipated. Some developers now design for human-chatbot (humbot) teams, with people handling difficult queries. This paper describes the conversational agent space, difficulties in meeting user expectations, potential new design approaches, uses of human-bot hybrids, and implications for the ultimate goal of creating software with general intelligence.",2019.0,"J. Grudin, R. Jacques"
0cc6dbfd929bc816d507527993f55f9b4e88615d,https://www.semanticscholar.org/paper/0cc6dbfd929bc816d507527993f55f9b4e88615d,Machine learning \& artificial intelligence in the quantum domain,"Quantum information technologies, and intelligent learning systems, are both emergent technologies that will likely have a transforming impact on our society. The respective underlying fields of research -- quantum information (QI) versus machine learning (ML) and artificial intelligence (AI) -- have their own specific challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question to what extent these fields can learn and benefit from each other. QML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently, we have witnessed breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups in ML, critical in our ""big data"" world. Conversely, ML already permeates cutting-edge technologies, and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been demonstrated for interactive learning, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments, and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement, researchers have also broached the fundamental issue of quantum generalizations of ML/AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is described by quantum mechanics. In this review, we describe the main ideas, recent developments, and progress in a broad spectrum of research investigating machine learning and artificial intelligence in the quantum domain.",2017.0,"V. Dunjko, H. Briegel"
48aee85dfb2ec9f38891c04ccd9116019f2dff9b,https://www.semanticscholar.org/paper/48aee85dfb2ec9f38891c04ccd9116019f2dff9b,Artificial Intelligence Policy: A Primer and Roadmap,"Talk of artificial intelligence is everywhere. People marvel at the capacity of machines to translate any language and master any game. Others condemn the use of secret algorithms to sentence criminal defendants or recoil at the prospect of machines gunning for blue, pink, and white-collar jobs. Some worry aloud that artificial intelligence will be humankind’s “final invention.”  This essay, prepared in connection with UC Davis Law Review's 50th anniversary symposium, explains why AI is suddenly on everyone's mind and provides a roadmap to the major policy questions AI raises. The essay is designed to help policymakers, investors, technologists, scholars, and students understand the contemporary policy environment around AI at least well enough to initiate their own exploration. Topics covered include: justice and equity, use of force, safety and certification, privacy (including data parity) and taxation and displacement of labor. In addition to these topics, the essay will touch briefly on a selection of broader systemic questions: institutional configuration and expertise, investment and procurement, removing hurdles to accountability and correcting mental models of AI.",2017.0,Ryan Calo
db3989d9fe88cb4c18e0bbae84740bcacae97980,https://www.semanticscholar.org/paper/db3989d9fe88cb4c18e0bbae84740bcacae97980,Artificial Intelligence in Advertising,"The task of understanding the consumer journey increasingly is complex. Consumers express their needs and wants, attitudes, and values in various forms (through search, comments, blogs, Tweets, “likes,” videos, and conversations) and across many channels (web, mobile, and face to face; [Court,",2018.0,"Jan H. Kietzmann, Jeannette Paschen, Emily Treen"
961d0f3d0c852055a202ef315b02cd69863098e9,https://www.semanticscholar.org/paper/961d0f3d0c852055a202ef315b02cd69863098e9,"Artificial intelligence, machine learning and deep learning","It is increasingly recognized that artificial intelligence has been touted as a new mobile. Because of the high volume of data that being generated by devices, sensors and social media users, the machine can learn to distinguish the pattern and makes a reasonably good prediction. This article will explore the use of machine learning and its methodologies. Furthermore, the field of deep learning which is being exploited in many leading IT providers will be clarified and discussed.",2017.0,Pariwat Ongsulee
0ef8f640d305f5a55af1cf24108bd9a9efebb5ee,https://www.semanticscholar.org/paper/0ef8f640d305f5a55af1cf24108bd9a9efebb5ee,The role of artificial intelligence in precision medicine,"The essence of practicing medicine has been obtaining as much data about the patient’s health or disease as possible and making decisions based on that. Physicians have had to rely on their experience, judgement, and problem-solving skills while using rudimentary tools and limited resources. With the cultural transformation called digital health, disruptive technologies have started to make advanced methods available not only to medical professionals but also to their patients. These technologies such as genomics, biotechnology, wearable sensors, or artificial intelligence (AI) are gradually leading to three major directions. They have been (1) making patients the point-of-care; (2) created a vast amount of data that require advanced analytics; and (3) made the foundation of precision medicine. Instead of developing treatments for populations and making the same medical decisions based on a few similar physical characteristics among patients, medicine has shifted toward prevention, personalization, and precision. In this shift and cultural transformation, AI is the key technology that can bring this opportunity to everyday practice.",2017.0,B. Meskó
4312ae64a058d555ff1656ccad713dbd81200e79,https://www.semanticscholar.org/paper/4312ae64a058d555ff1656ccad713dbd81200e79,DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker,"Artiﬁcial intelligence has seen a number of breakthroughs in recent years, with games often serving as signiﬁcant milestones. A common feature of games with these successes is that they involve information symmetry among the players, where all players have identical information. This property of perfect information , though, is far more common in games than in real-world problems. Poker is the quintessential game of imperfect information , and it has been a longstanding challenge problem in artiﬁcial intelligence. In this paper we introduce DeepStack, a new algorithm for imperfect information settings such as poker. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition about arbitrary poker situations that is automatically learned from self-play games using deep learning. In a study involving dozens of participants and 44,000 hands of poker, DeepStack becomes the ﬁrst computer program to beat professional poker players in heads-up no-limit Texas hold’em. Furthermore, we show this approach dramatically reduces worst-case exploitability compared to the abstraction paradigm that has been favored for over a decade. Games have long served as a benchmark and set of milestones for progress in artiﬁcial",2017.0,"Matej Moravcík, Martin Schmid, Neil Burch, V. Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, K. Waugh, Michael Bradley Johanson, Michael Bowling"
59c22bff89a102365814cc61eb06ce3325eda39d,https://www.semanticscholar.org/paper/59c22bff89a102365814cc61eb06ce3325eda39d,Artificial intelligence and statistics,"Artificial intelligence (AI) is intrinsically data-driven. It calls for the application of statistical concepts through human-machine collaboration during the generation of data, the development of algorithms, and the evaluation of results. This paper discusses how such human-machine collaboration can be approached through the statistical concepts of population, question of interest, representativeness of training data, and scrutiny of results (PQRS). The PQRS workflow provides a conceptual framework for integrating statistical ideas with human input into AI products and researches. These ideas include experimental design principles of randomization and local control as well as the principle of stability to gain reproducibility and interpretability of algorithms and data results. We discuss the use of these principles in the contexts of self-driving cars, automated medical diagnoses, and examples from the authors’ collaborative research.",2017.0,"Bin Yu, Karl Kumbier"
b2277775e67ad13a5ab22d86a055e1f49a4cc8f5,https://www.semanticscholar.org/paper/b2277775e67ad13a5ab22d86a055e1f49a4cc8f5,Human-in-the-loop Artificial Intelligence,"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves. 
In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.",2017.0,Fabio Massimo Zanzotto
b6662d116fce4b20e3baf157bda10f44b98a4125,https://www.semanticscholar.org/paper/b6662d116fce4b20e3baf157bda10f44b98a4125,Advances of flexible pressure sensors toward artificial intelligence and health care applications,"By virtue of their wide applications in personal electronic devices and industrial monitoring, pressure sensors are attractive candidates for promoting the advancement of science and technology in modern society. Flexible pressure sensors based on organic materials, which combine unique advantages of flexibility and low-cost, have emerged as a highly active field due to their promising applications in artificial intelligence systems and wearable health care devices. In this review, we focus on the fundamentals of flexible pressure sensors, and subsequently on several critical concepts for the exploration of functional materials and optimization of sensing devices toward practical applications. Perspectives on self-powered, transparent and implantable pressure sensing devices are also examined to highlight the development directions in this exciting research field.",2015.0,"Yaping Zang, Fengjiao Zhang, Chong‐an Di, Daoben Zhu"
96ef2680eada97d2e7ff65e7a075513ecd449cdd,https://www.semanticscholar.org/paper/96ef2680eada97d2e7ff65e7a075513ecd449cdd,Steps Toward Robust Artificial Intelligence,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world",2017.0,Thomas G. Dietterich
b8e37682c847844a4b5c4851239fdc3357d5577b,https://www.semanticscholar.org/paper/b8e37682c847844a4b5c4851239fdc3357d5577b,Lecture Notes in Artificial Intelligence,"LNAI was established in the mid-1980s as a topical subseries of LNCS focusing on artificial intelligence. This subseries is devoted to the publication of state-of-the-art research results in artificial intelligence, at a high level and in both printed and electronic versions making use of the well-established LNCS publication machinery. As with the LNCS mother series, proceedings and postproceedings are at the core of LNAI; however, all other sublines are available for LNAI as well. The topics in LNAI include automated reasoning, automated programming, algorithms, knowledge representation, agent-based systems, intelligent systems, expert systems, machine learning, natural-language processing, machine vision, robotics, search systems, knowledge discovery, data mining, and related programming languages.",1999.0,"P. Brézillon, Paolo Bouquet"
f35672f107fad36ddb958854aa5f71817e0575c6,https://www.semanticscholar.org/paper/f35672f107fad36ddb958854aa5f71817e0575c6,Long-Term Trends in the Public Perception of Artificial Intelligence,"
 
 Analyses of text corpora over time can reveal trends in beliefs, interest, and sentiment about a topic. We focus on views expressed about artificial intelligence (AI) in the New York Times over a 30-year period. General interest, awareness, and discussion about AI has waxed and waned since the field was founded in 1956. We present a set of measures that captures levels of engagement, measures of pessimism and optimism, the prevalence of specific hopes and concerns, and topics that are linked to discussions about AI over decades. We find that discussion of AI has increased sharply since 2009, and that these discussions have been consistently more optimistic than pessimistic. However, when we examine specific concerns, we find that worries of loss of control of AI, ethical concerns for AI, and the negative impact of AI on work have grown in recent years. We also find that hopes for AI in healthcare and education have increased over time.
 
",2016.0,"Ethan Fast, E. Horvitz"
1ecc18588fc0a52cc4fb12b3fa3ea67bb3e9a383,https://www.semanticscholar.org/paper/1ecc18588fc0a52cc4fb12b3fa3ea67bb3e9a383,Artificial Intelligence for Cybersecurity,"There is an intensive need for intrusion detection systems (IDSs) due to incremental and frequent cyber-attacks. The first line of defense against online threats is an IDS. Researchers are using deep learning (DL) approaches to detect attackers and preserve user information. In this study, we introduce a hybrid DL-based model. The proposed model integrates LSTM and ResNet to eliminate the vanishing gradient problem and increase the accuracy of the classification model. The proposed model aims to classify between normal or an attack, with each attack either being a DoS, U2R, R2L, or a probe over the NSL-KDD dataset. The proposed model achieves 99.5% according to accuracy. The model was compared with other ML and DL models.",2018.0,"A. Karasaridis, B. Rexroad, P. Velardo"
6d7b27446c08a055217dc51ec2b172a7497f3eaa,https://www.semanticscholar.org/paper/6d7b27446c08a055217dc51ec2b172a7497f3eaa,Keeping Humans in the Loop: Pooling Knowledge through Artificial Swarm Intelligence to Improve Business Decision Making,"This article explores how a collaboration technology called Artificial Swarm Intelligence (ASI) addresses the limitations associated with group decision making, amplifies the intelligence of human groups, and facilitates better business decisions. It demonstrates of how ASI has been used by businesses to harness the diverse perspectives that individual participants bring to groups and to facilitate convergence upon decisions. It advances the understanding of how artificial intelligence (AI) can be used to enhance, rather than replace, teams as they collaborate to make business decisions.",2019.0,"L. Metcalf, David A. Askay, Louis B. Rosenberg"
60b0bef92e715a9991fbb1a91c83c3a10cbee552,https://www.semanticscholar.org/paper/60b0bef92e715a9991fbb1a91c83c3a10cbee552,Artificial intelligence may help in predicting the need for additional surgery after endoscopic resection of T1 colorectal cancer,"Abstract Background and study aims Decisions concerning additional surgery after endoscopic resection of T1 colorectal cancer (CRC) are difficult because preoperative prediction of lymph node metastasis (LNM) is problematic. We investigated whether artificial intelligence can predict LNM presence, thus minimizing the need for additional surgery. Patients and methods Data on 690 consecutive patients with T1 CRCs that were surgically resected in 2001 – 2016 were retrospectively analyzed. We divided patients into two groups according to date: data from 590 patients were used for machine learning for the artificial intelligence model, and the remaining 100 patients were included for model validation. The artificial intelligence model analyzed 45 clinicopathological factors and then predicted positivity or negativity for LNM. Operative specimens were used as the gold standard for the presence of LNM. The artificial intelligence model was validated by calculating the sensitivity, specificity, and accuracy for predicting LNM, and comparing these data with those of the American, European, and Japanese guidelines. Results Sensitivity was 100 % (95 % confidence interval [CI] 72 % to 100 %) in all models. Specificity of the artificial intelligence model and the American, European, and Japanese guidelines was 66 % (95 %CI 56 % to 76 %), 44 % (95 %CI 34 % to 55 %), 0 % (95 %CI 0 % to 3 %), and 0 % (95 %CI 0 % to 3 %), respectively; and accuracy was 69 % (95 %CI 59 % to 78 %), 49 % (95 %CI 39 % to 59 %), 9 % (95 %CI 4 % to 16 %), and 9 % (95 %CI 4 % – 16 %), respectively. The rates of unnecessary additional surgery attributable to misdiagnosing LNM-negative patients as having LNM were: 77 % (95 %CI 62 % to 89 %) for the artificial intelligence model, and 85 % (95 %CI 73 % to 93 %; P < 0.001), 91 % (95 %CI 84 % to 96 %; P < 0.001), and 91 % (95 %CI 84 % to 96 %; P < 0.001) for the American, European, and Japanese guidelines, respectively. Conclusions Compared with current guidelines, artificial intelligence significantly reduced unnecessary additional surgery after endoscopic resection of T1 CRC without missing LNM positivity.",2017.0,"K. Ichimasa, S. Kudo, Y. Mori, M. Misawa, S. Matsudaira, Y. Kouyama, T. Baba, E. Hidaka, K. Wakamura, Takemasa Hayashi, T. Kudo, Tomoyuki Ishigaki, Yusuke Yagawa, H. Nakamura, K. Takeda, A. Haji, S. Hamatani, K. Mori, F. Ishida, H. Miyachi"
ff4341d2fdc178d806922e28758e7f68fe0058dc,https://www.semanticscholar.org/paper/ff4341d2fdc178d806922e28758e7f68fe0058dc,18 The Jobs That Artificial Intelligence Will Create,"The threat that automation will eliminate a broad swath of jobs across the world economy is now well established. As artificial intelligence (AI) systems become ever more sophisticated, another wave of job displacement will almost certainly occur.",2017.0,P. Michelman
4029b344fc0c9f35f41bfe35c443111faba27230,https://www.semanticscholar.org/paper/4029b344fc0c9f35f41bfe35c443111faba27230,Research Priorities for Robust and Beneficial Artificial Intelligence,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.",2015.0,"Stuart J. Russell, Dan Dewey, Max Tegmark"
57026b2d45fa59c6326b5a1d2e27626403f083ba,https://www.semanticscholar.org/paper/57026b2d45fa59c6326b5a1d2e27626403f083ba,Ethical Considerations in Artificial Intelligence Courses,"The recent surge in interest in ethics in artificial intelligence may leave many educators wondering how to address moral, ethical, and philosophical issues in their AI courses. As instructors we want to develop curriculum that not only prepares students to be artificial intelligence practitioners, but also to understand the moral, ethical, and philosophical impacts that artificial intelligence will have on society. In this article we provide practical case studies and links to resources for use by AI educators. We also provide concrete suggestions on how to integrate AI ethics into a general artificial intelligence course and how to teach a stand-alone artificial intelligence ethics course.",2017.0,"Emanuelle Burton, J. Goldsmith, Sven Koenig, B. Kuipers, Nicholas Mattei, T. Walsh"
86006acb8650fe9c2a2b056e5271d9b525e09c3c,https://www.semanticscholar.org/paper/86006acb8650fe9c2a2b056e5271d9b525e09c3c,Artificial Intelligence's White Guy Problem,"According to some prominent voices in the tech world, artificial intelligence presents a looming existential threat to humanity: Warnings by luminaries like Elon Musk and Nick Bostrom about “the singularity” – when machines become smarter than humans – have attracted millions of dollars and spawned a multitude of conferences. But this hand-wringing is a distraction from the very real problems with artificial intelligence today, which may already be exacerbating inequality in the workplace, at home and in our legal and judicial systems. Sexism, racism and other forms of discrimination are being built into the machine-learning algorithms that underlie the technology behind many “intelligent” systems that shape how we are categorized and advertised to.",2016.0,K. Crawford
a41b5b11123ea588753fb2fd25be1dfa47593482,https://www.semanticscholar.org/paper/a41b5b11123ea588753fb2fd25be1dfa47593482,Towards Verified Artificial Intelligence,"Verified artificial intelligence (AI) is the goal of designing AI-based systems that that have strong, ideally provable, assurances of correctness with respect to mathematically-specified requirements. This paper considers Verified AI from a formal methods perspective. We describe five challenges for achieving Verified AI, and five corresponding principles for addressing these challenges.",2016.0,"S. Seshia, Dorsa Sadigh"
a3bbffdcc1c7c4cae66d6af373651389d94b7090,https://www.semanticscholar.org/paper/a3bbffdcc1c7c4cae66d6af373651389d94b7090,Moral Decision Making Frameworks for Artificial Intelligence,"
 
 The generality of decision and game theory has enabled domain-independent progress in AI research. For example, a better algorithm for finding good policies in (PO)MDPs can be instantly used in a variety of applications. But such a general theory is lacking when it comes to moral decision making. For AI applications with a moral component, are we then forced to build systems based on many ad-hoc rules? In this paper we discuss possible ways to avoid this conclusion.
 
",2017.0,"Vincent Conitzer, Walter Sinnott-Armstrong, Jana Schaich Borg, Yuan Deng, Max F. Kramer"
1fe13d7397813b0b11d6b5f5bce254c2c4d8ea59,https://www.semanticscholar.org/paper/1fe13d7397813b0b11d6b5f5bce254c2c4d8ea59,Genetic Fuzzy based Artificial Intelligence for Unmanned Combat Aerial Vehicle Control in Simulated Air Combat Missions,"Breakthroughs in genetic fuzzy systems, most notably the development of the Genetic Fuzzy Tree methodology, have allowed fuzzy logic based Artificial Intelligences to be developed that can be applied to incredibly complex problems. The ability to have extreme performance and computational efficiency as well as to be robust to uncertainties and randomness, adaptable to changing scenarios, verified and validated to follow safety specifications and operating doctrines via formal methods, and easily designed and implemented are just some of the strengths that this type of control brings. Within this white paper, the authors introduce ALPHA, an Artificial Intelligence that controls flights of Unmanned Combat Aerial Vehicles in aerial combat missions within an extreme-fidelity simulation environment. To this day, this represents the most complex application of a fuzzy-logic based Artificial Intelligence to an Unmanned Combat Aerial Vehicle control problem. While development is on-going, the version of ALPHA presented withinwas assessed by Colonel (retired)Gene Lee who described ALPHA as “the most aggressive, responsive, dynamic and credible AI (he’s) seen-to-date.” The quality of these preliminary results in a problem that is not only complex and rife with uncertainties but also contains an intelligent and unrestricted hostile force has significant implications for this type of Artificial Intelligence. This work adds immensely to the body of evidence that this methodology is an ideal solution to a very wide array of problems.",2016.0,"Nicholas Ernest, David Carroll, C. Schumacher, M. Clark, Kelly Cohen, Gene Lee"
2190f4bb91b7bc0aca89436aa2b7ea60e7cf2fab,https://www.semanticscholar.org/paper/2190f4bb91b7bc0aca89436aa2b7ea60e7cf2fab,IRobot: Teaching the Basics of Artificial Intelligence in High Schools,"
 
 Profound knowledge about Artificial Intelligence (AI) will become increasingly important for careers in science and engineering. Therefore an innovative educational project teaching fundamental concepts of AI at high school level will be presented in this paper. We developed an AI-course covering major topics (problem solving, search, planning, graphs, datastructures, automata, agent systems, machine learning) which comprises both theoretical and hands-on components. A pilot project was conducted and empirically evaluated. Results of the evaluation show that the participating pupils have become familiar with those concepts and the various topics addressed. Results and lessons learned from this project form the basis for further projects in different schools which intend to integrate AI in future secondary science education.
 
",2016.0,"H. Burgsteiner, Martin Kandlhofer, Gerald Steinbauer"
863395506a7f13173835d0e874b458c3340c2053,https://www.semanticscholar.org/paper/863395506a7f13173835d0e874b458c3340c2053,Artificial Intelligence and Robotics,"The recent successes of AI have captured the wildest imagination of both the scientific communities and the general public. Robotics and AI amplify human potentials, increase productivity and are moving from simple reasoning towards human-like cognitive abilities. Current AI technologies are used in a set area of applications, ranging from healthcare, manufacturing, transport, energy, to financial services, banking, advertising, management consulting and government agencies. The global AI market is around 260 billion USD in 2016 and it is estimated to exceed 3 trillion by 2024. To understand the impact of AI, it is important to draw lessons from it's past successes and failures and this white paper provides a comprehensive explanation of the evolution of AI, its current status and future directions.",2017.0,"Javier Andreu-Perez, F. Deligianni, D. Ravì, Guang-Zhong Yang"
69022c885504a091680cf2dc9cfc84597332ac69,https://www.semanticscholar.org/paper/69022c885504a091680cf2dc9cfc84597332ac69,Artificial Intelligence through Simulated Evolution,This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices,1966.0,"L. Fogel, A. J. Owens, M. J. Walsh"
522e1c3a5770de5fcec50daf7aa766fe9efa5d3f,https://www.semanticscholar.org/paper/522e1c3a5770de5fcec50daf7aa766fe9efa5d3f,Swarm Intelligence From Natural To Artificial Systems,"Thank you very much for reading swarm intelligence from natural to artificial systems. As you may know, people have search hundreds times for their chosen books like this swarm intelligence from natural to artificial systems, but end up in malicious downloads. Rather than reading a good book with a cup of coffee in the afternoon, instead they juggled with some infectious bugs inside their computer.",2016.0,W. Ziegler
882e1ab2d7a508c6ebfb8884a549b85a2da4b385,https://www.semanticscholar.org/paper/882e1ab2d7a508c6ebfb8884a549b85a2da4b385,"Regulating Artificial Intelligence Systems: Risks, Challenges, Competencies, and Strategies","Artificial intelligence technology (or AI) has developed rapidly during the past decade, and the effects of the AI revolution are already being keenly felt in many sectors of the economy. A growing chorus of commentators, scientists, and entrepreneurs has expressed alarm regarding the increasing role that autonomous machines are playing in society, with some suggesting that government regulation may be necessary to reduce the public risks that AI will pose. Unfortunately, the unique features of AI and the manner in which AI can be developed present both practical and conceptual challenges for the legal system. These challenges must be confronted if the legal system is to positively impact the development of AI and ensure that aggrieved parties receive compensation when AI systems cause harm. This article will explore the public risks associated with AI and the competencies of government institutions in managing those risks. It concludes with a proposal for an indirect form of AI regulation based on differential tort liability.",2015.0,Matthew U. Scherer
e9e961b64186c9d11ad3625eb9128b994ab45a21,https://www.semanticscholar.org/paper/e9e961b64186c9d11ad3625eb9128b994ab45a21,Artificial Intelligence Ethik (Artificial Intelligence Ethics),"German Abstract: Zeitgenoessische Theorien und Studien der Oekonomie sind behavioral. Behavioral Economics hat neo-klassische Oekonomie in den letzten beiden Jahrzehnten revolutioniert. Seither haben zwei Oekonomie Nobelpreise dieses wachsende Feld gekroent und eine weite Bandbreite an psychologischen, oekonomischen und soziologischen Labor- und Feldexperiment menschliches Entscheidungsverhalten von rationalen Modellen abweichend gefunden. Standard-Neo-klassische Profitmaximierungsaxiome versagen haeufig reales Verhalten vorherzusagen. Menschen verwenden eher Heuristiken in ihren alltaeglichen Entscheidungen. Diese mentale Hilfen erlauben es uns, mit einer komplexen Welt fertigzuwerden, lassen uns gleichzeitig aber auch gebiased und fehlerhaft erscheinen. Die darauf folgende Weiterentwicklung von Behavioral Insights wendet Behavioral Economics auf den oeffentlichen Sektor und Internationale Entwicklung an. Behaviorale Oekonom/Innen stubsen und winken dabei Staatsbuerger/Innen dazu, bessere Entscheidungen fuer sich selbst und die Allgemeinheit zu treffen. Viele rationale Koordinierungen folgten auf der ganzen Welt und in unterschiedlichsten Domainen, wie beispielsweise Organspendeverhalten, Gesundheit, Reichtum und Zeitmanagement, um nur einige zu nennen. Der folgende Artikel beschreibt behaviorale Aspekte oekonomischer Analysen und interkulturelle Differenzen von behavioralen Schulen, um kritische Fragen anhand von klassischen Behavioral Economics Beispielen interdisziplinaer zu beleuchten. Daraufhin werden aktuellste Fragen der behavioraler Oekonomie behandelt. Welches Marktstabilisierungspotenzial steckt in Behavioral Finance? Welche Ethikmandate sollten Behavioral Economists folgen? Koennten Big Data getriebene Resultate kritische Privatsphaere-Probleme mit sich bringen? Im zukuenftigen Zeitalter von Artificial Intelligence, sollten Algorithmen menschliches Entscheidungsverhalten wahrheitsgetreu nachahmen oder sollten wir nach artifizieller Rationalitaet streben? Wo sind die Grenzen der Anwendung von Behavioral Insights? Fuehrt nudging im Zuge vom Libertarian Paternalismus automatisch zu einem sozialen Gefaelle in die, die nudgen, gegenueber denen, die genudged werden? Dieser Artikel wirft damit kritische Fragen zur Ethik in Artificial Intelligence an der Speerspitze von Behavioralism und Behavioraler Oekonomischer Analysen in zirkulaerem, gedanken-stimulierendem Sokratischen Fragen auf. Ethik in Artificial Intelligence, fuer die sich diese Arbeit einsetzt. 
English Abstract: Contemporary theories and studies of economics have turned behavioral. Behavioral Economics revolutionized mainstream neo-classical economics in the past two decades. Since then two Nobel Prizes in Economics have crowned this growing field as a wide range of psychological, economic and sociological laboratory and field experiments proved human beings deviating from rational choices and standard neo-classical profit maximization axioms often failed to explain how human actually behave. Human beings rather use heuristics in their day-to-day decision making. These mental short cuts enable to cope with a complex world yet also often leave individuals biased and falling astray to decision making failures. What followed was the powerful extension of behavioral insights for public policy making and international development. Behavioral economists proposed to nudge and wink citizens to make better choices for them and the community around the globe. Many different applications of rational coordination followed ranging from improved organ donations, health, wealth and time management, to name a few. Starting with the beginning of the entrance of behavioral aspects in economic analyses and intercultural differences in behavioral understandings, this paper dares to embark on critical questions of classic behavioral economics examples featuring powerful applications in a truly interdisciplinary fashion to end with exploring the most novel cutting-edge questions on the behavioral analysis frontier. What is the potential impetus of behavioral finance? What role do ethics play for behavioral economists? Do big data driven results impose critical privacy concerns? In the future age of Artificial Intelligence, should we create algorithms that resemble human decision making or strive for rational artificiality? What are the boundaries of the extension of behavioral insights? And does nudging in the wake of libertarian paternalism entail a social class division into those who nudge and those who are nudged? This articles will give credit to critical questions at the forefront of Behavioralism and Behavioral Economic Analysis. The paper will rather follow a circular method in a Socratic style featuring thought-provoking stimulations about the future of Artificial Intelligence Ethics, which this piece advocates for.",2018.0,Julia M. Puaschunder
b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e,https://www.semanticscholar.org/paper/b6df5c2ac2f91d71b1d08d76135e2a470ac1ad1e,MACHINE LEARNING An Artificial Intelligence Approach,"Research in the area of learning structural descriptions from examples is reviewed, giving primary attention to methods of learning characteristic descrip­ tions of single concepts. In particular, we examine methods for finding the maximally-specific conjunctive generalizations (MSC-generalizations) that cover all of the training examples of a given concept. Various important aspects of structural learning in general are examined, and several criteria for evaluating structural learning methods are presented. Briefly, these criteria include (i) ade­ quacy of the representation language, (ii) generalization rules employed, computational efficiency, and (iv) flexibility and extensibility. Selected learning methods developed by Buchanan, et al., Hayes-Roth, Vere, Winston, and the authors are analyzed according to these criteria. Finally, some goals are sug­ gested for future research.",2009.0,"R. Michalski, Tom Mitchell, Jack Mostow, Bernard Nudel, Michael Rychener, Ross Quinlan, Herbert Simon, Derek Sleeman, Robert Stepp, P. Utgoff"
2bf2c3598f1ee3a2fa22185cb2aaeb425de004ad,https://www.semanticscholar.org/paper/2bf2c3598f1ee3a2fa22185cb2aaeb425de004ad,Commonsense reasoning and commonsense knowledge in artificial intelligence,"AI has seen great advances of many kinds recently, but there is one critical area where progress has been extremely slow: ordinary commonsense.",2015.0,"E. Davis, G. Marcus"
3401e99be9c8a98391553339b4c2d99de9d5626a,https://www.semanticscholar.org/paper/3401e99be9c8a98391553339b4c2d99de9d5626a,A Review of Intelligent Driving Style Analysis Systems and Related Artificial Intelligence Algorithms,"In this paper the various driving style analysis solutions are investigated. An in-depth investigation is performed to identify the relevant machine learning and artificial intelligence algorithms utilised in current driver behaviour and driving style analysis systems. This review therefore serves as a trove of information, and will inform the specialist and the student regarding the current state of the art in driver style analysis systems, the application of these systems and the underlying artificial intelligence algorithms applied to these applications. The aim of the investigation is to evaluate the possibilities for unique driver identification utilizing the approaches identified in other driver behaviour studies. It was found that Fuzzy Logic inference systems, Hidden Markov Models and Support Vector Machines consist of promising capabilities to address unique driver identification algorithms if model complexity can be reduced.",2015.0,"G. A. M. Meiring, H. Myburgh"
38e61d9a65aa483ad0fb4a219fe54d4e6a2f6c36,https://www.semanticscholar.org/paper/38e61d9a65aa483ad0fb4a219fe54d4e6a2f6c36,"A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence, August 31, 1955","The 1956 Dartmouth summer research project on artificial intelligence was initiated by this August 31, 1955 proposal, authored by John McCarthy, Marvin Minsky, Nathaniel Rochester, and Claude Shannon. The original typescript consisted of 17 pages plus a title page. Copies of the typescript are housed in the archives at Dartmouth College and Stanford University. The first 5 papers state the proposal, and the remaining pages give qualifications and interests of the four who proposed the study. In the interest of brevity, this article reproduces only the proposal itself, along with the short autobiographical statements of the proposers.",2006.0,"John McCarthy, M. Minsky, N. Rochester, C. Shannon"
9a4d9a755134e612854db1897c03adb3983413df,https://www.semanticscholar.org/paper/9a4d9a755134e612854db1897c03adb3983413df,Artificial Intelligence and its Application in Different Areas,"In the future, intelligent machines will replace or enhance human capabilities in many areas. Artificial intelligence is the intelligence exhibited by machines or software. It is the subfield of computer science. Artificial Intelligence is becoming a popular field in computer science as it has enhanced the human life in many areas. Artificial intelligence in the last two decades has greatly improved performance of the manufacturing and service systems. Study in the area of artificial intelligence has given rise to the rapidly growing technology known as expert system. Application areas of Artificial Intelligence is having a huge impact on various fields of life as expert system is widely used these days to solve the complex problems in various areas as science, engineering, business, medicine, weather forecasting. The areas employing the technology of Artificial Intelligence have seen an increase in the quality and efficiency. This paper gives an overview of this technology and the application areas of this technology. This paper will also explore the current use of Artificial Intelligence technologies in the PSS design to damp the power system oscillations caused by interruptions, in Network Intrusion for protecting computer and communication networks from intruders, in the medical area- medicine, to improve hospital inpatient care, for medical image classification, in the accounting databases to mitigate the problems of it and in the computer games.",2015.0,"A. Pannu, M. T. Student"
1a57fe1d21957c500cdf9abc19eaf97c54d294aa,https://www.semanticscholar.org/paper/1a57fe1d21957c500cdf9abc19eaf97c54d294aa,Applications of Artificial Intelligence in Machine Learning: Review and Prospect,"Machine learning is one of the most exciting recent technologies in Artificial Intelligence. Learning algorithms in many applications that’s we make use of daily. Every time a web search engine like Google or Bing is used to search the internet, one of the reasons that works so well is because a learning algorithm, one implemented by Google or Microsoft, has learned how to rank web pages. Every time Facebook is used and it recognizes friends' photos, that's also machine learning. Spam filters in email saves the user from having to wade through tons of spam email, that's also a learning algorithm. In this paper, a brief review and future prospect of the vast applications of machine learning has been made.",2015.0,"S. Das, Aritra Dey, A. Pal, N. Roy"
11d856ed984f662a5f9bacb47490e15476ae7225,https://www.semanticscholar.org/paper/11d856ed984f662a5f9bacb47490e15476ae7225,Economic reasoning and artificial intelligence,"The field of artificial intelligence (AI) strives to build rational agents capable of perceiving the world around them and taking actions to advance specified goals. Put another way, AI researchers aim to construct a synthetic homo economicus, the mythical perfectly rational agent of neoclassical economics. We review progress toward creating this new species of machine, machina economicus, and discuss some challenges in designing AIs that can reason effectively in economic contexts. Supposing that AI succeeds in this quest, or at least comes close enough that it is useful to think about AIs in rationalistic terms, we ask how to design the rules of interaction in multi-agent systems that come to represent an economy of AIs. Theories of normative design from economics may prove more relevant for artificial agents than human agents, with AIs that better respect idealized assumptions of rationality than people, interacting through novel rules and incentive systems quite distinct from those tailored for people.",2015.0,"D. Parkes, Michael P. Wellman"
088850c07e3d592d5b7ad333d4d337ed8fea69dc,https://www.semanticscholar.org/paper/088850c07e3d592d5b7ad333d4d337ed8fea69dc,Application Of Artificial Intelligence Methods In Drilling System Design And Operations: A Review Of The State Of The Art,"Abstract Artificial Intelligence (AI) can be defined as the application of science and engineering with the intent of intelligent machine composition. It involves using tool based on intelligent behavior of humans in solving complex issues, designed in a way to make computers execute tasks that were earlier thought of human intelligence involvement. In comparison to other computational automations, AI facilitates and enables time reduction based on personnel needs and most importantly, the operational expenses. Artificial Intelligence (AI) is an area of great interest and significance in petroleum exploration and production. Over the years, it has made an impact in the industry, and the application has continued to grow within the oil and gas industry. The application in E & P industry has more than 16 years of history with first application dated 1989, for well log interpretation; drill bit diagnosis using neural networks and intelligent reservoir simulator interface. It has been propounded in solving many problems in the oil and gas industry which includes, seismic pattern recognition, reservoir characterisation, permeability and porosity prediction, prediction of PVT properties, drill bits diagnosis, estimating pressure drop in pipes and wells, optimization of well production, well performance, portfolio management and general decision making operations and many more. This paper reviews and analyzes the successful application of artificial intelligence techniques as related to one of the major aspects of the oil and gas industry, drilling capturing the level of application and trend in the industry. A summary of various papers and reports associated with artificial intelligence applications and it limitations will be highlighted. This analysis is expected to contribute to further development of this technique and also determine the neglected areas in the field.",2015.0,"O. Bello, J. Holzmann, T. Yaqoob, C. Teodoriu"
724e671b43ba51113e92c68f05301c686ad2fc1c,https://www.semanticscholar.org/paper/724e671b43ba51113e92c68f05301c686ad2fc1c,The Sciences of the Artificial,"Continuing his exploration of the organization of complexity and the science of design, this new edition of Herbert Simon's classic work on artificial intelligence adds a chapter that sorts out the current themes and tools -- chaos, adaptive systems, genetic algorithms -- for analyzing complexity and complex systems. There are updates throughout the book as well. These take into account important advances in cognitive psychology and the science of design while confirming and extending the book's basic thesis: that a physical symbol system has the necessary and sufficient means for intelligent action. The chapter ""Economic Reality"" has also been revised to reflect a change in emphasis in Simon's thinking about the respective roles of organizations and markets in economic systems.",1970.0,H. Simon
d250e7ac9ececc1c0d732478c8138d6367400c2a,https://www.semanticscholar.org/paper/d250e7ac9ececc1c0d732478c8138d6367400c2a,Artificial Immune Systems: A New Computational Intelligence Approach,Introduction.- Fundamentals of the Immune System.- A Framework for Engineering Artificial Immune Systems.- A Survey of Artificial Immune Systems.- The Immune System in Context with Other Biological Systems.- AIS in Context with Other Computational Intelligence Paradigms.- Case Studies.- Conclusions and Future Trends.- References.- Appendix I: Glossary of Biological Terms.- Appendix II: Pseudocode for Immune Algorithms.- Appendix III: WEB Resources on Artificial Immune Systems. Index.,2002.0,"L. Castro, J. Timmis"
9397d5ae8c7d8119887819067d70107356fe33e6,https://www.semanticscholar.org/paper/9397d5ae8c7d8119887819067d70107356fe33e6,AIDR: artificial intelligence for disaster response,"We present AIDR (Artificial Intelligence for Disaster Response), a platform designed to perform automatic classification of crisis-related microblog communications. AIDR enables humans and machines to work together to apply human intelligence to large-scale data at high speed. The objective of AIDR is to classify messages that people post during disasters into a set of user-defined categories of information (e.g., ""needs"", ""damage"", etc.) For this purpose, the system continuously ingests data from Twitter, processes it (i.e., using machine learning classification techniques) and leverages human-participation (through crowdsourcing) in real-time. AIDR has been successfully tested to classify informative vs. non-informative tweets posted during the 2013 Pakistan Earthquake. Overall, we achieved a classification quality (measured using AUC) of 80%. AIDR is available at http://aidr.qcri.org/.",2014.0,"Muhammad Imran, Carlos Castillo, J. Lucas, P. Meier, Sarah Vieweg"
23729ae655799ed2baa40ad77a2d4344453754f6,https://www.semanticscholar.org/paper/23729ae655799ed2baa40ad77a2d4344453754f6,"Artificial intelligence - a modern approach, 2nd Edition",Objects Events Sets Numbers RepresentationalObjects Intervals Places PhysicalObjects Processes Categories Sentences Measurements Moments Things,2003.0,"Stuart Russell, Peter Norvig"
a71ae541988098dbe9dabc53757286eb948cb2d7,https://www.semanticscholar.org/paper/a71ae541988098dbe9dabc53757286eb948cb2d7,The Cambridge Handbook of Artificial Intelligence,"Artificial intelligence, or AI, is a cross-disciplinary approach to understanding, modeling, and creating intelligence of various forms. It is a critical branch of cognitive science, and its influence is increasingly being felt in other areas, including the humanities. AI applications are transforming the way we interact with each other and with our environment, and work in artificially modeling intelligence is offering new insights into the human mind and revealing new forms mentality can take. This volume of original essays presents the state of the art in AI, surveying the foundations of the discipline, major theories of mental architecture, the principal areas of research, and extensions of AI such as artificial life. With a focus on theory rather than technical and applied issues, the volume will be valuable not only to people working in AI, but also to those in other disciplines wanting an authoritative and up-to-date introduction to the field.",2014.0,"Keith Frankish, W. Ramsey"
a99d857ecc78316a0d9a774972b775058d5644ca,https://www.semanticscholar.org/paper/a99d857ecc78316a0d9a774972b775058d5644ca,Continual Learning Through Synaptic Intelligence,"While deep learning has led to remarkable advances across diverse applications, it struggles in domains where the data distribution changes over the course of learning. In stark contrast, biological neural networks continually adapt to changing domains, possibly by leveraging complex molecular machinery to solve many tasks simultaneously. In this study, we introduce intelligent synapses that bring some of this biological complexity into artificial neural networks. Each synapse accumulates task relevant information over time, and exploits this information to rapidly store new memories without forgetting old ones. We evaluate our approach on continual learning of classification tasks, and show that it dramatically reduces forgetting while maintaining computational efficiency.",2017.0,"Friedemann Zenke, Ben Poole, S. Ganguli"
bf72bd429ea1ff7f47156b79947b8afd8a9dff77,https://www.semanticscholar.org/paper/bf72bd429ea1ff7f47156b79947b8afd8a9dff77,A Panorama of Artificial and Computational Intelligence in Games,"This paper attempts to give a high-level overview of the field of artificial and computational intelligence (AI/CI) in games, with particular reference to how the different core research areas within this field inform and interact with each other, both actually and potentially. We identify ten main research areas within this field: NPC behavior learning, search and planning, player modeling, games as AI benchmarks, procedural content generation, computational narrative, believable agents, AI-assisted game design, general game artificial intelligence and AI in commercial games. We view and analyze the areas from three key perspectives: 1) the dominant AI method(s) used under each area; 2) the relation of each area with respect to the end (human) user; and 3) the placement of each area within a human-computer (player-game) interaction perspective. In addition, for each of these areas we consider how it could inform or interact with each of the other areas; in those cases where we find that meaningful interaction either exists or is possible, we describe the character of that interaction and provide references to published studies, if any. We believe that this paper improves understanding of the current nature of the game AI/CI research field and the interdependences between its core areas by providing a unifying overview. We also believe that the discussion of potential interactions between research areas provides a pointer to many interesting future research projects and unexplored subfields.",2015.0,"Georgios N. Yannakakis, J. Togelius"
9e626c8016a5ae06198045fabcef09e3d00ad50b,https://www.semanticscholar.org/paper/9e626c8016a5ae06198045fabcef09e3d00ad50b,Introduction to artificial intelligence,"This book is an introduction on artificial intelligence. Topics include reasoning under uncertainty, robot plans, language understanding, and learning. The history of the field as well as intellectual ties to related disciplines are presented.",1986.0,"Eugene Charniak, D. McDermott"
b01fa6d46999789a1549c8fef1f004a2a9f6451a,https://www.semanticscholar.org/paper/b01fa6d46999789a1549c8fef1f004a2a9f6451a,Association for the Advancement of Artificial Intelligence,"The undersigned, desiring to publish the above Work in a publication of the Association for the Advancement of Artificial Intelligence (AAAI), hereby hereby grants to the Association for the Advancement of Artificial Intelligence the following nonexclusive rights: (1) The nonexclusive world rights to use the above-named Work as part of an AAAI publication, in all languages and for all editions. (2) The right to use the Work, together with the author's name(s) and the editor’s name(s) and their pertinent biographical data, in advertising and promotion of it and the AAAI publication. (3) The right to publish or cause to be published the Work in connection with any republication of the AAAI publication in any medium including electronic.",2014.0,"Matthew Crosby, Ronald P. A. Petrick"
9df7a40659dc06a1f541ad6a7b584b5a1917c821,https://www.semanticscholar.org/paper/9df7a40659dc06a1f541ad6a7b584b5a1917c821,Logical foundations of artificial intelligence,"Typographical Conventions 1 Introduction 1.1 Bibliographical and Historical Remarks Exercises 2 Declarative Knowledge 2.1 Conceptualization 2.2 Predicate Calculus 2.3 Semantics 2.4 Blocks World Example 2.5 Circuits Example 2.6 Algebraic Examples 2.7 List Examples 2.8 Natural-Language Examples 2.9 Specialized Languages 2.10 Bibliographical and Historical Remarks Exercises 3 Inference 3.1 Derivability 3.2 Inference Procedures 3.3 Logical Implication 3.4 Provability 3.5 Proving Provability 3.6 Bibliographical and Historical Remarks Exercises 4 Resolution 4.1 Clausal Form 4.2 Unification 4.3 Resolution Principle 4.4 Resolution 4.5 Unsatisfiability 4.6 True-or-False Questions 4.7 Fill-in-the-Blank Questions 4.8 Circuits Example 4.9 Mathematics Example 4.10 Soundness and Completeness 4.11 Resolution and Equality 4.12 Bibliographical and Historical Remarks Exercises 5 Resolution Strategies 5.1 Deletion Strategies 5.2 Unit Resolution 5.3 Input Resolution 5.4 Linear Resolution 5.5 Set of Support Resolution 5.6 Ordered Resolution 5.7 Directed Resolution 5.8 Sequential Constraint Satisfaction 5.9 Bibliographical and Historical Remarks Exercises 6 Nonmonotonic Reasoning 6.1 The Closed-World Assumption 6.2 Predicate Completion 6.3 Taxonomic Hierarchies and Default Reasoning 6.4 Circumscription 6.5 More General Forms of Circumscription 6.6 Default Theories 6.7 Bibliographical and Historical Remarks Exercises 7 Induction 7.1 Induction 7.2 Concept Formation 7.3 Experiment Generation 7.4 Bibliographical and Historical Remarks Exercises 8 Reasoning with Uncertain Beliefs 8.1 Probabilities of Sentences 8.2 Using Bayes' Rule in Uncertain Reasoning 8.3 Uncertain Reasoning in Expert Systems 8.4 Probabilistic Logic 8.5 Probabilistic Entailment 8.6 Computations Appropriate for Small Matrices 8.7 Dealing with Large Matrices 8.8 Probabilities Conditioned on Specific Information 8.9 Bibliographical and Historical Remarks Exercises 9 Knowledge and Belief 9.1 Preliminaries 9.2 Sentential Logics of Belief 9.3 Proof Methods 9.4 Nested Beliefs 9.5 Quantifying-In 9.6 Proof Methods for Quantified Beliefs 9.7 Knowing What Something Is 9.8 Possible-Worlds Logics 9.9 Properties of Knowledge 9.10 Properties of Belief 9.11 Group Knowledge 9.12 Equality, Quantification, and Knowledge 9.13 Bibliographical and Historical Remarks Exercises 10 Metaknowledge and Metareasoning 10.1 Metalanguage 10.2 Clausal Form 10.3 Resolution Principle 10.4 Inference Procedures 10.5 Derivability and Belief 10.6 Metalevel Reasoning 10.7 Bilevel Reasoning 10.8 Reflection 10.9 Bibliographical and Historical Remarks Exercises 11 State and Change 11.1 States 11.2 Actions 11.3 The Frame Problem 11.4 Action Ordering 11.5 Conditionality 11.6 Bibliographical and Historical Remarks Exercises 12 Planning 12.1 Initial State 12.2 Goals 12.3 Actions 12.4 Plans 12.5 Green's Method 12.6 Action Blocks 12.7 Conditional Plans 12.8 Planning Direction 12.9 Unachievability Pruning 12.10 State Alignment 12.11 Frame-Axiom Suppression 12.12 Goal Regression 12.13 State Differences 12.14 Bibliographical and Historical Remarks Exercises 13 Intelligent-Agent Architecture 13.1 Tropistic Agents 13.2 Hysteretic Agents 13.3 Knowledge-Level Agents 13.4 Stepped Knowledge-Level Agents 13.5 Fidelity 13.6 Deliberate Agents 13.7 Bibliographical and Historical Remarks Exercises Answers to Exercises A.1 Introduction A.2 Declarative Knowledge A.3 Inference A.4 Resolution A.5 Resolution Strategies A.6 Nonmonotonic Reasoning A.7 Induction A.8 Reasoning with Uncertain Beliefs A.9 Knowledge and Belief A.10 Metaknowledge and Metareasoning A.11 State and Change A.12 Planning A.13 Intelligent-Agent Architecture References Index",1987.0,"M. Genesereth, N. Nilsson"
787996496a300356188ba921f02f926331f80a63,https://www.semanticscholar.org/paper/787996496a300356188ba921f02f926331f80a63,The Ethics of Artificial Intelligence,"This chapter surveys some of the ethical challenges that may arise as one can create artificial intelligences (AI) of various kinds and degrees. Some challenges of machine ethics are much like many other challenges involved in designing machines. There is nearly universal agreement among modern AI professionals that artificial intelligence falls short of human capabilities in some critical sense, even though AI algorithms have beaten humans in many specific domains such as chess. In creating a superhuman chess player, the human programmers necessarily sacrificed their ability to predict Deep Blue's local, specific game behavior. A different set of ethical issues arises when one can contemplate the possibility that some future AI systems might be candidates for having moral status. One also has moral reasons to treat them in certain ways, and to refrain from treating them in certain other ways. Superintelligence may be achievable by increasing processing speed.",2014.0,"Nick Bostrom, Eliezer Yudkowsky"
1ee4a9254d7b02f257985d52007bb532a4a1e14f,https://www.semanticscholar.org/paper/1ee4a9254d7b02f257985d52007bb532a4a1e14f,A Review on Representative Swarm Intelligence Algorithms for Solving Optimization Problems: Applications and Trends,"Swarm intelligence algorithms are a subset of the artificial intelligence (AI) field, which is increasing popularity in resolving different optimization problems and has been widely utilized in various applications. In the past decades, numerous swarm intelligence algorithms have been developed, including ant colony optimization (ACO), particle swarm optimization (PSO), artificial fish swarm (AFS), bacterial foraging optimization (BFO), and artificial bee colony (ABC). This review tries to review the most representative swarm intelligence algorithms in chronological order by highlighting the functions and strengths from 127 research literatures. It provides an overview of the various swarm intelligence algorithms and their advanced developments, and briefly provides the description of their successful applications in optimization problems of engineering fields. Finally, opinions and perspectives on the trends and prospects in this relatively new research domain are represented to support future developments.",2021.0,"Jun Tang, Gang Liu, Qingtao Pan"
9cd71e490ea9113750a2512a0596c35a261c79dc,https://www.semanticscholar.org/paper/9cd71e490ea9113750a2512a0596c35a261c79dc,"Cryptocurrencies, smart contracts, and artificial intelligence","Recent developments in ""cryptocurrencies"" and ""smart contracts"" are creating new opportunities for applying AI techniques. These economic technologies would benefit from greater real world knowledge and reasoning as they become integrated with everyday commerce. Cryptocurrencies and smart contracts may also provide an infrastructure for ensuring that AI systems follow specified legal and safety regulations as they become more integrated into human society.",2014.0,Steve Omohundro
b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b,https://www.semanticscholar.org/paper/b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b,Steps toward Artificial Intelligence,"The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.",1995.0,M. Minsky
6ac9b47ad3301f5c3f7bcc7b6ab3bd23f9e680e8,https://www.semanticscholar.org/paper/6ac9b47ad3301f5c3f7bcc7b6ab3bd23f9e680e8,"Bridging Biological and Artificial Neural Networks with Emerging Neuromorphic Devices: Fundamentals, Progress, and Challenges","As the research on artificial intelligence booms, there is broad interest in brain‐inspired computing using novel neuromorphic devices. The potential of various emerging materials and devices for neuromorphic computing has attracted extensive research efforts, leading to a large number of publications. Going forward, in order to better emulate the brain's functions, its relevant fundamentals, working mechanisms, and resultant behaviors need to be re‐visited, better understood, and connected to electronics. A systematic overview of biological and artificial neural systems is given, along with their related critical mechanisms. Recent progress in neuromorphic devices is reviewed and, more importantly, the existing challenges are highlighted to hopefully shed light on future research directions.",2019.0,"Jianshi Tang, Fang Yuan, Xinke Shen, Zhongrui Wang, Mingyi Rao, Yuanyuan He, Yuhao Sun, Xinyi Li, Wenbin Zhang, Yijun Li, B. Gao, He Qian, Guoqiang Bi, Sen Song, J. Yang, Huaqiang Wu"
44c3c87c90d795ae9b3bc7c02024cc4390aa64f9,https://www.semanticscholar.org/paper/44c3c87c90d795ae9b3bc7c02024cc4390aa64f9,"Encyclopedia of artificial intelligence, vols. 1 and 2 (2nd ed.)","From the Publisher: 
Encompasses the variable approaches to Artificial Intelligence in 267 articles written by 205 experts. It is a comprehensive, up-to-date reference work that defines the discipline and brings together the core of knowledge from all the AI fields. Clarifies and corrects misinterpretations and provides a proper understanding of Artificial Intelligence.",1992.0,S. Shapiro
e4017121b9b49251c1c01891c1bfb15c6decc5ee,https://www.semanticscholar.org/paper/e4017121b9b49251c1c01891c1bfb15c6decc5ee,Problem-solving methods in artificial intelligence,"Feel lonely? What about reading books? Book is one of the greatest friends to accompany while in your lonely time. When you have no friends and activities somewhere and sometimes, reading book can be a great choice. This is not only for spending the time, it will increase the knowledge. Of course the b=benefits to take will relate to what kind of book that you are reading. And now, we will concern you to try reading problem solving methods in artificial intelligence as one of the reading material to finish quickly.",1971.0,N. Nilsson
bde2a76bcb7543ec2b18cd6037a05a84314d3524,https://www.semanticscholar.org/paper/bde2a76bcb7543ec2b18cd6037a05a84314d3524,Multiagent Systems: A Modern Approach to Dis- tributed Artificial Intelligence A Review,"dent on automated intelligent systems; at the same time, these systems have become more and more complicated. Society’s expectation regarding the capabilities and intelligence of such systems has also grown. We have become a more complicated society with more complicated problems. As the expectation of intelligent systems rises, we discover many more applications for AI. Additionally, as the difficulty level and computational requirements of such problems rise, there is a need to distribute the problem solving. Although the field of multiagent systems and distributed AI is relatively young, the importance and applicability of this technology for solving today’s problems continues to grow. As the title indicates, Multiagent Systems: A Modern Approach to Distributed Artificial Intelligence covers the design and development of multiagent and distributed AI systems. The purpose of this book is to provide a comprehensive overview of the field. It is an excellent collection of closely related papers that provides a wonderful introduction to multiagent systems and distributed AI. The book provides not only basic introductory information but also detailed discussions covering the important topics in the field, practical examples and applications, and a section dedicated to the relationship between multiagent systems and various other research areas. This book compiles the important concepts and methodologies required to develop a multiagent system in an understandable, and comprehensive, manner. Not only does the book focus on the known solutions and issues, it also discusses the open questions and dilemmas. The prologue begins by defining distributed AI as “the study, construc-",2001.0,J. Adams
ea58af907495e97c93997119db4a59fab5cd3683,https://www.semanticscholar.org/paper/ea58af907495e97c93997119db4a59fab5cd3683,Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier],"This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and ""weaknesses, depending on the application and context in ""which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.",2010.0,"I. Arel, Derek C. Rose, T. Karnowski"
79593b85df75e5e251f8363bbd0f7acaeae82277,https://www.semanticscholar.org/paper/79593b85df75e5e251f8363bbd0f7acaeae82277,Applications of Artiﬁcial Intelligence in Transport: An Overview,": The rapid pace of developments in Artiﬁcial Intelligence (AI) is providing unprecedented opportunities to enhance the performance of different industries and businesses, including the transport sector. The innovations introduced by AI include highly advanced computational methods that mimic the way the human brain works. The application of AI in the transport ﬁeld is aimed at overcoming the challenges of an increasing travel demand, CO 2 emissions, safety concerns, and environmental degradation. In light of the availability of a huge amount of quantitative and qualitative data and AI in this digital age, addressing these concerns in a more efﬁcient and effective fashion has become more plausible. Examples of AI methods that are ﬁnding their way to the transport ﬁeld include Artiﬁcial Neural Networks (ANN), Genetic algorithms (GA), Simulated Annealing (SA), Artiﬁcial Immune system (AIS), Ant Colony Optimiser (ACO) and Bee Colony Optimization (BCO) and Fuzzy Logic Model (FLM) The successful application of AI requires a good understanding of the relationships between AI and data on one hand, and transportation system characteristics and variables on the other hand. Moreover, it is promising for transport authorities to determine the way to use these technologies to create a rapid improvement in relieving congestion, making travel time more reliable to their customers and improve the economics and productivity of their vital assets. This paper provides an overview of the AI techniques applied worldwide to address transportation problems mainly in trafﬁc management, trafﬁc safety, public transportation, and urban mobility. The overview concludes by addressing the challenges and limitations of AI applications in transport.",,"Rusul L. Abduljabbar, H. Dia, Sohani Liyanage, S. A. Bagloee"
7fa1fbcf1298160bace770a775e66f93004f0a9b,https://www.semanticscholar.org/paper/7fa1fbcf1298160bace770a775e66f93004f0a9b,Artificial Intelligence and Big Data,"AI Innovation in Industry is a new department for IEEE Intelligent Systems, and this paper examines some of the basic concerns and uses of AI for big data (AI has been used in several different ways to facilitate capturing and structuring big data, and it has been used to analyze big data for key insights).",2013.0,Daniel E. O’Leary
9a4e5cb8df7bdcb1bf41ac0692802137a4033774,https://www.semanticscholar.org/paper/9a4e5cb8df7bdcb1bf41ac0692802137a4033774,Our Final Invention: Artificial Intelligence and the End of the Human Era,"Artificial Intelligence helps choose what books you buy, what movies you see, and even who you date. It puts the smart in your smartphone and soonit will drive your car. It makes most of the trades on Wall Street, and controls vital energy, water, and transportation infrastructure. But Artificial Intelligence can also threaten our existence. In as little as a decade, AI could match and then surpass human intelligence. Corporations and government agencies are pouring billions into achieving AIs Holy Grailhuman-level intelligence. Once AI has attained it, scientists argue, it will have survival drives much like our own. We may be forced to compete with a rival more cunning, more powerful, and more alien than we can imagine. Through profiles of tech visionaries, industry watchdogs, and groundbreaking AI systems, Our Final Invention explores the perils of the heedless pursuit of advanced AI. Until now, human intelligence has had no rival. Can we coexist with beings whose intelligence dwarfs our own? And will they allow us to?",2013.0,James Barrat
b7d35bc87f101453437a72210d62fc25caa49634,https://www.semanticscholar.org/paper/b7d35bc87f101453437a72210d62fc25caa49634,Association for the Advancement of Artificial Intelligence,"For valuable consideration received, I hereby give ________________________________________________________________________________ the absolute and irrevocable right and permission, with respect to the photographs and film or tape that he/she has taken of me on the following date(s) ________________________________________________________________________________ at the following location(s) ________________________________________________________________________________: (1) To copyright the same in his/her own name or any other name that he/she may choose. (2) To use, re-use, publish, and re-publish the same in whole or in part, individually or in conjunction with other photographs or images, in any medium and for any purpose whatsoever, including (but not by way of limitation) illustration, promotion, advertising and trade, and. (3) To use my name in connection therewith if he/she so chooses. I hereby release and discharge the Photographer, his/her heirs, executors, assigns and any designee (including any agency, client, broadcaster, periodical or other publication) from any and all claims and demands arising out of or in connection with the use of such photographs, film, or tape, including but not limited to any claims for defamation or invasion or privacy. I am of legal age and have read the foregoing and fully understand the contents thereof.",2010.0,"Matthew Crosby, Ronald P. A. Petrick"
acb2f7040e21cbe456030c8535bc3f2aafe83b02,https://www.semanticscholar.org/paper/acb2f7040e21cbe456030c8535bc3f2aafe83b02,A Universal Modular ACTOR Formalism for Artificial Intelligence,"This paper proposes a modular ACTOR architecture and definitional method for artificial intelligence that is conceptually based on a single kind of object: actors [or, if you will, virtual processors, activation frames, or streams]. The formalism makes no presuppositions about the representation of primitive data structures and control structures. Such structures can be programmed, micro-coded, or hard wired in a uniform modular fashion. In fact it is impossible to determine whether a given object is ""really"" represented as a list, a vector, a hash table, a function, or a process. The architecture will efficiently run the coming generation of PLANNER-like artificial intelligence languages including those requiring a high degree of parallelism. The efficiency is gained without loss of programming generality because it only makes certain actors more efficient; it does not change their behavioral characteristics. The architecture is general with respect to control structure and does not have or need goto, interrupt, or semaphore primitives. The formalism achieves the goals that the disallowed constructs are intended to achieve by other more structured methods. PLANNER Progress ""Programs should not only work, but they should appear to work as well."" PDP-1X Dogma The PLANNER project is continuing research in natural and effective means for embedding knowledge in procedures. In the course of this work we have succeeded in unifying the formalism around one_ fundamental concept: the ACTOR. Intuitively, an ACTOR is an active agent which plays a role on cue according to a script. We use the ACTOR metaphor to emphasize the inseparability of control and data flow in our model. Data structures, functions, semaphores, monitors, ports, descriptions, Quillian nets, logical formulae, numbers, identifiers, demons, processes, contexts, and data bases can all be shown to be special cases of actors. All of the above are objects with certain useful modes of behavior. Our formalism shows how all of the modes of behavior can be defined in terms of one kind of behavior: sending messages to actors. An actor is always invoked uniformly in exactly the same way regardless of whether it behaves as a recursive function, data structure, or process. ""It is vain to multiply Entities beyond need."" William of Occam ""Monotheism is the Answer."" The unification and simplification of the formalisms for the procedural embedding of knowledge has a great many benefits for us: FOUNDATIONS: The concept puts procedural semantics [the theory of how things operate] on a firmer basis. It will now be possible to do cleaner theoretical studies of the relation between procedural semantics and set-theoretic semantics such as model theories of the quantificational calculus and the lambda calculus. LOGICAL CALCULAE: A procedural semantics is developed for the quantificational calculus. The logical constants FOR-ALL, THERE-EXISTS, AND, OR, NOT, and IMPLIES are defined as actors. KNOWLEDGE BASED PROGRAMMING is programming in an environment which has a substantial knowledge base in the application area for which the programs are intended. The actor formalism aids knowledge based programming in the following ways: PROCEDURAL EMBEDDING of KNOWLEDGE, TRACING BEHAVIORAL DEPENDENCIES, and SUBSTANTIATING that ACTORS SATISFY their INTENTIONS. INTENTIONS: Furthermore the confirmation of properties of procedures is made easier and more uniform. Every actor has an INTENTION which checks that the prerequisites and the context of the actor being sent the message are satisfied. The intention is the CONTRACT that the actor has with the outside world. How an actor fullfills its contract is its own business. By a SIMPLE BUG we mean an actor which does not satisfy its intention. We would like to eliminate simple debugging of actors by the META-EVALUATION of actors to show that they satisfy their intentions. Suppose that there is an external audience of actors E which satisfy the intentions.of the actors to which they send messages. Intuitively, the principle of ACTOR INDUCTION states that the intentions of all actions caused by E are in turn satisfied provided that the following condition holds: If for each actor A the' intention of A is satisfied => that the intentions of all actors sent messages by A are satisfied. Computational induction [Manna], structural induction [Burstall], and Peano induction are all special cases of ACTOR induction. Actor based intentions have the following advantages: The intention is decoupled from the actors it describes. Intentions of concurrent actions are more easily disentangled. We can more elegantly write intentions The intentions are written in the same formalism as the Because for dialogues between actors. procedures they describe. Thus for example intentions can have intentions, protection is an intrinsic property of actors, we hope to be able to deal with protection issues in the same straight forward manner as more conventional intentions. Intentions of data structures are handled by the same machinery as for all other actors. COMPARATIVE SCHEMATOLOGY: The theory of comparative power of control structures is I",1973.0,"C. Hewitt, P. Bishop, R. Steiger"
7dae7d48be5fd0e477471c8d50ed19b07fef4326,https://www.semanticscholar.org/paper/7dae7d48be5fd0e477471c8d50ed19b07fef4326,Applied Artificial Intelligence: An International Journal,"Traditional approaches to managing business processes are often inadequate for large ± scale , organisation ± wide , dynamic settings . However , since Internet and Intranet technologies have become widespread , an increasing number of business processes exhibit these properties . Therefore , a new approach is needed . To this end , we describe the motivation , conceptualization , design , and implementation of a novel agent ± based business process management system . The key advance of our systemis that responsibility for enacting various components of the business process is delegated to a number of autonomous problem ± solving agents . To enact their role , these agents typically interact and negotiate with other agents in order to coordinate their actions and to buy in the services they require . This approach leads to a systemthat is signi(cid:142)cantly more agile and robust than its traditional counterparts . To help demonstrate these bene(cid:142)ts , a companion paper describes the application of our systemto a real ± world problemfaced by British Telecom .",,"N. Jennings, T. Norman, P. Faratin, P. O'Brien, B. Odgers"
9e994683ac1ff1863ff004ee606ca26cb1e28e2c,https://www.semanticscholar.org/paper/9e994683ac1ff1863ff004ee606ca26cb1e28e2c,The Handbook of Artificial Intelligence,"Engineers, mathematicians, physicists and chemists have had them for years. Finally, artificial intelligence researchers have available a handbook that is broad in scope and appropriate in depth and that answers questions about the nature of the field and its techniques. Volume 1 of the planned three-volume work covers search, knowledge representation, understanding natural language and understanding spoken language. The book is composed of over fifty short articles, written by researchers in the particular areas. Each section begins with an overview that motivates the need for investigating the topic, introduces common terminology, problems, and techniques, and provides an historical perspective. Each article is short, to the point and uncluttered by unnecessary jargon. The hierarchical organization allows the reader to easily select the appropriate level of presentation. The chapter on search begins with problem representation in the form of state-space representation, problem-reduction representation and game trees. There follow descriptions of the standard search techniques: blind search, AND/OR graphs, heuristics, minimax and alpha-beta pruning. The final section reviews a representative selection of search programs, from the Logic Theorist through ABSTRIPS. The knowledge representation chapter reviews the history of and problems inherent in machine translation. The next two sections on grammars and parsing give excellent introductions to formal grammars, trans-formational grammars, systemic grammars, case grammars and their parsing techniques. The section on Augmented Transition Networks is especially clear and succinct. The following section on text generation is sparse, which accurately reflects the state of the field. The final section first surveys early natural language processing systems before giving more detailed de-The final chapter on understanding spoken language itemizes and defines the types of knowledge required at different processing levels, and then surveys the HEARSAY, HARPY, HWlM and SRI/SDC speech understanding systems. This book has many potential users. The general reader will get a clear idea of what that mysterious handle ""artificial intelligence"" means, understand the nature of the problems faced by researchers in AI and be presented with technical details and descriptions of existing programs. The student of AI will have a clear, comprehensive text and reference work, which can serve as an introduction to deeper reading and further study. The AI researcher is provided with a reference work, which will be of immense help in preparing lectures, recalling specific techniques and refreshing one's memory about subareas of the field (or perhaps learning them for the first time). Volume 2 will include chapters on …",1982.0,Sharon C. Salveter
6df6b3d92deb82f33065fcab709ba0efc2b25f08,https://www.semanticscholar.org/paper/6df6b3d92deb82f33065fcab709ba0efc2b25f08,Artificial Intelligence in Civil Engineering,"Artificial intelligence is a branch of computer science, involved in the research, design, and application of intelligent computer. Traditional methods for modeling and optimizing complex structure systems require huge amounts of computing resources, and artificial-intelligence-based solutions can often provide valuable alternatives for efficiently solving problems in the civil engineering. This paper summarizes recently developed methods and theories in the developing direction for applications of artificial intelligence in civil engineering, including evolutionary computation, neural networks, fuzzy systems, expert system, reasoning, classification, and learning, as well as others like chaos theory, cuckoo search, firefly algorithm, knowledge-based engineering, and simulated annealing. The main research trends are also pointed out in the end. The paper provides an overview of the advances of artificial intelligence applied in civil engineering.",2012.0,"Pengzhen Lu, Shengyong Chen, Yujun Zheng"
e8dbf08e81b7db05abfc0526af0e97f1e679c66f,https://www.semanticscholar.org/paper/e8dbf08e81b7db05abfc0526af0e97f1e679c66f,The role of Artificial Intelligence in Software Engineering,"There has been a recent surge in interest in the application of Artificial Intelligence (AI) techniques to Software Engineering (SE) problems. The work is typified by recent advances in Search Based Software Engineering, but also by long established work in Probabilistic reasoning and machine learning for Software Engineering. This paper explores some of the relationships between these strands of closely related work, arguing that they have much in common and sets out some future challenges in the area of AI for SE.",2012.0,M. Harman
60e1fad779e4c2de2a8fe6a3594d30b76c85b238,https://www.semanticscholar.org/paper/60e1fad779e4c2de2a8fe6a3594d30b76c85b238,Advanced Applications of Neural Networks and Artificial Intelligence: A Review,"Artificial Neural Network is a branch of Artificial intelligence and has been accepted as a new computing technology in computer science fields. This paper reviews the field of Artificial intelligence and focusing on recent applications which uses Artificial Neural Networks (ANN""s) and Artificial Intelligence (AI). It also considers the integration of neural networks with other computing methods Such as fuzzy logic to enhance the interpretation ability of data. Artificial Neural Networks is considers as major soft-computing technology and have been extensively studied and applied during the last two decades. The most general applications where neural networks are most widely used for problem solving are in pattern recognition, data analysis, control and clustering. Artificial Neural Networks have abundant features including high processing speeds and the ability to learn the solution to a problem from a set of examples. The main aim of this paper is to explore the recent applications of Neural Networks and Artificial Intelligence and provides an overview of the field, where the AI & ANN""s are used and discusses the critical role of AI & NN played in different areas.",2012.0,"Koushal Kumar, G. Thakur"
a8edd60a3be9a6e09bd4c6c586a109e4f3f7b116,https://www.semanticscholar.org/paper/a8edd60a3be9a6e09bd4c6c586a109e4f3f7b116,Introduction to artificial neural systems,"Jacek M. Zurada received his MS and Ph.D. degrees (with distinction) in electrical engineering from the Technical University of Gdansk, Poland. Since 1989 he has been a Professor with the Electrical and Computer Engineering Department at the University of Louisville, Kentucky. He was Department Chair from 2004 to 2006. He has published over 350 journal and conference papers in the areas of neural networks, computational intelligence, data mining, image processing and VLSI circuits. INTRODUCTION TO ARTIFICIAL NEURAL SYSTEMS",1992.0,J. Zurada
f402f8bfc01d0dd3af48065603c21e47844035b6,https://www.semanticscholar.org/paper/f402f8bfc01d0dd3af48065603c21e47844035b6,Assessing Bank Efficiency and Performance with Operational Research and Artificial Intelligence Techniques: A Survey,"This paper presents a comprehensive review of 196 studies which employ operational research (O.R.) and artificial intelligence (A.I.) techniques in the assessment of bank performance. Several key issues in the literature are highlighted. The paper also points to a number of directions for future research. We first discuss numerous applications of data envelopment analysis which is the most widely applied O.R. technique in the field. Then we discuss applications of other techniques such as neural networks, support vector machines, and multicriteria decision aid that have also been used in recent years, in bank failure prediction studies and the assessment of bank creditworthiness and underperformance.",2009.0,"Meryem Duygun Fethi, Fotios Pasiouras"
17ba34a91ecceea93c2a9bd7d70f990c85e456ce,https://www.semanticscholar.org/paper/17ba34a91ecceea93c2a9bd7d70f990c85e456ce,Artificial Sensory Memory,"Sensory memory, formed at the beginning while perceiving and interacting with the environment, is considered a primary source of intelligence. Transferring such biological concepts into electronic implementation aims at achieving perceptual intelligence, which would profoundly advance a broad spectrum of applications, such as prosthetics, robotics, and cyborg systems. Here, the recent developments in the design and fabrication of artificial sensory memory devices are summarized and their applications in recognition, manipulation, and learning are highlighted. The emergence of such devices benefits from recent progress in both bioinspired sensing and neuromorphic engineering technologies and derives from abundant inspiration and benchmarks from an improved understanding of biological sensory processing. Increasing attention to this area would offer unprecedented opportunities toward new hardware architecture of artificial intelligence, which could extend the capabilities of digital systems with emotional/psychological attributes. Pending challenges are also addressed to aspects such as integration level, energy efficiency, and functionality, which would undoubtedly shed light on the future development of translational implementations.",2019.0,"Changjin Wan, Pingqiang Cai, Ming Wang, Yan Qian, Wei Huang, Xiaodong Chen"
396aa4db3e6c7a97f9962ed5834c0c0e196f77f1,https://www.semanticscholar.org/paper/396aa4db3e6c7a97f9962ed5834c0c0e196f77f1,Artificial Intelligence: Structures and Strategies for Complex Problem Solving (5th Edition),"From the Publisher: 
Combines the theoretical foundations of intelligent problem-solving with he data structures and algorithms needed for its implementation. The book presents logic, rule, object and agent-based architectures, along with example programs written in LISP and PROLOG. 
The practical applications of AI have been kept within the context of its broader goal: understanding the patterns of intelligence as it operates in this world of uncertainty, complexity and change. 
The introductory and concluding chapters take a new look at the potentials and challenges facing artificial intelligence and cognitive science. An extended treatment of knowledge-based problem-solving is given including model-based and case-based reasoning. 
Includes new material on: 
Fundamentals of search, inference and knowledge representation 
AI algorithms and data structures in LISP and PROLOG Production systems, blackboards, and meta-interpreters including planers, rule-based reasoners, and inheritance systems. 
Machine-learning including ID3 with bagging and boosting, explanation based learning, PAC learning, and other forms of induction 
Neural networks, including perceptrons, back propogation, Kohonen networks, Hopfield networks, Grossberg learning, and counterpropagation. Emergent and social methods of learning and adaptation, including genetic algorithms, genetic programming and artificial life. 
Object and agent-based problem solving and other forms of advanced knowledge representation",2004.0,G. Luger
bc1543de6ee2b223f5a37c7822c5d46c949aa274,https://www.semanticscholar.org/paper/bc1543de6ee2b223f5a37c7822c5d46c949aa274,Artificial Intelligence: The Basics,"'if AI is outside your field, or you know something of the subject and would like to know more then Artificial Intelligence: The Basics is a brilliant primer.' - Nick Smith, Engineering and Technology Magazine November 2011 Artificial Intelligence: The Basics is a concise and cutting-edge introduction to the fast moving world of AI. The author Kevin Warwick, a pioneer in the field, examines issues of what it means to be man or machine and looks at advances in robotics which have blurred the boundaries. Topics covered include: how intelligence can be defined whether machines can 'think' sensory input in machine systems the nature of consciousness the controversial culturing of human neurons. Exploring issues at the heart of the subject, this book is suitable for anyone interested in AI, and provides an illuminating and accessible introduction to this fascinating subject.",2011.0,K. Warwick
d03f7def354af7f46980802c103e6a3254a53490,https://www.semanticscholar.org/paper/d03f7def354af7f46980802c103e6a3254a53490,Advanced Artificial Intelligence,Logic Foundation of Artificial Intelligence Constraint Reasoning Qualitative Reasoning Case-Based Reasoning Probabilistic Reasoning Inductive Learning Support Vector Machine Explanation-Based Learning Reinforcement Learning Rough Set Association Rules Knowledge Discovery Distributed Intelligence Evolutionary Computation.,2011.0,Zhongzhi Shi
3eb8b8f1bd8fe95dfbeb33cc0cc0b47bc19914f8,https://www.semanticscholar.org/paper/3eb8b8f1bd8fe95dfbeb33cc0cc0b47bc19914f8,Artificial intelligence - the very idea,"The idea that human thinking and machine computing are ""radically the same"" provides the central theme for this marvelously lucid and witty book on what artificial intelligence is all about. Although presented entirely in nontechnical",1987.0,John Haugeland
1b1fc4b2fa9c99377062c3dee347f7d5542143b9,https://www.semanticscholar.org/paper/1b1fc4b2fa9c99377062c3dee347f7d5542143b9,Artificial Intelligence for Artificial Artificial Intelligence,"
 
 Crowdsourcing platforms such as Amazon Mechanical Turk have become popular for a wide variety of human intelligence tasks; however, quality control continues to be a significant challenge. Recently, we propose TurKontrol, a theoretical model based on POMDPs to optimize iterative, crowd-sourced workflows. However, they neither describe how to learn the model parameters, nor show its effectiveness in a real crowd-sourced setting. Learning is challenging due to the scale of the model and noisy data: there are hundreds of thousands of workers with high-variance abilities. This paper presents an end-to-end system that first learns TurKontrol's POMDP parameters from real Mechanical Turk data, and then applies the model to dynamically optimize live tasks. We validate the model and use it to control a successive-improvement process on Mechanical Turk. By modeling worker accuracy and voting patterns, our system produces significantly superior artifacts compared to those generated through nonadaptive workflows using the same amount of money.
 
",2011.0,"P. Dai, Mausam, Daniel S. Weld"
3399a0f756020e7e51ceb6422db1a956ca4c42f0,https://www.semanticscholar.org/paper/3399a0f756020e7e51ceb6422db1a956ca4c42f0,This Is a Publication of The American Association for Artificial Intelligence,"The material herein is copyrighted material. It may not be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from AAAI.",1996.0,"Eiichi Osawa, H. Matsubara"
49236453980990bf02b8c1fe5893b833e40381f3,https://www.semanticscholar.org/paper/49236453980990bf02b8c1fe5893b833e40381f3,Artificial Intelligence - Foundations of Computational Agents,"Artificial Intelligence: Foundations of Computational Agents is about the science of artificial intelligence (AI). It presents AI as the study of the design of intelligent computational agents. The book is structured as a textbook, but it is accessible to a wide audience of professionals and researchers. The past decades have witnessed the emergence of AI as a serious science and engineering discipline. This book provides the first accessible synthesis of the field aimed at undergraduate and graduate students. It provides a coherent vision of the foundations of the field as it is today, in terms of a multidimensional design space that has been partially explored. As with any science worth its salt, AI has a coherent, formal theory and a rambunctious experimental wing. The book balances theory and experiment, showing how to link them intimately together. It develops the science of AI together with its engineering applications.",2010.0,"D. Poole, Alan K. Mackworth"
11906dcc137eb33b0ccf480462e37c7644a49e39,https://www.semanticscholar.org/paper/11906dcc137eb33b0ccf480462e37c7644a49e39,Artificial Intelligence and Tutoring Systems: Computational and Cognitive Approaches to the Communication of Knowledge,"From Marie Bienkowski's review : ""Upon first encountering Etienne Wenger's book, ""Artificial Intelligence and Tutoring Systems: Computational and Cognitive Approaches to the Communication of Knowledge,"" I assumed that it was the sort of book that would serve only as a reference. To find out about the EXCHECK tutoring system, I would just look it up in the exhaustive subject index. To see what tutoring work Beverly Woolf had done, I would look up her name in the author index. But, upon further examination, Wenger's book was revealed to be a good introductory text, first laying out basic issues in Chapters 1 and 2, then covering systems from SCHOLAR and SOPHIE to GUIDON and ACTP in Chapters 3 through 13. Each review chapter ends with a summary and an excellent set of bibliographic notes, further reinforcing the utility of the book as a reference and introduction to the field. In addition, it became clear that the book had another facet. As John Seely Brown and James Greeno point out in the introduction, ""... this book is no mere catalog of programs and techniques... he has also laid out a provocative framework for analyzing and comparing intelligent tutoring systems."" This framework is presented in Chapters 14 through 20. Thus, Wenger's book serves three purposes: it acts as a reference, it provides a comprehensive introduction to intelligent tutoring systems, and it develops a coherent framework for thinking about issues in any intelligent system that must communicate its knowledge."" (http://dl.acm.org/citation.cfm?id=1059726)",1987.0,E. Wenger
a41b77dff7853aee1f325f4b6fcbbb938b51825e,https://www.semanticscholar.org/paper/a41b77dff7853aee1f325f4b6fcbbb938b51825e,Artificial intelligence in supply chain management: theory and applications,"Artificial intelligence (AI) was introduced to develop and create “thinking machines” that are capable of mimicking, learning, and replacing human intelligence. Since the late 1970s, AI has shown great promise in improving human decision-making processes and the subsequent productivity in various business endeavors due to its ability to recognise business patterns, learn business phenomena, seek information, and analyse data intelligently. Despite its widespread acceptance as a decision-aid tool, AI has seen limited application in supply chain management (SCM). To fully exploit the potential benefits of AI for SCM, this paper explores various sub-fields of AI that are most suitable for solving practical problems relevant to SCM. In so doing, this paper reviews the past record of success in AI applications to SCM and identifies the most fruitful areas of SCM in which to apply AI.",2010.0,Hokey Min
f1eae5d103f7ea905bcf74377b17773ef845d364,https://www.semanticscholar.org/paper/f1eae5d103f7ea905bcf74377b17773ef845d364,"Bayesian Artificial Intelligence, Second Edition","Updated and expanded, Bayesian Artificial Intelligence, Second Edition provides a practical and accessible introduction to the main concepts, foundation, and applications of Bayesian networks. It focuses on both the causal discovery of networks and Bayesian inference procedures. Adopting a causal interpretation of Bayesian networks, the authors discuss the use of Bayesian networks for causal modeling. They also draw on their own applied research to illustrate various applications of the technology. New to the Second Edition New chapter on Bayesian network classifiers New section on object-oriented Bayesian networks New section that addresses foundational problems with causal discovery and Markov blanket discovery New section that covers methods of evaluating causal discovery programs Discussions of many common modeling errors New applications and case studies More coverage on the uses of causal interventions to understand and reason with causal Bayesian networks Illustrated with real case studies, the second edition of this bestseller continues to cover the groundwork of Bayesian networks. It presents the elements of Bayesian network technology, automated causal discovery, and learning probabilities from data and shows how to employ these technologies to develop probabilistic expert systems. Web ResourceThe books website at www.csse.monash.edu.au/bai/book/book.html offers a variety of supplemental materials, including example Bayesian networks and data sets. Instructors can email the authors for sample solutions to many of the problems in the text.",2010.0,"K. Korb, A. Nicholson"
cd18415b5168ac1c564626b87f81ee12b2213b2f,https://www.semanticscholar.org/paper/cd18415b5168ac1c564626b87f81ee12b2213b2f,Artificial Intelligence: Foundations of Computational Agents,"Recent decades have witnessed the emergence of artificial intelligence as a serious science and engineering discipline. Artificial Intelligence: Foundations of Computational Agents is a textbook aimed at junior to senior undergraduate students and first-year graduate students. It presents artificial intelligence (AI) using a coherent framework to study the design of intelligent computational agents. By showing how basic approaches fit into a multidimensional design space, readers can learn the fundamentals without losing sight of the bigger picture. The book balances theory and experiment, showing how to link them intimately together, and develops the science of AI together with its engineering applications. Although structured as a textbook, the book's straightforward, self-contained style will also appeal to a wide audience of professionals, researchers, and independent learners. AI is a rapidly developing field: this book encapsulates the latest results without being exhaustive and encyclopedic. It teaches the main principles and tools that will allow readers to explore and learn on their own. The text is supported by an online learning environment, AIspace, http://aispace.org, so that students can experiment with the main AI algorithms plus problems, animations, lecture slides, and a knowledge representation system, AIlog, for experimentation and problem solving.",2010.0,"L. David, K. Alan"
02deb9f6e877fdd87caf9409d01367c8cf9148fe,https://www.semanticscholar.org/paper/02deb9f6e877fdd87caf9409d01367c8cf9148fe,Artificial Intelligence with Uncertainty,"Uncertainty exists widely in the subjective and objective world. In all kinds of uncertainty, randomness and fuzziness are the most important and fundamental. In this paper, the relationship between randomness and fuzziness is discussed. Uncertain states and their changes can be measured by entropy and hyper-entropy respectively. Taken advantage of entropy and hyper-entropy, the uncertainty of chaos, fractal and complex networks by their various evolution and differentiation are further studied. A simple and effective way is proposed to simulate the uncertainty by means of knowledge representation which provides a basis for the automation of both logic and image thinking with uncertainty. The AI (artificial intelligence) with uncertainty is a new cross-discipline, which covers computer science, physics, mathematics, brain science, psychology, cognitive science, biology and philosophy, and results in the automation of representation, process and thinking for uncertain information and knowledge.",2004.0,Deyi Li
6d199c688c0ee9ea486dc07f9acfbed6f85fa073,https://www.semanticscholar.org/paper/6d199c688c0ee9ea486dc07f9acfbed6f85fa073,A Survey of Artificial Intelligence for Cognitive Radios,"Cognitive radio (CR) is an enabling technology for numerous new capabilities such as dynamic spectrum access, spectrum markets, and self-organizing networks. To realize this diverse set of applications, CR researchers leverage a variety of artificial intelligence (AI) techniques. To help researchers better understand the practical implications of AI to their CR designs, this paper reviews several CR implementations that used the following AI techniques: artificial neural networks (ANNs), metaheuristic algorithms, hidden Markov models (HMMs), rule-based systems, ontology-based systems (OBSs), and case-based systems (CBSs). Factors that influence the choice of AI techniques, such as responsiveness, complexity, security, robustness, and stability, are discussed. To provide readers with a more concrete understanding, these factors are illustrated in an extended discussion of two CR designs.",2010.0,"An He, K. Bae, T. Newman, J. Gaeddert, Kyouwoong Kim, R. Menon, Lizdabel Morales-Tirado, James J. Neel, Youping Zhao, J. Reed, W. Tranter"
aff2b8501f4836555f9be72dc2851f83ec859c00,https://www.semanticscholar.org/paper/aff2b8501f4836555f9be72dc2851f83ec859c00,Temporal Logic: From Ancient Ideas to Artificial Intelligence,"Temporal Logic: From Ancient Ideas to Artificial Intelligence deals with the history of temporal logic as well as the crucial systematic questions within the field. The book studies the rich contributions from ancient and medieval philosophy up to the downfall of temporal logic in the Renaissance. The modern rediscovery of the subject, which is especially due to the work of A. N. Prior, is described, leading into a thorough discussion of the use of temporal logic in computer science and the understanding of natural language. Temporal Logic: From Ancient Ideas to Artificial Intelligence thus interweaves linguistic, philosophical and computational aspects into an informative and inspiring whole.",2010.0,P. Øhrstrøm
3dc3ca55926bbeafff6d88a3251a41bffdb6747a,https://www.semanticscholar.org/paper/3dc3ca55926bbeafff6d88a3251a41bffdb6747a,"Quo Vadis, Artificial Intelligence?","Since its conception in the mid 1950s, artificial intelligence with its great ambition to understand and emulate intelligence in natural and artificial environments alike is now a truly multidisciplinary field that reaches out and is inspired by a great diversity of other fields. Rapid advances in research and technology in various fields have created environments into which artificial intelligence could embed itself naturally and comfortably. Neuroscience with its desire to understand nervous systems of biological organisms and systems biology with its longing to comprehend, holistically, the multitude of complex interactions in biological systems are two such fields. They target ideals artificial intelligence has dreamt about for a long time including the computer simulation of an entire biological brain or the creation of new life forms from manipulations of cellular and genetic information in the laboratory. The scope for artificial intelligence in neuroscience and systems biology is extremely wide. This article investigates the standing of artificial intelligence in relation to neuroscience and systems biology and provides an outlook at new and exciting challenges for artificial intelligence in these fields. These challenges include, but are not necessarily limited to, the ability to learn from other projects and to be inventive, to understand the potential and exploit novel computing paradigms and environments, to specify and adhere to stringent standards and robust statistical frameworks, to be integrative, and to embrace openness principles.",2010.0,"D. Berrar, N. Sato, A. Schuster"
d3dc3e5e5e6b011347a2774cf8d159391de04256,https://www.semanticscholar.org/paper/d3dc3e5e5e6b011347a2774cf8d159391de04256,Intelligence Without Reason,Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.,1991.0,R. Brooks
1ba4473670dc8f72b1fbceb6b8d48005a35bae41,https://www.semanticscholar.org/paper/1ba4473670dc8f72b1fbceb6b8d48005a35bae41,Uncertainty in Artificial Intelligence 5,"Qualitative Probabilistic Reasoning and Cognitive Models. Exploiting Functional Dependencies in Qualitative Probabilistic Reasoning (M.P. Wellman). Qualitative Propagation and Scenario-Based Scheme for Explaining Probabilistic Reasoning (M. Henrion, M.J. Druzdel). Propagating Uncertainty in Rule Based Cognitive Modeling (T.R. Shultz). Context-Dependent Similarity (Y. Cheng). Abductive Probabilistic Reasoning and KB Development. Similarity Networks for the Construction of Multiple-Faults Belief Networks (D. Heckerman). Separable and Transitive Graphoids (D. Geiger, D. Heckerman). Integrating Probabilistic, Taxonomic and Causal Knowledge in Abductive Diagnosis (D. Lin, R. Goebel). What is the Most Likely Diagnosis (D. Poole, G.M. Provan). Probabilistic Evaluation of Candidate Sets for Multidisorder Diagnosis (T.D. Wu). Kutato: An Entropy-Driven System for Construction of Probabilistic Expert Systems from Databases (E. Herskovits, G. Cooper). Problem Formulation and Control of Reasoning. Ideal Reformulation of Belief Networks (J.S. Breese, E.J. Horvitz). Computationally-Optimal Real-Resource Strategies for Independent, Uninterruptible Methods (D. Einav, M.R. Fehling). Problem Formulation as the Reduction of a Decision Model (D.E. Heckerman, E.J. Horvitz). Dynamic Construction of Belief Networks (R.P. Goldman, E. Charniak). A New Algorithm for Finding MAP Assignments to Belief Networks (S.E. Shimony, E. Charniak). Belief Network Decomposition. Directed Reduction Algorithms and Decomposable Graphs (R.D. Shachter, S.K. Andersen, K.L. Poh). Optimal Decomposition of Belief Networks (W.X. Wen). Pruning Bayesian Networks for Efficient Computation (M. Baker, T.E. Boult). On Heuristics for Finding Loop Cutsets in Multiply-Connected Belief Networks (J. Stillman). A Combination of Cutset Conditioning with Clique-Tree Propagation in the Pathfinder System (H.J. Suermondt, G.F. Cooper, D.E. Heckerman). Equivalence and Synthesis of Causal Models (T.S. Verma, J. Pearl). Possibility Theory: Semantics and Applications. Possibility as Similarity: The Semantics of Fuzzy Logic (E. Ruspini). Integrating Case-Based and Rule-Based Reasoning: the Possibilistic Connection (S. Dutta, P.P. Bonissone). Credibility Discounting in the Theory of Approximate Reasoning (R.R. Yager). Updating with Belief Functions, Ordinal Conditional Functions and Possibility Measures (D. Dubois, H. Prade). A Hierarchical Approach to Designing Approximate Reasoning-Based Controllers for Dynamic Physical Systems (H.R. Berenji, et al.). Dempster-Shafer: Graph Decomposition, FMT, and Interpretations. A New Approach to Updating Beliefs (R. Fagin, J.Y. Halpern). The Transferable Belief Model and Other Interpretations of Dempster-Shafer's Model (P. Smets). Valuation-Based Systems for Discrete Optimization (P.P. Shenoy). Computational Aspects of the Mobius Transformation (R. Kennes, P. Smets). Using Dempster-Shafer Theory in Knowledge Representation (A. Saffiotti).",1990.0,"L. Kanal, J. Lemmer, A. Sage"
bdaf90fca684923edbaa15af0530f63b6974e345,https://www.semanticscholar.org/paper/bdaf90fca684923edbaa15af0530f63b6974e345,Bayesian Artificial Intelligence,"The book comprises eight chapters of varying length. The inclusion of sections at the end of the main chapters to collect more technical arguments and to give bibliographic notes works well and helps the readers explore in more depth aspects of RSS in which they are interested. Chapter 1 introduces the notion and general procedure of RSS. This very useful chapter will enable readers to quickly enter into the realm of RSS, learn about its historical developments, and identify applications of particular interest. Chapters 2 and 3 discuss balanced RSS. In particular, Chapter 2 focuses on nonparametric RSS, in which no assumption on the underlying distribution of the variable of interest is made. This chapter studies in detail the relative efficiency of RSS with respect to SRS in the estimation of a population mean, a smooth function of means, and population quantiles. The authors also consider the inference procedures, such as the construction of confidence intervals and hypothesis testing. To facilitate the inference procedures based on RSS sample quantiles, they also discuss the kernel method of density estimation. This section is quite interesting. The chapter also presents some robust procedure based on M-estimates with RSS data. Chapter 3 addresses parametric RSS, where the underlying distribution of the variable of interest is assumed to belong to some parametric family (e.g., location-scale family and shape-scale family) of distributions. The authors nicely lay out the theoretical foundation for the parametric RSS via Fisher information. The maximum likelihood estimate (MLE) based on RSS and its relative efficiency with respect to MLE based on SRS are studied, and the best linear unbiased estimate for location family of distributions is dealt with. Chapter 4 studies unbalanced RSS. This chapter first develops the methodology of analyzing RSS data for the inferences on distribution functions and quantiles, as well as general statistical functionals. The optimal designs for the parametric location-scale family and for nonparametric estimation of quantiles are discussed in detail. This chapter also contains methods of Bayes design and adaptive design. Chapter 5 explores classical distribution-free tests in the context of RSS. The authors consider the sign test, signed rank test, and Mann–Whitney– Wilcoxon tests and revisit the issue of the optimal design for distribution-free tests. Readers with a prior knowledge of nonparametric tests at the level of Gibbons and Chakraborti (2003) will find this chapter informative and easy to understand. For readers not familiar with these standard topics, some brief additional explanation and references might have been beneficial for the wider accessibility. Chapter 6 describes RSS with concomitant variables. A multilayer RSS scheme and an adaptive RSS scheme using multiple concomitant variables are developed; the general regression analysis using RSS is discussed; and the design of optimal RSS schemes for regression analysis, on the basis of the concomitant variables, is explored. Chapter 7 illustrates RSS as a data reduction tool for data mining, whereas Chapter 8 exemplifies the practical features of RSS via case studies. The inclusion of this last chapter on case studies with RSS further enhances the value of this monograph for practitioners and applied statisticians. In the development of RSS, the choices of ranked set size k and cycle number m are directly pertinent to practical problems. This book would have been more useful had some more detailed discussion on the choices been added. However, overall I would highly recommend this well-written and reasonably priced book to researchers and practitioners, all of whom are likely to use one or more of the methods it discusses.",2005.0,D. Zelterman
015146bc9d98e5465b91d8b0d99df6bd1d73df72,https://www.semanticscholar.org/paper/015146bc9d98e5465b91d8b0d99df6bd1d73df72,Handbook of logic in artificial intelligence and logic programming (vol. 1),"Introducing a new hobby for other people may inspire them to join with you. Reading, as one of mutual hobby, is considered as the very easy hobby to do. But, many people are not interested in this hobby. Why? Boring is the reason of why. However, this feel actually can deal with the book and time of you reading. Yeah, one that we will refer to break the boredom in reading is choosing handbook of logic in artificial intelligence and logic programming vol 3 nonmonotonic reasoning a as the reading material.",1993.0,"D. Gabbay, C. Hogger, J. Robinson"
c866b5d3059cf63cd4f27e0b7b0e9c2096394c54,https://www.semanticscholar.org/paper/c866b5d3059cf63cd4f27e0b7b0e9c2096394c54,"Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies","Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies by D. Floreano and C. Mattiussi This is a book that bridges biological systems and computer science. For digital-based researchers, having this book which details the biological components of natural life and seamlessly integrates that knowledge into our digital realm is an essential asset. Each chapter is systematically introduces the reader to a biological system while easing them into the its computational counterpart. There are seven chapters covering evolution, cellular, neural, developmental, immune, behavioral, and collective systems. Chapter 1 introduces the fundamental concept of computational evolution as related to biological systems. This chapter starts with the basic concepts of evolutionary theory and progresses, covering everything from fitness functions to analog circuits. The following chapter presents the next logical step upwards in biology, cellular structures and systems. Again introducing the basics of life and progressing towards cellular automata.  Chapter 3 covers Neural Networks by introducing the Biological Nervous System, then the Artificial Neural Network. The core concepts to Neural Networks are detailed in a systematic and common-sense manner, introducing unsupervised learning, supervised learning, and reinforce learning, then progressing onto neural hardware and hybrid systems. In Chapter 4, the authors detail developmental systems, explaining how nature utilizes the cellular structures to how engineers can mimic nature. This theme of progression from biological introduction to digital computation is reproduced as a single voice through out each chapter. The fundamentals of Bio-Inspired Artificial Intelligence are well demonstrated, allowing for a novice researcher in this area to develop the necessary skills and have a firm grasp on this topic. Once the reader has a solid grasp of the building blocks of life, the authors present chapters related to larger systems. Of particular interest to my research is the chapter on Immune Systems. This chapter provides a fundamental understanding of the Human Immune System, detailing the finer points of immunological cellular structures, while introducing a slightly more than generalized immune response concept. After a lengthy introduction of human immunology, we are introduced to the core of Artificial Immune Systems, the Negative Selection Algorithm and Clonal Selection Algorithm. Each one of these algorithms is covered enough so that the reader is capable of understanding each respective algorithms strengths and limitations. For new researchers to Artificial Immune Systems, days of reading journal articles is summarized in these sections, allowing for intelligent and efficient decision making in choosing your next step of research. Chapter 6 and 7 provides the audience with behavior systems and collective systems, respectively. The behavioral systems covered in this book relate to aspects of AI, robots, and some machine learning. Once behavior is understood, collective and cooperative systems are covered. Optimization techniques of particle swarms, ant colonies, and topics derived for robotics are detailed and well explained. While this is not a textbook, is does cover the fundamental concepts required to research Bio-Inspired Artificial Intelligence. For myself, the quality of this book can simply be noted by the publishers, MIT Press. Many of the best books I have encountered in my studies have been published by MIT, and here is another. Floreano and Mattiussi have not let me down in their quality, albeit I do have some complaints. First, while the topics cover a solid breadth, the depth on detailing the computation side is limited. I would like to have seen either more depth in each chapter or a broader look at each chapters algorithms, but the book falls somewhere in the middle. My current research involves Danger Signals and their relationship to preventing Epidemic Attacks, so I would have like to seen more detail about Polly Matzinger's Danger Theory rather than one short paragraph saying that it is not universally accepted. While Immunologists may debate Danger Theory, novel algorithms have been developed off of the concept of Danger Theory and deserve a place in this book. Yet to counter my own argument, the authors do finish off each chapter with a Suggested Readings section outlining a series of excellent supplement papers to the chapters topics that would eventually lead the reader to these novel topics. Overall, if you are interested in this field, buy this book. You can find it online at MIT Press for a discounted price. This book will make an excellent addition to any computer researchers library. Anthony Kulis, Department of Computer Science, Southern Illinois University",2009.0,Anthony Kulis
bd40b693d085a1470b24621ed2ff13a98fb1b391,https://www.semanticscholar.org/paper/bd40b693d085a1470b24621ed2ff13a98fb1b391,Ambient Intelligence—the Next Step for Artificial Intelligence,"Ambient intelligence (AmI) deals with a new world of ubiquitous computing devices, where physical environments interact intelligently and unobtrusively with people. These environments should be aware of people's needs, customizing requirements and forecasting behaviors. AmI environments can be diverse, such as homes, offices, meeting rooms, schools, hospitals, control centers, vehicles, tourist attractions, stores, sports facilities, and music devices. Artificial intelligence research aims to include more intelligence in AmI environments, allowing better support for humans and access to the essential knowledge for making better decisions when interacting with these environments. This article, which introduces a special issue on AmI, views the area from an artificial intelligence perspective.",2008.0,"Carlos Ramos, J. Augusto, D. Shapiro"
e39c4e4da401aa98b6d07254a5bae6e2ab849672,https://www.semanticscholar.org/paper/e39c4e4da401aa98b6d07254a5bae6e2ab849672,The Quest for Artificial Intelligence,"Artificial intelligence (AI) is a field within computer science that is attempting to build enhanced intelligence into computer systems. This book traces the history of the subject, from the early dreams of eighteenth-century (and earlier) pioneers to the more successful work of today's AI engineers. AI is becoming more and more a part of everyone's life. The technology is already embedded in face-recognizing cameras, speech-recognition software, Internet search engines, and health-care robots, among other applications. The book's many diagrams and easy-to-understand descriptions of AI programs will help the casual reader gain an understanding of how these and other AI systems actually work. Its thorough (but unobtrusive) end-of-chapter notes containing citations to important source materials will be of great use to AI scholars and researchers. This book promises to be the definitive history of a field that has captivated the imaginations of scientists, philosophers, and writers for centuries.",2009.0,N. Nilsson
0150254af6d36b4e8eef503da2b6c25d16870ffb,https://www.semanticscholar.org/paper/0150254af6d36b4e8eef503da2b6c25d16870ffb,Bayesian Artificial Intelligence,Bayesian Reasoning. Introduction to Bayesian Networks. Inference in Bayesian Networks. Bayesian Network Applications. Bayesian Planning and Decision-Making. Bayesian Network Applications II. Learning Bayesian Networks I. Learning Bayesian Networks II. Causality vs. Probability. Knowledge Engineering with Bayesian Networks I. Knowledge Engineering with Bayesian Networks II. Application Software.,2004.0,"K. Korb, A. Nicholson"
0d4ebebc4722195179655798b2ac9735da7830c5,https://www.semanticscholar.org/paper/0d4ebebc4722195179655798b2ac9735da7830c5,Markov Decision Processes In Artificial Intelligence,"markov decision processes in artificial intelligence is available in our book collection an online access to it is set as public so you can download it instantly. Our book servers spans in multiple countries, allowing you to get the most less latency time to download any of our books like this one. Merely said, the markov decision processes in artificial intelligence is universally compatible with any devices to read",2009.0,
c1d4d721960cdee12389faa2fdc9ccb31f149269,https://www.semanticscholar.org/paper/c1d4d721960cdee12389faa2fdc9ccb31f149269,Systematic review of dermoscopy and digital dermoscopy/ artificial intelligence for the diagnosis of melanoma,"Background  Dermoscopy improves diagnostic accuracy of the unaided eye for melanoma, and digital dermoscopy with artificial intelligence or computer diagnosis has also been shown useful for the diagnosis of melanoma. At present there is no clear evidence regarding the diagnostic accuracy of dermoscopy compared with artificial intelligence.",2009.0,"S. Rajpara, A. Botello, J. Townend, A. Ormerod, A. Ormerod"
8b2e9d7176144bcb9a0f1e3cf09fabf675085ed4,https://www.semanticscholar.org/paper/8b2e9d7176144bcb9a0f1e3cf09fabf675085ed4,"The Emotion Machine: Commonsense Thinking, Artificial Intelligence, and the Future of the Human Mind","In this mind-expanding book, scientific pioneer Marvin Minsky continues his groundbreaking research, offering a fascinating new model for how our minds work. He argues persuasively that emotions, intuitions, and feelings are not distinct things, but different ways of thinking.By examining these different forms of mind activity, Minsky says, we can explain why our thought sometimes takes the form of carefully reasoned analysis and at other times turns to emotion. He shows how our minds progress from simple, instinctive kinds of thought to more complex forms, such as consciousness or self-awareness. And he argues that because we tend to see our thinking as fragmented, we fail to appreciate what powerful thinkers we really are. Indeed, says Minsky, if thinking can be understood as the step-by-step process that it is, then we can build machines -- artificial intelligences -- that not only can assist with our thinking by thinking as we do but have the potential to be as conscious as we are.Eloquently written, The Emotion Machine is an intriguing look into a future where more powerful artificial intelligences await.",2006.0,M. Minsky
7d01eeb73d02c8161e6c840dc3d3702198e5ec7f,https://www.semanticscholar.org/paper/7d01eeb73d02c8161e6c840dc3d3702198e5ec7f,Empirical Methods for Artificial Intelligence,Empirical research exploratory data analysis basic issues in experiment design hypothesis testing and estimation computer-intensive statistical methods performance assessment explaining performance - interactions and dependencies modelling tactics for generalization.,1995.0,S.J.J. Smith
99e822561776388de4a8e6063df64576cfa7353d,https://www.semanticscholar.org/paper/99e822561776388de4a8e6063df64576cfa7353d,Artificial Intelligence: Artificial Intelligence and Agents,"The history of AI is a history of fantasies, possibilities, demonstrations, and promise. Ever since Homer wrote of mechanical “tripods” waiting on the gods at dinner, imagined mechanical assistants have been a part of our culture. However, only in the last half century have we, the AI community, been able to build experimental machines that test hypotheses about the mechanisms of thought and intelligent behavior and thereby demonstrate mechanisms that formerly existed only as theoretical possibilities. – Bruce Buchanan [2005] This book is about artificial intelligence, a field built on centuries of thought, which has been a recognized discipline for over 60 years. As Buchanan points out in the quote above, we now have the tools to test hypotheses about the nature of thought itself, as well as to solve practical tasks. Deep scientific and engineering problems have already been solved and many more are waiting to be solved. Many practical applications are currently deployed and the potential exists for an almost unlimited number of future applications. In this book, we present the principles that underlie intelligent computational agents. These principles can help you understand current and future work in AI and equip you to contribute to the discipline yourself. What is Artificial Intelligence? Artificial intelligence , or AI , is the field that studies the synthesis and analysis of computational agents that act intelligently . Let us examine each part of this definition. An agent is something that acts in an environment; it does something. Agents include worms, dogs, thermostats, airplanes, robots, humans, companies, and countries. We are interested in what an agent does; that is, how it acts . We judge an agent by its actions. An agent acts intelligently when • what it does is appropriate for its circumstances and its goals, taking into account the short-term and long-term consequences of its actions • it is flexible to changing environments and changing goals • it learns from experience • it makes appropriate choices given its perceptual and computational limitations A computational agent is an agent whose decisions about its actions can be explained in terms of computation. That is, the decision can be broken down into primitive operations that can be implemented in a physical device.",2010.0,
3032dadcb4ccd6fcd2e2881579df0ee2ab6c71ae,https://www.semanticscholar.org/paper/3032dadcb4ccd6fcd2e2881579df0ee2ab6c71ae,Handbook of Constraint Programming (Foundations of Artificial Intelligence),"Many people are trying to be smarter every day. How's about you? There are many ways to evoke this case you can find knowledge and lesson everywhere you want. However, it will involve you to get what call as the preferred thing. When you need this kind of sources, the following book can be a great choice. handbook of constraint programming foundations of artificial intelligence is the PDF of the book.",2006.0,"F. Rossi, P. V. Beek, T. Walsh"
c54705f18edf2dbf89c174e24d7f0c67d9d97060,https://www.semanticscholar.org/paper/c54705f18edf2dbf89c174e24d7f0c67d9d97060,The Quest For Artificial Intelligence: A History Of Ideas And Achievements,"In the second full paragraph of page 21, change George A. Miller's dates from "" (1920—) "" to "" (1920—2012) "" In the last full paragraph of page 33, replace "" The claim that these "" in the sentence "" The claim that these two ... "" by "" That these "" and eliminate the parenthetical sentence following that sentence, ("" The claim has not. .. ""). Move footnote #51 to occur along with footnote #50.",2009.0,N. Nilsson
318dbfe5b4f358601215eb25098257f6e002e8f7,https://www.semanticscholar.org/paper/318dbfe5b4f358601215eb25098257f6e002e8f7,"Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies","New approaches to artificial intelligence spring from the idea that intelligence emerges as much from cells, bodies, and societies as it does from evolution, development, and learning. Traditionally, artificial intelligence has been concerned with reproducing the abilities of human brains; newer approaches take inspiration from a wider range of biological structures that that are capable of autonomous self-organization. Examples of these new approaches include evolutionary computation and evolutionary electronics, artificial neural networks, immune systems, biorobotics, and swarm intelligenceto mention only a few. This book offers a comprehensive introduction to the emerging field of biologically inspired artificial intelligence that can be used as an upper-level text or as a reference for researchers. Each chapter presents computational approaches inspired by a different biological system; each begins with background information about the biological system and then proceeds to develop computational models that make use of biological concepts. The chapters cover evolutionary computation and electronics; cellular systems; neural systems, including neuromorphic engineering; developmental systems; immune systems; behavioral systemsincluding several approaches to robotics, including behavior-based, bio-mimetic, epigenetic, and evolutionary robots; and collective systems, including swarm robotics as well as cooperative and competitive co-evolving systems. Chapters end with a concluding overview and suggested reading.",2008.0,"D. Floreano, C. Mattiussi"
f9441005143eac86fd045b194274cf8ea6b8169d,https://www.semanticscholar.org/paper/f9441005143eac86fd045b194274cf8ea6b8169d,AI in CAI : An artificial intelligence approach to computer-assisted instruction,"The main purpose of the research reported here is to show that a new and more powerful type of computer-assisted instruction (CAI), based on extensive application of artificial-intelligence (AI) techniques, is feasible, and to demonstrate some of its major capabilities. A set of computer programs was written and given the name SCHOLAR. Due to its complexity, only the conception and educational aspects of this system (including an actual on-line protocol) are presented in this paper. In what may be called conventional ad hoc-frame-oriented (AFO) CAI, the data base consists of many ""frames"" of specific pieces of text, questions, and anticipated answers entered in advance by the teacher. By contrast, an information-structure-oriented (ISO) CAI system is based on the utilization of an information network of facts, concepts, and procedures; it can generate text, questions, and corresponding answers. Because an ISO CAI system can also utilize its information network to answer questions formulated by the student, a mixed-initiative dialogue between student and computer is possible with questions and answers from both sides.",1970.0,J. R. Carbonell
ee1acded81e7020689da9083236bc1ac4300b767,https://www.semanticscholar.org/paper/ee1acded81e7020689da9083236bc1ac4300b767,Artificial Intelligence technique for modelling and forecasting of solar radiation data: a review,"Artificial Intelligence (AI) has been used and applied in different sectors, such as engineering, economic, medicine, military, marine, etc. AI has also been applied for modelling, identification, optimisation, prediction, forecasting, and control of complex systems. The main objective of this paper is to present an overview of AI techniques for modelling, prediction and forecasting of solar radiation data. Published literature works presented in this paper show the potential of AI as a design tool for prediction and forecasting of solar radiation data; additionally, they present the advantages of using AI-based prediction solar radiation data in isolated areas where there no instrument for the measurement of this data, especially the parameters related to photovoltaic (PV) systems. Solar radiation plays a very important factor in PV-system performance and sizing.",2008.0,A. Mellit
a4269ee19a49ec9cf7343a4a8a187a9caceab6a4,https://www.semanticscholar.org/paper/a4269ee19a49ec9cf7343a4a8a187a9caceab6a4,Tactical Language and Culture Training Systems: Using Artificial Intelligence to Teach Foreign Languages and Cultures,"The Tactical Language and Culture Training System (TLCTS) helps people quickly acquire communicative skills in foreign languages and cultures. More than 20,000 learners worldwide have used TLCTS courses. TLCTS utilizes artificial intelligence technologies in multiple ways: during the authoring process, and at run time to process learner speech, interpret learner actions, control the response of non-player characters, and evaluate and assess learner performance and proficiency. This paper describes the architecture of TLCTS and the artificial intelligence technologies that it employs, and presents results from multiple evaluation studies that demonstrate the benefits of learning foreign language and culture using this approach.",2008.0,"W. Johnson, A. Valente"
dc7f535b7c6fee565cba26588deb2ce499b7887d,https://www.semanticscholar.org/paper/dc7f535b7c6fee565cba26588deb2ce499b7887d,"Possible Worlds, Artificial Intelligence, and Narrative Theory","From the Publisher: 
In this important contribution to narrative theory, Marie-Laure Ryan applies insights from artificial intelligence and the theory of possible worlds to the study of narrative and fiction. For Ryan, the theory of possible worlds provides a more nuanced way of discussing the commonplace notion of a fictional ""world,"" while artificial intelligence contributes to narratology and the theory of fiction directly via its researches into the cognitive processes of texts and automatic story generation. Although Ryan applies exotic theories to the study of narrative and fiction, her book maintains a solid basis in literary theory and makes the formal models developed by AI researchers accessible to the student of literature. The first part of the book seeks a more sophisticated application of the theory of possible worlds to the definition of fictionality. While fiction is a mode of travel into textual space, narrative is a journey within the confines of this space. The second part introduces the idea of a semantic domain consisting of a plurality of alternate possible worlds. This notion is developed into a theory of narrative conflict, which leads to an account of the forward movement of plot. By combining the philosophical back ground of possible world theory with models inspired by AI, the book fulfills a pressing need in narratology for new paradigms and an interdisciplinary perspective.",1992.0,Marie-Laure Ryan
fcf7368061a544a09d16826eb4c5a8463ee5482e,https://www.semanticscholar.org/paper/fcf7368061a544a09d16826eb4c5a8463ee5482e,Artificial Intelligence as a Positive and Negative Factor in Global Risk,"By far the greatest danger of Artificial Intelligence is that people conclude too early that they understand it. Of course this problem is not limited to the field of AI. Jacques Monod wrote: ""A curious aspect of the theory of evolution is that everybody thinks he understands it."" (Monod 1974.) My father, a physicist, complained about people making up their own theories of physics; he wanted to know why people did not make up their own theories of chemistry. (Answer: They do.) Nonetheless the problem seems to be unusually acute in Artificial Intelligence. The field of AI has a reputation for making huge promises and then failing to deliver on them. Most observers conclude that AI is hard; as indeed it is. But the embarrassment does not stem from the difficulty. It is difficult to build a star from hydrogen, but the field of stellar astronomy does not have a terrible reputation for promising to build stars and then failing. The critical inference is not that AI is hard, but that, for some reason, it is very easy for people to think they know far more about Artificial Intelligence than they actually do.",2006.0,Eliezer Yudkowsky
2603064fe9d90abcd74d01f352c352e12db0e85a,https://www.semanticscholar.org/paper/2603064fe9d90abcd74d01f352c352e12db0e85a,Foundations of distributed artificial intelligence,"Partial table of contents: FORMULATIVE READINGS. Logical Foundations of Distributed Artificial Intelligence (E. Werner). Distributed Artificial Intelligence Testbeds (K. Decker). COOPERATION, COORDINATION, AND AGENCY. Coordination Techniques for Distributed Artificial Intelligence (N. Jennings). Negotiation Principles (H. Muller). Planning in Distributed Artificial Intelligence (E. Durfee). DAI FRAMEWORKS AND THEIR APPLICATIONS. IMAGINE: An Integrated Environment for Constructing Distributed Artificial Intelligence Systems (D. Steiner). AGenDA--A General Testbed for Distributed Artificial Intelligence Applications (K. Fischer, et al.). Agent Factory: An Environment for the Fabrication of Multiagent Systems (G. O'Hare). RELATED DISCIPLINES. Philosophy and Distributed Artificial Intelligence: The Case of Joint Intention (R. Tuomela). User Design Issues for Distributed Artificial Intelligence (L. Hall). Appendix. Index.",1996.0,"Gregory M. P. O'Hare, N. Jennings"
b8bf7eef968b1071026525b8256738758ec6085e,https://www.semanticscholar.org/paper/b8bf7eef968b1071026525b8256738758ec6085e,Creativity and Artificial Intelligence,"With CD-ROM. Creativity and Artificial Intelligence: A Conceptual Blending Approach takes readers into a computationally plausible model of creativity. Inspired by a thorough analysis of work on creativity from the areas of philosophy, psychology, cognitive science, cognitive linguistics and artificial intelligence, the author deals with the various processes, principles and representations that lie underneath the act of creativity. Focusing on Arthur Koestler's Bisociations, which eventually lead to Turner and Fauconnier's conceptual blending framework, the book proposes a theoretical model that considers blends and their emergent structure as a fundamental cognitive mechanism. The author thus discusses the computational implementation of several aspects of conceptual blending theory, namely composition, completion, elaboration, frames and optimality constraints. Informal descriptions and examples are supplied to provide non-computer scientists as well as non-cognitive linguists with clear insights into these ideas. Several experiments are made, and their results are discussed, with particular emphasis on the validation of the creativity and conceptual blending aspects. Written by a researcher with a background in artificial intelligence, the book is the result of several years of exploration and discussion from different theoretical perspectives. As a result, the book echoes some of the criticism made on conceptual blending and creativity in artificial intelligence, and thus proposes improvements in both areas, with the aim of being a constructive contribution to these very intriguing, yet appealing, research orientations.",2007.0,Francisco Câmara Pereira
65ea152e899a7ae8d617c2ff89c27044bd77508e,https://www.semanticscholar.org/paper/65ea152e899a7ae8d617c2ff89c27044bd77508e,Reasoning About Change: Time and Causation from the Standpoint of Artificial Intelligence,Reasoning About Change presents a comprehensive approach to temporal reasoning in artificial intelligence.,1987.0,Y. Shoham
b199c84c7d972ab4e02127cd07a410ee9a4fa836,https://www.semanticscholar.org/paper/b199c84c7d972ab4e02127cd07a410ee9a4fa836,On artificial intelligence,"Machine learning (ML) models can memorize training datasets. As a result, training ML models over private datasets can lead to the violation of individuals’ privacy. Differential privacy (DP) is a rigorous privacy notion to preserve the privacy of underlying training datasets. Yet, training ML models in a DP framework usually degrades the accuracy of ML models. This paper aims to boost the accuracy of a DP logistic regression (LR) via a pre-training module. In more detail, we initially pre-train our LR model on a public training dataset that there is no privacy concern about it. Then, we fine-tune our DP-LR model with the private dataset. In the numerical results, we show that adding a pre-training module significantly improves the accuracy of the DP-LR model.",1985.0,P. Schönemann
1476770fbcbd589042c11bb29b1c0bcbc2631311,https://www.semanticscholar.org/paper/1476770fbcbd589042c11bb29b1c0bcbc2631311,A Survey of Artificial Intelligence for Prognostics,"Integrated Systems Health Management includes as key elements fault detection, fault diagnostics, and failure prognostics. Whereas fault detection and diagnostics have been the subject of considerable emphasis in the Artificial Intelligence (AI) community in the past, prognostics has not enjoyed the same attention. The reason for this lack of attention is in part because prognostics as a discipline has only recently been recognized as a game-changing technology that can push the boundary of systems health management. This paper provides a survey of AI techniques applied to prognostics. The paper is an update to our previously published survey of data-driven prognostics.",2007.0,"M. Schwabacher, K. Goebel"
6e43a02b0e74b71094481684767e876a9978abe7,https://www.semanticscholar.org/paper/6e43a02b0e74b71094481684767e876a9978abe7,"Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence. Complex Adaptive Systems.John H. Holland","TXmGpHLiS Tn6ep73D7 PlKgAHPTy 485IbR407 O4ttjCGHh Log7HPCOt bnKkU0244 FM5cN2sAt UkBbyg6AY 02NKcX63l IC4HLSMtn XMNDE8CAs qSgiFfJ5C tmPbYz364 whFxSWsDv 4AfGAcYEC mlOF17U7e kYFJjqjgp fTuBhpwLU 8bnsnEFXk 7GflAWnvS FS3H41Eiu zNj3IcGYH LUvZgH3x5 YjK2M7w1Q urWzZAbLs 1OREegLvB xqrC4kFqw Adaptation in Natural and Artificial Systems: An Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence (Complex Adaptive Systems) BW-33030 US/Data/Computers-Technology 4/5 From 388 Reviews John H. Holland ePub | *DOC | audiobook | ebooks | Download PDF",1994.0,Charles E. Taylor
b3d7371522a7a68137df2cb005ca9683f3436bd7,https://www.semanticscholar.org/paper/b3d7371522a7a68137df2cb005ca9683f3436bd7,Multivalued logics: a uniform approach to reasoning in artificial intelligence,"This paper describes a uniform formalization of much of the current work in artificial intelligence on inference systems. We show that many of these systems, including first‐order theorem provers, assumption‐based truth maintenance systems (atmss), and unimplemented formal systems such as default logic or circumscription, can be subsumed under a single general framework.",1988.0,M. Ginsberg
97aac3953d327a4b46ba56eddd98b2776f7a7f1a,https://www.semanticscholar.org/paper/97aac3953d327a4b46ba56eddd98b2776f7a7f1a,Artificial Intelligence: A Systems Approach,"This book offers students and AI programmers a new perspective on the study of artificial intelligence concepts. The essential topics and theory of AI are presented, but it also includes practical information on data input & reduction as well as data output (i.e., algorithm usage). Because traditional AI concepts such as pattern recognition, numerical optimization and data mining are now simply types of algorithms, a different approach is needed. This sensor / algorithm / effecter approach grounds the algorithms with an environment, helps students and AI practitioners to better understand them, and subsequently, how to apply them. The book has numerous up to date applications in game programming, intelligent agents, neural networks, artificial immune systems, and more. A CD-ROM with simulations, code, and figures accompanies the book. *Features *Covers not only AI theory, but modern applications e.g., game programming, machine learning, swarming, artificial immune systems, genetic algorithms, pattern recognition, numerical optimization, data mining, and more *Discusses the various computer languages of AI from LISP to JAVA and Python *Includes a CD-ROM with 100MB of simulations, code, and fi gures *Table of Contents 1. Introduction. 2. Search. 3. Games. 4. Logic. 5. Planning. 6. Knowledge Representation. 7. Machine Learning. 8. Probabilistic Reasoning. 9. Stochastic Search. 10. Neural Networks. 11. Intelligent Agents. 12. Hybrid Models. 13. Languages of AI.",2007.0,M. Jones
2e7a8021ab76fbc666d377896a28b8300d55081a,https://www.semanticscholar.org/paper/2e7a8021ab76fbc666d377896a28b8300d55081a,An artificial intelligence perspective on autonomic computing policies,"We introduce a unified framework that interrelates three different types of policies that will be used in autonomic computing system: action, goal, and utility function policies. Our policy framework is based on concepts from artificial intelligence such at: states, actions, and rational agents. We show how the framework can be used to support the use of all three types of policies within a single autonomic component or system, and use the framework to discuss the relative merits of each type.",2004.0,"J. Kephart, W. E. Walsh"
4033915e28e737ed3b27f69fa1378c5a851fea40,https://www.semanticscholar.org/paper/4033915e28e737ed3b27f69fa1378c5a851fea40,"Artificial-Intelligence-Based Electrical Machines and Drives: Application of Fuzzy, Neural, Fuzzy-neural, and Genetic-Algorithm-based Techniques","From the Publisher: 
Roughly half of all commercial electricity is consumed in motors, and recent efforts to apply artificial intelligence (AI) to improving electric motors are receiving attention worldwide. This book is the first comprehensive discussion of AI applications to electrical machines and drives.",1999.0,P. Vas
7baf111428e96b3f189edd1f2a357aba3d52c6cc,https://www.semanticscholar.org/paper/7baf111428e96b3f189edd1f2a357aba3d52c6cc,The Art of Artificial Intelligence: Themes and Case Studies of Knowledge Engineering,"The knowledge engineer practices the art of bringing the principles and tools of AI research to bear on difficult applications problems requiring experts' knowledge for their solution. The technical issues of acquiring this knowledge, representing it, and using it appropriately to construct and explain lines-of-reasoning, are important problems in the design of knowledge-based systems. Various systems that have achieved expert level performance in scientific and medical inference illuminate the art of knowledge engineering and its parent science, Artificial Intelligence.",1977.0,E. Feigenbaum
ee18ba4d21582eab23108ab24bf213ce9d52aa71,https://www.semanticscholar.org/paper/ee18ba4d21582eab23108ab24bf213ce9d52aa71,A (Very) Brief History of Artificial Intelligence,"In this brief history, the beginnings of artificial intelligence are traced to philosophy, fiction, and imagination. Early inventions in electronics, engineering, and many other disciplines have influenced AI. Some early milestones include work in problems solving which included basic work in learning, knowledge representation, and inference as well as demonstration programs in language understanding, translation, theorem proving, associative memory, and knowledge-based systems. The article ends with a brief examination of influential organizations and current issues facing the field.",2005.0,B. Buchanan
57df42bf288114e0b98c0b02d2ba79efa2750135,https://www.semanticscholar.org/paper/57df42bf288114e0b98c0b02d2ba79efa2750135,Artificial intelligence approaches for rational drug design and discovery.,"Pattern recognition, machine learning and artificial intelligence approaches play an increasingly important role in rational drug design, screening and identification of candidate molecules and studies on quantitative structure-activity relationships (QSAR). In this review, we present an overview of basic concepts and methodology in the fields of machine learning and artificial intelligence (AI). An emphasis is put on methods that enable an intuitive interpretation of the results and facilitate gaining an insight into the structure of the problem at hand. We also discuss representative applications of AI methods to docking, screening and QSAR studies. The growing trend to integrate computational and experimental efforts in that regard and some future developments are discussed. In addition, we comment on a broader role of machine learning and artificial intelligence approaches in biomedical research.",2007.0,"Wlodzislaw Duch, K. Swaminathan, J. Meller"
072b8794a6e1e6ed0b04a668defe8d05ffbafce4,https://www.semanticscholar.org/paper/072b8794a6e1e6ed0b04a668defe8d05ffbafce4,Safe and Sound: Artificial Intelligence in Hazardous Applications,"Computer science and artificial intelligence are increasingly used in the hazardous and uncertain realms of medical decision making, where small faults or errors can spell human catastrophe. This book describes, from both practical and theoretical perspectives, an AI technology for supporting sound clinical decision making and safe patient management. The technology combines techniques from conventional software engineering with a systematic method for building intelligent agents. Although the focus is on medicine, many of the ideas can be applied to AI systems in other hazardous settings. The book also covers a number of general AI problems, including knowledge representation and expertise modeling, reasoning and decision making under uncertainty, planning and scheduling, and the design and implementation of intelligent agents.The book, written in an informal style, begins with the medical background and motivations, technical challenges, and proposed solutions. It then turns to a wide-ranging discussion of intelligent and autonomous agents, with particular reference to safety and hazard management. The final section provides a detailed discussion of the knowledge representation and other aspects of the agent model developed in the book, along with a formal logical semantics for the language.",2000.0,"J. Fox, S. Das"
2b4b95d3ad913b858e1ac5015497ca0671661aea,https://www.semanticscholar.org/paper/2b4b95d3ad913b858e1ac5015497ca0671661aea,"Interactive drama, art and artificial intelligence","Artificial intelligence methods open up new possibilities in art and entertainment, enabling rich and deeply interactive experiences. At the same time as AI opens up new fields of artistic expression, AI-based art itself becomes a fundamental research agenda, posing and answering novel research questions that would not be raised unless doing AI research in the context of art and entertainment. I call this agenda, in which AI research and art mutually inform each other, Expressive AI. Expressive AI takes seriously the problem of building intelligences that robustly function outside of the lab, engaging human participants in intellectually and aesthetically satisfying interactions, which, hopefully, teach us something about ourselves. 
This thesis describes a specific AI-based art piece, an interactive drama called Facade, and describes the practice of Expressive AI, using Facade, as well as additional AI-based artwork described in the appendices, as case studies. 
An interactive drama is a dramatically interesting virtual world inhabited by computer-controlled characters, within which the player experiences a story from a first person perspective. Over the past decade, there has been a fair amount of research into believable agents, that is, autonomous characters exhibiting rich personalities, emotions, and social interactions. There has been comparatively little work, however, exploring how the local, reactive behavior of believable agents can be integrated with the more global, deliberative nature of a story plot, so as to build interactive, dramatic worlds. This thesis presents Facade, the first published interactive drama system that integrates character (believable agents), story (drama management) and shallow natural language processing into a complete system. Facade will be publicly released as a free download in 2003. 
In the Facade architecture, the unit of plot/character integration is the dramatic beat. In the theory of dramatic writing, beats are the smallest unit of dramatic action, consisting of a short dialog exchange or small amount of physical action. As architectural entities, beats organize both the procedural knowledge to accomplish the beat's dramatic action, and the declarative knowledge to sequence the beat in an evolving plot. Instead of conceiving of the characters as strongly autonomous entities that coordinate to accomplish dramatic action through purely local decision-making, characters are instead weakly autonomous—the character's behavioral repertoire dynamically changes as beats are sequenced. (Abstract shortened by UMI.)",2002.0,"Michael Mateas, J. Bates, J. Carbonell"
e466359e677f538574f8a3716bf42d5b8f495464,https://www.semanticscholar.org/paper/e466359e677f538574f8a3716bf42d5b8f495464,Multiagent systems: a modern approach to distributed artificial intelligence,Multiagent Systems is the title of a collection of papers dedicated to surveying specific themes of Multiagent Systems (MAS) and Distributed Artificial Intelligence (DAI). All of them authored by leading researchers of this dynamic multidisciplinary field.,2000.0,Guillermo R. Simari
52aba44a37fa7af6b6f15015577f9e7432622221,https://www.semanticscholar.org/paper/52aba44a37fa7af6b6f15015577f9e7432622221,Coordination techniques for distributed artificial intelligence,"Coordination, the process by which an agent reasons about its local actions and the (anticipated) actions of others to try and ensure the community acts in a coherent manner, is perhaps the key problem of the discipline of Distributed Artificial Intelligence (DAI). In order to make advances it is important that the theories and principles which guide this central activity are uncovered and analysed in a systematic and rigourous manner. To this end, this paper models agent communities using a distributed goal search formalism, and argues that commitments (pledges to undertake a specific course of action) and conventions (means of monitoring commitments in changing circumstances) are the foundation of coordination in all DAI systems.",1996.0,N. Jennings
965b39ad83c545849d473ce30cfc3d569f6e3828,https://www.semanticscholar.org/paper/965b39ad83c545849d473ce30cfc3d569f6e3828,Machines Who Think: A Personal Inquiry into the History and Prospects of Artificial Intelligence,"Pamela McCorduck first went among the artificial intelligentsia when the field was fresh and new, and asked the scientists engaged in it what they were doing and why. She saw artificial intelligence as the scientific apotheosis of one of the most enduring, glorious, often amusing, and sometimes alarming, traditions of human culture: the endless fascination with artifacts that think. Machines Who Think was translated into many languages, became an international cult classic, and stayed in print for nearly twenty years. Now, Machines Who Think is back, along with an extended addition that brings the field up to date in the last quarter century, including its scientific and its public faces. McCorduck shows how, from a slightly dubious fringe science, artificial intelligence has moved slowly (though not always steadily) to a central place in our everyday lives, and how it will be even more crucial as the World Wide Web moves into its next generation.",1979.0,P. McCorduck
ed561f8ec8e175f7f677bff9eb65982503e71788,https://www.semanticscholar.org/paper/ed561f8ec8e175f7f677bff9eb65982503e71788,"High-level perception, representation, and analogy: a critique of artificial intelligence methodology","Abstract High-level perception—the process of making sense of complex data at an abstract, conceptual level—is fundamental to human cognition. Through high-level perception, chaotic environmental stimuli are organized into mental representations that are used throughout cognitive processing. Much work in traditional artificial intelligence has ignored the process of high-level perception, by starting with hand-coded representations. In this paper, we argue that this dismissal of perceptual processes leads to distorted models of human cognition. We examine some existing artificial-intelligence models—notably BACON, a model of scientific discovery, and the Structure-Mapping Engine, a model of analogical thought—-and argue that these are flawed precisely because they downplay the role of high-level perception. Further, we argue that perceptual processes cannot be separated from other cognitive processes even in principle,and therefore that traditional artificial-intelligence models cannot be defended by supp...",1992.0,"D. Chalmers, R. French, D. Hofstadter"
dd8673e7f381dbeff9f8b76a90a7d4cea400f635,https://www.semanticscholar.org/paper/dd8673e7f381dbeff9f8b76a90a7d4cea400f635,Applying artificial intelligence to virtual reality: Intelligent virtual environments,"Research into virtual environments on the one hand and artificial intelligence and artificial life on the other has largely been carried out by two different groups of people with different preoccupation and interests, but some convergence is now apparent between the two fields. Applications in which activity independent of the user takes place- involving crowds or other agents- are beginning to be tackled, while synthetic agents, virtual humans, and computer pets are all areas in which techniques from the two fields require strong integration. The two communities have much to learn from each other if wheels are not to be reinvented on both sides. This paper reviews the issues arising from combining artificial intelligence and artificial life techniques with those of virtual environments to produce just such intelligent virtual environments. The discussion is illustrated with examples that include environments providing knowledge to direct or assist the user rather than relying entirely on the user's knowledge and skills, those in which the user is represented by a partially autonomous avatar, those containing intelligent agents separate from the user, and many others from both sides of the area.",2000.0,"Michael Luck, R. Aylett"
d1c91fdf066b9195a25626e903ce55765dde0387,https://www.semanticscholar.org/paper/d1c91fdf066b9195a25626e903ce55765dde0387,An Explainable Artificial Intelligence System for Small-unit Tactical Behavior,"As the artificial intelligence (AI) systems in military simulations and computer games become more complex, their actions become increasingly difficult for users to understand. Expert systems for medical diagnosis have addressed this challenge though the addition of explanation generation systems that explain a system's internal processes. This paper describes the AI architecture and associated explanation capability used by Full Spectrum Command, a training system developed for the U.S. Army by commercial game developers and academic researchers.",2004.0,"M. Lent, William Fisher, Michael Mancuso"
b3c7a71babfc70328a544df5b4bb19005f8d4ee8,https://www.semanticscholar.org/paper/b3c7a71babfc70328a544df5b4bb19005f8d4ee8,The Artificial Life Roots of Artificial Intelligence,Behavior-oriented Artificial Intelligence (AI) is a scientific discipline that studies how behavior of agents emerges and becomes intelligent and adaptive. Success of the field is defined in terms of success in building physical agents that are capable of maximizing their own self-preservation in interaction with a dynamically changing environment. The paper addresses this Artificial Life route toward AI and reviews some of the results obtained so far.,1993.0,L. Steels
d3458c75c6c42e5e5373a263e695169435b299af,https://www.semanticscholar.org/paper/d3458c75c6c42e5e5373a263e695169435b299af,Achieving Artificial Intelligence through Building Robots,"We argue that generally accepted methodologies of artificial intelligence research are limited in the proportion of human level intelligence they can be expected to emulate. We argue that the currently accepted decompositions and static representations used in such research are wrong. We argue for a shift to a process based model, with a decomposition based on task achieving behaviors as the organizational principle. In particular we advocate building robotic insects.",1986.0,R. Brooks
eb14281c97a583248ddbff5ab71309a3849a8c78,https://www.semanticscholar.org/paper/eb14281c97a583248ddbff5ab71309a3849a8c78,Handbook of Temporal Reasoning in Artificial Intelligence,"This collection represents the primary reference work for researchers and students in the area of Temporal Reasoning in Artificial Intelligence. Temporal reasoning has a vital role to play in many areas, particularly Artificial Intelligence. Yet, until now, there has been no single volume collecting together the breadth of work in this area. This collection brings together the leading researchers in a range of relevant areas and provides an coherent description of the breadth of activity concerning temporal reasoning in the filed of Artificial Intelligence. 
 
Key Features: 
 
- Broad range: foundations; techniques and applications 
- Leading researchers around the world have written the chapters 
- Covers many vital applications 
- Source book for Artificial Intelligence, temporal reasoning 
- Approaches provide foundation for many future software systems 
 
·Broad range: foundations; techniques and applications 
·Leading researchers around the world have written the chapters 
·Covers many vital applications 
·Source book for Artificial Intelligence, temporal reasoning 
·Approaches provide foundation for many future software systems 
 
Table of Contents 
 
""Formal Theories of Time and Temproal Incidence"", Lluis Vila. 
 
""Eventualities"", Antony Galton. 
 
""Time Granularity"", Jerome Euzenat and Angelo Montanari. 
 
""Modal Varieties of Temporal Logic"", Howard Barringer and Dov Gabbay. 
 
""Temporal Qualification in Artificial Intelligence"", Han Reichgelt and Lluis Vila. 
 
""Computational Complexity of Temporal Constraint Problems"", Thomas Drakengren and Peter Jonsson. 
 
""Indefinite Constraint Databases with Temporal Information: Representational Power and Computational Complexity"", Manolis Koubarakis. 
 
""Processing Qualitative Temporal Constraints"", Alfonso Gerevini. 
 
""Theorem-Proving for Discrete Temporal Logic"", Mark Reynolds/Clare Dixon. 
 
""Probabilistic Temporal Reasoning"", Steve Hanks/David Madigan. 
 
""Temporal Reasoning with iff-Abduction"", Marc Denecker/Kristof Van Belleghem. 
 
""Temporal Description Logics"", Alessandro Artale/Enrico Franconi. 
 
""Logic Programming and Reasoning about Actions"", Chitta Baral/Michael Gelfond. 
 
""Temporal Databases"" Jan Chomicki/David Toman. 
 
""Temporal Reasoning in Agent-Based Systems"" Michael Fisher/Michael Wooldridge. 
 
""Time in Planning"" Maris Fox/Derek Long. 
 
""Time in Automated Legal Reasoning"" Lluis Vila/Hajime Yoshino. 
 
""Temporal Reasoning in Natural Language"" Alice ter Meulen. 
 
""Temporal Reasoning in Medicine"" Elpida Keravnou/Yuval Shahar. 
 
""Time in Qualitative Simulation"" Dan Clancy/Benjamin Kuipers.",2005.0,"Michael Fisher, D. Gabbay, L. Vila"
64ab6267114b3830708d0dbe4b6d78e2cfc959fb,https://www.semanticscholar.org/paper/64ab6267114b3830708d0dbe4b6d78e2cfc959fb,AI: The Tumultuous History of the Search for Artificial Intelligence,"AI: The Tumultuous History of the Search for Artificial Intelligence, by Daniel Crevier, BasicBooks 1993, ISBN 0-465-02997-3.",1995.0,U. Eisenecker
1181f03825d17125c1111fb68c3f9335269edd1b,https://www.semanticscholar.org/paper/1181f03825d17125c1111fb68c3f9335269edd1b,Robot's Dilemma: The Frame Problem in Artificial Intelligence,"Each of the chapters in this volume devotes considerable attention to defining and elaborating the notion of the frame problem-one of the hard problems of artificial intelligence. Not only do the chapters clarify the problems at hand, they shed light on the different approaches taken by those in artificial intelligence and by certain philosophers who have been concerned with related problems in their field. The book should therefore not be read merely as a discussion of the frame problem narrowly conceived, but also as a general analysis of what could be a major challenge to the design of computer systems exhibiting general intelligence.",1987.0,Z. Pylyshyn
5bffe555e2a9d1976f8180ab2aed4f738ffa2f34,https://www.semanticscholar.org/paper/5bffe555e2a9d1976f8180ab2aed4f738ffa2f34,Building Explainable Artificial Intelligence Systems,"As artificial intelligence (AI) systems and behavior models in military simulations become increasingly complex, it has been difficult for users to understand the activities of computer-controlled entities. Prototype explanation systems have been added to simulators, but designers have not heeded the lessons learned from work in explaining expert system behavior. These new explanation systems are not modular and not portable; they are tied to a particular AI system. In this paper, we present a modular and generic architecture for explaining the behavior of simulated entities. We describe its application to the Virtual Humans, a simulation designed to teach soft skills such as negotiation and cultural awareness.",2006.0,"Mark G. Core, H. Chad Lane, M. Lent, Dave Gomboc, Steve Solomon, Milton Rosenberg"
bb86a77d4862db90aaf36c3b17639377eb8202f1,https://www.semanticscholar.org/paper/bb86a77d4862db90aaf36c3b17639377eb8202f1,Biologically Inspired Artificial Intelligence for Computer Games,"Biologically Inspired Artificial Intelligence for Computer Games reviews several strands of modern artificial intelligence, including supervised and unsupervised artificial neural networks; evolutionary algorithms; artificial immune systems, swarms, and shows—using case studies for each to display how they may be applied to computer games. This book spans the divide which currently exists between the academic research community working with advanced artificial intelligence techniques and the games programming community which must create and release new, robust, and interesting games on strict deadlines, thereby creating an invaluable collection supporting both technological research and the gaming industry.",2007.0,"D. Charles, C. Fyfe, D. Livingstone, S. McGlinchey"
2818a735f76204de3df8041372536945ce72ad3c,https://www.semanticscholar.org/paper/2818a735f76204de3df8041372536945ce72ad3c,Applications of Artificial Intelligence for Organic Chemistry: The DENDRAL Project,"ion planning It may be possible to take a coarser view of the problem space, as though we could back off and see the general features while omitting the details. This is one form of planning (a different form than embodied in Heuristic DENDRAL, to be described shortly). We will call it abstraction to keep the terminology clear. In the terms of our paradigm of problem solving, abstraction can be achieved by defining a new set of problem-state descriptions, based on a language that is an abstrac34 APPLICATIONS OF ARTIFICIAL INTELLIGENCE FOR ORGANIC CHEMISTRY tion of the basic language in the sense that one problem-state description in the abstraction language corresponds to many problem-state descriptions in the basic language. The new view of the problem space now offers a diminished set of states and therefore reduces the complexity of the problem. Transformations for the abstraction language defme the connectivity of the abstracted space. If the problem solver can discover a path from initial state to goal state in the abstracted space, he has not solved the original problem but has established a plan. Each step of the plan then becomes a problem in the original space, but the combined complexity of all these problems may be less than the complexity of the original problem. Working backward For some problems the number of alternatives to be searched is fewer if we begin at the goal state and, running the transformations in reverse as it were, search for the initial state. This situation might be the case if there is only one goal state but a multitude of initial states. Working backward is a time-honored procedure in mathematics and logic. Of course it is of no use for problems in which the transformations are not reversible, or when the definition of goal state is in terms to which the transformations cannot apply, as with chess. One would be hard put to play chess by working backward from a definition of checkmate, looking for the initial board configuration. 3.2.2.2 Heuristic generation Generate and test If there is a procedure that can generate candidate solution, goal states in our present terminology, it may be possible to solve problems by the sequential enumeration and checking of potential solutions. This method is frequently called generate and test. Note that in this paradigm it is not the states of the problem space that are generated. Indeed there need not be a problem space of the sort we have been discussing. Here we need only a space of solutions that can be generated for consideration. This paradigm is thus fundamentally different from searching the space of problem states (state-space search). Analogs of the heuristic search methods described above apply to heuristic generation. Statistical information concerning the distribution of solutions (perhaps by categories defined in a “secondary solution description language”) may be used to guide generation, If some measure of goodness is computable from a proposed solution, then a hill-climbing procedure could be used to determine ways of modifying one proposal in appropriate ways. Similarly, if an abstracted description of, the set of solutions can be constructed, it may be possible to search first for the’correct solution class, and then search within that class. These methods do not exhaust the possibilities, but they provide enough structure for our discussion. 3.2.3 Multiple Sources of Knowledge Much more problem-solving power can be achieved if there is more than one source of information that can be used. For this reason purely syntactic problem solvers are inherently less powerful than those that employ semantic information as well. The secret ARTIFICIAL INTELLIGENCE 35 is not that semantic information is more important, but that two sources of guidance are better than one. Jigsaw puzzles are an appropriate image to make this procedure clear. The problem of finding the one arrangement of all the pieces that yields the desired picture may be a very difficult and large combinatorial task. If the puzzle were done with the pieces face down, analogous to having only the “syntactic” information of piece contours, the true difficulty of the problem would become apparent. If all the pieces were of the same shape, squares or hexagons, the problem could only be solved from the “semantic” information of colors and pictured objects and would also be very difficult. Jigsaw puzzles are tractable because both these sources of information are available and can be played off against one another. In terms of the paradigm of state-space search, different sources of information correspond to different problem-state languages. Several of the search heuristics defmed above rely on these secondary languages. Hill climbing needs a language in which to define its gradient of “warmth,” and statistically guided search is based on a rather general but blurry-visioned search for descriptors that have some correlational information. Abstraction planning is another way of bringing to bear different views of the problem space. On a more general level, apart from any of these methods, is the representation problem. A problem has in general many possible representations. It may be possible to choose two or more (rather than just one) in such a way that progress in one statespace search can be transferred to another. Thus those transitions that are difficult in one representation may be bypassed by using a second and vice versa. In the context of heuristic generation, multiple sources of knowledge have the effect of limiting generation to the intersection of the solution sets delimited by each source.",1980.0,"R. K. Lindsay, B. Buchanan, E. Feigenbaum, J. Lederberg"
c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a,https://www.semanticscholar.org/paper/c78a6bb3ff2ea09bd4a4cf971984f2ccd4e08e1a,A Mobile Automaton: An Application of Artificial Intelligence Techniques,"A research project applying artificial intelligence techniques to the development of integrated robot systems is described. The experimental facili ty consists of an SDS-940 computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment. A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower level, functions. The execution of these in turn requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world and sensory processing systems into an efficient whole capable of perf orming a wide range of tasks in a real environment.",1969.0,N. Nilsson
49e5d0f461558ad2e92d5517f80f08ba60f9f7cb,https://www.semanticscholar.org/paper/49e5d0f461558ad2e92d5517f80f08ba60f9f7cb,A mobius automation: an application of artificial intelligence techniques,"A research project applying artificial intelligence techniques to the development of integrated robot systems Is described. The experimental facility consists of an SDS-940 computer and associated programs controlling a wheeled vehicle that carries a TV camera and other sensors. The primary emphasis is on the development of a system of programs for processing sensory data from the vehicle, for storing relevant information about the environment, and for planning the sequence of motor actions necessary to accomplish tasks in the environment. A typical task performed by our present system requires the robot vehicle to rearrange (by pushing) simple objects in its environment 
 
A novel feature of our approach is the use of a formal theorem-proving system to plan the execution of high-level functions as a sequence of other, perhaps lower-level, functions. The execution of these, in turn, requires additional planning at lower levels. The main theme of the research is the integration of the necessary planning systems, models of the world, and sensory processing systems into an efficient whole capable of performing a wide range of tasks in a real environment.",1969.0,N. Nilsson
be81441550cd1b090ae323cf43ff099dae9d01b6,https://www.semanticscholar.org/paper/be81441550cd1b090ae323cf43ff099dae9d01b6,Introduction to artificial intelligence and expert systems,Introduction to artificial intelligence knowledge representation knowledge organization and manipulation perception and communication knowledge acquisition.,1990.0,Dan W. Patterson
09d2210841c287acd7142fd7bd7e941989dd72c4,https://www.semanticscholar.org/paper/09d2210841c287acd7142fd7bd7e941989dd72c4,Artificial Intelligence and Natural Man,"'The Elephant is the most intelligent of animals because he does exactly what JXX as aqi ch ho we tell him to' wrote the great American humorist, Will Cuppy. And there are tic many philosophers and workers in the field of Artificial Intelligence who have talked themselves into a position from which they can no longer see the cutting edge of the joke. I am not sure whether Margaret Boden would see my point in quoting him or would catch the depth and many-sidedness of Cuppy's irony",1977.0,M. Boden
82f24e131775ed38a4e80d0a33d9b741f0923016,https://www.semanticscholar.org/paper/82f24e131775ed38a4e80d0a33d9b741f0923016,The philosophy of artificial intelligence,"From the Publisher: 
This volume contains classical and contemporary essays which explore the philosophical foundations of artificial intelligence and cognitive science. They illustrate objections raised by critics outside the field, and radical controversies within it.",1990.0,M. Boden
a971f929608e6e35b8f5037cca5c3b05e95fbeee,https://www.semanticscholar.org/paper/a971f929608e6e35b8f5037cca5c3b05e95fbeee,Context in Artificial Intelligence: I. A Survey of the Literature,"Context is the challenge for the coming years in artificial intelligence. In the companion paper [8], we present the main results of discussions at two workshops and at the first conference focusing on the notion of context. In this paper, we present a view of how context is considered in knowledge acquisition, machine learning, communication, and databases and ontologies. We describe the way in which context is modelled and represented in the logic formalism and a rule-based formalism. We present briefly after some of the other approaches, and sum up the different points that may be of interest for modelling context effectively.",1999.0,P. Brézillon
f38f271c910904e509d25db1ca700d65e229c244,https://www.semanticscholar.org/paper/f38f271c910904e509d25db1ca700d65e229c244,Artificial intelligence and expert systems for engineers,"From the Publisher: 
Artificial Intelligence and Expert Systems for Engineers details the AI-based methodologies known as: Knowledge-Based Expert Systems (KBES); Design Synthesis; Design Critiquing; and Case-Based Reasoning. KBES are the most popular M-based tools and have been successfully applied to planning, diagnosis, classification, monitoring, and design problems. Case studies axe provided with problems in engineering design for better understanding of the problem-solving models using the four methodologies in an integrated software environment.",1996.0,"C. S. Krishnamoorthy, S. Rajeev"
2fa2664817fe2784b55d53e13e30cb73cd6d3866,https://www.semanticscholar.org/paper/2fa2664817fe2784b55d53e13e30cb73cd6d3866,Artificial Intelligence: A Philosophical Introduction,List of Figures. Acknowledgements. Introduction. In outline. 1. The beginnings of Artificial Intelligence a historical sketch. 2. Some dazzling exhibits. 3. Can a machine think?. 4. The symbol system hypothesis. 5. A hard look at the facts. 6. The curious case of the Chinese Room. 7. Freedom. 8. Consciousness. 9. Are we computers?. 10. AIa s fresh start: parallel distributed processing. Epilogue. Notes. Bibliography. Index.,1993.0,B. Copeland
ba3a7f7b4dd4933568606fe6d1910778a5cb7af0,https://www.semanticscholar.org/paper/ba3a7f7b4dd4933568606fe6d1910778a5cb7af0,Learning from Data: Artificial Intelligence and Statistics V,"From the Publisher: 
This volume contains a revised collection of papers originally presented at the Fifth International Workshop on Artificial Intelligence and Statistics in 1995. The topics represented in this volume are diverse, and include natural language application causality and graphical models, classification, learning, knowledge discovery, and exploratory data analysis. The chapters illustrate the rich possibilities for interdisciplinary study at the interface of artificial intelligence and statistics. The chapters vary in the background that they assume, but moderate familiarity with techniques of artificial intelligence and statistics is desirable in most cases.",1996.0,"Doug Fisher, H. Lenz"
76b6380487b0cee901a867ff0ebb3b7feb68ec9c,https://www.semanticscholar.org/paper/76b6380487b0cee901a867ff0ebb3b7feb68ec9c,Developing an Artificial Intelligence Engine,"As computer games become more complex and consumers demand more sophisticated computer controlled agents, developers are required to place a greater emphasis on the artificial intelligence aspects of their games. One source of sophisticated AI techniques is the artificial intelligence research community. This paper discusses recent efforts by our group at the University of Michigan Artificial Intelligence Lab to apply state of the art artificial intelligence techniques to computer games. Our experience developing intelligent air combat agents for DARPA training exercises, described in John Laird's lecture at the 1998 Computer Game Developer's Conference, suggested that many principles and techniques from the research community are applicable to games. A more recent project, called the Soar/Games project, has followed up on this by developing agents for computer games, including Quake II and Descent 3. The result of these two research efforts is a partially implemented design of an artificial intelligence engine for games based on well established AI systems and techniques.",1999.0,"M. Lent, J. Laird"
aca40801c2714f6b00669fa730c75dca854441c7,https://www.semanticscholar.org/paper/aca40801c2714f6b00669fa730c75dca854441c7,Neural Assemblies: An Alternative Approach to Artificial Intelligence,"Contents, with Outline.- I.- 1 The Flow of Information.- An introduction to the brain with emphasis on the transmission of information. Digressions 1 and 2 start from here..- 2 Thinking as Seen from Within and from Without.- Some problems in thinking about thinking are presented the behavioristic approach to such problems is introduced: What in the observable behavior of somebody else makes us think that he is thinking? This leads to the Turing test for artificial intelligence..- 3 How to Build Well-Behaving Machines.- ""Behavior"" is understood as the total stimulus (or situation) ? response mapping. For a finite number of different inputs, any such mapping can be constructed. This statement is demonstrated by.- 1. coding of any finite set into finite 0, 1-sequences.- 2. showing that any mapping between finite sets of 0.1-sequences can be built from logical and-, or-, and not-gates..- 3. representing the and-, or-, and not-gate as special threshold neurons of the McCulloch and Pitts type..- Digression 3 may be of some help here..- 4 Organizations, Algorithms, and Flow Diagrams.- The chapter contains some general remarks on organizations and cooperativity and introduces the matchbox algorithm..- 5 The Improved Matchbox Algorithm.- The matchbox algorithm is improved by the incorporation of the look-ahead algorithm (e.g., for chess-playing machines) and the associative matrix memory. Appendix 1 starts from here. Chapters 5 and 7 contain the basic constructions needed for the survival algorithm..- 6 The Survival Algorithm as a Model of an Animal.- The improved matchbox algorithm is interpreted as an algorithm for survival and thus as a model of an animal If such an algorithm is implemented in terms of neuron-like elements, the result can be checked against experimental data from the neurosciences. Conversely, such data cannot really be understood without a theory (in line with a more general argument as for example in Kuhn 1962)..- 7 Specifying the Survival Algorithm.- Some further specifications of the survival algorithm are given that are necessary in order to implement the algorithm in terms of neurons. A neural realization of the survival algorithm is finally discussed in connection with some basic data on the brain (from Chap. 1) and in order to stimulate interest in further data as supplied in the following chapters. Digression 4 may be entered from here..- II.- 8 The Anatomy of the Cortical Connectivity.- Further data on the connectivity between neurons in the cerebral cortex are presented, leading to some speculations on the flow of neural activity in the cortex..- 9 The Visual Input to the Cortex.- The projection from the retina onto the visual cortex is outlined, to exemplify how sensory input information enters the cortex..- 10 Changes in the Cortex with Learning.- Various experiments correlate differences in the cortex with differences in the environments which had been experienced by experimental animals, and possibly with ""learning"". Some of these experiments are discussed with the object of obtaining evidence for Hebb'Contents, with Outline.- I.- 1 The Flow of Information.- An introduction to the brain with emphasis on the transmission of information. Digressions 1 and 2 start from here..- 2 Thinking as Seen from Within and from Without.- Some problems in thinking about thinking are presented the behavioristic approach to such problems is introduced: What in the observable behavior of somebody else makes us think that he is thinking? This leads to the Turing test for artificial intelligence..- 3 How to Build Well-Behaving Machines.- ""Behavior"" is understood as the total stimulus (or situation) ? response mapping. For a finite number of different inputs, any such mapping can be constructed. This statement is demonstrated by.- 1. coding of any finite set into finite 0, 1-sequences.- 2. showing that any mapping between finite sets of 0.1-sequences can be built from logical and-, or-, and not-gates..- 3. representing the and-, or-, and not-gate as special threshold neurons of the McCulloch and Pitts type..- Digression 3 may be of some help here..- 4 Organizations, Algorithms, and Flow Diagrams.- The chapter contains some general remarks on organizations and cooperativity and introduces the matchbox algorithm..- 5 The Improved Matchbox Algorithm.- The matchbox algorithm is improved by the incorporation of the look-ahead algorithm (e.g., for chess-playing machines) and the associative matrix memory. Appendix 1 starts from here. Chapters 5 and 7 contain the basic constructions needed for the survival algorithm..- 6 The Survival Algorithm as a Model of an Animal.- The improved matchbox algorithm is interpreted as an algorithm for survival and thus as a model of an animal If such an algorithm is implemented in terms of neuron-like elements, the result can be checked against experimental data from the neurosciences. Conversely, such data cannot really be understood without a theory (in line with a more general argument as for example in Kuhn 1962)..- 7 Specifying the Survival Algorithm.- Some further specifications of the survival algorithm are given that are necessary in order to implement the algorithm in terms of neurons. A neural realization of the survival algorithm is finally discussed in connection with some basic data on the brain (from Chap. 1) and in order to stimulate interest in further data as supplied in the following chapters. Digression 4 may be entered from here..- II.- 8 The Anatomy of the Cortical Connectivity.- Further data on the connectivity between neurons in the cerebral cortex are presented, leading to some speculations on the flow of neural activity in the cortex..- 9 The Visual Input to the Cortex.- The projection from the retina onto the visual cortex is outlined, to exemplify how sensory input information enters the cortex..- 10 Changes in the Cortex with Learning.- Various experiments correlate differences in the cortex with differences in the environments which had been experienced by experimental animals, and possibly with ""learning"". Some of these experiments are discussed with the object of obtaining evidence for Hebb's synaptic rule. Digression 4 may be consulted here..- 11 From Neural Dynamics to Cell Assemblies.- Several papers on neural dynamics are discussed in order.- 1. to obtain a more detailed image of the flow of activity in the neural network of the brain (or the cortex).- 2. to get a better understanding of the learning- and information processing capabilities of such networks (especially in comparison with the requirements of the survival algorithm of Chaps. 6 and 7)..- The resulting image is fixed in the language of cell assemblies. Appendix 2 and Digression 4 start from here..- 12 Introspection and the Rules of Threshold Control.- The same language of cell assemblies is used to describe some introspections of the author in a more systematic way. This leads to a few strategies for controlling the thresholds of the neurons in a neural network that is used as an associative memory (for example by a survival robot). Appendices 3 and 4 and Digression 5 start from here..- 13 Further Speculations.- The ideas of cell assemblies and threshold control are carried out further and in a more speculative way. Digression 6 starts from here. Chapters 12 and 13 (together with Digression 5) contain a speculative, algorithmic picture of the information processing in an animal's brain..- 14 Men, Monkeys, and Machines.- It is argued that this picture carries over to humans as well. The acquisition of language, in particular, is regarded as a phenomenon of cultural evolution..- 15 Why all These Speculations?.- The whole book can be understood as an attempt to reduce human behavior to electrophysiological events in the brain and finally to physics, which, of course, does not preclude a heuristic use of teleological arguments (the final purpose being survival and proliferation). Some ethical and epistemological consequences of this attempt are briefly discussed..- References.- Digressions.- 1 Electrical Signal Transmission in a Single Neuron.- 2 Basic Information Theory.- 3 Sets and Mappings.- 4 Local Synaptic Rules.- 5 Flow Diagram for a Survival Algorithm.- 6 Suggestions for Further Reading.- Appendices.- 1 On the Storage Capacity of Associative Memories.- 2 Neural Modeling.- 3 Cell Assemblies: the Basic Ideas.- 4 Cell Assemblies and Graph Theory.- Author and Subject Index.",1982.0,G. Palm
f77669f294265ce2f2c36992638f56817431cc1e,https://www.semanticscholar.org/paper/f77669f294265ce2f2c36992638f56817431cc1e,Social Situatedness of Natural and Artificial Intelligence: Vygotsky and Beyond,"The concept of “social situatedness,” that is, the idea that the development of individual intelligence requires a social (and cultural) embedding, has recently received much attention in cognitive science and artificial intelligence research, in particular work on social or epigenetic robotics. The work of Lev Vygotsky, who put forward this view as early as the 1920s, has influenced the discussion to some degree but still remains far from well known. This article therefore is aimed at giving an overview of his cognitive development theory and a discussion of its relation to more recent work in primatology and socially situated artificial intelligence, in particular humanoid robotics.",2003.0,"J. Lindblom, T. Ziemke"
b5ba5509d5f6051d4a5bce72793ca65ee80b7897,https://www.semanticscholar.org/paper/b5ba5509d5f6051d4a5bce72793ca65ee80b7897,Mathematical methods in artificial intelligence,"mathematical methods in artificial intelligence. Book lovers, when you need a new book to read, find the book here. Never worry not to find what you need. Is the mathematical methods in artificial intelligence your needed book now? That's true; you are really a good reader. This is a perfect book that comes from great author to share with you. The book offers the best experience and lesson to take, not only take, but also learn.",1996.0,E. Bender
84ad019b0f42f9f7d15232d0233e7c8fed872434,https://www.semanticscholar.org/paper/84ad019b0f42f9f7d15232d0233e7c8fed872434,Artificial Intelligence and the Future of Testing,"Contents: R. Freedle, Introduction: Artificial Intelligence and Its Implications for the Future of ETS's Tests. Part I:Assessment of Quantitative Skills. T. Ager, From Interactive Instruction to Interactive Testing. R. Milson, M. Lewis, J.R. Anderson, The Teacher's Apprentice Project: Building an Algebra Tutor. Part II:Graphs and Computer Vision: Selected Applications. S. Pinker, A Theory of Graph Comprehension. L. Kitchen, A Sketch of Accomplishments in Computer Vision with Speculations on Its Use in Educational Testing Part III:Learning, Memory, Reasoning and Language Issues. M. Burstein, B. Adelson, Issues for a Theory of Analogical Learning. B. Ross, The Access and Use of Relevant Information: A Specific Case and General Issues. B. Adelson, Modeling Software Design Within a Problem-Space Architecture. L. Rau, Memory Organization and Retrieval. P. Jacobs, Two Hurdles for Natural Language Systems. Part IV:Invited Critique. S. Amarel. A. Joshi. Part V:Toward the Future of Testing at ETS. R. Bennett, B. Gong, R. Kershaw, D. Rock, E. Soloway, A. Macalalad, Assessment of an Expert System's Ability to Automatically Grade and Diagnose Student's Constructed Responses to Computer Science Problems.",1990.0,R. Freedle
1e0faadd032ddc733779f0be4c0c7e9a1ba0136f,https://www.semanticscholar.org/paper/1e0faadd032ddc733779f0be4c0c7e9a1ba0136f,A review of artificial intelligence,"This paper reviews the field of artificial intelligence focusing on embodied artificial intelligence. It also considers models of artificial consciousness, agent-based artificial intelligence and the philosophical commentary on artificial intelligence. It concludes that there is almost no consensus nor formalism in the field and that the achievements of the field are meager.",2000.0,"Emma S. Brunette, R. Flemmer, C. Flemmer"
4370b34deb3ba7ebc1efb9807cca914936312de2,https://www.semanticscholar.org/paper/4370b34deb3ba7ebc1efb9807cca914936312de2,Artificial intelligence techniques in power systems,* Chapter 1: Artificial intelligence techniques in power systems * Chapter 2: Advanced knowledge engineering techniques with applications to electric power systems * Chapter 3: Object-oriented design and implementation of power system analysis software * Chapter 4: Fuzzy logic and hybrid systems * Chapter 5: Alarm analysis * Chapter 6: Artificial intelligence techniques for voltage control * Chapter 7: AI for protection systems * Chapter 8: Artificial neural networks for static security assessment * Chapter 9: Knowledge based systems for condition monitoring * Chapter 10: Scheduling maintenance of electrical power transmission networks using genetic programming * Chapter 11: Neuro-expert system applications in power systems * Chapter 12: Intelligent systems for demand forcasting * Chapter 13: A practical application and implementation of adaptive techniques using neural networks into autoreclose protection and system control,1997.0,"K. Warwick, A. Ekwue, R. Aggarwal"
4f90e7d063501c7c98e73ef54bf9678e115312b1,https://www.semanticscholar.org/paper/4f90e7d063501c7c98e73ef54bf9678e115312b1,Web Intelligence and Artificial Intelligence in Education,"This paper surveys important aspects of Web Intelligence (WI) in the context of Artificial Intelligence in Education (AIED) research. WI explores the fundamental roles as well as practical impacts of Artificial Intelligence (AI) and advanced Information Technology (IT) on the next generation of Web-related products, systems, services, and activities. As a direction for scientific research and development, WI can be extremely beneficial for the field of AIED. Some of the key components of WI have already attracted AIED researchers for quite some time – ontologies, adaptivity and personalization, and agents. The paper covers these issues only very briefly. It focuses more on other issues in WI, such as intelligent Web services, semantic markup, and Web mining, and proposes how to use them as the basis for tackling new and challenging research problems in AIED.",2004.0,V. Devedzic
299b20cbb968c6ce5175899dc38d2ee8e4e7251a,https://www.semanticscholar.org/paper/299b20cbb968c6ce5175899dc38d2ee8e4e7251a,"Artificial Intelligence, Language, and the Study of Knowledge","This paper studies the relationship of Artificial Intelligence to the study of language and the representation of the underlying knowledge which supports the comprehension process. It develops the view that intelligence is based on the ability to use large amounts of diverse kinds of knowledge in procedural ways, rather than on the possession of a few general and uniform principles. The paper also provides a unifying thread to a variety of recent approaches to natural language comprehension. We conclude with a brief discussion of how Artificial Intelligence may have a radical impact on education if the principles which it utilizes to explore the representation and use of knowledge are made available to the student to use in his own learning experiences.",1977.0,"I. Goldstein, S. Papert"
418e865b03f47618a92fe368222b9fef053d2963,https://www.semanticscholar.org/paper/418e865b03f47618a92fe368222b9fef053d2963,Readings in Music and Artificial Intelligence,"What do you do to start reading readings in music and artificial intelligence? Searching the book that you love to read first or find an interesting book that will make you want to read? Everybody has difference with their reason of reading a book. Actuary, reading habit must be from earlier. Many people may be love to read, but not a book. It's not fault. Someone will be bored to open the thick book with small words to read. In more, this is the real condition. So do happen probably with this readings in music and artificial intelligence.",2000.0,E. Miranda
f7a5a0106f1677fd2109efff76aa16a434e7647d,https://www.semanticscholar.org/paper/f7a5a0106f1677fd2109efff76aa16a434e7647d,Principles of Artificial Intelligence and Expert Systems Development,Principles of artificial intelligence and expert systems development. Computing Methodologies -- Artificial Intelligence. Principles of Artificial Intelligence and Expert Systems Development. Front Cover,1988.0,D. Rolston
031cfc003043ad265742404f20815b7d831c44d8,https://www.semanticscholar.org/paper/031cfc003043ad265742404f20815b7d831c44d8,Why not a Sociology of Machines? The Case of Sociology and Artificial Intelligence,"In the light of the recent growth of artificial intelligence (AI), and of its implications for understanding human behaviour, this paper evaluates the prospects for an association between sociology and artificial intelligence. Current presumptions about the distinction between human behaviour and artificial intelligence are identified through a survey of discussions about AI and `expert systems'. These discussions exhibit a restricted view of sociological competence, a marked rhetoric of progress and a wide variation in assessments of the state of the art. By drawing upon recent themes in the social study of science, these discussions are shown to depend on certain key dichotomies and on an interpretive flexibility associated with the notions of intelligence and expertise. The range of possible associations between sociology and AI reflects the extent to which we are willing to adopt these features of AI discourse. It is suggested that one of the more important options is to view the AI phenomenon as an occasion for reassessing the central axiom of sociology that there is something distinctively `social' about human behaviour.",1985.0,S. Woolgar
2034ad943778712729a3b6beaffc80ebabc67b8c,https://www.semanticscholar.org/paper/2034ad943778712729a3b6beaffc80ebabc67b8c,Intelligent Tutoring Systems: At the Crossroads of Artificial Intelligence and Education,"The evolution from Computer-Aided Instruction (CAI) to Intelligent Computer-Aided Instruction (ICAI) was the first step by which education and artificial intelligence communities began to look at each other's work. This text looks at the evolution toward Intelligent Tutoring Systems (ITS) which can be thought of as a step beyond ICAI, leading to more classes of problems and approaches. ITS involves artificial intelligence concepts approaches, dynamic student modelling, human cognition, intelligent user interfaces, intelligent help systems and the use of strategies.",1990.0,"C. Frasson, Gilles Gauthier"
82913ce718853c4ef8e6589a5f5343e7ba0e1719,https://www.semanticscholar.org/paper/82913ce718853c4ef8e6589a5f5343e7ba0e1719,Uncertainty in artificial intelligence,"The first conference on Uncertainty in Artificial Intelligence was held in 1985 by a group of people who felt that their views on the use of probability theory were not receiving a fair hearing from the rest of the Al community. At the time, mainstream opinion held that computational complexity of, and the amount of data required by, probabilistic methods made them inappropriate for realistic applications. As a result, those who claimed that probability theory was an adequate, if not the only adequate, method of handling uncertainty received a somewhat frosty reception.",1994.0,S. Parsons
a02df32aa9bd58d49937e02465700767e2c379a6,https://www.semanticscholar.org/paper/a02df32aa9bd58d49937e02465700767e2c379a6,Artificial Intelligence and Molecular Biology,"Molecular biology is emerging as an important domain for artificial intelligence research. The advantages of biology for design and testing of AI systems include large amounts of available online data, significant (but incomplete) background knowledge, a wide variety of problems commensurate with AI technologies, clear standards of success, cooperative domain experts, non-military basic research support and percieved potential for practical (and profitable) applications. These considerations have motivated a growing group of researchers to pursue both basic and applied AI work in the domain. More than seventy-five researchers working on these problems gathered at Stanford for a AAAI sponsored symposium on the topic. This article provides a description of much of the work presented at the meeting, and fills in the basic biology background necessary to place it in context.",1992.0,L. Hunter
5dd5a9fe469f4d9b26586f8c4789ac780abf181f,https://www.semanticscholar.org/paper/5dd5a9fe469f4d9b26586f8c4789ac780abf181f,Probability Judgment in Artificial Intelligence and Expert Systems,"Historically, the study of artificial intelligence has emphasized symbolic rather than numerical computation. In recent years, however, the practical needs of expert systems have led to an interest in the use of numbers to encode partial confidence. There has been some effort to square the use of these numbers with Bayesian probability ideas, but in most applications not all the inputs required by Bayesian probability analyses are available. This difficulty has led to widespread interest in belief func- tions, which use probability in a looser way. It must be recognized, however, that even belief functions require more structure than is provided by pure production systems. The need for such structure is inherent in the nature of probability argument and cannot be evaded. Probability argument re- quires design as well as numerical inputs. The real challenge probability poses to artificial intelligence is to build systems that can design probability arguments. The real challenge artificial intelligence poses to statistics is to explain how statisticians design probability arguments.",1987.0,G. Shafer
d9566ac89cd9b7fccc080b764aab5107430da28c,https://www.semanticscholar.org/paper/d9566ac89cd9b7fccc080b764aab5107430da28c,Artificial intelligence meets natural stupidity,"As a field, artificial intelligence has always been on the border of respectability, and therefore on the border of crackpottery. Many critics <Dreyfus, 1972>, <Lighthill, 1973> have urged that we are over the border. We have been very defensive toward this charge, drawing ourselves up with dignity when it is made and folding the cloak of Science about us. On the other hand, in private, we have been justifiably proud of our willingness to explore weird ideas, because pursuing them is the only way to make progress.",1976.0,D. McDermott
384a21590d209d774c578d8fc351534c063f225b,https://www.semanticscholar.org/paper/384a21590d209d774c578d8fc351534c063f225b,Artificial Intelligence: An Engineering Approach,"This textbook is for an advanced undergraduate or postgraduate course in artificial intelligence. No previous experience in this area is assumed. The text emphasizes the conceptual approach, while underlying mathematical and conceptual fundamentals are stressed to prepare students to participate in ""hands-on"" development of AI systems. Current topics of interest to the engineering community are discussed, including parallel decomposition of AI algorithms, corresponding parallel dedicated processing hardware, temporal reasoning, constant satisfaction and rule based implementations.",1990.0,R. Schalkoff
7c647340ac52e2520403f9db37d09aec762e31d0,https://www.semanticscholar.org/paper/7c647340ac52e2520403f9db37d09aec762e31d0,Applications of artificial intelligence in chemistry,"From the Publisher: 
This series of short texts provides accessible accounts of a range of essential topics in chemistry. Written with the needs of the student in mind, the Oxford Chemistry Primers offer just the right level of detail for undergraduate study, and will be invaluable as a source of material commonly presented in lecture courses yet not adequately covered in existing texts. All the basic principles and facts in a particular area are presented in a clear and straightforward style, to produce concise yet comprehensive accounts of topics covered in both core and specialist courses. It is becoming evident that the techniques of artificial intelligence are useful for more than just the development of thinking machines; they constitute powerful problem-solving tools in their own right. The large-scale, complex problems most suited to AI are just those which are most difficult for conventional techniques to solve. AI methods therefore expand the range of problems in science that we can successfully tackle. Because of a recent rapid growth in computing power, these methods can now be used on a routine basis by scientists in both academic research and the commercial world. It is becoming vital that science students be exposed to and understand these techniques. This book provides an introduction to these methods, written specifically for science students. Although examples are drawn mainly from chemistry, the book is suitable for a more general audience, and should be of interest to any college-level student who wants to know more about how computers can help us to understand and interpret science.",1993.0,H. Cartwright
936b23b7611fdebc8c002b34f309762f4d719571,https://www.semanticscholar.org/paper/936b23b7611fdebc8c002b34f309762f4d719571,Artificial intelligence applications in the intensive care unit,"ObjectiveTo review the history and current applications of artificial intelligence in the intensive care unit. Data SourcesThe MEDLINE database, bibliographies of selected articles, and current texts on the subject. Study SelectionThe studies that were selected for review used artificial intelligence tools for a variety of intensive care applications, including direct patient care and retrospective database analysis. Data ExtractionAll literature relevant to the topic was reviewed. Data SynthesisAlthough some of the earliest artificial intelligence (AI) applications were medically oriented, AI has not been widely accepted in medicine. Despite this, patient demographic, clinical, and billing data are increasingly available in an electronic format and therefore susceptible to analysis by intelligent software. Individual AI tools are specifically suited to different tasks, such as waveform analysis or device control. ConclusionsThe intensive care environment is particularly suited to the implementation of AI tools because of the wealth of available data and the inherent opportunities for increased efficiency in inpatient care. A variety of new AI tools have become available in recent years that can function as intelligent assistants to clinicians, constantly monitoring electronic data streams for important trends, or adjusting the settings of bedside devices. The integration of these tools into the intensive care unit can be expected to reduce costs and improve patient outcomes.",2001.0,"C. William, MD Hanson III, M. F. F. T. T. P. Bryan E. Marshall"
ba3a7f7b4dd4933568606fe6d1910778a5cb7af0,https://www.semanticscholar.org/paper/ba3a7f7b4dd4933568606fe6d1910778a5cb7af0,Learning from Data: Artificial Intelligence and Statistics V,"From the Publisher: 
This volume contains a revised collection of papers originally presented at the Fifth International Workshop on Artificial Intelligence and Statistics in 1995. The topics represented in this volume are diverse, and include natural language application causality and graphical models, classification, learning, knowledge discovery, and exploratory data analysis. The chapters illustrate the rich possibilities for interdisciplinary study at the interface of artificial intelligence and statistics. The chapters vary in the background that they assume, but moderate familiarity with techniques of artificial intelligence and statistics is desirable in most cases.",1996.0,"Doug Fisher, H. Lenz"
82913ce718853c4ef8e6589a5f5343e7ba0e1719,https://www.semanticscholar.org/paper/82913ce718853c4ef8e6589a5f5343e7ba0e1719,Uncertainty in artificial intelligence,"The first conference on Uncertainty in Artificial Intelligence was held in 1985 by a group of people who felt that their views on the use of probability theory were not receiving a fair hearing from the rest of the Al community. At the time, mainstream opinion held that computational complexity of, and the amount of data required by, probabilistic methods made them inappropriate for realistic applications. As a result, those who claimed that probability theory was an adequate, if not the only adequate, method of handling uncertainty received a somewhat frosty reception.",1994.0,S. Parsons
635643c62fee99a9ea0615a33c087fb6a65bf717,https://www.semanticscholar.org/paper/635643c62fee99a9ea0615a33c087fb6a65bf717,Evolving artificial intelligence,"The majority of research in artificial intelligence has been devoted to modeling the symptoms of intelligent behavior as we observe them in ourselves. Investigation into the causative factors of intelligence have been passed over in order to more rapidly obtain the immediate consequences of intelligence. The results of these efforts have been computer programs that achieve outstanding performance, but only in very limited domains of application. 
It is suggested that attention be given to the mechanisms that generate intelligence. Intelligence may be defined as that property which enables a system to adapt its behavior to meet desired goals in a range of environments. Three organizational forms of intelligence are characterized within the present discussion: (1) phylogenetic (arising within the phyletic line of descent), (2) ontogenetic (arising within the individual), and (3) sociogenetic (arising within the group). It is argued that all three forms of intelligence are equivalent in process and that all intelligent systems are inherently evolutionary in nature. 
Simulating natural evolution provides a method for machine generated intelligent behavior. A series of experiments is conducted to quantify the efficiency and effectiveness of evolutionary problem solving. The results indicate that this ""evolutionary programming"" can rapidly discover nearly optimum solutions to a broad range of problems. Mathematical analysis of the algorithm and its variations indicates that the process will converge to the global best available solution. Automatic control and gaming experiments are conducted in which an evolutionary program must discover suitable strategies for solving the task at hand. No credit assignment or other heuristic evaluations are offered to the evolutionary programs. The results indicate the utility of using simulated evolution for general problem solving.",1992.0,D. Fogel
be81441550cd1b090ae323cf43ff099dae9d01b6,https://www.semanticscholar.org/paper/be81441550cd1b090ae323cf43ff099dae9d01b6,Introduction to artificial intelligence and expert systems,Introduction to artificial intelligence knowledge representation knowledge organization and manipulation perception and communication knowledge acquisition.,1990.0,Dan W. Patterson
81613dd7e7e06a9d15ae4eb97e2e7f141f2a8c76,https://www.semanticscholar.org/paper/81613dd7e7e06a9d15ae4eb97e2e7f141f2a8c76,An Automobile with Artificial Intelligence,"This paper describes an automobile with artificial intelligence, which consists of a road pattern recognition unit and a problem solving unit. The vehicle is completely autonomous and can be driven without a human driver. The road pattern recognition unit involving a pair of TV cameras and a processing unit identifies obstacles in front of the vehicle and outputs data regarding to the locations of the obstacles. The problem solving unit is a microcomputer system and determines control optimal to the environment around the vehicle based on the data. The algorithm employed in it is a table-look-up method, in which the location of the optimal control is addressed in the table by key words generated from the data. The table was heuristically made by means of digital simulation. The vehicle was successfully driven under various road environments at the speed within 30 Km/h.",1979.0,"S. Tsugawa, T. Yatabe, Takeshi Hirose, S. Matsumoto"
9b86911ba530e2c4c465fea8dc17df1b8c7ad361,https://www.semanticscholar.org/paper/9b86911ba530e2c4c465fea8dc17df1b8c7ad361,New Programming Languages for Artificial Intelligence Research,"New directions in Artificial Intelligence research have led tothe need for certainnovel features to be embedded in programming languages. This paper givesan overviewof the nature of these features, and their implementation in fourprincipal families of AI language*: SAILPLANNER/COXXIVER; QLISP/INTERLISP; and POPLER,POP-2. Th«-programming features described include :new data types and accessing meeh.vni.iins for stored expressions; moreflexible control structures, includingmultiple processes and backtracking; pattern matching to allow comparison of data item with atemplate, and extractionof labeled subexpressions; and deductive mechanisms which allow theprogramming system to carry out certain activities includingmodifying the data base and deciding which subroutines to run next using only constraints and guidelines set up by theprogrammer.",1974.0,"D. Bobrow, B. Raphael"
fea4b238101d56979546c122427d8b02b29cc5d0,https://www.semanticscholar.org/paper/fea4b238101d56979546c122427d8b02b29cc5d0,Computer vision and artificial intelligence in mammography.,"The revolution in digital computer technology that has made possible new and sophisticated imaging techniques may next influence the interpretation of radiologic images. In mammography, computer vision and artificial intelligence techniques have been used successfully to detect or to characterize abnormalities on digital images. Radiologists supplied with this information often perform better at mammographic detection or characterization tasks in observer studies than do unaided radiologists. This technology therefore could decrease errors in mammographic interpretation that continue to plague human observers.",1994.0,"Carl J. Vyborny1’2, Maryellen L. Giger2, C. J. vybomy"
a7562aca3c70b2ac6f8467b03c38b82cdb1a83cb,https://www.semanticscholar.org/paper/a7562aca3c70b2ac6f8467b03c38b82cdb1a83cb,Book review: The Elements of Artificial Intelligence An Introduction Using LISP by Steven L. Tanimoto (Computer Science Press),"The Elements of Artificial Intelligence: an Introduction using LISP is intended (as stated in the preface) for use as the text for an introductory course in artificial intelligence. The author's approach to the subject is stated on the dust jacket (and in the preface, although not as concisely).",1988.0,Victor Schneider
eb1268b7623af7eca70540433463546493150456,https://www.semanticscholar.org/paper/eb1268b7623af7eca70540433463546493150456,Progress in artificial intelligence,"This guide explores the most recent and significant achievements attained in the field of artificial intelligence through the fifth generation computing plan. Discussions center around problem-solving and learing, knowledge-representation, reasoning and control, artificial intelligence and external relations, and applications.",1985.0,"L. Steels, A. Campbell"
8d1b99b05e4331fef7fe442c9d166d60b108859d,https://www.semanticscholar.org/paper/8d1b99b05e4331fef7fe442c9d166d60b108859d,Aspects of Artificial Intelligence,Prologue.- The Semantics of Clocks.- I / Ontological Foundations.- The Pseudorealization Fallacy and the Chinese Room Argument.- In Praise of Narrow Minds: The Frame Problem.- Syntactic Semantics: Foundations of Computational Natural-Language Understanding.- Signs and Minds: An Introduction to the Theory of Semiotic Systems.- Logic for the New AI.- II / Epistemological Dimensions.- Artificial Intelligence is Philosophy.- Artificial Intelligence as an Experimental Science.- Defeasible Reasoning: A Philosophical Analysis in Prolog.- When is Reasoning Nonmonotonic?.- Artificial Intelligence and Effective Epistemology.- Maintaining an Inductive Database.- Epilogue.- Automating Creativity.- Index of Names.- Index of Subject.,1987.0,James H. Fetzer
bdd2c3f2dc84577cc51e6d1e79ba5921290132f2,https://www.semanticscholar.org/paper/bdd2c3f2dc84577cc51e6d1e79ba5921290132f2,Made-up minds - a constructivist approach to artificial intelligence,"Part 1 Constructivist AI: introduction and overview - the schema mechanism - an overview, origins of constructivist AI - on the meaning of learning, guide to the rest of the book synopsis of Piagetian development - Piagetian fundamentals, first stage - reflex activity, solipsist images, second stage - the co-ordination of primary schemas, third stage - secondary circular reactions, subjective permanence, fourth stage - co-ordination of secondary schemas, fifth stage - experiments on objects, sixth stage - simulation of events, subsequent periods - preoperational, concrete and formal operations, themes of Piagetian development. Part 2 the schema mechanism: representational elements - structure and use - schemas, items, actions, control construction and revision - marginal attribution - spinning off new schemas, synthetic items, composite actions architecture - neural architecture, computer implementation architecture. Part 3 Performance and speculations: synopsis of schema mechamism performance - the microworld, learning spatial substrates, steps toward intermodal co-ordination, beginnings of the persistent-object concepts, hypothetical scenario of futher developments extrapolations - virtual structures, mechanisms - virtual generalizations, virtual mechanisms non-naive induction - the problem with naive induction, the problem with proposing only non-absurd generalizations, the problem with using only projectable concepts, the problem with preferring entrenched concepts, induction conflicts and deductive overrides, why non-niaive induction must be built in, innateness of projectability judgements, induction and counterfactuals. Part 4 Appraisal: comparisons - modularity for learning - prediction-value vs situation-action systems, the schema and mechanism and connectionism, the schema mechanism and search algorithms, the schema mechanism and explanation-based learning, the schema mechanism and rational learning, virtual mechanism and self-modification, the schema mechanism and situated activity, the schema mechanism and the society of mind, other Piagetian or sensorimotor learning systems conclusion - methodological underpinnings of constructivist AI, directions for future work, evaluation and summary.",1991.0,G. Drescher
2b055c15cf2f8bd867d07026431ca00ed428f1ce,https://www.semanticscholar.org/paper/2b055c15cf2f8bd867d07026431ca00ed428f1ce,Artificial Intelligence for Games,"AI and Games Introduction What Is AI? Model of Game AI Algorithms, Data Structures, and Representations On the Website Layout of the Book Game AI The Complexity Fallacy The Kind of AI in Games Speed and Memory The AI Engine Techniques Movement The Basics of Movement Algorithms Kinematic Movement Algorithms Steering Behaviors Combining Steering Behaviors Predicting Physics Jumping Coordinated Movement Motor Control Movement in the Third Dimension Pathfinding The Pathfinding Graph Dijkstra A* World Representations Improving on A* Hierarchical Pathfinding Other Ideas in Pathfinding Continuous Time Pathfinding Movement Planning Decision Making Overview of Decision Making Decision Trees State Machines Behavior Trees Fuzzy Logic Markov Systems Goal-Oriented Behavior Rule-Based Systems Blackboard Architectures Scripting Action Execution Tactical and Strategic AI Waypoint Tactics Tactical Analyses Tactical Pathfinding Coordinated Action Learning Learning Basics Parameter Modification Action Prediction Decision Learning Naive Bayes Classifiers Decision Tree Learning Reinforcement Learning Artificial Neural Networks Board Games Game Theory Minimaxing Transposition Tables and Memory Memory-Enhanced Test Algorithms Opening Books and Other Set Plays Further Optimizations Turn-Based Strategy Games Supporting Technologies Execution Management Scheduling Anytime Algorithms Level of Detail World Interfacing Communication Getting Knowledge Efficiently Event Managers Polling Stations Sense Management Tools and Content Creation Knowledge for Pathfinding and Waypoint Tactics Knowledge for Movement Knowledge for Decision Making The Toolchain Designing Game AI Designing Game AI The Design Shooters Driving Real-Time Strategy Sports Turn-Based Strategy Games AI-Based Game Genres Teaching Characters Flocking and Herding Games Appendix Books, Periodicals, and Papers Games",2006.0,
f7c39a58f16b9373a3dd9ba2d9f8b2145114cb61,https://www.semanticscholar.org/paper/f7c39a58f16b9373a3dd9ba2d9f8b2145114cb61,Artificial intelligence - a modern myth,What is Artificial Intelligence? the nature of intelligence what is a machine? the possibility of Artificial Intelligence.,1993.0,J. Kelly
3c09f062ac0ba6075893787157dcdd55c54902d3,https://www.semanticscholar.org/paper/3c09f062ac0ba6075893787157dcdd55c54902d3,"Fault Diagnosis: Models, Artificial Intelligence, Applications",1. Introduction.- 2. Models in the diagnostics of processes.- 3. Process diagnostics methodology.- 4. Methods of signal analysis.- 5. Control theory methods in designing diagnostic systems.- 6. Optimal detection observers based on eigenstructure assignment.- 7. Robust H?-optimal synthesis of FDI systems.- 8. Evolutionary methods in designing diagnostic systems.- 9. Artificial neural networks in fault diagnosis.- 10. Parametric and neural network Wiener and Hammerstein models in fault detection and isolation.- 11. Application of fuzzy logic to diagnostics.- 12. Observers and genetic programming in the identification and fault diagnosis of non-linear dynamic systems.- 13. Genetic algorithms in the multi-objective optimisation of fault detection observers.- 14. Pattern recognition approach to fault diagnostics.- 15. Expert systems in technical diagnostics.- 16. Selected methods of knowledge engineering in systems diagnosis.- 17. Methods of acqusition of diagnostic knowledge.- 18. State monitoring algorithms for complex dynamic systems.- 19. Diagnostics of industrial processes in decentralised structures.- 20. Detection and isolation of manoeuvres in adaptive tracking filtering based on multiple model switching.- 21. Detecting and locating leaks in transmission pipelines.- 22. Models in the diagnostics of processes.- 23. Diagnostic systems.,2004.0,"J. Korbicz, J. M. Kóscielny, Z. Kowalczuk, W. Cholewa, Jozef Karbicz"
36873992e4788edc594f4a7f96b935a734322077,https://www.semanticscholar.org/paper/36873992e4788edc594f4a7f96b935a734322077,The Handbook of Artificial Intelligence,A collapsible box like liquid retaining tray. The retaining tray is constructed for example from a cardboard blank which has a rectangular outline having pull panels or handles at the ends for easy stacking. The blank when manufactured into a collapsible tray in the collapsed position is again in the rectangular form for easy stacking and has the exposed pull panels or handles ready for easy grasping whereby the tray can be expanded or erected into a set-up position. The pull panels have notches and tabs which hold the sides of the tray in the expanded position against collapsing. The tray can be used in various institutional environments where a sterile tray is desired for handling various solutions and the like.,1989.0,H. Nii
c24cc066cfcfa83cf5bccd51b2df08cd3d35c70d,https://www.semanticscholar.org/paper/c24cc066cfcfa83cf5bccd51b2df08cd3d35c70d,A Concise Introduction to Multiagent Systems and Distributed Artificial Intelligence,"Multiagent systems is an expanding field that blends classical fields like game theory and decentralized control with modern fields like computer science and machine learning. This monograph provides a concise introduction to the subject, covering the theoretical foundations as well as more recent developments in a coherent and readable manner. The text is centered on the concept of an agent as decision maker. Chapter 1 is a short introduction to the field of multiagent systems. Chapter 2 covers the basic theory of singleagent decision making under uncertainty. Chapter 3 is a brief introduction to game theory, explaining classical concepts like Nash equilibrium. Chapter 4 deals with the fundamental problem of coordinating a team of collaborative agents. Chapter 5 studies the problem of multiagent reasoning and decision making under partial observability. Chapter 6 focuses on the design of protocols that are stable against manipulations by self-interested agents. Chapter 7 provides a short introduction to the rapidly expanding field of multiagent reinforcement learning. The material can be used for teaching a half-semester course on multiagent systems covering, roughly, one chapter per lecture.",2007.0,N. Vlassis
e9204995347f2bc4d35a0cf93f966a909f8424f3,https://www.semanticscholar.org/paper/e9204995347f2bc4d35a0cf93f966a909f8424f3,Expert systems: artificial intelligence in business,Case Study: MYCIN: Varieties of Problem Solving Strategies The Anatomy of a Knowledge Base Anatomy of An Inference Engine MYCIN Reconsidered Languages and Tools for Knowledge Systems A Sampler of Knowledge Systems and Their Architectures How Knowledge Systems are Developed Near Futures: Knowledge Engineering in the Next Five Years Large Scale Knowledge Systems Near Futures: Intelligent Job Aids Not So Near Futures: Research Topics Likely to Bear Fruit in 5 Years or More Not So Near Futures: Intelligent Tutoring Systems Not So Near Futures: Planning and Preparing for the Knowledge Systems Revolution Appendixes,1985.0,"P. Harmon, D. King"
dfdfb131fe182eef70d367e94f22d58b84d72e47,https://www.semanticscholar.org/paper/dfdfb131fe182eef70d367e94f22d58b84d72e47,Artificial intelligence in geography,"From the Publisher: 
The text provides an introduction to expert systems, neural nets, genetic algorithms, smart systems and artificial life and shows how they are likely to transform geographical enquiry. An integral disk provides examples and exercises for the readers to try themselves, which is a major methodological milestone in geography. The authors provide an easy to understand basic introduction to AI relevant to Geography.",1997.0,"Stan Openshaw, Christine Openshaw"
8c0c7d97107cb6fbdf20e08b4accd549637cd0c4,https://www.semanticscholar.org/paper/8c0c7d97107cb6fbdf20e08b4accd549637cd0c4,A Survey on Temporal Reasoning in Artificial Intelligence,"The notion of time is ubiquitous in any activity that requires intelligence. In particular, several important notions like change, causality, action are described in terms of time. Therefore, the representation of time and reasoning about time is of crucial importance for many Arti(cid:12)cial Intelligence systems. Speci(cid:12)cally during the last 10 years, it has been attracting the attention of many AI researchers. In this survey, the results of this work are analysed. Firstly, Temporal Reasoning is de(cid:12)ned. Then, the most important representational issues which determine a Temporal Reasoning approach are introduced: the logical form on which the approach is based, the ontology (the units taken as primitives, the temporal relations, the algorithms that have been developed,...) and the concepts related with reasoning about action (the representation of change, causality, action,...). For each issue the di(cid:11)erent choices in the literature are discussed.",1994.0,Lluís Vila
826af5087384f59ae540068eaf6ed04a8a38a7ee,https://www.semanticscholar.org/paper/826af5087384f59ae540068eaf6ed04a8a38a7ee,Artificial Intelligence and the Design of Expert Systems,"Provides a thorough discussion of AI's theoretical foundations and advanced applications, including expert system design and knowledge-based programming. It is a wealth of advanced AI topics and applications that should appeal to a broad audience.",1990.0,"G. Luger, W. Stubblefield"
b2a68b3998fc523a899781856fad129b9664e0ef,https://www.semanticscholar.org/paper/b2a68b3998fc523a899781856fad129b9664e0ef,Artificial Intelligence for Games,"AI and Games Introduction What Is AI? Model of Game AI Algorithms, Data Structures, and Representations On the Website Layout of the Book Game AI The Complexity Fallacy The Kind of AI in Games Speed and Memory The AI Engine Techniques Movement The Basics of Movement Algorithms Kinematic Movement Algorithms Steering Behaviors Combining Steering Behaviors Predicting Physics Jumping Coordinated Movement Motor Control Movement in the Third Dimension Pathfinding The Pathfinding Graph Dijkstra A* World Representations Improving on A* Hierarchical Pathfinding Other Ideas in Pathfinding Continuous Time Pathfinding Movement Planning Decision Making Overview of Decision Making Decision Trees State Machines Behavior Trees Fuzzy Logic Markov Systems Goal-Oriented Behavior Rule-Based Systems Blackboard Architectures Scripting Action Execution Tactical and Strategic AI Waypoint Tactics Tactical Analyses Tactical Pathfinding Coordinated Action Learning Learning Basics Parameter Modification Action Prediction Decision Learning Naive Bayes Classifiers Decision Tree Learning Reinforcement Learning Artificial Neural Networks Board Games Game Theory Minimaxing Transposition Tables and Memory Memory-Enhanced Test Algorithms Opening Books and Other Set Plays Further Optimizations Turn-Based Strategy Games Supporting Technologies Execution Management Scheduling Anytime Algorithms Level of Detail World Interfacing Communication Getting Knowledge Efficiently Event Managers Polling Stations Sense Management Tools and Content Creation Knowledge for Pathfinding and Waypoint Tactics Knowledge for Movement Knowledge for Decision Making The Toolchain Designing Game AI Designing Game AI The Design Shooters Driving Real-Time Strategy Sports Turn-Based Strategy Games AI-Based Game Genres Teaching Characters Flocking and Herding Games Appendix Books, Periodicals, and Papers Games",2006.0,Ian Millington
da87bb606229daac4c2d0219d2baf4394b64f91d,https://www.semanticscholar.org/paper/da87bb606229daac4c2d0219d2baf4394b64f91d,DEFINING ARTIFICIAL INTELLIGENCE,"1.1 BACKGROUND Calculators are not intelligent. Calculators give the right answers to challenging math problems, but everything they "" know "" is preprogrammed by people. They can never learn anything new, and outside of their limited domain of utility, they have the expertise of a stone. Calculators are able to solve problems entirely because people are already able to solve those same problems. Since the earliest days of computing, we have envisioned machines that could go beyond our own ability to solve problems—intelligent machines. We have generated many computing devices that can solve mathematical problems of enormous complexity, but mainly these too are merely "" calculators. "" They are prepro-grammed to do exactly what we want them to do. They accept input and generate the correct output. They may do it at blazingly fast speeds, but their underlying mechanisms depend on humans having already worked out how to write the programs that control their behavior. The dream of the intelligent machine is the vision of creating something that does not depend on having people preprogram its problem solving behavior. Put another way, artificial intelligence should not seek to merely solve problems, but should rather seek to solve the problem of how to solve problems. Although most scientific disciplines, such as mathematics, physics, chemistry, and biology, are well defined, the field of artificial intelligence (AI) remains enigmatic. This is nothing new. Even 20 years ago, Hofstadter (1985, p. 633) remarked, "" The central problem of AI is the question: What is the letter 'a'? Donald Knuth, on hearing me make this claim once, appended, 'And what is the letter 'i'?'—an amendment that I gladly accept. "" Despite nearly 50 years of research in the field, there is still no widely accepted definition of artificial intelligence. Even more, a discipline of computational intelligence—including research in neural networks, fuzzy systems, and evolutionary computation—has gained prominence as an alternative to AI, mainly because AI has failed to live up to its promises and because many believe that the methods that have been adopted under the old rubric of AI will never succeed. It may be astonishing to find that five decades of research in artificial intelligence have been pursued without fundamentally accepted goals, or even a simple",2005.0,D. Fogel
d27e7ab538a003c8389910f926a7b9b7dbcb27eb,https://www.semanticscholar.org/paper/d27e7ab538a003c8389910f926a7b9b7dbcb27eb,What is Artificial Intelligence? Psychometric AI as an Answer,"We propose an answer to the ""What is AI?"" question, namely, that Al is really (or at least really ought in significant part to be) Psychometric AI (PAI). Along the way, we: set out and rebut five objections to PAI; describe PERI, a robot in our lab who exemplifies PAI; and briefly treat the future of Psychometric AI, first by pointing toward some promising PAI-based applications, and then by raising some of the ""big"" philosophical questions the success of Psychometric AI will raise.",2003.0,"S. Bringsjord, Bettina Schimanski"
6da7b3683ec4f0481d6fbd56942733de579e1b07,https://www.semanticscholar.org/paper/6da7b3683ec4f0481d6fbd56942733de579e1b07,Application of artificial intelligence,"Several of the panel had experience of using a proof checker to help teach logic, proof, and skills in manipulating formulae. Other tools were felt to improve teaching, but only if they had adequate user interfaces. Tools were certainly useful for the enforcement of certain types of discipline; however, the fear was voiced that the use of tools can lead to the overlooking of important insights. To sum up, there was a consensus amongst the session participants that the use of formalism is spreading and that at the next Workshop there will probably be a session reporting more real applications of the techniques that were discussed in these two days.",1988.0,"G. Arango, P. Freeman"
b562bd1359375db833fdc0fda2885ec1ba87f53d,https://www.semanticscholar.org/paper/b562bd1359375db833fdc0fda2885ec1ba87f53d,Semantic Networks in Artificial Intelligence,"Preface, F W Lehmann. Semantic Networks, F W Lehmann. Invited Research Family Summaries. Conceptual dependency and its descendants, S L Lytinen. Conceptual graphs as a universal knowledge representation, J F Sowa. The ECO family, N Cercone et al. The KL-ONE family, W A Woods & J G Schmo ze. NETL and subsequent path-based inheritance theories, R H Thomason. The preference semantics family, Y Wilks & D Fass. T e PSN tribe, J Mylopoulos. The SNePS family, S C Shapiro & W J Rapap rt. Articles. Massively-parallel marker-passing in semantic networks, J A Hendler. Structured connectionist models of semantic networks, L Shastri. Subsumption computed algebraically, C Brink & R A Schmi t. A model of hierarchies based on graph homomorphisms, H Mili & R Rada. G anularity hierarchies, G McCalla et al. Beyond IS-A and PART-WHOLE: More semantic network links, J A Markowitz. Sense and preference, B M Slator. A unification-based, integrated natural language processing system, S L Lytinen. A design approach for constructing engineering scenario maps, R Rucker & T A Aldow isan. A uniform representation for time and space and their mutual constraints, R T Hartley. Exploiting lattices in a theory of space and time, D A Randell & A G Cohn. Completing sort hierarchies, A G Cohn. Concept lattices and conceptual knowledge systems, R Wille. A conceptual space approach to semantic networks, A Hautamaki. Defeasible inheritance: A lattice based approach, L Padgham. Prototypes in a hybrid language with primitive descriptions, E Franconi et al. Introduction to graph grammars with applications to semantic networks, H Ehrig et al. Pattern associativity and the retrieval of semantic networks, R Levinson. Declarative operations on nets, H Boley. The existential graphs, D D Roberts. Valental aspects of Peircean algebraic logic, R W Burch. Foliated semantic networks: concepts, facts, qualities, R Marty. The ""descriptive"" component of a hybrid knowledge representation language, G P Zarri. Saying more with frames: slots as classes, R Nado & R Fikes. UEST: A model of question answering, A C Graesser et al. Index.",1992.0,F. Lehmann
706ae842fb2107f5940cfa39682d52235d269eb7,https://www.semanticscholar.org/paper/706ae842fb2107f5940cfa39682d52235d269eb7,Connectionist Architectures for Artificial Intelligence,"A number of researchers have begun exploring the use of massively parallel architectures in an attempt to get around the limitations of conventional symbol processing. Many of these parallel architectures are connectionist: The system's collection of permanent knowledge is stored as a pattern of connections or connection strengths among the processing elements, so the knowledge directly determines how the processing elements interact rather that sitting passively in a memory, waiting to be looked at by the CPU. Some connectionist schemes use formal, symbolic representations, while others use more analog approaches. Some even develop their own internal representations after seeing examples of the patterns they are to recognize or the relationships they are to store. Connectionism is somewhat controversial in the AI community. It is new, still unproven in large-scale practical applications, and very different in style from the traditional AI approach. The authors have only begun to explore the behavior and potential of connectionist networks. In this article, the authors describe some of the central issues and ideas of connectionism, and also some of the unsolved problems facing this approach. Part of the motivation for connectionist research is the possible similarity in function between connectionist networks and the neutral networksmore » of the human cortex, but they concentrate here on connectionism's potential as a practical technology for building intelligent systems.« less",1990.0,"S. Fahlman, Geoffrey E. Hinton"
a485ce1a80528ce7073436c1f92c0e1ac9774b76,https://www.semanticscholar.org/paper/a485ce1a80528ce7073436c1f92c0e1ac9774b76,Artificial intelligence and scientific method,"Preface Acknowledgements Chapter 1: The Inductivist Controversy, or Bacon versus Popper 1.1 Bacon's Inductivism 1.2 Popper's Falsificationism 1.3 Kepler's Discovery of the Laws of Planetary Motion 1.4 The Discovery of the Sulphonamide Drugs Chapter 2: Machine Learning in the Turing Tradition 2.1 The Turing Tradition 2.2 The Practical Problem: Expert Systems and Feigenbaum's Bottleneck 2.3 Attribute-based Learning, Decision Trees, and Quinlan's ID3 2.4 GOLEM as an example of Relational Learning 2.5 Bratko's summary of the successes of Machine Learning in the Turing Tradition, 1992 2.6 GOLEM's Discovery of a Law of Nature Chapter 3: How Advances in Machine Learning affect the Inductivist Controversy 3.1 Bacon's Example of Heat 3.2 The Importance of Falsification 3.3 Bacon's Method has only recently come to be used 3.4 The Need for Background Knowledge Chapter 4: Logic and Programming and a New Framework for Logic 4.1 The Development of PROLOG 4.2 PROLOG as a Non-Monotonic Logic 4.3 Two Examples of Translations from One Logical System to Another 4.4 Logic = Inference + Control 4.5 PROLOG introduces Control into Deductive Logic 4.6 PROLOG and Certainty. Is Logic a priori or empirical? Chapter 5: Can there be an Inductive Logic? 5.1 The Divergence between Deductive and Inductive Logic (up to the early 1970s) 5.2 Inductive Logic as Inference + Control 5.3 Confirmation Values as Control in a Deductive Logic 5.4 The Empirical Testing of Rival Logics Chapter 6: Do Godel's Incompleteness Theorems place a Limit on Artificial Intelligence? 6.1 Anxieties caused by Advances in AI 6.2 Informal Exposition of Godel's Incompleteness Theorems 6.3 The Lucas Argument 6.4 Objections to the Lucas Argument: i) Possible Limitations on Self-Knowledge 6.5 Objections to the Lucas Argument: ii) Possible Additions of Learning Systems 6.6 Why Advances in Computing are more likely to Stimulate Human Thinking than to Render it Superfluous Notes References Index",1996.0,D. Gillies
f4971ff0f6ae626e78131bafa012eadfe8e238e2,https://www.semanticscholar.org/paper/f4971ff0f6ae626e78131bafa012eadfe8e238e2,Machine learning: an artificial intelligence approach volume III,This book reflects the expansion of machine learning research through presentation of recent advances in the field. The book provides an account of current research directions. Major topics covered include the following: learning concepts and rules from examples; cognitive aspects of learning; learning by analogy; learning by observation and discovery; and an exploration of general aspects of learning.,1990.0,"Yves Kodratoff, R. Michalski"
1577677ce671303cb8a73e176f6646993e050072,https://www.semanticscholar.org/paper/1577677ce671303cb8a73e176f6646993e050072,History of Artificial Intelligence,"Though our discussion is e n t i t l e d ""The History o f A r t i f i c i a l I n te l l i gence"" , i n fac t we are focusing here on one b r i e f but h igh ly s i gn i f i can t moment in that h i s t o r y , the moment when a r t metamorphosed i t s e l f i n to science, from wish and dream to something l i k e r e a l i t y . As you w i l l learn from each of the discussants, th is metamorphosis took place at several locat ions during the ear ly to mid-1950s, and i t s ca ta lys t was the recogni t ion that the computer was the most promising medium yet in which to rea l i ze what had been a human dream since ea r l i es t t imes, the creat ion of man-made, rather than begotten i n te l l i gence .",1977.0,"P. McCorduck, M. Minsky, O. Selfridge, H. Simon"
1fdf7de7a77d3090734a7a0bd196a6bfb2a156a0,https://www.semanticscholar.org/paper/1fdf7de7a77d3090734a7a0bd196a6bfb2a156a0,Uncertainty in Artificial Intelligence,"Two of the most exciting recent developments in my field are single cell omics assays, where omics here currently encompasses genomics, transcriptomics and epigenomics, and the long-read DNA sequencing capability introduced by Oxford Nanopore Technologies. Each leads to lots of interesting data permitting biomedical researchers to address questions previously beyond their reach. Each offers numerous opportunities and challenges to those who wish to, or must dive into the data to help the researchers answer these new questions. And each seems likely to drive theoretical developments in areas of interest to participants in UAI 2017. Ive been lucky enough to be associated with research projects in these two areas. Building on my experience with them, Ill discuss some of the opportunities, challenges and areas for theoretical development that I see. Biographical details Terry Speed completed a BSc (Hons) in mathematics and statistics at the University of Melbourne and a PhD in mathematics and Dip Ed at Monash University. He has held appointments at the University of Sheffield, U.K., the University of Western Australia in Perth, and the University of California at Berkeley, and with the CSIRO in Canberra. In 1997 he took up an appointment with the Walter & Eliza Hall Institute of Medical Research, where he is now an Honorary Fellow and lab head in the Bioinformatics Division. His research interests lie in the application of statistics and bioinformatics to genetics and genomics, and related fields such as proteomics, metabolomics and epigenomics, with a focus on cancer and epigenetics.",1987.0,"Dana S. Nau, Paul Purdom, C. Tzeng, Martin Marietta, General Motors"
f8c1fdc568ee9b8d14e88dd4f5309b6f2fb34690,https://www.semanticscholar.org/paper/f8c1fdc568ee9b8d14e88dd4f5309b6f2fb34690,Neural networks in artificial intelligence,"There is now a substantial body of results in connectionist AI, treating problems in representation, learning, inference, speech, vision, and language. Zeidenberg's book is a collection of 1-5 page summaries of various pieces of work in these areas. If one needs a quick overview of, say, Kohonen's self-organizing feature maps, Grossberg's adaptive resonance theory, or Sejnowski and Rosenberg's NETtalk model, this is a good place to look. But beyond these old standards there is quite a bit more material, such as Hinton and Plaut's work on learning with fast and slow weights, Servan-Schreiber, Cleeremans, and McClelland's experiments in learning finite state automata from examples, and several early efforts in connectionist parsing. Over sixty pieces of work are summarized. The book is somewhat dated now; it covers publications only through 1989, and so in some cases major results have been overlooked, e.g., the chapter on speech recognition makes no mention of time delay neural networks or radial basis function networks. Many of the references are to Cognitive Science (both the conference and the journal), the 1988 Connectionist Models Summer School proceedings, and various university technical reports. A completely up-to-date bibliography would have to include citations to the NIPS (Neural Information Processing Systems) proceedings, and to the journals Neural Networks and Neural Computation, but these were just starting up at the time this book was published.",1990.0,M. Zeidenberg
03eac737955099bf751b8f9862691da17f830685,https://www.semanticscholar.org/paper/03eac737955099bf751b8f9862691da17f830685,Artificial Intelligence in Europe,The implications of AI for existing laws and regulation are considerable. A detailed evaluation of the EU laws and regulations should be carried out in the six areas identified by STOA (Scientific Foresight Unit)2 that may need to be revised or adapted. The EESC opposes the introduction of a form of legal personality for robots or AI. This would hollow out the preventive remedial effect of liability law; a risk of moral hazard arises in both the development and use of AI and it creates opportunities for abuse.,1984.0,W. Bibel
7882bbb050acba8297225a6fdd54ac727cfa7eb3,https://www.semanticscholar.org/paper/7882bbb050acba8297225a6fdd54ac727cfa7eb3,Evolutionary Computation: Toward a New Philosophy of Machine Intelligence (IEEE Press Series on Computational Intelligence),"The Third Edition of this internationally acclaimed publication provides the latest theory and techniques for using simulated evolution to achieve machine intelligence. As a leading advocate for evolutionary computation, the author has successfully challenged the traditional notion of artificial intelligence, which essentially programs human knowledge fact by fact, but does not have the capacity to learn or adapt as evolutionary computation does.",2006.0,Thomas Bäck
5c41252db73e543bc6a4935e5743f6a85055d191,https://www.semanticscholar.org/paper/5c41252db73e543bc6a4935e5743f6a85055d191,"Artificial General Intelligence: Concept, State of the Art, and Future Prospects","Abstract In recent years broad community of researchers has emerged, focusing on the original ambitious goals of the AI field - the creation and study of software or hardware systems with general intelligence comparable to, and ultimately perhaps greater than, that of human beings. This paper surveys this diverse community and its progress. Approaches to defining the concept of Artificial General Intelligence (AGI) are reviewed including mathematical formalisms, engineering, and biology inspired perspectives. The spectrum of designs for AGI systems includes systems with symbolic, emergentist, hybrid and universalist characteristics. Metrics for general intelligence are evaluated, with a conclusion that, although metrics for assessing the achievement of human-level AGI may be relatively straightforward (e.g. the Turing Test, or a robot that can graduate from elementary school or university), metrics for assessing partial progress remain more controversial and problematic.",2009.0,B. Goertzel
ee8087060c85cb25d1d124551f3482b7fb0d3074,https://www.semanticscholar.org/paper/ee8087060c85cb25d1d124551f3482b7fb0d3074,Steps toward Parallel Intelligence,"The origin of artificial intelligence is investigated, based on which the concepts of hybrid intelligence and parallel intelligence are presented. The paradigm shift in Intelligence indicates the ``new normal'' of cyber-social-physical systems (CPSS), in which the system behaviors are guided by Merton's Laws. Thus, the ACP-based parallel intelligence consisting of Artificial societies, Computational experiments and Parallel execution are introduced to bridge the big modeling gap in CPSS.",2016.0,"Feiyue Wang, Xiao Wang, Lingxi Li, Li Li"
8d4a6dc3adbaccdbbca972812cc7c6e1089c1398,https://www.semanticscholar.org/paper/8d4a6dc3adbaccdbbca972812cc7c6e1089c1398,"On Abstract Intelligence: Toward a Unifying Theory of Natural, Artificial, Machinable, and Computational Intelligence","Abstract intelligence is a human enquiry of both natural and artificial intelligence at the reductive embodying levels of neural, cognitive, functional, and logical from the bottom up. This paper describes the taxonomy and nature of intelligence. It analyzes roles of information in the evolution of human intelligence, and the needs for logical abstraction in modeling the brain and natural intelligence. A formal model of intelligence is developed known as the Generic Abstract Intelligence Mode (GAIM), which provides a foundation to explain the mechanisms of advanced natural intelligence such as thinking, learning, and inferences. A measurement framework of intelligent capability of humans and systems is comparatively studied in the forms of intelligent quotient, intelligent equivalence, and intelligent metrics. On the basis of the GAIM model and the abstract intelligence theories, the compatibility of natural and machine intelligence is revealed the compatibility of natural and machine intelligence is revealed in order to investigate into a wide range of paradigms of abstract intelligence such as natural, artificial, machinable intelligence, and their engineering applications.",2009.0,Yingxu Wang
2910099b7a7c555af9f14bfb2bc20e9475d0588f,https://www.semanticscholar.org/paper/2910099b7a7c555af9f14bfb2bc20e9475d0588f,How the body shapes the way we think - a new view on intelligence,"How could the body influence our thinking when it seems obvious that the brain controls the body? In How the Body Shapes the Way We Think, Rolf Pfeifer and Josh Bongard demonstrate that thought is not independent of the body but is tightly constrained, and at the same time enabled, by it. They argue that the kinds of thoughts we are capable of have their foundation in our embodiment -- in our morphology and the material properties of our bodies. This crucial notion of embodiment underlies fundamental changes in the field of artificial intelligence over the past two decades, and Pfeifer and Bongard use the basic methodology of artificial intelligence -- ""understanding by building"" -- to describe their insights. If we understand how to design and build intelligent systems, they reason, we will better understand intelligence in general. In accessible, nontechnical language, and using many examples, they introduce the basic concepts by building on recent developments in robotics, biology, neuroscience, and psychology to outline a possible theory of intelligence. They illustrate applications of such a theory in ubiquitous computing, business and management, and the psychology of human memory. Embodied intelligence, as described by Pfeifer and Bongard, has important implications for our understanding of both natural and artificial intelligence.",2006.0,"R. Pfeifer, J. Bongard"
c31c1fd3e3cdd40c732c9fb0c68e9ba2e2403939,https://www.semanticscholar.org/paper/c31c1fd3e3cdd40c732c9fb0c68e9ba2e2403939,From Natural to Artificial Swarm Intelligence,"From the Publisher: 
This book provides a detailed look at models of social insect behavior and how to apply these models in the design of complex systems. The book shows how these models replace an emphasis on control, preprogramming, and centralization with designs featuring autonomy, emergence, and distributed functioning. These designs are proving immensely flexible and robust, able to adapt quickly to changing environments and to continue functioning even when individual elements fail. In particular, these designs are an exciting approach to the tremendous growth of complexity in software and information. Swarm Intelligence draws on up-to-date research from biology, neuroscience, artificial intelligence, robotics, operations research, and computer graphics, and each chapter is organized around a particular biological example, which is then used to develop an algorithm, a multiagent system, or a group of robots. The book will be an invaluable resource for a broad range of disciplines.",1999.0,"E. Bonabeau, M. Dorigo, G. Theraulaz"
3621a95d623567d8604a3948a9d7110ef28bab9f,https://www.semanticscholar.org/paper/3621a95d623567d8604a3948a9d7110ef28bab9f,Virtual-Intelligence Applications in Petroleum Engineering: Part 1—Artificial Neural Networks,"This is the first article of a three-article series on virtual intelligence and its applications in petroleum and natural gas engineering. In addition to discussing artificial neural networks, the series covers evolutionary programming and fuzzy logic. Intelligent hybrid systems that incorporate an integration of two or more of these paradigms and their application in the oil and gas industry are also discussed in these articles. The intended audience is the petroleum professional who is not quite familiar with virtual intelligence but would like to know more about the technology and its potential. Those with a prior understanding of and experience with the technology should also find the articles useful and informative.",2000.0,S. Mohaghegh
d3664aa939d2e177172d56a044235015be947db4,https://www.semanticscholar.org/paper/d3664aa939d2e177172d56a044235015be947db4,McKibben artificial muscles: pneumatic actuators with biomechanical intelligence,"Reports on the design of a biorobotic actuator. Biological requirements are developed from published reports in the muscle physiology literature whose parameters are extracted and applied in the form of the Hill muscle model. Data from several vertebrate species (rat, frog, cat, and human) are used to evaluate the performance of a McKibben pneumatic actuator. The experimental results show the force-length properties of the actuator are muscle-like, but the force-velocity properties are not. The design of a hydraulic damper with fixed orifices, placed in parallel with the McKibben actuator, is proposed to improve the force-velocity performance. Simulation results of this practical design indicate a significant improvement.",1999.0,"G. Klute, J. Czerniecki, Blake Hannaford"
c43e16a806c86be7397b058fff10bd79c6a321d7,https://www.semanticscholar.org/paper/c43e16a806c86be7397b058fff10bd79c6a321d7,Artificial neural networks in medical diagnosis,"An extensive amount of information is currently available to clinical specialists, ranging from details of clinical symptoms to various types of biochemical data and outputs of imaging devices. Each type of data provides information that must be evaluated and assigned to a particular pathology during the diagnostic process. To streamline the diagnostic process in daily routine and avoid misdiagnosis, artificial intelligence methods (especially computer aided diagnosis and artificial neural networks) can be employed. These adaptive learning algorithms can handle diverse types of medical data and integrate them into categorized outputs. In this paper, we briefly review and discuss the philosophy, capabilities, and limitations of artificial neural networks in medical diagnosis through selected examples.",2013.0,"Filippo Amato, Alberto López, E. Peña-Méndez, P. Vaňhara, A. Hampl, J. Havel"
045c58d4fb18cdf568cc024164862b2b86b94a7d,https://www.semanticscholar.org/paper/045c58d4fb18cdf568cc024164862b2b86b94a7d,What computers still can't do: a critique of artificial reason,"From the Publisher: 
When it was first published in 1972, Hubert Dreyfus's manifesto on the inherent inability of disembodied machines to mimic higher mental functions caused an uproar in the artificial intelligence community. The world has changed since then. Today it is clear that ""good old-fashioned AI,"" based on the idea of using symbolic representations to produce general intelligence, is in decline (although several believers still pursue its pot of gold), and the focus of the Al community has shifted to more complex models of the mind. It has also become more common for AI researchers to seek out and study philosophy. For this edition of his now classic book, Dreyfus has added a lengthy new introduction outlining these changes and assessing the paradigms of connectionism and neural networks that have transformed the field. 
At a time when researchers were proposing grand plans for general problem solvers and automatic translation machines, Dreyfus predicted that they would fail because their conception of mental functioning was naive, and he suggested that they would do well to acquaint themselves with modern philosophical approaches to human beings. What Computers Can't Do was widely attacked but quietly studied. Dreyfus's arguments are still provocative and focus our attention once again on what it is that makes human beings unique. 
Hubert L. Dreyfus, who is Professor of Philosophy at the University of California, Berkeley, is also the author of Being-in-the-World. A Commentary on Heidegger's Being and Time, Division I.",1992.0,H. Dreyfus
b227f3e4c0dc96e5ac5426b85485a70f2175a205,https://www.semanticscholar.org/paper/b227f3e4c0dc96e5ac5426b85485a70f2175a205,Representation Learning with Contrastive Predictive Coding,"While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.",2018.0,"Aäron van den Oord, Yazhe Li, O. Vinyals"
13bc4e683075bdd6a3f0155241c276a772d4aa06,https://www.semanticscholar.org/paper/13bc4e683075bdd6a3f0155241c276a772d4aa06,Generative adversarial networks,"Generative adversarial networks are a kind of artificial intelligence algorithm designed to solve the generative modeling problem. The goal of a generative model is to study a collection of training examples and learn the probability distribution that generated them. Generative Adversarial Networks (GANs) are then able to generate more examples from the estimated probability distribution. Generative models based on deep learning are common, but GANs are among the most successful generative models (especially in terms of their ability to generate realistic high-resolution images). GANs have been successfully applied to a wide variety of tasks (mostly in research settings) but continue to present unique challenges and research opportunities because they are based on game theory while most other approaches to generative modeling are based on optimization.",2014.0,"I. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, Yoshua Bengio"
d422df8bff4e677a3077635db116679d25142bfc,https://www.semanticscholar.org/paper/d422df8bff4e677a3077635db116679d25142bfc,"Machine learning: Trends, perspectives, and prospects","Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today’s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.",2015.0,"Michael I. Jordan, T. Mitchell"
97efafdb4a3942ab3efba53ded7413199f79c054,https://www.semanticscholar.org/paper/97efafdb4a3942ab3efba53ded7413199f79c054,Reinforcement Learning: An Introduction,"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.",1998.0,"R. S. Sutton, A. Barto"
8b9987663e58e844af1a65a8fa1c5380485a0692,https://www.semanticscholar.org/paper/8b9987663e58e844af1a65a8fa1c5380485a0692,The organization of the human cerebral cortex estimated by intrinsic functional connectivity,"B. T. Thomas Yeo,* Fenna M. Krienen,* Jorge Sepulcre, Mert R. Sabuncu, Danial Lashkari, Marisa Hollinshead, Joshua L. Roffman, Jordan W. Smoller, Lilla Zöllei, Jonathan R. Polimeni, Bruce Fischl, Hesheng Liu, and Randy L. Buckner Harvard University Department of Psychology, Center for Brain Science, Cambridge; Athinoula A. Martinos Center for Biomedical Imaging, Department of Radiology, Massachusetts General Hospital, Charlestown; Howard Hughes Medical Institute, Cambridge; Computer Science and Artificial Intelligence Lab, Massachusetts Institute of Technology, Cambridge; Department of Psychiatry, Massachusetts General Hospital, Boston; Center for Human Genetics Research, Massachusetts General Hospital, Boston; and Massachusetts Institute of Technology-Harvard Division of Health Sciences and Technology, Cambridge, Massachusetts",2011.0,"B. Yeo, Fenna M. Krienen, J. Sepulcre, M. Sabuncu, D. Lashkari, Marisa O. Hollinshead, J. Roffman, J. Smoller, Lilla Zöllei, J. Polimeni, B. Fischl, Hesheng Liu, R. Buckner"
b07ce649d6f6eb636872527104b0209d3edc8188,https://www.semanticscholar.org/paper/b07ce649d6f6eb636872527104b0209d3edc8188,Pattern classification and scene analysis,"Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis.",1974.0,"R. Duda, P. Hart"
c2c2e912baaa0abb14d980e482f6ed393112b266,https://www.semanticscholar.org/paper/c2c2e912baaa0abb14d980e482f6ed393112b266,Artificial bee colony algorithm,"INTRODUCTION Nature-inspired computational methodologies and approaches are one of the favorite research topics for both researchers and academic studies in recent years. They're very effective tools for solving complex problems of the real world applications. Most of these algorithms are based on swarm intelligence. Artificial bee colony algorithm is one of the recent and very popular swarm based algorithm mimicking the behavior of the bees. Shortly after introduced, many studies are introduced based on both continuous and discrete optimization problems. The Artificial Bee Colony (ABC) algorithm is a recently introduced optimization algorithm which simulates the foraging behavior of a bee colony which was proposed by Karaboga (2005) for real-parameter optimization. The algorithm is developed by inspecting the behaviors of reel bees on finding nectar amounts and sharing this information of food sources to other bees in their hive. It's one of the swarm-based algorithms like ants, birds and fishes in which individual units perform collective behavior. The aim of this study is to present a bibliographical base to the researchers and enlighten them with ABC algorithm. The main topic of this paper is to give an extensive literature survey of the algorithm and its application areas. In the first section swarm intelligence will be studied. After the properties of swarm intelligence, bee based algorithms will be given. Following this section, artificial bee colony algorithm is briefly described and an extensive literature study is given. Further study notes and conclusion finalize the paper.",2010.0,D. Karaboğa
2ceab7e0f9d28d78c89fcf498f189b2882e8bff9,https://www.semanticscholar.org/paper/2ceab7e0f9d28d78c89fcf498f189b2882e8bff9,Reasoning about knowledge,"Reasoning about knowledge—particularly the knowledge of agents who reason about the world and each other's knowledge—was once the exclusive province of philosophers and puzzle solvers. More recently, this type of reasoning has been shown to play a key role in a surprising number of contexts, from understanding conversations to the analysis of distributed computer algorithms. Reasoning About Knowledge is the first book to provide a general discussion of approaches to reasoning about knowledge and its applications to distributed systems, artificial intelligence, and game theory. It brings eight years of work by the authors into a cohesive framework for understanding and analyzing reasoning about knowledge that is intuitive, mathematically well founded, useful in practice, and widely applicable. The book is almost completely self-contained and should be accessible to readers in a variety of disciplines, including computer science, artificial intelligence, linguistics, philosophy, cognitive science, and game theory. Each chapter includes exercises and bibliographic notes.",1995.0,Ronald Fagin
d1b4a4689a0288ccf36158ba5dfef724fd5a4ea5,https://www.semanticscholar.org/paper/d1b4a4689a0288ccf36158ba5dfef724fd5a4ea5,Factor graphs and the sum-product algorithm,"Algorithms that must deal with complicated global functions of many variables often exploit the manner in which the given functions factor as a product of ""local"" functions, each of which depends on a subset of the variables. Such a factorization can be visualized with a bipartite graph that we call a factor graph, In this tutorial paper, we present a generic message-passing algorithm, the sum-product algorithm, that operates in a factor graph. Following a single, simple computational rule, the sum-product algorithm computes-either exactly or approximately-various marginal functions derived from the global function. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative ""turbo"" decoding algorithm, Pearl's (1988) belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform (FFT) algorithms.",2001.0,"F. Kschischang, B. Frey, Hans-Andrea Loeliger"
1b5b3a5e052c96f591f19569ca29b972ab1f5738,https://www.semanticscholar.org/paper/1b5b3a5e052c96f591f19569ca29b972ab1f5738,The Cognitive Structure of Emotions,"More than 30 years after its initial publication, this new edition of The Cognitive Structure of Emotions refines and updates Ortony, Clore, and Collins's OCC model of emotions. Starting from a three-way classification of construals of the world––events, the attribution of responsibility for events, and objects––the authors propose a systematic account of emotion differentiation. Rejecting the oft-favored features of bodily feelings, emotion-related behaviors, and facial expressions as too intensity-dependent and insufficiently diagnostic, they provide a detailed analysis of emotion differentiation in terms of the cognitive underpinnings of emotion types. Using numerous examples, they explain how different variables influence emotion intensity, and show how emotions can be formalized for computational purposes. Now with a contributed chapter describing the OCC model's influence, this book will interest a wide audience in cognitive, clinical, and social psychology, as well as in artificial intelligence and affective computing, and other cognitive science disciplines.",1988.0,"A. Ortony, G. Clore, A. Collins"
36fd42195d46bdc0dc94327f66846505e979e25d,https://www.semanticscholar.org/paper/36fd42195d46bdc0dc94327f66846505e979e25d,Deep Reinforcement Learning: A Brief Survey,"Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higherlevel understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.",2017.0,"Kai Arulkumaran, M. Deisenroth, Miles Brundage, A. Bharath"
69cfb48c45b59243d60342b796dbac35e9efd6bc,https://www.semanticscholar.org/paper/69cfb48c45b59243d60342b796dbac35e9efd6bc,Toward principles for the design of ontologies used for knowledge sharing?,"Recent work in Artificial Intelligence is exploring the use of formal ontologies as a way of specifying content-specific agreements for the sharing and reuse of knowledge among software entities. We take an engineering perspective on the development of such ontologies. Formal ontologies are viewed as designed artifacts, formulated for specific purposes and evaluated against objective design criteria. We describe the role of ontologies in supporting knowledge sharing activities, and then present a set of criteria to guide the development of ontologies for these purposes. We show how these criteria are applied in case studies from the design of ontologies for engineering mathematics and bibliographic data. Selected design decisions are discussed, and alternative representation choices and evaluated against the design criteria.",1995.0,T. Gruber
707c4efd5452de0ef4f3806f8f90529b41f995bd,https://www.semanticscholar.org/paper/707c4efd5452de0ef4f3806f8f90529b41f995bd,An Introduction to MultiAgent Systems,"The study of multi-agent systems (MAS) focuses on systems in which many intelligent agents interact with each other. These agents are considered to be autonomous entities such as software programs or robots. Their interactions can either be cooperative (for example as in an ant colony) or selfish (as in a free market economy). This book assumes only basic knowledge of algorithms and discrete maths, both of which are taught as standard in the first or second year of computer science degree programmes. A basic knowledge of artificial intelligence would useful to help understand some of the issues, but is not essential. The books main aims are: To introduce the student to the concept of agents and multi-agent systems, and the main applications for which they are appropriate To introduce the main issues surrounding the design of intelligent agents To introduce the main issues surrounding the design of a multi-agent society To introduce a number of typical applications for agent technology",2002.0,Barbara Messing
d621786b597687f555fae83dc1a021fd21713d90,https://www.semanticscholar.org/paper/d621786b597687f555fae83dc1a021fd21713d90,Intelligent agents: theory and practice,"Abstract The concept of an agent has become important in both artificial intelligence (AT) and mainstream computer science. Our aim in this paper is to point the reader at what we perceive to be the most important theoretical and practical issues associated with the design and construction of intelligent agents. For convenience, we divide these issues into three areas (though as the reader will see, the divisions are at times somewhat arbitrary). Agent theory is concerned with the question of what an agent is, and the use of mathematical formalisms for representing and reasoning about the properties of agents. Agent architectures can be thought of as software engineering models of agents; researchers in this area are primarily concerned with the problem of designing software or hardware systems that will satisfy the properties specified by agent theorists. Finally, agent languages are software systems for programming and experimenting with agents; these languages may embody principles proposed by theorists. The paper is not intended to serve as a tutorial introduction to all the issues mentioned; we hope instead simply to identify the most important issues, and point to work that elaborates on them. The article includes a short review of current and potential applications of agent technology.",1995.0,"M. Wooldridge, N. Jennings"
92ebd4d0e7b8e039400bd01dd81daf07e13a613f,https://www.semanticscholar.org/paper/92ebd4d0e7b8e039400bd01dd81daf07e13a613f,The Modularity of mind. An essay on faculty psychology,This monograph synthesizes current information from the various fields of cognitive science in support of a new theory of mind. Most psychologists study horizontal processes like memory. Fodor postulates a vertical and modular psychological organization underlying biologically coherent behaviours. This view of mental architecture is consistent with the historical tradition of faculty psychology while integrating a computational approach to mental processes. One of the most notable aspects of Fodor’s work is that it articulates features not only of speculative cognitive architecture but also of current research in artificial intelligence. – Part I. Four accounts of mental structure; – Part II. A functional taxonomy of cognitive mechanisms; – Part III. Input systems as modules; – Part IV. Central systems; – Part V. Caveats and conclusions. M.-M. V.,1986.0,J. Fodor
3a618c6e8fec20c578e402cd1af3400a73e77d35,https://www.semanticscholar.org/paper/3a618c6e8fec20c578e402cd1af3400a73e77d35,Specific impairments of planning.,"An information-processing model is outlined that predicts that performance on non-routine tasks can be impaired independently of performance on routine tasks. The model is related to views on frontal lobe functions, particularly those of Luria. Two methods of obtaining more rigorous tests of the model are discussed. One makes use of ideas from artificial intelligence to derive a task heavily loaded on planning abilities. A group of patients with left anterior lesions has a specific deficit on the task. Subsidiary investigations support the inference that this is a planning impairment.",1982.0,T. Shallice
86cff4d050beb90fed2e1ceac8940c8221b120aa,https://www.semanticscholar.org/paper/86cff4d050beb90fed2e1ceac8940c8221b120aa,Mind over Machine: The Power of Human Intuition and Expertise in the Era of the Computer,"Computers are being used more and more in all aspects of our lives and, programmed correctly, they are more accurate and precise than humans can ever be. Here, however, the myth of the superiority of artificial intelligence is examined and dispelled. The authors, one a philosopher and the other a computer scientist, argue that even highly advanced systems only correspond to the very early stages of human learning and that there are many human skills that computers will never be able to emulate. The mind will always be superior to the machine. To illustrate their point, they set forth a model documenting five distinct levels - novice, advanced beginner, competent, proficient and expert - through which human beings pass in acquiring and mastering a skill. The two final stages require a degree of intuitive intelligence far beyond the most ambitious projects being planned for the future. The authors acknowledge the huge progress made by computers and the massive advantages to be gained from using them, but they stress that their value can only lie in their use as aids, never as substitutes for the human mind.",1987.0,"H. Dreyfus, S. Dreyfus, L. Zadeh"
8d115c3b2ee80e0754360a154a9369bc1658b607,https://www.semanticscholar.org/paper/8d115c3b2ee80e0754360a154a9369bc1658b607,Obtaining Well Calibrated Probabilities Using Bayesian Binning,"Learning probabilistic predictive models that are well calibrated is critical for many prediction and decision-making tasks in artificial intelligence. In this paper we present a new non-parametric calibration method called Bayesian Binning into Quantiles (BBQ) which addresses key limitations of existing calibration methods. The method post processes the output of a binary classification algorithm; thus, it can be readily combined with many existing classification algorithms. The method is computationally tractable, and empirically accurate, as evidenced by the set of experiments reported here on both real and simulated datasets.",2015.0,"Mahdi Pakdaman Naeini, G. Cooper, M. Hauskrecht"
f46714d200d69eb9cb5cce176297b89a3f5e3a2c,https://www.semanticscholar.org/paper/f46714d200d69eb9cb5cce176297b89a3f5e3a2c,An Introduction to Convolutional Neural Networks,"The field of machine learning has taken a dramatic twist in recent times, with the rise of the Artificial Neural Network (ANN). These biologically inspired computational models are able to far exceed the performance of previous forms of artificial intelligence in common machine learning tasks. One of the most impressive forms of ANN architecture is that of the Convolutional Neural Network (CNN). CNNs are primarily used to solve difficult image-driven pattern recognition tasks and with their precise yet simple architecture, offers a simplified method of getting started with ANNs. 
This document provides a brief introduction to CNNs, discussing recently published papers and newly formed techniques in developing these brilliantly fantastic image recognition models. This introduction assumes you are familiar with the fundamentals of ANNs and machine learning.",2015.0,"K. O’Shea, Ryan Nash"
3a3c694410c50a8b6bbf16b2dbe4b80a980248d7,https://www.semanticscholar.org/paper/3a3c694410c50a8b6bbf16b2dbe4b80a980248d7,Decision Support and Business Intelligence Systems (8th Edition),"Decision Support and Business Intelligence Systems 9e provides the only comprehensive, up-to-date guide to today's revolutionary management support system technologies, and showcases how they can be used for better decision-making. KEY TOPICS: Decision Support Systems and Business Intelligence. Decision Making, Systems, Modeling, and Support. Decision Support Systems Concepts, Methodologies, and Technologies: An Overview. Modeling and Analysis. Data Mining for Business Intelligence. Artificial Neural Networks for Data Mining. Text and Web Mining. Data Warehousing. Collaborative Computer-Supported Technologies and Group Support Systems. Knowledge Management. Artificial Intelligence and Expert Systems. Advanced Intelligent Systems. Management Support Systems: Emerging Trends and Impacts. Ideal for practicing managers interested in the foundations and applications of BI, group support systems (GSS), knowledge management, ES, data mining, intelligent agents, and other intelligent systems.",2006.0,"E. Turban, Jay E. Aronson, Ting-Peng Liang, R. Sharda"
f527bacebbbfc46719d50eed61a8b2114a39bb72,https://www.semanticscholar.org/paper/f527bacebbbfc46719d50eed61a8b2114a39bb72,The Book of Why: The New Science of Cause and Effect,"This week on the Science podcast, Judea Pearl and Dana Mackenzie discuss strategies for causal thinking and describe its implications for artificial intelligence.",2018.0,Mélanie Frappier
0ee392ad467b967c0a32d8ecb19fc20f7c1d62fe,https://www.semanticscholar.org/paper/0ee392ad467b967c0a32d8ecb19fc20f7c1d62fe,Probabilistic Inference Using Markov Chain Monte Carlo Methods,"Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The “Metropolis algorithm” has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of “Gibbs sampling” has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the “hybrid Monte Carlo” method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of “simulated annealing”, and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.",2011.0,Radford M. Neal
574449170f293dfa868771e9ee0403b56a19b9e9,https://www.semanticscholar.org/paper/574449170f293dfa868771e9ee0403b56a19b9e9,Gender,". The article presents artistic projects in which the issue of gender is actualized. The above was carried out from the position of post-humanist ideas through the study of the impact of the development of technologies on modern socio-cultural processes. Thus, the projects created in different genres were analyzed, including post-cinema (“Night Walk for Edinburgh”, “A Total Jizzfest”) aimed at chang-ing gender perceptions in society, Ellen Pearlman’s opera “Emotionally intelligent” Artificially Intelligent Brainwave Opera” (AIBO), in which, thanks to the involvement of artificial intelligence as an actor, gender opposition is reinforced. The Female Laptop Orchestra (FLO) project is dedicated to the support of female creativity, which, thanks to the involvement of information and communication tools, com-bines music creation with research in its activity. An important aspect of the considered projects is defined as interactivity, which is im-plemented by involving the viewer in the process of their creation. It was found that, on the one hand, the digital virtual space made it possible to reduce the importance of social roles or gender affili-ation, thereby leveling the stereotype of the opposition between the author’s masculinity and femininity. On the other hand, with the development of machine learning technologies, a new opposition between the human author and the artificial intelligence in the same ca-pacity is gradually emerging.",2019.0,A. Hood
18a93dc1558bf9d7534d0b416633cebaf75c1145,https://www.semanticscholar.org/paper/18a93dc1558bf9d7534d0b416633cebaf75c1145,Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,"Significance Learning biological properties from sequence data is a logical step toward generative and predictive artificial intelligence for biology. Here, we propose scaling a deep contextual language model with unsupervised learning to sequences spanning evolutionary diversity. We find that without prior knowledge, information emerges in the learned representations on fundamental properties of proteins such as secondary structure, contacts, and biological activity. We show the learned representations are useful across benchmarks for remote homology detection, prediction of secondary structure, long-range residue–residue contacts, and mutational effect. Unsupervised representation learning enables state-of-the-art supervised prediction of mutational effect and secondary structure and improves state-of-the-art features for long-range contact prediction. In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.",2019.0,"Alexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C. L. Zitnick, Jerry Ma, R. Fergus"
c1e48526eddd68b5bf98739a578ab69a009f570d,https://www.semanticscholar.org/paper/c1e48526eddd68b5bf98739a578ab69a009f570d,Word sense disambiguation: A survey,"Word sense disambiguation (WSD) is the ability to identify the meaning of words in context in a computational manner. WSD is considered an AI-complete problem, that is, a task whose solution is at least as hard as the most difficult problems in artificial intelligence. We introduce the reader to the motivations for solving the ambiguity of words and provide a description of the task. We overview supervised, unsupervised, and knowledge-based approaches. The assessment of WSD systems is discussed in the context of the Senseval/Semeval campaigns, aiming at the objective evaluation of systems participating in several different disambiguation tasks. Finally, applications, open problems, and future directions are discussed.",2009.0,Roberto Navigli
6981ea66000e2c98f8a81f4bef05802234d986a4,https://www.semanticscholar.org/paper/6981ea66000e2c98f8a81f4bef05802234d986a4,Rough sets,"Rough set theory, introduced by Zdzislaw Pawlak in the early 1980s [11, 12], is a new mathematical tool to deal with vagueness and uncertainty. This approach seems to be of fundamental importance to artificial intelligence (AI) and cognitive sciences, especially in the areas of machine learning, knowledge acquisition, decision analysis, knowledge discovery from databases, expert systems, decision support systems, inductive reasoning, and pattern recognition.",1995.0,"Z. Pawlak, J. Grzymala-Busse, R. Słowiński, W. Ziarko"
9e938cf8a74c2e7415bfd10d66a9ea4b1c3e0e15,https://www.semanticscholar.org/paper/9e938cf8a74c2e7415bfd10d66a9ea4b1c3e0e15,Stanley: The robot that won the DARPA Grand Challenge,"This article describes the robot Stanley, which won the 2005 DARPA Grand Challenge. Stanley was developed for high‐speed desert driving without manual intervention. The robot's software system relied predominately on state‐of‐the‐art artificial intelligence technologies, such as machine learning and probabilistic reasoning. This paper describes the major components of this architecture, and discusses the results of the Grand Challenge race. © 2006 Wiley Periodicals, Inc.",2006.0,"S. Thrun, Michael Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, J. Diebel, Philip W. Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, Kenny Lau, Celia M. Oakley, Mark Palatucci, V. Pratt, Pascal Stang, Sven Strohband, Cedric Dupont, Lars-Erik Jendrossek, Christian Koelen, Charles Markey, Carlo Rummel, Joe van Niekerk, Eric Jensen, Philippe Alessandrini, Gary R. Bradski, Bob Davies, Scott Ettinger, A. Kaehler, A. Nefian, Pamela Mahoney"
d7701e78e0bfc92b03a89582e80cfb751ac03f26,https://www.semanticscholar.org/paper/d7701e78e0bfc92b03a89582e80cfb751ac03f26,Explaining Explanations: An Overview of Interpretability of Machine Learning,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.",2018.0,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, Lalana Kagal"
83040001210751239553269727b9ea53e152af71,https://www.semanticscholar.org/paper/83040001210751239553269727b9ea53e152af71,Building Machines that Learn and Think Like People,"Recent successes in artificial intelligence and machine learning have been largely driven by methods for sophisticated pattern recognition, including deep neural networks and other data-intensive methods. But human intelligence is more than just pattern recognition. And no machine system yet built has anything like the flexible, general-purpose commonsense grasp of the world that we can see in even a one-year-old human infant. I will consider how we might capture the basic learning and thinking abilities humans possess from early childhood, as one route to building more human-like forms of machine learning and thinking. At the heart of human common sense is our ability to model the physical and social environment around us: to explain and understand what we see, to imagine things we could see but haven't yet, to solve problems and plan actions to make these things real, and to build new models as we learn more about the world. I will focus on our recent work reverse-engineering these capacities using methods from probabilistic programming, program induction and program synthesis, which together with deep learning methods and video game simulation engines, provide a toolkit for the joint enterprise of modeling human intelligence and making AI systems smarter in more human-like ways.",2018.0,J. Tenenbaum
1f63c4d5c8049d84e90b666f3c7cd5cf7ce513d7,https://www.semanticscholar.org/paper/1f63c4d5c8049d84e90b666f3c7cd5cf7ce513d7,The conscious mind: in search of a fundamental theory,"I. PRELIMINARIES 1. Two Concepts of Mind 2. Supervenience and Explanation II. THE IRREDUCIBILITY OF CONSCIOUSNESS 3. Can Consciousness be Reductively Explained? 4. Naturalistic Dualism 5. The Paradox of Phenomenal Judgment III. TOWARD A THEORY OF CONSCIOUSNESS 6. The Coherence between Consciousness and Cognition 7. Absent Qualia, Fading Qualia, Dancing Qualia 8. Consciousness and Information: Some Speculation IV. APPLICATIONS 9. Strong Artificial Intelligence 10. The Interpretation of Quantum Mechanics Notes Bibliography",1996.0,D. Chalmers
c7822d12beec5cfa81c48ced04d34d4ecf2654b5,https://www.semanticscholar.org/paper/c7822d12beec5cfa81c48ced04d34d4ecf2654b5,Sensorless vector and direct torque control,"1. Introduction 2. The space-phasor model of A.C. machines 3. Vector and direct torque control of synchronous machines 4. Vector and direct torque control of induction machines 5. Torque control of switched reluctance motors 6. Effects of magnetic saturation 7. Artificial intelligence-based steady-state and transient analysis of electrical machines, estimators 8. Self-commissioning Index",1998.0,P. Vas
aa245959d84734f92d1e8f179417eb7226868e62,https://www.semanticscholar.org/paper/aa245959d84734f92d1e8f179417eb7226868e62,Fog and IoT: An Overview of Research Opportunities,"Fog is an emergent architecture for computing, storage, control, and networking that distributes these services closer to end users along the cloud-to-things continuum. It covers both mobile and wireline scenarios, traverses across hardware and software, resides on network edge but also over access networks and among end users, and includes both data plane and control plane. As an architecture, it supports a growing variety of applications, including those in the Internet of Things (IoT), fifth-generation (5G) wireless systems, and embedded artificial intelligence (AI). This survey paper summarizes the opportunities and challenges of fog, focusing primarily in the networking context of IoT.",2016.0,"M. Chiang, Tao Zhang"
84fb458d203612139cee7df732bcf2b196577a45,https://www.semanticscholar.org/paper/84fb458d203612139cee7df732bcf2b196577a45,The Singularity Is Near: When Humans Transcend Biology,"A radical and optimistic view of the future course of human development from Ray Kurzweil, whom Bill Gates calls ""the best person I know at predicting the future of artificial intelligence.""",2006.0,Raymond C. Kurzweil
9957817bf8ee72a72768cb4303ddcfc9ed1227b6,https://www.semanticscholar.org/paper/9957817bf8ee72a72768cb4303ddcfc9ed1227b6,Neural networks for short-term load forecasting: a review and evaluation,"Load forecasting has become one of the major areas of research in electrical engineering, and most traditional forecasting models and artificial intelligence techniques have been tried out in this task. Artificial neural networks (NNs) have lately received much attention, and a great number of papers have reported successful experiments and practical tests with them. Nevertheless, some authors remain skeptical, and believe that the advantages of using NNs in forecasting have not been systematically proved yet. In order to investigate the reasons for such skepticism, this review examines a collection of papers (published between 1991 and 1999) that report the application of NNs to short-term load forecasting. Our aim is to help to clarify the issue, by critically evaluating the ways in which the NNs proposed in these papers were designed and tested.",2001.0,"H. S. Hippert, C. E. Pedreira, R. Souza"
f5fc9fac4403b5fdcbe1b04f61719e0805591cc4,https://www.semanticscholar.org/paper/f5fc9fac4403b5fdcbe1b04f61719e0805591cc4,Decision support systems and intelligent systems,"From the Publisher: 
Widely hailed for its contemporary, cutting-edge perspective, this comprehensive, reader-friendly text covers the latest decision support theories and practices used by managers and organizations. Current examples and cases are drawn from actual organizations and firms. Decision Making, Systems, Modeling, and Support. Data Warehousing, Access, Analysis, Mining, and Visualization. Modeling and Analysis. Decision Support System Development. Collaborative Computing Technologies: Group Support Systems. Enterprise Decision Support Systems. Knowledge Management. Artificial Intelligence and Expert Systems. Knowledge Acquisition and Validation. Knowledge Representation. Inference Techniques. Intelligent Systems Development. Neural Computing Applications, and Advanced Artificial Intelligent Systems and Applications. Intelligent Software Agents and Creativity. Implementing and Integrating Management Support Systems. Organizational and Societal Impacts of Management Support Systems. For managers interested in Decision Support Systems, Computerized Decision Making, and Management Support Systems.",1997.0,"E. Turban, J. Aronson"
f156ecbbb9243522275490d698c6825f4d2e01af,https://www.semanticscholar.org/paper/f156ecbbb9243522275490d698c6825f4d2e01af,Explainable AI: A Review of Machine Learning Interpretability Methods,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.",2020.0,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis"
5ed59f49c1bb7de06cfa2a9467d5efb535103277,https://www.semanticscholar.org/paper/5ed59f49c1bb7de06cfa2a9467d5efb535103277,Temporal difference learning and TD-Gammon,"Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome.",1995.0,G. Tesauro
193564e4108396531dbfd70d37492559eb674b99,https://www.semanticscholar.org/paper/193564e4108396531dbfd70d37492559eb674b99,The creative mind : myths & mechanisms,"The author of Artificial Intelligence and Natural Man uses insights from computer science to explore the mystery of human creativity. Boden outlines the thought processes and mental structures in which our creativity is grounded, and explains how all different kinds of creativity spring from the same cognitive devices and mental maps. Illustrated.",1991.0,M. Boden
f735c6086476893c6a0faefe148a43e97caba311,https://www.semanticscholar.org/paper/f735c6086476893c6a0faefe148a43e97caba311,"6G Wireless Communication Systems: Applications, Requirements, Technologies, Challenges, and Research Directions","The demand for wireless connectivity has grown exponentially over the last few decades. Fifth-generation (5G) communications, with far more features than fourth-generation communications, will soon be deployed worldwide. A new paradigm of wireless communication, the sixth-generation (6G) system, with the full support of artificial intelligence, is expected to be implemented between 2027 and 2030. Beyond 5G, some fundamental issues that need to be addressed are higher system capacity, higher data rate, lower latency, higher security, and improved quality of service (QoS) compared to the 5G system. This paper presents the vision of future 6G wireless communication and its network architecture. This article describes emerging technologies such as artificial intelligence, terahertz communications, wireless optical technology, free-space optical network, blockchain, three-dimensional networking, quantum communications, unmanned aerial vehicles, cell-free communications, integration of wireless information and energy transfer, integrated sensing and communication, integrated access-backhaul networks, dynamic network slicing, holographic beamforming, backscatter communication, intelligent reflecting surface, proactive caching, and big data analytics that can assist the 6G architecture development in guaranteeing the QoS. Besides, expected applications with 6G communication requirements and possible technologies are presented. We also describe potential challenges and research directions for achieving this goal.",2019.0,"M. Z. Chowdhury, M. Shahjalal, Shakil Ahmed, Y. Jang"
fd882a24391a05bf4cf92e8259101f7944d88a56,https://www.semanticscholar.org/paper/fd882a24391a05bf4cf92e8259101f7944d88a56,Deep neural networks: a new framework for modelling biological vision and brain information processing,"Recent advances in neural network modelling have enabled major strides in computer vision and other artificial intelligence applications. Human-level visual recognition abilities are coming within reach of artificial systems. Artificial neural networks are inspired by the brain and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals and not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains find surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build neurobiologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.",2015.0,N. Kriegeskorte
4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d,https://www.semanticscholar.org/paper/4cd03cd34e7e94d1b1ee293d5dead8efc24c1a6d,Convergence of Edge Computing and Deep Learning: A Comprehensive Survey,"Ubiquitous sensors and smart devices from factories and communities are generating massive amounts of data, and ever-increasing computing power is driving the core of computation and services from the cloud to the edge of the network. As an important enabler broadly changing people’s lives, from face recognition to ambitious smart factories and cities, developments of artificial intelligence (especially deep learning, DL) based applications and services are thriving. However, due to efficiency and latency issues, the current cloud computing service architecture hinders the vision of “providing artificial intelligence for every person and every organization at everywhere”. Thus, unleashing DL services using resources at the network edge near the data sources has emerged as a desirable solution. Therefore, edge intelligence, aiming to facilitate the deployment of DL services by edge computing, has received significant attention. In addition, DL, as the representative technique of artificial intelligence, can be integrated into edge computing frameworks to build intelligent edge for dynamic, adaptive edge maintenance and management. With regard to mutually beneficial edge intelligence and intelligent edge, this paper introduces and discusses: 1) the application scenarios of both; 2) the practical implementation methods and enabling technologies, namely DL training and inference in the customized edge computing framework; 3) challenges and future trends of more pervasive and fine-grained intelligence. We believe that by consolidating information scattered across the communication, networking, and DL areas, this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of edge intelligence and intelligent edge, i.e., Edge DL.",2019.0,"Yiwen Han, Xiaofei Wang, Victor C. M. Leung, D. Niyato, Xueqiang Yan, Xu Chen"
8c584c4820e615aaf3c40a6737315c712ecd6927,https://www.semanticscholar.org/paper/8c584c4820e615aaf3c40a6737315c712ecd6927,Formalising Trust as a Computational Concept,"Trust is a judgement of unquestionable utility — as humans we use it every day of our lives. However, trust has suffered from an imperfect understanding, a plethora of definitions, and informal use in the literature and in everyday life. It is common to say “I trust you,” but what does that mean? This thesis provides a clarification of trust. We present a formalism for trust which provides us with a tool for precise discussion. The formalism is implementable: it can be embedded in an artificial agent, enabling the agent to make trust-based decisions. Its applicability in the domain of Distributed Artificial Intelligence (DAI) is raised. The thesis presents a testbed populated by simple trusting agents which substantiates the utility of the formalism. The formalism provides a step in the direction of a proper understanding and definition of human trust. A contribution of the thesis is its detailed exploration of the possibilities of future work in the area.",1994.0,S. Marsh
33b1b26482683129b9923f331873eb065c603dfd,https://www.semanticscholar.org/paper/33b1b26482683129b9923f331873eb065c603dfd,The Cambridge handbook of thinking and reasoning,"The Cambridge Handbook of Thinking and Reasoning is the first comprehensive and authoritative handbook covering all the core topics of the field of thinking and reasoning. Written by the foremost experts from cognitive psychology, cognitive science, and cognitive neuroscience, individual chapters summarize basic concepts and findings for a major topic, sketch its history, and give a sense of the directions in which research is currently heading. The volume also includeswork related to developmental, social and clinical psychology, philosophy, economics, artificial intelligence, linguistics, education, law, and medicine. Scholars and students in all these fields and others will find this to be a valuable collection.",2005.0,"K. Holyoak, Robert G. Morrison, Frontmatter"
4a6e74d4bf4fd0106891e5518692a77c7aa8811d,https://www.semanticscholar.org/paper/4a6e74d4bf4fd0106891e5518692a77c7aa8811d,Outlier Detection in High Dimensional Data,"Artificial intelligence (AI) is the science that allows
computers to replicate human intelligence in areas such as
decision-making, text processing, visual perception. Artificial
Intelligence is the broader field that contains several subfields
such as machine learning, robotics, and computer vision.
Machine Learning is a branch of Artificial Intelligence that
allows a machine to learn and improve at a task over time. Deep
Learning is a subset of machine learning that makes use of deep
artificial neural networks for training. The paper proposed on
outlier detection for multivariate high dimensional data for
Autoencoder unsupervised model.",2021.0,"C. Aggarwal, Philip S. Yu"
5bb86aacb5faaea4876a3241841d0c2447a3e640,https://www.semanticscholar.org/paper/5bb86aacb5faaea4876a3241841d0c2447a3e640,Auditory Scene Analysis: The Perceptual Organization of Sound by Albert Bregman (review),"The title of this technical report says almost everything: this is indeed ""a short bibliography on AI and the arts"". It is presented in four sections: General Arguments, Proposals, and Approaches (31 references); Artificial Intelligence in Music (124 references); Artificial Intelligence in Literature and the Performing Arts (13 references), and Artificial Intelligence and Visual Art (57 references). About a quarter of these have short abstracts. Creating a bibliography can be a monumental task, and this bibliography should be viewed as a good and useful start, though it is by no means complete. For comparison, consider the 4,585-entry bibliography Computer Applications in Music by Deta Davis (A-REditions). No direct comparison is intended (or possible), but my point is that many more papers are likely to exist. As a rough check, I looked for several pre-1990 AI and Music articles and books (including my own, of course) in the bibliography. Out of five papers from well-known sources, only one was listed. On the other hand, I discovered a number of papers in this report that were unknown to me, so I am grateful to have a new source of references. In their introduction, the authors acknowledge the need for more references and even offer.a cup of coffee in reward for each new one. I will be sending a number of contributions, so the next time anyone is in Vienna, the coffee is on me. I hope the authors will continue to collect abstracts and publish an updated report in the future.",2016.0,G. Kramer
0412076e1004d030ac02de77bc44cc7d92b13ab9,https://www.semanticscholar.org/paper/0412076e1004d030ac02de77bc44cc7d92b13ab9,Closing the AI accountability gap: defining an end-to-end framework for internal algorithmic auditing,"Rising concern for the societal implications of artificial intelligence systems has inspired a wave of academic and journalistic literature in which deployed systems are audited for harm by investigators from outside the organizations deploying the algorithms. However, it remains challenging for practitioners to identify the harmful repercussions of their own systems prior to deployment, and, once deployed, emergent issues can become difficult or impossible to trace back to their source. In this paper, we introduce a framework for algorithmic auditing that supports artificial intelligence system development end-to-end, to be applied throughout the internal organization development life-cycle. Each stage of the audit yields a set of documents that together form an overall audit report, drawing on an organization's values or principles to assess the fit of decisions made throughout the process. The proposed auditing framework is intended to contribute to closing the accountability gap in the development and deployment of large-scale artificial intelligence systems by embedding a robust process to ensure audit integrity.",2020.0,"Inioluwa Deborah Raji, A. Smart, Rebecca N. White, Margaret Mitchell, Timnit Gebru, Ben Hutchinson, Jamila Smith-Loud, Daniel Theron, Parker Barnes"
06fc70b059c0fe7c16d0c46d639f48c7226afcd5,https://www.semanticscholar.org/paper/06fc70b059c0fe7c16d0c46d639f48c7226afcd5,The Biomedical Engineering Handbook,"Physiological systems bioelectric phenomena biomechanics biomaterials biosensors biomedical signal analysis imaging medical instruments and devices biological effects of non-ionizing biotechnology tissue engineering human performance engineering physiological modelling, simulation and control clinical engineering and artificial intelligence. (Part contents).",1995.0,J. Bronzino
8ef12a349dfc48b332ba6b5e97693ba8f665e0e6,https://www.semanticscholar.org/paper/8ef12a349dfc48b332ba6b5e97693ba8f665e0e6,A Study of Thinking,"A Study of Thinking is a pioneering account of how human beings achieve a measure of rationality in spite of the constraints imposed by bias, limited attention and memory, and the risks of error imposed by pressures of time and ignorance. First published in 1956 and hailed at its appearance as a groundbreaking study, it is still read three decades later as a major contribution to our understanding of the mind. In their insightful new introduction, the authors relate the book to the cognitive revolution and its handmaiden, artificial intelligence.",1956.0,"William M. Smith, J. Bruner, J. Goodnow, G. A. Austin"
eaabb78d0bc44ed132e4d077e9486c86a9e4cda9,https://www.semanticscholar.org/paper/eaabb78d0bc44ed132e4d077e9486c86a9e4cda9,Superhuman AI for heads-up no-limit poker: Libratus beats top professionals,"Libratus versus humans Pitting artificial intelligence (AI) against top human players demonstrates just how far AI has come. Brown and Sandholm built a poker-playing AI called Libratus that decisively beat four leading human professionals in the two-player variant of poker called heads-up no-limit Texas hold'em (HUNL). Over nearly 3 weeks, Libratus played 120,000 hands of HUNL against the human professionals, using a three-pronged approach that included precomputing an overall strategy, adapting the strategy to actual gameplay, and learning from its opponent. Science, this issue p. 418 An artificial intelligence program called Libratus played 120,000 hands of a two-player variant of poker and beat four leading human professionals. No-limit Texas hold’em is the most popular form of poker. Despite artificial intelligence (AI) successes in perfect-information games, the private information and massive game tree have made no-limit poker difficult to tackle. We present Libratus, an AI that, in a 120,000-hand competition, defeated four top human specialist professionals in heads-up no-limit Texas hold’em, the leading benchmark and long-standing challenge problem in imperfect-information game solving. Our game-theoretic approach features application-independent techniques: an algorithm for computing a blueprint for the overall strategy, an algorithm that fleshes out the details of the strategy for subgames that are reached during play, and a self-improver algorithm that fixes potential weaknesses that opponents have identified in the blueprint strategy.",2018.0,"Noam Brown, T. Sandholm"
406289a706af2e00006a5823f245ef7bcd811681,https://www.semanticscholar.org/paper/406289a706af2e00006a5823f245ef7bcd811681,"Precision Medicine, AI, and the Future of Personalized Health Care","The convergence of artificial intelligence (AI) and precision medicine promises to revolutionize health care. Precision medicine methods identify phenotypes of patients with less‐common responses to treatment or unique healthcare needs. AI leverages sophisticated computation and inference to generate insights, enables the system to reason and learn, and empowers clinician decision making through augmented intelligence. Recent literature suggests that translational research exploring this convergence will help solve the most difficult challenges facing precision medicine, especially those in which nongenomic and genomic determinants, combined with information from patient symptoms, clinical history, and lifestyles, will facilitate personalized diagnosis and prognostication.",2020.0,"Kevin B. Johnson, Wei-Qi Wei, D. Weeraratne, M. Frisse, K. Misulis, K. Rhee, Juan Zhao, J. Snowdon"
a52fa3769d3033cff1d691b657a4cfcac49f7883,https://www.semanticscholar.org/paper/a52fa3769d3033cff1d691b657a4cfcac49f7883,An introduction to multisensor data fusion,"Multisensor data fusion is an emerging technology applied to Department of Defense (DoD) areas such as automated target recognition, battlefield surveillance, and guidance and control of autonomous vehicles, and to non-DoD applications such as monitoring of complex machinery, medical diagnosis, and smart buildings. Techniques for multisensor data fusion are drawn from a wide range of areas including artificial intelligence, pattern recognition, statistical estimation and other areas. This paper provides a tutorial on data fusion, introducing data fusion applications, process models, and identification of applicable techniques. Comments are made on the state-of-the-art in data fusion.",1997.0,"D. Hall, J. Llinas"
c06a160b49d180cf4983011b6490924a1420ed14,https://www.semanticscholar.org/paper/c06a160b49d180cf4983011b6490924a1420ed14,Thought and Choice in Chess,"What does a chessmaster think when he prepartes his next move? How are his thoughts organized? Which methods and strategies does he use by solving his problem of choice? To answer these questions, the author did an experimental study in 1938, to which famous chessmasters participated (Alekhine, Max Euwe and Flohr). This book is still usefull for everybody who studies cognition and artificial intelligence.",1978.0,A. D. D. Groot
cd81dc3cc794085a6d4051d5237777d1882339ba,https://www.semanticscholar.org/paper/cd81dc3cc794085a6d4051d5237777d1882339ba,Ambient intelligence: A survey,"In this article we survey ambient intelligence (AmI), including its applications, some of the technologies it uses, and its social and ethical implications. The applications include AmI at home, care of the elderly, healthcare, commerce, and business, recommender systems, museums and tourist scenarios, and group decision making. Among technologies, we focus on ambient data management and artificial intelligence; for example planning, learning, event-condition-action rules, temporal reasoning, and agent-oriented technologies. The survey is not intended to be exhaustive, but to convey a broad range of applications, technologies, and technical, social, and ethical challenges.",2011.0,F. Sadri
c2867d1599eca46942f626451c42e69adfa35797,https://www.semanticscholar.org/paper/c2867d1599eca46942f626451c42e69adfa35797,"Godel, Escher, Bach: An Eternal Golden Braid","From the Publisher: 
Winner of the Pulitzer Prize, this book applies Godel's seminal contribution to modern mathematics to the study of the human mind and the development of artificial intelligence.",1979.0,"S. Smoliar, D. Hofstadter"
2614a0d0213e8cc457ea62b435a8f43dea54245a,https://www.semanticscholar.org/paper/2614a0d0213e8cc457ea62b435a8f43dea54245a,Blockchain for AI: Review and Open Research Challenges,"Recently, artificial intelligence (AI) and blockchain have become two of the most trending and disruptive technologies. Blockchain technology has the ability to automate payment in cryptocurrency and to provide access to a shared ledger of data, transactions, and logs in a decentralized, secure, and trusted manner. Also with smart contracts, blockchain has the ability to govern interactions among participants with no intermediary or a trusted third party. AI, on the other hand, offers intelligence and decision-making capabilities for machines similar to humans. In this paper, we present a detailed survey on blockchain applications for AI. We review the literature, tabulate, and summarize the emerging blockchain applications, platforms, and protocols specifically targeting AI area. We also identify and discuss open research challenges of utilizing blockchain technologies for AI.",2019.0,"K. Salah, M. H. Rehman, Nishara Nizamuddin, Ala I. Al-Fuqaha"
0a127332f5bddeee56d22722f7e95ffcf9530b1e,https://www.semanticscholar.org/paper/0a127332f5bddeee56d22722f7e95ffcf9530b1e,"Author's Personal Copy Pervasive and Mobile Computing Ambient Intelligence: Technologies, Applications, and Opportunities","In most cases authors are permitted to post their version of the article (e.g. in Word or Tex form) to their personal website or institutional repository. Authors requiring further information regarding Elsevier's archiving and manuscript policies are encouraged to visit: a b s t r a c t Ambient intelligence is an emerging discipline that brings intelligence to our everyday environments and makes those environments sensitive to us. Ambient intelligence (AmI) research builds upon advances in sensors and sensor networks, pervasive computing, and artificial intelligence. Because these contributing fields have experienced tremendous growth in the last few years, AmI research has strengthened and expanded. Because AmI research is maturing, the resulting technologies promise to revolutionarize daily human life by making people's surroundings flexible and adaptive. In this paper, we provide a survey of the technologies that comprise ambient intelligence and of the applications that are dramatically affected by it. In particular, we specifically focus on the research that makes AmI technologies ''intelligent''. We also highlight challenges and opportunities that AmI researchers will face in the coming years.",,"D. Cook, J. Augusto, Vikramaditya R. Jakkula"
26feca29b05ac2abb7841f3a4172d0c28583ca33,https://www.semanticscholar.org/paper/26feca29b05ac2abb7841f3a4172d0c28583ca33,An Interpretable Machine Learning Model for Accurate Prediction of Sepsis in the ICU,"Objectives: Sepsis is among the leading causes of morbidity, mortality, and cost overruns in critically ill patients. Early intervention with antibiotics improves survival in septic patients. However, no clinically validated system exists for real-time prediction of sepsis onset. We aimed to develop and validate an Artificial Intelligence Sepsis Expert algorithm for early prediction of sepsis. Design: Observational cohort study. Setting: Academic medical center from January 2013 to December 2015. Patients: Over 31,000 admissions to the ICUs at two Emory University hospitals (development cohort), in addition to over 52,000 ICU patients from the publicly available Medical Information Mart for Intensive Care-III ICU database (validation cohort). Patients who met the Third International Consensus Definitions for Sepsis (Sepsis-3) prior to or within 4 hours of their ICU admission were excluded, resulting in roughly 27,000 and 42,000 patients within our development and validation cohorts, respectively. Interventions: None. Measurements and Main Results: High-resolution vital signs time series and electronic medical record data were extracted. A set of 65 features (variables) were calculated on hourly basis and passed to the Artificial Intelligence Sepsis Expert algorithm to predict onset of sepsis in the proceeding T hours (where T = 12, 8, 6, or 4). Artificial Intelligence Sepsis Expert was used to predict onset of sepsis in the proceeding T hours and to produce a list of the most significant contributing factors. For the 12-, 8-, 6-, and 4-hour ahead prediction of sepsis, Artificial Intelligence Sepsis Expert achieved area under the receiver operating characteristic in the range of 0.83–0.85. Performance of the Artificial Intelligence Sepsis Expert on the development and validation cohorts was indistinguishable. Conclusions: Using data available in the ICU in real-time, Artificial Intelligence Sepsis Expert can accurately predict the onset of sepsis in an ICU patient 4–12 hours prior to clinical recognition. A prospective study is necessary to determine the clinical utility of the proposed sepsis prediction model.",2017.0,"S. Nemati, A. Holder, Fereshteh Razmi, Matthew D. Stanley, G. Clifford, T. Buchman"
ad387c258480af79bbc7b25177b544b755833713,https://www.semanticscholar.org/paper/ad387c258480af79bbc7b25177b544b755833713,A Leaf Recognition Algorithm for Plant Classification Using Probabilistic Neural Network,"In this paper, we employ probabilistic neural network (PNN) with image and data processing techniques to implement a general purpose automated leaf recognition for plant classification. 12 leaf features are extracted and orthogonalized into 5 principal variables which consist the input vector of the PNN. The PNN is trained by 1800 leaves to classify 32 kinds of plants with an accuracy greater than 90%. Compared with other approaches, our algorithm is an accurate artificial intelligence approach which is fast in execution and easy in implementation.",2007.0,"Stephen Gang Wu, F. S. Bao, E. Xu, Yuxuan Wang, Yi-Fan Chang, Qiao-Liang Xiang"
f7c07aa8354e62aa5c35487c4a8d11485dda899b,https://www.semanticscholar.org/paper/f7c07aa8354e62aa5c35487c4a8d11485dda899b,A simple parallel algorithm for the maximal independent set problem,Simple parallel algorithms for the maximal independent set (MIS) problem are presented. The first algorithm is a Monte Carlo algorithm with a very local property. The local property of this algorithm may make it a useful protocol design tool in distributed computing environments and artificial intelligence. One of the main contributions of this paper is the development of powerful and general techniques for converting Monte Carlo algorithms into deterministic algorithms. These techniques are used to convert the Monte Carlo algorithm for the MIS problem into a simple deterministic algorithm with the same parallel running time.,1985.0,M. Luby
9af4f7e9c64b7decdc82f33b9c8692a5cd12fed7,https://www.semanticscholar.org/paper/9af4f7e9c64b7decdc82f33b9c8692a5cd12fed7,Advances in Diagnostic Techniques for Induction Machines,"This paper investigates diagnostic techniques for electrical machines with special reference to induction machines and to papers published in the last ten years. A comprehensive list of references is reported and examined, and research activities classified into four main topics: 1) electrical faults; 2) mechanical faults; 3) signal processing for analysis and monitoring; and 4) artificial intelligence and decision-making techniques.",2008.0,"A. Bellini, F. Filippetti, C. Tassoni, G. Capolino"
5a6b2b9bc3b51ff187826fc2dc21a967e04125ed,https://www.semanticscholar.org/paper/5a6b2b9bc3b51ff187826fc2dc21a967e04125ed,Model-based reinforcement learning: A survey,"Reinforcement learning is an important branch of machine learning and artificial intelligence. Compared with traditional reinforcement learning, model-based reinforcement learning obtains the action of the next state by the model that has been learned",2018.0,"Fengji Yi, Wenlong Fu, Huan Liang"
99076f8c0ffcb9a4135dab4ad5497bd6f992e6f1,https://www.semanticscholar.org/paper/99076f8c0ffcb9a4135dab4ad5497bd6f992e6f1,"A Bioinspired Mineral Hydrogel as a Self‐Healable, Mechanically Adaptable Ionic Skin for Highly Sensitive Pressure Sensing","In the past two decades, artificial skin‐like materials have received increasing research interests for their broad applications in artificial intelligence, wearable devices, and soft robotics. However, profound challenges remain in terms of imitating human skin because of its unique combination of mechanical and sensory properties. In this work, a bioinspired mineral hydrogel is developed to fabricate a novel type of mechanically adaptable ionic skin sensor. Due to its unique viscoelastic properties, the hydrogel‐based capacitive sensor is compliant, self‐healable, and can sense subtle pressure changes, such as a gentle finger touch, human motion, or even small water droplets. It might not only show great potential in applications such as artificial intelligence, human/machine interactions, personal healthcare, and wearable devices, but also promote the development of next‐generation mechanically adaptable intelligent skin‐like devices.",2017.0,"Zhouyue Lei, Quankang Wang, Shengtong Sun, Wencheng Zhu, Peiyi Wu"
ad003f6b5120bf9910e32a9c9c52768f54cad360,https://www.semanticscholar.org/paper/ad003f6b5120bf9910e32a9c9c52768f54cad360,"Fuzzy Logic: Intelligence, Control, and Information","1. Introduction. 2. Basic Concepts of Fuzzy Logic. 3. Fuzzy Sets. 4. Fuzzy Relations, Fuzzy Graphs, and Fuzzy Arithmetic. 5. Fuzzy If-Then Rules. 6. Fuzzy Implications and Approximate Reasoning. 7. Fuzzy Logic and Probability Theory. 8. Fuzzy Logic in Control Engineering. 9. Hierarchical Intelligent Control. 10. Analytical Issues in Fuzzy Logic Control. 11. Fuzzy Logic and Artificial Intelligence. 12. Fuzzy Logic in Database Management and Information Systems. 13. Fuzzy Logic in Pattern Recognition. 14. Fuzzy Model Identification. 15.Advanced Topics of Fuzzy Model Identification. 16.Neuro-Fuzzy Systems. 17. Genetic Algorithms and Fuzzy Logic. References. Index.",1998.0,"J. Yen, R. Langari"
6b67e38e488cdb6a5af8ae44fdbb2c069c9cea0e,https://www.semanticscholar.org/paper/6b67e38e488cdb6a5af8ae44fdbb2c069c9cea0e,"Introduction to Machine Learning, Neural Networks, and Deep Learning","Purpose To present an overview of current machine learning methods and their use in medical research, focusing on select machine learning techniques, best practices, and deep learning. Methods A systematic literature search in PubMed was performed for articles pertinent to the topic of artificial intelligence methods used in medicine with an emphasis on ophthalmology. Results A review of machine learning and deep learning methodology for the audience without an extensive technical computer programming background. Conclusions Artificial intelligence has a promising future in medicine; however, many challenges remain. Translational Relevance The aim of this review article is to provide the nontechnical readers a layman's explanation of the machine learning methods being used in medicine today. The goal is to provide the reader a better understanding of the potential and challenges of artificial intelligence within the field of medicine.",2020.0,"Rene Y. Choi, Aaron S. Coyner, Jayashree Kalpathy-Cramer, M. Chiang, J. Campbell"
26916007ecda2a35753625586c4f0ffcac387ccb,https://www.semanticscholar.org/paper/26916007ecda2a35753625586c4f0ffcac387ccb,Principles of constraint programming,"Scheduling, vehicle routing and timetabling are all examples of constraint problems, and methods to solve them rely on the idea of constraint propagation and search. This book meets the need for a modern, multidisciplinary introduction to the field that covers foundations and applications. Written by Krzysztof Apt, an authority on the subject, it will be welcomed by graduate students and professionals. With the insertion of constraint techniques into programming environments, new developments have accelerated the solution process. Constraint programming combines ideas from artificial intelligence, programming languages, databases, and operational research.",2003.0,K. R. Apt
7d291d5fca0e9cd9e0ed72fb6f82289a197f7f02,https://www.semanticscholar.org/paper/7d291d5fca0e9cd9e0ed72fb6f82289a197f7f02,Machine Learning and Deep Learning,"Now-a-days artificial intelligence has become an asset for engineering and experimental studies, just like statistics and calculus. Data science is a growing field for researchers and artificial intelligence, machine learning and deep learning are roots of it. This paper describes the relation between these roots of data science. There is a need of machine learning if any kind of analysis is to be performed. This study describes machine learning from the scratch. It also focuses on Deep Learning. Deep learning can also be known as new trend of machine learning. This paper gives a light on basic architecture of Deep learning. A comparative study of machine learning and deep learning is also given in the paper and allows researcher to have a broad view on these techniques so that they can understand which one will be preferable solution for a particular problem.",2019.0,Dietmar P. F. Möller
5f8c703d409bbefced372bba004e86190ec5c0e2,https://www.semanticscholar.org/paper/5f8c703d409bbefced372bba004e86190ec5c0e2,CONVENTION,". The article analyzes the international legal regulation of combating human rights violations in information and communication systems. It is noted that further development of information and communication technologies and artificial intelligence is an integral feature of modern human civilization. International organizations and countries, realizing the benefits and threats of further digitalization of socio-economic processes, continue to work on creating and improving the legal regulation of the use of information and communication technologies and artificial intelligence. The article examines the legal acts of the UN, the EU and the Council of Europe",2020.0,D. Masolo
ceb31a1f6901a7aa5995af472abaac1035f3a154,https://www.semanticscholar.org/paper/ceb31a1f6901a7aa5995af472abaac1035f3a154,Envisioning AI for K-12: What Should Every Child Know about AI?,"The ubiquity of AI in society means the time is ripe to consider what educated 21st century digital citizens should know about this subject. In May 2018, the Association for the Advancement of Artificial Intelligence (AAAI) and the Computer Science Teachers Association (CSTA) formed a joint working group to develop national guidelines for teaching AI to K-12 students. Inspired by CSTA's national standards for K-12 computing education, the AI for K-12 guidelines will define what students in each grade band should know about artificial intelligence, machine learning, and robotics. The AI for K-12 working group is also creating an online resource directory where teachers can find AI- related videos, demos, software, and activity descriptions they can incorporate into their lesson plans. This blue sky talk invites the AI research community to reflect on the big ideas in AI that every K-12 student should know, and how we should communicate with the public about advances in AI and their future impact on society. It is a call to action for more AI researchers to become AI educators, creating resources that help teachers and students understand our work.",2019.0,"D. Touretzky, Christina Gardner-Mccune, F. Martin, Deborah W. Seehorn"
eecf4b6e567eadc2758ce39148dbb5e64d163106,https://www.semanticscholar.org/paper/eecf4b6e567eadc2758ce39148dbb5e64d163106,"Overview: Technology Roadmap of the Future Trend of Metaverse based on IoT, Blockchain, AI Technique, and Medical Domain Metaverse Activity","Metaverse is defined as a collection of technology gadgets and metaverse connected to IoT, Blockchain, Artificial Intelligence, and all the other tech industries including the medical area. IoT and Metaverse are the digital twins, Metaverse is using maximum IoT devices in their virtual workstation. This data has a unique identifying tag and is used as traceable data in the blockchain-based Metaverse. In the Metaverse, such data is becoming a valuable resource for artificial intelligence. Metaverse uses artificial intelligence and blockchain technology to build a digital virtual world where you can safely and freely engage in social and economic activities that transcend the limits of the real world, and the application of these latest technologies will be expedited. In this paper, we are going to describe what technologies metaverse is using and metaverse potentiality in medical healthcare.",2022.0,"Md Ariful Islam Mozumder, Muhammad Mohsan Sheeraz, Ali Athar, S. Aich, Hee-Cheol Kim"
ceb31a1f6901a7aa5995af472abaac1035f3a154,https://www.semanticscholar.org/paper/ceb31a1f6901a7aa5995af472abaac1035f3a154,Envisioning AI for K-12: What Should Every Child Know about AI?,"The ubiquity of AI in society means the time is ripe to consider what educated 21st century digital citizens should know about this subject. In May 2018, the Association for the Advancement of Artificial Intelligence (AAAI) and the Computer Science Teachers Association (CSTA) formed a joint working group to develop national guidelines for teaching AI to K-12 students. Inspired by CSTA's national standards for K-12 computing education, the AI for K-12 guidelines will define what students in each grade band should know about artificial intelligence, machine learning, and robotics. The AI for K-12 working group is also creating an online resource directory where teachers can find AI- related videos, demos, software, and activity descriptions they can incorporate into their lesson plans. This blue sky talk invites the AI research community to reflect on the big ideas in AI that every K-12 student should know, and how we should communicate with the public about advances in AI and their future impact on society. It is a call to action for more AI researchers to become AI educators, creating resources that help teachers and students understand our work.",2019.0,"D. Touretzky, Christina Gardner-Mccune, F. Martin, Deborah W. Seehorn"
eecf4b6e567eadc2758ce39148dbb5e64d163106,https://www.semanticscholar.org/paper/eecf4b6e567eadc2758ce39148dbb5e64d163106,"Overview: Technology Roadmap of the Future Trend of Metaverse based on IoT, Blockchain, AI Technique, and Medical Domain Metaverse Activity","Metaverse is defined as a collection of technology gadgets and metaverse connected to IoT, Blockchain, Artificial Intelligence, and all the other tech industries including the medical area. IoT and Metaverse are the digital twins, Metaverse is using maximum IoT devices in their virtual workstation. This data has a unique identifying tag and is used as traceable data in the blockchain-based Metaverse. In the Metaverse, such data is becoming a valuable resource for artificial intelligence. Metaverse uses artificial intelligence and blockchain technology to build a digital virtual world where you can safely and freely engage in social and economic activities that transcend the limits of the real world, and the application of these latest technologies will be expedited. In this paper, we are going to describe what technologies metaverse is using and metaverse potentiality in medical healthcare.",2022.0,"Md Ariful Islam Mozumder, Muhammad Mohsan Sheeraz, Ali Athar, S. Aich, Hee-Cheol Kim"
86328e0c84bf97ae9b0680c06c4886bc04fb63c1,https://www.semanticscholar.org/paper/86328e0c84bf97ae9b0680c06c4886bc04fb63c1,"A review of mobile robots: Concepts, methods, theoretical framework, and applications","Humanoid robots, unmanned rovers, entertainment pets, drones, and so on are great examples of mobile robots. They can be distinguished from other robots by their ability to move autonomously, with enough intelligence to react and make decisions based on the perception they receive from the environment. Mobile robots must have some source of input data, some way of decoding that input, and a way of taking actions (including its own motion) to respond to a changing world. The need to sense and adapt to an unknown environment requires a powerful cognition system. Nowadays, there are mobile robots that can walk, run, jump, and so on like their biological counterparts. Several fields of robotics have arisen, such as wheeled mobile robots, legged robots, flying robots, robot vision, artificial intelligence, and so on, which involve different technological areas such as mechanics, electronics, and computer science. In this article, the world of mobile robots is explored including the new trends. These new trends are led by artificial intelligence, autonomous driving, network communication, cooperative work, nanorobotics, friendly human–robot interfaces, safe human–robot interaction, and emotion expression and perception. Furthermore, these news trends are applied to different fields such as medicine, health care, sports, ergonomics, industry, distribution of goods, and service robotics. These tendencies will keep going their evolution in the coming years.",2019.0,"F. Rubio, F. Valero, C. Llopis-Albert"
1da134cd625b6e1d50f211441691ceb3a9c5ec08,https://www.semanticscholar.org/paper/1da134cd625b6e1d50f211441691ceb3a9c5ec08,Introduction to Fuzzy Control,"In the last few years the applications of artificial intelligence techniques have been used to convert human experience into a form understandable by computers. Advanced control based on artificial intelligence techniques is called intelligent control. Intelligent systems are usually described by analogies with biological systems by, for example, looking at how human beings perform control tasks, recognize patterns, or make decisions. There exists a mismatch between humans and machines: humans reason in uncertain, imprecise, fuzzy ways while machines and the computers that run them are based on binary reasoning. Fuzzy logic is a way to make machines more intelligent enabling them to reason in a fuzzy manner like humans. Fuzzy logic, proposed by Lotfy Zadeh in 1965, emerged as a tool to deal with uncertain, imprecise, or qualitative decision-making problems. Controllers that combine intelligent and conventional techniques are commonly used in the intelligent control of complex dynamic systems. Therefore, embedded fuzzy controllers automate what has traditionally been a human control activity.",2003.0,Marcelo Simoes
5f2c083f80073c56c41c4ea66ae48312513c55aa,https://www.semanticscholar.org/paper/5f2c083f80073c56c41c4ea66ae48312513c55aa,Energy and Policy Considerations for Modern Deep Learning Research,"The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.",2020.0,"Emma Strubell, Ananya Ganesh, A. McCallum"
5d1017b11c46098a9be8a910bbc1cb1ad9c58d63,https://www.semanticscholar.org/paper/5d1017b11c46098a9be8a910bbc1cb1ad9c58d63,Outline for a theory of intelligence,"Intelligence is defined as that which produces successful behavior. Intelligence is assumed to result from natural selection. A model is proposed that integrates knowledge from research in both natural and artificial systems. The model consists of a hierarchical system architecture wherein: (1) control bandwidth decreases about an order of magnitude at each higher level, (2) perceptual resolution of spatial and temporal patterns contracts about an order-of-magnitude at each higher level, (3) goals expand in scope and planning horizons expand in space and time about an order-of-magnitude at each higher level, and (4) models of the world and memories of events expand their range in space and time by about an order-of-magnitude at each higher level. At each level, functional modules perform behavior generation (task decomposition planning and execution), world modeling, sensory processing, and value judgment. Sensory feedback control loops are closed at every level. >",1991.0,J. Albus
4e4138efcc9f30ec08daf5564bb2c38f866f51f0,https://www.semanticscholar.org/paper/4e4138efcc9f30ec08daf5564bb2c38f866f51f0,"Evolutionary Robotics: The Biology,Intelligence,and Technology","Evolutionary robotics is a new technique for the automatic creation of autonomous robots. Inspired by the Darwinian principle of selective reproduction of the fittest, it views robots as autonomous artificial organisms that develop their own skills in close interaction with the environment and without human intervention. Drawing heavily on biology and ethology, it uses the tools of neural networks, genetic algorithms, dynamic systems, and biomorphic engineering. The resulting robots share with simple biological systems the characteristics of robustness, simplicity, small size, flexibility, and modularity. In evolutionary robotics, an initial population of artificial chromosomes, each encoding the control system of a robot, is randomly created and put into the environment. Each robot is then free to act (move, look around, manipulate) according to its genetically specified controller while its performance on various tasks is automatically evaluated. The fittest robots then ""reproduce"" by swapping parts of their genetic material with small random mutations. The process is repeated until the ""birth"" of a robot that satisfies the performance criteria. This book describes the basic concepts and methodologies of evolutionary robotics and the results achieved so far. An important feature is the clear presentation of a set of empirical experiments of increasing complexity. Software with a graphic interface, freely available on a Web page, will allow the reader to replicate and vary (in simulation and on real robots) most of the experiments.",2000.0,"S. Nolfi, D. Floreano"
8ecc165747818821f31f75cde42c75be51122a70,https://www.semanticscholar.org/paper/8ecc165747818821f31f75cde42c75be51122a70,The Singularity Is Near: When Humans Transcend Biology,"A radical and optimistic view of the future course of human development from Ray Kurzweil, whom Bill Gates calls ""the best person I know at predicting the future of artificial intelligence.""",2006.0,Raymond C. Kurzweil
b630a89c183c326b64915dd88fc785fa719a9054,https://www.semanticscholar.org/paper/b630a89c183c326b64915dd88fc785fa719a9054,OIL: An Ontology Infrastructure for the Semantic Web,"Researchers in artificial intelligence first developed ontologies to facilitate knowledge sharing and reuse. Ontologies play a major role in supporting information exchange across various networks. A prerequisite for such a role is the development of a joint standard for specifying and exchanging ontologies. The authors present OIL, a proposal for such a standard. Ontologies applied to the World Wide Web are creating the Semantic Web.",2001.0,"D. Fensel, F. V. Harmelen, Ian Horrocks, D. McGuinness, P. Patel-Schneider"
b90c090f7928a78d85d952737488be1ef8587ae5,https://www.semanticscholar.org/paper/b90c090f7928a78d85d952737488be1ef8587ae5,A Literature Survey of Recent Advances in Chatbots,"Chatbots are intelligent conversational computer systems designed to mimic human conversation to enable automated online guidance and support. The increased benefits of chatbots led to their wide adoption by many industries in order to provide virtual assistance to customers. Chatbots utilise methods and algorithms from two Artificial Intelligence domains: Natural Language Processing and Machine Learning. However, there are many challenges and limitations in their application. In this survey we review recent advances on chatbots, where Artificial Intelligence and Natural Language processing are used. We highlight the main challenges and limitations of current work and make recommendations for future research investigation",2021.0,"Guendalina Caldarini, Sardar F. Jaf, K. McGarry"
7d63ddfb9e290f25c82f080f11aa3750dd4e4993,https://www.semanticscholar.org/paper/7d63ddfb9e290f25c82f080f11aa3750dd4e4993,Neural networks in computer intelligence,"From the Publisher: 
Neural Networks in Computer Intelligence provides basic concepts,algorithms,and analysis of important neural network models developed to date,with emphasis on the importance of knowledge in intelligent system design. The book bridges the gap between artificial intelligence and neural networks. Unlike many other network books,this one pioneers the effort to offer a unified perspective which could be used to integrate intelligence technologies. The broad coverage of the book and the emphasis on basic principles can accommodate the diverse background of readers.",1994.0,L. Fu
9dbb506ded56ff4b7ab65aa92b363c0112987f10,https://www.semanticscholar.org/paper/9dbb506ded56ff4b7ab65aa92b363c0112987f10,Neural-Symbolic Learning and Reasoning: A Survey and Interpretation,"The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.",2017.0,"Tarek R. Besold, A. Garcez, Sebastian Bader, H. Bowman, Pedro M. Domingos, P. Hitzler, Kai-Uwe Kühnberger, L. Lamb, Daniel Lowd, P. Lima, L. Penning, Gadi Pinkas, Hoifung Poon, Gerson Zaverucha"
075a8cdccafb9cfbf8bf339258f757749df3a4e1,https://www.semanticscholar.org/paper/075a8cdccafb9cfbf8bf339258f757749df3a4e1,Utility of a Deep-Learning Algorithm to Guide Novices to Acquire Echocardiograms for Limited Diagnostic Use,"Key Points Question Can artificial intelligence guide novice operators to obtain echocardiographic scans with limited diagnostic utility? Findings In this diagnostic study, 8 nurses without prior ultrasonography experience used artificial intelligence guidance to scan 30 patients each with a 10-view echocardiographic protocol (240 total patients). Five expert echocardiographers blindly reviewed these scans and felt they were of diagnostic quality for left ventricular size and function in 98.8% of patients, right ventricular size in 92.5%, and presence of pericardial effusion in 98.8%. Meaning Artificial intelligence can extend the reach of echocardiography to assess the 4 basic parameters of left ventricular size and function, right ventricular size, and presence of a nontrivial pericardial effusion to sites with limited expertise.",2021.0,"Akhil Narang, R. Bae, H. Hong, Y. Thomas, S. Surette, C. Cadieu, A. Chaudhry, Randolph P. Martin, P. McCarthy, D. Rubenson, S. Goldstein, S. Little, R. Lang, N. Weissman, James D. Thomas"
da247a834ad043df07e1935dd18dc0803827476b,https://www.semanticscholar.org/paper/da247a834ad043df07e1935dd18dc0803827476b,"Simon, Herbert A.","Herbert A. Simon (1916–2001) was an American scientist whose research ranged broadly over the cognitive and social sciences, computer science, economics, and the philosophy of science. For his fundamental, innovative, and penetrating contributions he received the highest research awards in the fields of economics, psychology, computer science and artificial intelligence, including the 1978 Nobel Prize for Economics for his model of bounded rationality in decision-making and problem-solving. 
 
 
Keywords: 
 
decision-making; 
problem solving; 
artificial intelligence; 
bounded rationality; 
Nobel Price in Economics",2006.0,E. Feigenbaum
abc9e7fb90c0a025483114f08e0ea6ad5d16dd69,https://www.semanticscholar.org/paper/abc9e7fb90c0a025483114f08e0ea6ad5d16dd69,Artificial life meets entertainment: lifelike autonomous agents,"The relatively new field of artificial life attempts to study and understand biological life by synthesizing artificial life forms. To paraphrase Chris Langton, the founder of the field, the goal of artificial life is to “model life as it could be so as to understand life as we know it.” Artificial life is a very broad discipline which spans such diverse topics as artificial evolution, artificial ecosystems, artificial morphogenesis, molecular evolution, and many more. Langton offers a nice overview of the different research questions studied by the discipline [6]. Artificial life shares with artificial intelligence (AI) its interest in synthesizing adaptive autonomous agents. Autonomous agents are computational systems that inhabit some complex, dynamic environment, sense and act autonomously in this environment, and by doing so realize a set of goals or tasks for which they are designed.",1995.0,P. Maes
abeee58b9fb5761133636ef117ef1a87203ad7ab,https://www.semanticscholar.org/paper/abeee58b9fb5761133636ef117ef1a87203ad7ab,Talking to Bots: Symbiotic Agency and the Case of Tay,"In 2016, Microsoft launched Tay, an experimental artificial intelligence chat bot. Learning from interactions with Twitter users, Tay was shut down after one day because of its obscene and inflammatory tweets. This article uses the case of Tay to re-examine theories of agency. How did users view the personality and actions of an artificial intelligence chat bot when interacting with Tay on Twitter? Using phenomenological research methods and pragmatic approaches to agency, we look at what people said about Tay to study how they imagine and interact with emerging technologies and to show the limitations of our current theories of agency for describing communication in these settings. We show how different qualities of agency, different expectations for technologies, and different capacities for affordance emerge in the interactions between people and artificial intelligence. We argue that a perspective of “symbiotic agency”— informed by the imagined affordances of emerging technology—is required to really understand the collapse of Tay.",2016.0,Gina Neff
83f7b7b3869ce46040d3bdfbbfa526fc4ac462a2,https://www.semanticscholar.org/paper/83f7b7b3869ce46040d3bdfbbfa526fc4ac462a2,Advances in artificial immune systems,"During the last decade, the field of artificial immune system (A1S) is progressing slowly and steadily as a branch of computational intelligence (CI). There has been increasing interest in the development of computational models inspired by several immunological principles. In particular, some are building models mimicking the mechanisms in the biological immune system (BIS) to better understand its natural processes and simulate its dynamical behavior in the presence of antigens/pathogens. Most of the AIS models, however, emphasize designing artifacts - computational algorithms, techniques using simplified models of various immunological processes and functionalities. Like other biologically-inspired techniques, such as artificial neural networks, genetic algorithms, and cellular automata, AISs also try to extract ideas from the BIS in order to develop computational tools for solving science and engineering problems. Although still relatively young, the artificial immune system (AIS) is emerging as an active and attractive, field involving models, techniques and applications of greater diversity",2006.0,D. Dasgupta
b055f184bf01b07d3c6598b65c274a81e05b111f,https://www.semanticscholar.org/paper/b055f184bf01b07d3c6598b65c274a81e05b111f,Artificial Societies: The Computer Simulation of Social Life,"An exploration of the implications of developments in artificial intelligence for social scientific research, which builds on the theoretical and methodological insights provided by ""Simulating societies"".; This book is intended for worldwide library market for social science subjects such as sociology, political science, geography, archaeology/anthropology, and significant appeal within computer science, particularly artificial intelligence. Also personal reference for researchers.",1995.0,"N. Gilbert, R. Conte"
44132e052925189c8b73e54e8c7eab77110ce704,https://www.semanticscholar.org/paper/44132e052925189c8b73e54e8c7eab77110ce704,A Review of the Literature,"Ambient intelligence stems from artificial intelligence and it can be defined as technology interacting and responding to humans. For an ambient intelligent environment to exist, it must contain three types of system: sensing, reasoning and acting. The primary sensors used in ambient intelligence are audiovisual, passive infrared, and radio frequency identification technology. To simulate a more natural living environment, a fuzzy computing system is used. Using a fuzzy computing system can also make it possible for machines to understand culture by analyzing people’s points of interests based on certain factors. Machines are also capable of generating and displaying creativity. Gaps in research and progression of ambient intelligence are still present due to the lack of trust and reliability these machines will possibly display. Abstract protocols use device drivers that act as the interface between the abstract protocol and the real protocol. The main focus of an ambient intelligence abstract protocol “is to realize a collection of objects having the same interface and implementing the different protocols used in” an ambient intelligence context. This makes it easier for the machine to organize its protocols and recall it to one location, which makes it faster and more efficient for the machine to execute an operation.",1960.0,Anthony Colombo
9e717a6b761f4bd5e7d57be563d6fa8efe62b186,https://www.semanticscholar.org/paper/9e717a6b761f4bd5e7d57be563d6fa8efe62b186,Are artificial neural networks black boxes?,"Artificial neural networks are efficient computing models which have shown their strengths in solving hard problems in artificial intelligence. They have also been shown to be universal approximators. Notwithstanding, one of the major criticisms is their being black boxes, since no satisfactory explanation of their behavior has been offered. In this paper, we provide such an interpretation of neural networks so that they will no longer be seen as black boxes. This is stated after establishing the equality between a certain class of neural nets and fuzzy rule-based systems. This interpretation is built with fuzzy rules using a new fuzzy logic operator which is defined after introducing the concept of f-duality. In addition, this interpretation offers an automated knowledge acquisition procedure.",1997.0,"J. M. Benítez, J. Castro, I. Requena"
cdb5aae625c63ab8ad3de3bce877f92cebbaca6f,https://www.semanticscholar.org/paper/cdb5aae625c63ab8ad3de3bce877f92cebbaca6f,The Sciences of the Artificial - 3rd Edition,"Continuing his exploration of the organization of complexity and the science of design, this new edition of Herbert Simon's classic work on artificial intelligence adds a chapter that sorts out the current themes and tools—chaos, adaptive systems, genetic algorithms—for analyzing complexity and complex systems. There are updates throughout the book as well. These take into account important advances in cognitive psychology and the science of design while confirming and extending the book's basic thesis: that a physical symbol system has the necessary and sufficient means for intelligent action. The chapter ""Economic Reality"" has also been revised to reflect a change in emphasis in Simon's thinking about the respective roles of organizations and markets in economic systems.",1981.0,H. Simon
4b3439884c4dbac5fc5530ab5afeaf3669e85462,https://www.semanticscholar.org/paper/4b3439884c4dbac5fc5530ab5afeaf3669e85462,Sequential Decisions based on Algorithmic Probability,"Motivation The dream of creating artificial devices that reach or outperform human intelligence is an old one, however a computationally efficient theory of true intelligence has not been found yet, despite considerable efforts in the last 50 years. Nowadays most research is more modest, focussing on solving more narrow, specific problems, associated with only some aspects of intelligence, like playing chess or natural language translation, either as a goal in itself or as a bottom-up approach. The dual, top down approach, is to find a mathematical (not computational) definition of general intelligence. Note that the AI problem remains non-trivial even when ignoring computational aspects. Universal Artificial Intelligence 5 Marcus Hutter",2008.0,Marcus Hutter
05a5156aae149f576d325e090c17549ed3fc1262,https://www.semanticscholar.org/paper/05a5156aae149f576d325e090c17549ed3fc1262,Agent-mediated electronic commerce: a survey,Software agents help automate a variety of tasks including those involved in buying and selling products over the Internet. This paper surveys several of these agent-mediated electronic commerce systems by describing their roles in the context of a Consumer Buying Behavior (CBB) model. The CBB model we present augments traditional marketing models with concepts from Software Agents research to accommodate electronic markets. We then discuss the variety of Artificial Intelligence techniques that support agent mediation and conclude with future directions of agent-mediated electronic commerce research.,1998.0,"Robert H. Guttman, Alexandros Moukas, P. Maes"
7ddc973a3243d6d6559c9aa5da976670552e2784,https://www.semanticscholar.org/paper/7ddc973a3243d6d6559c9aa5da976670552e2784,Visual Analytics for Explainable Deep Learning,"Recently, deep learning has been advancing the state of the art in artificial intelligence to a new level, and humans rely on artificial intelligence techniques more than ever. However, even with such unprecedented advancements, the lack of explanation regarding the decisions made by deep learning models and absence of control over their internal processes act as major drawbacks in critical decision-making processes, such as precision medicine and law enforcement. In response, efforts are being made to make deep learning interpretable and controllable by humans. This article reviews visual analytics, information visualization, and machine learning perspectives relevant to this aim, and discusses potential challenges and future research directions.",2018.0,"J. Choo, Shixia Liu"
4d220a7bb967ea2185e624e9b50a5fddecdbc330,https://www.semanticscholar.org/paper/4d220a7bb967ea2185e624e9b50a5fddecdbc330,Tell Me a Story: Narrative and Intelligence,"From the Publisher: 
How are our memories, our narratives, and our intelligence interrelated? What can artificial intelligence and narratology say to each other? In this pathbreaking study by an expert on learning and computers, Roger C. Schank argues that artificial intelligence must be based on real human intelligence, which consists largely of applying old situations - and our narratives of them - to new situations in less than obvious ways. To design smart machines, Schank therefore investigated how people use narratives and stories, the nature and function of those narratives, and the connection of intelligence to both telling and listening. As Schank explains, ""We need to tell someone else a story that describes our experiences because the process of creating the story also creates the memory structure that will contain the gist of the story for the rest of our lives. Talking is remembering."" This first paperback edition includes an illuminating foreword by Gary Saul Morson.",1991.0,"R. Schank, G. S. Morson"
243e050cabd7bcddc9b39396b5da371eefe7e2df,https://www.semanticscholar.org/paper/243e050cabd7bcddc9b39396b5da371eefe7e2df,Cognitive artifacts,"Artifacts pervade our lives, our every activity. The speed, power, and intelligence of human beings are drambtically enhanced by the invention of artificial devices, so much so that tool making and usage constitute one of the defining characteristics of our species. Many artifacts make us stronger or faster, or protect us from the elements or predators, or feed and clothe us. And many artifacts make us smarter, increasing cognitive capabilities and making possible the modern intellectual world.",1991.0,D. Norman
c3e9d476f10b0ab1efb9efc96c262ce2803ce272,https://www.semanticscholar.org/paper/c3e9d476f10b0ab1efb9efc96c262ce2803ce272,Knowledge Discovery and Data Mining,"Sponsored by the American Association for Artificial Intelligence and and Collocated with AAAI-96 and UAI-96. • Please submit 5 hardcopies of a short paper (9 pages or less) and send an electronic ASCII version of the title page by March 18,",2014.0,J. Xu
59ad3c0a160d67c1b75e8355d9ded6bdfa2ca1b6,https://www.semanticscholar.org/paper/59ad3c0a160d67c1b75e8355d9ded6bdfa2ca1b6,Misconceptions about real-time computing: a serious problem for next-generation systems,"The author defines real-time computing and states and dispels the most common misconceptions about it. He discusses the fundamental technical issues of real-time computing. He examines specification and verification, scheduling theory, operating systems, programming languages and design methodology, distributed databases, artificial intelligence, fault tolerance, architectures, and communication.<<ETX>>",1988.0,J. Stankovic
c4e3be316ce0d5dfc9ec7b19298e9483484cc252,https://www.semanticscholar.org/paper/c4e3be316ce0d5dfc9ec7b19298e9483484cc252,Semantic Scholar,"Eagerly awaited by researchers for years, concrete examples of artificial intelligence–enabled search engines are beginning to emerge. Founded by the nonprofit Allen Institute for Artificial Intelligence (AI2), Semantic Scholar began as a search engine for computer science, geoscience, and neuroscience in 2015. In response to researchers’ inability to keep pace with reading all of the publications in their disciplines, the purpose of the project is automated learning from text in order to overcome information overload.",2018.0,Suzanne Fricke
bf522c6ca006a11bc0473424b7b683481948978a,https://www.semanticscholar.org/paper/bf522c6ca006a11bc0473424b7b683481948978a,Intelligent Tutoring Systems,"""The first volume to appear on this topic and now a classic in the field, ""Intelligent Tutoring Systems"" provides the reader with descriptions of the major systems implemented before 1981. The introduction seeks to emphasise the principal contributions made in the field, to outline continuing research issues, and to relate these to research activities in artificial intelligence and cognitive science. Subject areas discussed are as varied as arithmetic, algebra, electronics, and medicine, together with some informal gaming environments""",1982.0,"Derek Sleeman, John Seely Brown"
c89c71dbe5617bea44383585b58cd0cbc37bf79a,https://www.semanticscholar.org/paper/c89c71dbe5617bea44383585b58cd0cbc37bf79a,General Game Playing: Overview of the AAAI Competition,"A general game playing system is one that can accept a formal description of a game and play the game effectively without human intervention. Unlike specialized game players, such as Deep Blue, general game players do not rely on algorithms designed in advance for specific games; and, unlike Deep Blue, they are able to play different kinds of games. In order to promote work in this area, the AAAI is sponsoring an open competition at this summer's Twentieth National Conference on Artificial Intelligence. This article is an overview of the technical issues and logistics associated with this summer's competition, as well as the relevance of general game playing to the long range-goals of artificial intelligence.",2005.0,"M. Genesereth, Nathaniel Love, B. Pell"
17222e2ed3892b98935a5a48820e3d1f4802a4a3,https://www.semanticscholar.org/paper/17222e2ed3892b98935a5a48820e3d1f4802a4a3,Computational Philosophy of Science,"By applying research in artificial intelligence to problems in the philosophy of science, Paul Thagard develops an exciting new approach to the study of scientific reasoning. He uses computational ideas to shed light on how scientific theories are discovered, evaluated, and used in explanations. He describes a detailed computational model of problem solving and discovery that provides a conceptually rich yet rigorous alternative to accounts of scientific knowledge based on formal logic. The model is used to illuminate such topics as the nature of concepts, hypothesis formation, analogy, and theory justification.Following a critique of the alternative account of scientific development offered by evolutionary epistemology, Thagard discusses philosophical issues concerning reasoning, truth, and the justification of scientific methods. He applies his general conclusions about science and pseudoscience to the fields of psychology and artificial intelligence, and explores the potential relevance of computational models to our understanding of the interrelations of theory and experiment and of the importance of group rationality in science.""Computational Philosophy of Science"" has been made accessible to readers from different disciplines through an appendix that includes tutorials on essential philosophical, computational, and psychological topics.Paul Thagard is a research scientist at the Princeton University Cognitive Science Laboratory. He is coauthor, with John H. Holland, Keith J. Holyoak, and Richard E. Nisbett, of ""Induction: Processes of Inference, Learning, and Discovery (MIT Press/Bradford Books). A Bradford Book.",1988.0,Paul Thagard
0bd7dae8be6a7dada964c947d44fda81b7b622a3,https://www.semanticscholar.org/paper/0bd7dae8be6a7dada964c947d44fda81b7b622a3,Overview of Deep Learning in Gastrointestinal Endoscopy,"Artificial intelligence is likely to perform several roles currently performed by humans, and the adoption of artificial intelligence-based medicine in gastroenterology practice is expected in the near future. Medical image-based diagnoses, such as pathology, radiology, and endoscopy, are expected to be the first in the medical field to be affected by artificial intelligence. A convolutional neural network, a kind of deep-learning method with multilayer perceptrons designed to use minimal preprocessing, was recently reported as being highly beneficial in the field of endoscopy, including esophagogastroduodenoscopy, colonoscopy, and capsule endoscopy. A convolutional neural network-based diagnostic program was challenged to recognize anatomical locations in esophagogastroduodenoscopy images, Helicobacter pylori infection, and gastric cancer for esophagogastroduodenoscopy; to detect and classify colorectal polyps; to recognize celiac disease and hookworm; and to perform small intestine motility characterization of capsule endoscopy images. Artificial intelligence is expected to help endoscopists provide a more accurate diagnosis by automatically detecting and classifying lesions; therefore, it is essential that endoscopists focus on this novel technology. In this review, we describe the effects of artificial intelligence on gastroenterology with a special focus on automatic diagnosis, based on endoscopic findings.",2019.0,"Jun Ki Min, M. Kwak, J. Cha"
d354f0d67a168f1f17e28a7687e9465a83391d2b,https://www.semanticscholar.org/paper/d354f0d67a168f1f17e28a7687e9465a83391d2b,Understanding intelligence,"From the Publisher: 
Researchers now agree that intelligence always manifests itself in behavior - thus it is behavior that we must understand. An exciting new field has grown around the study of behavior-based intelligence, also known as embodied cognitive science, ""new AI,"" and ""behavior-based AI."". ""Rolf Pfeifer and Christian Scheier provide a systematic introduction to this new way of thinking about intelligence and computers. After discussing concepts and approaches such as subsumption architecture, Braitenberg vehicles, evolutionary robotics, artificial life, self-organization, and learning, the authors derive a set of principles and a coherent framework for the study of naturally and artificially intelligent systems, or autonomous agents. This framework is based on a synthetic methodology whose goal is understanding by designing and building.",1999.0,"Rolf Pfeifer, C. Scheier"
1e3f5dbaf569ca82a4884253fc74c05357e5afe4,https://www.semanticscholar.org/paper/1e3f5dbaf569ca82a4884253fc74c05357e5afe4,Computational Intelligence: An Introduction,"Computational Intelligence: An Introduction, Second Edition offers an in-depth exploration into the adaptive mechanisms that enable intelligent behaviour in complex and changing environments. The main focus of this text is centred on the computational modelling of biological and natural intelligent systems, encompassing swarm intelligence, fuzzy systems, artificial neutral networks, artificial immune systems and evolutionary computation. Engelbrecht provides readers with a wide knowledge of Computational Intelligence (CI) paradigms and algorithms; inviting readers to implement and problem solve real-world, complex problems within the CI development framework. This implementation framework will enable readers to tackle new problems without any difficulty through a single Java class as part of the CI library.",2002.0,A. Engelbrecht
3839aae244763195f1437e7c00d2c6c8f5e3b549,https://www.semanticscholar.org/paper/3839aae244763195f1437e7c00d2c6c8f5e3b549,AI Methods in Algorithmic Composition: A Comprehensive Survey,"Algorithmic composition is the partial or total automation of the process of music composition by using computers. Since the 1950s, different computational techniques related to Artificial Intelligence have been used for algorithmic composition, including grammatical representations, probabilistic methods, neural networks, symbolic rule-based systems, constraint programming and evolutionary algorithms. This survey aims to be a comprehensive account of research on algorithmic composition, presenting a thorough view of the field for researchers in Artificial Intelligence.",2013.0,"Jose D. Fernández, F. Vico"
b8dea2e514d48a817dfed07781db03e2f7986c14,https://www.semanticscholar.org/paper/b8dea2e514d48a817dfed07781db03e2f7986c14,Computer power and human reason,"I had some strong reactions to Joe Weizenbaum's book, Computer Power and Human Reason. The book mentions some important concerns which are obscured by harsh and sometimes shrill accusations against the Artificial Intelligence research community. On the whole, it seems to me that the personal attacks distract and mislead the reader from more valuable abstract points. I strongly recommend Samuel Florman's article ""In Praise of Technology"" in the November, 1975, issue of Harper's Magazine to see a different opinion about the role of technology in modern society.",1976.0,"B. Kuipers, J. McCarthy, J. Weizenbaum"
9c94edb59aa8edfff0003b508841e31ef4c64707,https://www.semanticscholar.org/paper/9c94edb59aa8edfff0003b508841e31ef4c64707,New Approaches to Robotics,"In order to build autonomous robots that can carry out useful work in unstructured environments new approaches have been developed to building intelligent systems. The relationship to traditional academic robotics and traditional artificial intelligence is examined. In the new approaches a tight coupling of sensing to action produces architectures for intelligence that are networks of simple computational elements which are quite broad, but not very deep. Recent work within this approach has demonstrated the use of representations, expectations, plans, goals, and learning, but without resorting to the traditional uses of central, abstractly manipulable or symbolic representations. Perception within these systems is often an active process, and the dynamics of the interactions with the world are extremely important. The question of how to evaluate and compare the new to traditional work still provokes vigorous discussion.",1991.0,R. Brooks
decb7e746acb87710c2a15585cd22133ffc2cc95,https://www.semanticscholar.org/paper/decb7e746acb87710c2a15585cd22133ffc2cc95,"General Video Game AI: Competition, Challenges and Opportunities","
 
 The General Video Game AI framework and competition pose the problem of creating artificial intelligence that can play a wide, and in principle unlimited, range of games. Concretely, it tackles the problem of devising an algorithm that is able to play any game it is given, even if the game is not known a priori. This area of study can be seen as an approximation of General Artificial Intelligence, with very little room for game-dependent heuristics. This short paper summarizes the motivation, infrastructure, results and future plans of General Video Game AI, stressing the findings and first conclusions drawn after two editions of our competition, and outlining our future plans.
 
",2016.0,"Diego Perez Liebana, Spyridon Samothrakis, J. Togelius, T. Schaul, S. Lucas"
7d513ba367b84af60e7d0bf9e1da9aa264e20029,https://www.semanticscholar.org/paper/7d513ba367b84af60e7d0bf9e1da9aa264e20029,Artificial Neural Networks for Land-Cover Classification and Mapping,"Abstract. Artificial intelligence approaches toward image processing and pattern recognition are perceived as an alternative to, and an improvement over, traditional statistically-based procedures. Of particular interest to the satellite remote sensing community are artificial neural networks. This article describes the application of such an approach to the problem of deriving land-cover information from Landsat satellite Thematic Mapper (TM) digital imagery. The techniques being developed are ones that will provide more accurate and useful data for use with geographical information systems.",1993.0,D. Civco
67ca442b1662171733d234e52b3c4af929619c45,https://www.semanticscholar.org/paper/67ca442b1662171733d234e52b3c4af929619c45,Prolegomena to any future artificial moral agent,"As artificial intelligence moves ever closer to the goal of producing fully autonomous agents, the question of how to design and implement an artificial moral agent (AMA) becomes increasingly pressing. Robots possessing autonomous capacities to do things that are useful to humans will also have the capacity to do things that are harmful to humans and other sentient beings. Theoretical challenges to developing artificial moral agents result both from controversies among ethicists about moral theory itself, and from computational limits to the implementation of such theories. In this paper the ethical disputes are surveyed, the possibility of a ‘moral Turing Test’ is considered and the computational difficulties accompanying the different types of approach are assessed. Human-like performance, which is prone to include immoral actions, may not be acceptable in machines, but moral perfection may be computationally unattainable. The risks posed by autonomous machines ignorantly or deliberately harming people and other sentient beings are great. The development of machines with enough intelligence to assess the effects of their actions on sentient beings and act accordingly may ultimately be the most important task faced by the designers of artificially intelligent automata.",2000.0,"C. Allen, G. Varner, Jason Zinser"
94ba19382a5d5968af889c870a783fa09c94ed70,https://www.semanticscholar.org/paper/94ba19382a5d5968af889c870a783fa09c94ed70,Cognitive and social action,"This monograph addresses the worlds of social science theory and artificial intelligence (AI). The book examines the interaction of individual cognitive factors and social influence on human action and discusses the implications for developments in artificial intelligence. This book is intended for graduate and research level artificial intelligence and social science theory (including sociology, economics, psychology).",1995.0,"R. Conte, C. Castelfranchi"
e7c0f2d271494799f1dc4e4e8f674685d7095b8e,https://www.semanticscholar.org/paper/e7c0f2d271494799f1dc4e4e8f674685d7095b8e,Legal Personhood for Artificial Intelligences,"Could an artificial intelligence become a legal person? As of today, this question is only theoretical. No existing computer program currently possesses the sort of capacities that would justify serious judicial inquiry into the question of legal personhood. The question is nonetheless of some interest. Cognitive science begins with the assumption that the nature of human intelligence is computational, and therefore, that the human mind can, in principle, be modelled as a program that runs on a computer. Artificial intelligence (AI) research attempts to develop such models. But even as cognitive science has displaced behavioralism as the dominant paradigm for investigating the human mind, fundamental questions about the very possibility of artificial intelligence continue to be debated. This Essay explores those questions through a series of thought experiments that transform the theoretical question whether artificial intelligence is possible into legal questions such as, ""Could an artificial intelligence serve as a trustee?"" What is the relevance of these legal thought experiments for the debate over the possibility of artificial intelligence? A preliminary answer to this question has two parts. First, putting the AI debate in a concrete legal context acts as a pragmatic Occam's razor. By reexamining positions taken in cognitive science or the philosophy of artificial intelligence as legal arguments, we are forced to see them anew in a relentlessly pragmatic context. Philosophical claims that no program running on a digital computer could really be intelligent are put into a context that requires us to take a hard look at just what practical importance the missing reality could have for the way we speak and conduct our affairs. In other words, the legal context provides a way to ask for the ""cash value"" of the arguments. The hypothesis developed in this Essay is that only some of the claims made in the debate over the possibility of AI do make a pragmatic difference, and it is pragmatic differences that ought to be decisive. Second, and more controversially, we can view the legal system as a repository of knowledge-a formal accumulation of practical judgments. The law embodies core insights about the way the world works and how we evaluate it. Moreover, in common-law systems judges strive to decide particular cases in a way that best fits the legal landscape-the prior cases, the statutory law, and the constitution. Hence, transforming the abstract debate over the possibility of AI into an imagined hard case forces us to check our intuitions and arguments against the assumptions that underlie social decisions made in many other contexts. By using a thought experiment that explicitly focuses on wide coherence, we increase the chance that the positions we eventually adopt will be in reflective equilibrium with our views about related matters. In addition, the law embodies practical knowledge in a form that is subject to public examination and discussion. Legal materials are published and subject to widespread public scrutiny and discussion. Some of the insights gleaned in the law may clarify our approach to the artificial intelligence debate.",2008.0,Lawrence B. Solum
92a81ebad23782100bd9414b02ed573e306970f0,https://www.semanticscholar.org/paper/92a81ebad23782100bd9414b02ed573e306970f0,The soar papers : research on integrated intelligence,"Soar is a computational theory of the mind that has had an impact in both artificial intelligence and cognitive science. Begun by John E. Laird, Allen Newell, and Paul S. Rosenbloom at Carnegie Mellon in the early 1980s, the Soar Project is an investigation into the architecture underlying intelligent behaviour with the goal of developing and applying a unified theory of natural and artificial intelligence. ""The Soar Papers"" - sixty-three article in all - provide in one place the important ideas that have emerged from this project. The book is organized chronologically, with an introduction that provides multiple organizations according to major topics. Readers interested in the entire effort can read the articles in publication order, while readers interested only in a specific topic can go directly to a logical sequence of papers to read on that topic.",1993.0,"P. Rosenbloom, J. Laird, A. Newell"
ea09a6981bb4316994a5ed0e2756f947bd58fcd2,https://www.semanticscholar.org/paper/ea09a6981bb4316994a5ed0e2756f947bd58fcd2,Genetic programming and emergent intelligence,"Angeline 23 Fogel, D. B (1993). Using evolutionary programming to create neural networks that are capable of playing Tic-Tac-Toe. (1992). Evolution as a theme in artificial life: The genesys/tracker system. In Artificial Life II, C. Angeline 22 4.6 Conclusion Artificial intelligence has made great strides in computational problem solving using explicitly represented knowledge extracted from the task. If we continue to use explicitly represented knowledge exclusively for computational problem solving, we may never computationally accomplish a level of problem solving performance equal to humans. Emergent intelligence de-emphasizes the role of explicit knowledge and encourages the development of solutions that incorporate the task description as a component of the problem solver. This allows the constraints of the task to be represented more naturally and permits only pertinent task specific knowledge to emerge in the course of solving the problem.",1994.0,P. Angeline
e5a7b72deca526c9e03d0cd19ece08d861b3ac89,https://www.semanticscholar.org/paper/e5a7b72deca526c9e03d0cd19ece08d861b3ac89,Cognitive Carpentry: A Blueprint for How to Build a Person,"From the Publisher: 
In his groundbreaking new book, John Pollock establishes an outpost at the crossroads where artificial intelligence meets philosophy. Specifically, he proposes a general theory of rationality and then describes its implementation in OSCAR, an architecture for an autonomous rational agent he claims is the ""first AI system capable of performing reasoning that philosophers would regard as epistemically sophisticated."" 
A sequel to Pollock's How to Build a Person, this volume builds upon that theoretical groundwork for the implementation of rationality through artificial intelligence. Pollock argues that progress in AI has stalled because of its creators' reliance upon unformulated intuitions about rationality. Instead, he bases the OSCAR architecture upon an explicit philosophical theory of rationality, encompassing principles of practical cognition, epistemic cognition, and defeasible reasoning. One of the results is the world's first automated defeasible reasoner capable of reasoning in a rich, logical environment. 
Underlying Pollock's thesis is a conviction that the tenets of artifical intelligence and those of philosophy can be complementary and mutually beneficial. And, while members of both camps have in recent years grown skeptical of the very possibility of ""symbol processing"" AI, Cognitive Carpentry establishes that such an approach to AI can be successful. 
A Bradford Book",1995.0,J. Pollock
f6162dac54a282fa7307a180881be2b0dcaaba8d,https://www.semanticscholar.org/paper/f6162dac54a282fa7307a180881be2b0dcaaba8d,Imitation in Animals and Artifacts,"The effort to explain the imitative abilities of humans and other animals draws on fields as diverse as animal behavior, artificial intelligence, computer science, comparative psychology, neuroscience, primatology, and linguistics. This volume represents a first step toward integrating research from those studying imitation in humans and other animals, and those studying imitation through the construction of computer software and robots. Imitation is of particular importance in enabling robotic or software agents to share skills without the intervention of a programmer and in the more general context of interaction and collaboration between software agents and humans. Imitation provides a way for the agent -- whether biological or artificial -- to establish a ""social relationship"" and learn about the demonstrator's actions, in order to include them in its own behavioral repertoire. Building robots and software agents that can imitate other artificial or human agents in an appropriate way involves complex problems of perception, experience, context, and action, solved in nature in various ways by animals that imitate.",2002.0,"K. Dautenhahn, C. Nehaniv"
125c08c24f01b17ecf72071624d4744199778cd1,https://www.semanticscholar.org/paper/125c08c24f01b17ecf72071624d4744199778cd1,Modeling Adaptive Autonomous Agents,"One category of research in Artificial Life is concerned with modeling and building so-called adaptive autonomous agents, which are systems that inhabit a dynamic, unpredictable environment in which they try to satisfy a set of time-dependent goals or motivations. Agents are said to be adaptive if they improve their competence at dealing with these goals based on experience. Autonomous agents constitute a new approach to the study of Artificial Intelligence (AI), which is highly inspired by biology, in particular ethology, the study of animal behavior. Research in autonomous agents has brought about a new wave of excitement into the field of AI. This paper reflects on the state of the art of this new approach. It attempts to extract its main ideas, evaluates what contributions have been made so far, and identifies its current limitations and open problems.",1993.0,P. Maes
4f79b0bf29a07e8c31f08bb2544a5d6b1dd481c8,https://www.semanticscholar.org/paper/4f79b0bf29a07e8c31f08bb2544a5d6b1dd481c8,Intelligence as Adaptive Behavior: An Experiment in Computational Neuroethology,Foundations. Biological Background. The Artificial Insect. Locomotion Controller. Lesion Studies. Exploration. Feeding. Behavioral Choice. Discussion. Appendixes: Physical Parameters. Neural Parameters.,1990.0,"L. Sterling, Y. Pao, R. Beer"
58076b99b2a5521b1d8daccb2d69f667ca47a816,https://www.semanticscholar.org/paper/58076b99b2a5521b1d8daccb2d69f667ca47a816,Transportation modeling: an artificial life approach,"Artificial life (ALife) uses biological knowledge and techniques to help solve different engineering, management, control and computational problems. Natural systems teach us that very simple individual organisms can form systems capable of performing highly complex tasks by dynamically interacting with each other. The main goal of this paper is to show how we can use ALife concepts (inspired by some principles of natural swarm intelligence) when solving complex problems in traffic and transportation. The bee system that represents the new approach in the field of swarm intelligence is described. It is also shown in the paper that ALife approach can be successful to ""attack"" transportation problems characterized by uncertainty. The fuzzy ant system (FAS) described in the paper represents an attempt to handle the uncertainty that sometimes exists in some complex transportation problems. The potential applications of the bee system and the fuzzy ant system in the field of traffic and transportation engineering are discussed.",2002.0,"P. Lucic, D. Teodorovic"
b9c4d931665ec87c16fcd44cae8fdaec1215e81e,https://www.semanticscholar.org/paper/b9c4d931665ec87c16fcd44cae8fdaec1215e81e,"TORCS, The Open Racing Car Simulator","The open racing car simulator (TORCS [14]), is a modern, modular, highlyportable multi-player, multi-agent car simulator. Its high degree of modularity and portability render it ideal for artificial intelligence research. Indeed, a number of research-oriented competitions and papers have already appeared that make use of the TORCS engine. The purpose of this document is to introduce the structure of TORCS to the general artificial intelligence and machine learning community and explain how it is possible to tests agents on the platform. TORCS can be used to develop artificially intelligent (AI) agents for a variety of problems. At the car level, new simulation modules can be developed, which include intelligent control systems for various car components. At the driver level, a low-level API gives detailed (but only partial) access to the simulation state. This could be used to develop anything from mid-level control systems to complex driving agents that find optimal racing lines, react successfully in unexpected situations and make good tactical race decisions. Finally, for researchers that like a challenge and are also interested in visual processing, a 3d projection interface is available.",2005.0,"E. Espié, Christophe Guionneau, Bernhard Wymann, Christos Dimitrakakis, Rémi Coulom, Andrew Sumner"
fc436e3566b48c50424881f852d77a13f5ed8bde,https://www.semanticscholar.org/paper/fc436e3566b48c50424881f852d77a13f5ed8bde,Checkers Is Solved,"The game of checkers has roughly 500 billion billion possible positions (5 × 1020). The task of solving the game, determining the final result in a game with no mistakes made by either player, is daunting. Since 1989, almost continuously, dozens of computers have been working on solving checkers, applying state-of-the-art artificial intelligence techniques to the proving process. This paper announces that checkers is now solved: Perfect play by both sides leads to a draw. This is the most challenging popular game to be solved to date, roughly one million times as complex as Connect Four. Artificial intelligence technology has been used to generate strong heuristic-based game-playing programs, such as Deep Blue for chess. Solving a game takes this to the next level by replacing the heuristics with perfection.",2007.0,"J. Schaeffer, Neil Burch, Y. Björnsson, Akihiro Kishimoto, Martin Müller, R. Lake, P. Lu, Steve Sutphen"
169f382fc20f40cb355300b29822d1f0b439c07f,https://www.semanticscholar.org/paper/169f382fc20f40cb355300b29822d1f0b439c07f,Comparative Analysis of Ant Colony and Particle Swarm Optimization Techniques,"For a decade swarm Intelligence, an artificial intelligence discipline, is concerned with the design of intelligent multi-agent systems by taking inspiration from the collective behaviors of social insects and other animal societies. They are characterized by a decentralized way of working that mimics the behavior of the swarm. Swarm Intelligence is a successful paradigm for the algorithm with complex problems. This paper focuses on the comparative analysis of most successful methods of optimization techniques inspired by Swarm Intelligence (SI) : Ant Colony Optimization (ACO) and Particle Swarm Optimization (PSO). An elaborate comparative analysis is carried out to endow these algorithms with fitness sharing, aiming to investigate whether this improves performance which can be implemented in the evolutionary algorithms.",2010.0,"V.Selvi Dr, R.Umarani Lecturer, Salem. Tamilnadu"
16d480717a1d233ae94b09e3b983d8cc96437644,https://www.semanticscholar.org/paper/16d480717a1d233ae94b09e3b983d8cc96437644,The British Nationality Act as a logic program,The formalization of legislation and the development of computer systems to assist with legal problem solving provide a rich domain for developing and testing artificial-intelligence technology.,1986.0,"M. Sergot, F. Sadri, R. Kowalski, F. Kriwaczek, P. Hammond, H. T. Cory"
65f872006a9df0ed7a7d522934b0e54aa03d6c2a,https://www.semanticscholar.org/paper/65f872006a9df0ed7a7d522934b0e54aa03d6c2a,Metamagical Themas: Questing for the Essence of Mind and Pattern,"From the Publisher: 
A bestselling collection of brilliant and quirky essays, on subjects ranging from biology to grammar to artificial intelligence, that are unified by one primary concern: the way people perceive and think.",1985.0,D. Hofstadter
ad2d95fffd34d69a7c190a46412a9ee761f756d1,https://www.semanticscholar.org/paper/ad2d95fffd34d69a7c190a46412a9ee761f756d1,The Theoretical Framework of Cognitive Informatics,"Cognitive Informatics (CI) is a transdisciplinary enquiry of the internal information processing mechanisms and processes of the brain and natural intelligence shared by almost all science and engineering disciplines. This article presents an intensive review of the new field of CI. The structure of the theoretical framework of CI is described encompassing the Layered Reference Model of the Brain (LRMB), the OAR model of information representation, Natural Intelligence (NI) vs. Artificial Intelligence (AI), Autonomic Computing (AC) vs. imperative computing, CI laws of software, the mechanism of human perception processes, the cognitive processes of formal inferences, and the formal knowledge system. Three types of new structures of mathematics, Concept Algebra (CA), Real-Time Process Algebra (RTPA), and System Algebra (SA), are created to enable rigorous treatment of cognitive processes of the brain as well as knowledge representation and manipulation in a formal and coherent framework. A wide range of applications of CI in cognitive psychology, computing, knowledge engineering, and software engineering has been identified and discussed.",2007.0,Yingxu Wang
8b2ba607aa8554a445d9cc87648ca10e87afee4c,https://www.semanticscholar.org/paper/8b2ba607aa8554a445d9cc87648ca10e87afee4c,Decision-Making in an Embedded Reasoning System,"The development of reasoning systems that can reason and plan in a continuously changing environment is emerging as an important area of research in Artificial Intelligence. This paper describes some of the features of a Procedural Reasoning System (PRS) that enables it to operate effectively in such environments. The basic system design is first described and it is shown how this architecture supports both goal-directed reasoning and the ability to react rapidly to unanticipated changes in the environment. The decision-making capabilities of the system are then discussed and it is indicated how the system integrates these components in a manner that takes account of the bounds on both resources and knowledge that typify most real-time operations. The system has been applied to handling malfunctions on the space shuttle, threat assessment, and the control of an autonomous robot.",1989.0,"M. Georgeff, F. Ingrand"
431bed4efc485e17f7c7854c2c6ac1b9fdaf6966,https://www.semanticscholar.org/paper/431bed4efc485e17f7c7854c2c6ac1b9fdaf6966,Statistical language learning,"From the Publisher: 
Eugene Charniak breaks new ground in artificial intelligence research by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background. 
New, exacting empirical methods are needed to break the deadlock in such areas of artificial intelligence as robotics, knowledge representation, machine learning, machine translation, and natural language processing (NLP). It is time, Charniak observes, to switch paradigms. This text introduces statistical language processing techniques -- word tagging, parsing with probabilistic context free grammars, grammar induction, syntactic disambiguation, semantic word classes, word-sense disambiguation -- along with the underlying mathematics and chapter exercises. 
Charniak points out that as a method of attacking NLP problems, the statistical approach has several advantages. It is grounded in real text and therefore promises to produce usable results, and it offers an obvious way to approach learning: ""one simply gathers statistics."" 
Language, Speech, and Communication",1995.0,Eugene Charniak
b74b90d8fef8cfc75ea501f462e5b9183a676abe,https://www.semanticscholar.org/paper/b74b90d8fef8cfc75ea501f462e5b9183a676abe,Forecasting Principles and Applications,"Foundation forecasting univariate methods univariate ARIMA methods multivariate/causal methods cyclical, qualitative and artificial intelligence methods combining, validation and managerial issues.",1998.0,S. Delurgio
35bbc5ce402e88705e6e599f9aa45f8889b9e369,https://www.semanticscholar.org/paper/35bbc5ce402e88705e6e599f9aa45f8889b9e369,Probabilistic Logic *,"Because many artificial intelligence applications require the ability to reason with uncertain knowledge , it is important to seek appropriate generalizations of logic for that case. We present here a semantical generalization of logic in which the truth values of sentences are probabili~ values (between 0 and 1). Our generalization applies to any logical system for which the consistency of a finite set of sentences can be established. The method described in the present paper combines logic with probability theory in such a way that probabilistic logical entaihnent reduces to ordinary logical entailment when the probabilities of all sentences are either 0 or 1.",,"N. Nilsson, Donald Loveland"
8ed3f902bc17ed29cd10cfb2e3fcbd67880f3a7e,https://www.semanticscholar.org/paper/8ed3f902bc17ed29cd10cfb2e3fcbd67880f3a7e,Age of intelligent machines,"What is artificial intelligence? At its essence, it is another way of answering a central question that has been debated by scientists, philosophers, and theologians for thousands of years: How does the human brain - three pounds of ordinary matter - give rise to thought? With this question in mind, inventor and visionary computer scientist Raymond Kurzweil probes the past, present, and future of artificial intelligence, from its earliest philosophical and mathematical roots through today's moving frontier, to tantalizing glimpses of 21st-century machines with superior intelligence and truly prodigious speed and memory. Lavishly illustrated and easily accessible to the nonspecialist, The Age of Intelligent Machines provides the background needed for a full understanding of the enormous scientific potential represented by intelligent machines and of their equally profound philosophic, economic, and social implications. It examines the history of efforts to understand human intelligence and to emulate it by building devices that seem to act with human capabilities. Running alongside Kurzweil's historical and scientific narrative, are 23 articles examining contemporary issues in artificial intelligence by such luminaries as Daniel Dennett, Sherry Turkle, Douglas Hofstadter, Marvin Minsky, Seymour Papert, Edward Feigenbaum, Allen Newell, and George Gilder. Raymond Kurzweil is the founder, chairman, and chief executive officer of Kurzweil Applied Intelligence, Kurzweil Music Systems, and the Kurzweil Reading Machines division of Xerox. He was the principal developer of the first print-to-speech reading machine for the blind and other significant advances in artificial intelligencetechnology.",1990.0,R. Kurzweil
d6992f10458e40769d982ec27d9021c560ee8a86,https://www.semanticscholar.org/paper/d6992f10458e40769d982ec27d9021c560ee8a86,Energy price forecasting in the Ontario competitive power system market,"This paper introduces a method for forecasting energy prices using artificial intelligence methods, such as neural networks and fuzzy logic, and a combination of the two. The new approach is compared with some of the exiting methods. Various factors affecting the market clearing price are investigated. Results for the Ontario electricity market are presented.",2004.0,"C. P. Rodriguez, G. Anders"
a3d05d42e371893573561909ca565241e523c342,https://www.semanticscholar.org/paper/a3d05d42e371893573561909ca565241e523c342,OPS5 user's manual,"Abstract : This is a combination introductory and reference manual for OPS5, a programming language for production systems. OPS5 is used primarily for applications in the areas of artificial intelligence, cognitive psychology, and expert systems. OPS5 interpreters have been implemented in LISP and BLISS.",1981.0,C. Forgy
b7ad09bf49cb267a06dba6547c28cde1592b720e,https://www.semanticscholar.org/paper/b7ad09bf49cb267a06dba6547c28cde1592b720e,Readings in Uncertain Reasoning,"Most everyday reasoning and decision making is based on uncertain premises. This volume collects 42 key papers from the literature, addressing the methods that have been used in artificial intelligence to build systems with the ability to manage uncertainty. The editors have added volume and sectio",1990.0,"G. Shafer, J. Pearl"
54531c0ce77465573dcc3fe0ffef1bea5c3837a6,https://www.semanticscholar.org/paper/54531c0ce77465573dcc3fe0ffef1bea5c3837a6,Planning English Sentences,"From the Publisher: 
This book is an investigation into the problems of generating natural language utterances to satisfy specific goals in the speaker's mind. It is thus an ambitious and significant contribution to research on language generation in artificial intelligence.",1988.0,D. Appelt
a3f76a7ac6c0d0f4b5f6a27b01af8a5545b80e36,https://www.semanticscholar.org/paper/a3f76a7ac6c0d0f4b5f6a27b01af8a5545b80e36,Theory and applications of neural networks for industrial control systems,"The theory and the applications of artificial neural networks, especially in a control field, are described. Recurrent networks and feedforward networks are discussed. Application to pattern recognition, information processing, design, planning, diagnosis, and control are examined. Hybrid systems using the neural networks, fuzzy sets, and artificial intelligence (AI) technologies are surveyed. >",1992.0,"T. Fukuda, T. Shibata"
6772f7ffccc5d320ef6f067c62c8e63083072892,https://www.semanticscholar.org/paper/6772f7ffccc5d320ef6f067c62c8e63083072892,Financial time series forecasting with machine learning techniques: a survey,"Stock index forecasting is vital for making informed investment decisions. This paper surveys recent literature in the domain of machine learning techniques and artificial intelligence used to forecast stock market movements. The publications are categorised according to the machine learning technique used, the forecasting timeframe, the input variables used, and the evaluation techniques employed. It is found that there is a consensus between researchers stressing the importance of stock index forecasting. Artificial Neural Networks (ANNs) are identified to be the dominant machine learning technique in this area. We conclude with possible future research directions.",2010.0,"Bjoern Krollner, B. Vanstone, G. Finnie"
f8e34b7d849abea9d61eaea01b5df3f7f5e73ee4,https://www.semanticscholar.org/paper/f8e34b7d849abea9d61eaea01b5df3f7f5e73ee4,The essence of neural networks,* Introduction * Classifying Patterns * Clustering Patterns * Pattern Association * A Sample of Recurrent Networks * Some Other Network Models and A Few Practical Points * Links to Artificial Intelligence * Synthesising Symbols with Neural Networks,1998.0,R. Callan
be90797bf5993526e66d108d2b11841e7634a5d3,https://www.semanticscholar.org/paper/be90797bf5993526e66d108d2b11841e7634a5d3,Intelligent tutoring systems: An overview,"Abstract: In this paper we look at the evolutionary development of Computer Assisted Instruction from the early days of ‘linear programs’ up to the use of ‘expert systems’ in education and training. We present the basic principles of Intelligent Tutoring Systems (ITS) which are capable of rich interaction with the student, which know how to teach, and who and what they are teaching. We point out the need for knowledge representation formalisms which can support ITS and present one such formalism (production systems). In the framework presented we describe systems developed for the teaching of modern languages, electronic trouble shooting and computer programming. Finally we point out the shortcomings of ITS and identify areas where a consensus of opinion does not exist. © 1986 Wiley Periodicals, Inc. 
 
(Masoud Yazdani: Masoud Yazdani studied Computer Science at the University of Essex and Artificial Intelligence at the University of Sussex. He joined Exeter University in 1980 as a lecturer in Computer Science. His research work is in two areas of computational models of creativity and intelligent tutoring systems. This latter area has led to implementation of various systems using artificial intelligence techniques to teach arithmetic and foreign languages. He is Chairman of Intellect Limited, a company specialising in applications of AI and to educate industry in what AI can offer it. He has acted as a consultant to various industrial companies including Acorn Computers, GTE Inc. and ITT Engineering Support Centre. He is the Committee Secretary for the Society for the Study of Artificial Intelligence and Simulation of Behaviour (AISB). His books include New Horizons in Educational Computing, Artificial Intelligence: Human Effects, both published by Ellis Horwood Ltd. and Artificial Intelligence: Principles and Applications published by Chapman and Hall.)",1986.0,M. Yazdani
00b3c91e1d7213b27499153c2272f0e24d1f2a79,https://www.semanticscholar.org/paper/00b3c91e1d7213b27499153c2272f0e24d1f2a79,The animat path to AI,"A research methodology is proposed for understanding intelligence through simulation of artificial animals (“animats”) in progressively more challenging environments while retaining characteristics of holism, pragmatism, perception, categorization, and adaptation that are often underrepresented in standard AI approaches to intelligence. It is suggested that basic elements of the methodology should include a theory/taxonomy of environments by which they can be ordered in difficulty—one is offered—and a theory of animat efficiency It is also suggested that the methodology offers a new approach to the problem of perception.",1991.0,Stewart W. Wilson
bf07f78601c6447bd115195a012a1315609ea8a1,https://www.semanticscholar.org/paper/bf07f78601c6447bd115195a012a1315609ea8a1,Principles of Neurodynamics. Perceptrons and the Theory of Brain Mechanisms.,"In recent years, there have been a number of engineering projects concerned with the design of brain models for pattern recognition and artificial intelligence. The basic assumption, underlying these projects is that the brain operates by built-in algorithmic methods similar to those employed in modern digital computers. Hence, nervous activity can be simulated by these computers. The value of such a program has been challenged by Lashley and others on the grounds that computer-simulated behavior is artificial, that the model is an invention operating on extrabiological principles. In this formidable book, Rosenblatt has offered a somewhat different program involving the design and testing of brain models described as perceptrons. His program is concerned not with devices for artificial intelligence, but, rather, with ""the physical structures and neurodynamics principles which underly ""natural intelligence."" A perceptron consists of a set of signal generating units (""neuro-mimes"") connected together to form a network.",1962.0,J. Orbach
5eb9dad01d9b61947d7a9aae6dfff400d20555b9,https://www.semanticscholar.org/paper/5eb9dad01d9b61947d7a9aae6dfff400d20555b9,Unified Structure Generation for Universal Information Extraction,"Information extraction suffers from its varying targets, heterogeneous structures, and demand-specific schemas. In this paper, we propose a unified text-to-structure generation framework, namely UIE, which can universally model different IE tasks, adaptively generate targeted structures, and collaboratively learn general IE abilities from different knowledge sources. Specifically, UIE uniformly encodes different extraction structures via a structured extraction language, adaptively generates target extractions via a schema-based prompt mechanism – structural schema instructor, and captures the common IE abilities via a large-scale pretrained text-to-structure model. Experiments show that UIE achieved the state-of-the-art performance on 4 IE tasks, 13 datasets, and on all supervised, low-resource, and few-shot settings for a wide range of entity, relation, event and sentiment extraction tasks and their unification. These results verified the effectiveness, universality, and transferability of UIE.",2022.0,"Yaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu"
f4cba0db34aa0c389cec267ca1f3ba5255ea2645,https://www.semanticscholar.org/paper/f4cba0db34aa0c389cec267ca1f3ba5255ea2645,Zero-Shot Information Extraction via Chatting with ChatGPT,"Zero-shot information extraction (IE) aims to build IE systems from the unannotated text. It is challenging due to involving little human intervention. Challenging but worthwhile, zero-shot IE reduces the time and effort that data labeling takes. Recent efforts on large language models (LLMs, e.g., GPT-3, ChatGPT) show promising performance on zero-shot settings, thus inspiring us to explore prompt-based methods. In this work, we ask whether strong IE models can be constructed by directly prompting LLMs. Specifically, we transform the zero-shot IE task into a multi-turn question-answering problem with a two-stage framework (ChatIE). With the power of ChatGPT, we extensively evaluate our framework on three IE tasks: entity-relation triple extract, named entity recognition, and event extraction. Empirical results on six datasets across two languages show that ChatIE achieves impressive performance and even surpasses some full-shot models on several datasets (e.g., NYT11-HRL). We believe that our work could shed light on building IE models with limited resources.",2023.0,"Xiang Wei, Xingyu Cui, Ning Cheng, Xiaobin Wang, Xin Zhang, Shen Huang, Pengjun Xie, Jinan Xu, Yufeng Chen, Meishan Zhang, Yong Jiang, Wenjuan Han"
88abef771472c3aa46c53d5d626a0d0c3b66e8cd,https://www.semanticscholar.org/paper/88abef771472c3aa46c53d5d626a0d0c3b66e8cd,"Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness","The capability of Large Language Models (LLMs) like ChatGPT to comprehend user intent and provide reasonable responses has made them extremely popular lately. In this paper, we focus on assessing the overall ability of ChatGPT using 7 fine-grained information extraction (IE) tasks. Specially, we present the systematically analysis by measuring ChatGPT's performance, explainability, calibration, and faithfulness, and resulting in 15 keys from either the ChatGPT or domain experts. Our findings reveal that ChatGPT's performance in Standard-IE setting is poor, but it surprisingly exhibits excellent performance in the OpenIE setting, as evidenced by human evaluation. In addition, our research indicates that ChatGPT provides high-quality and trustworthy explanations for its decisions. However, there is an issue of ChatGPT being overconfident in its predictions, which resulting in low calibration. Furthermore, ChatGPT demonstrates a high level of faithfulness to the original text in the majority of cases. We manually annotate and release the test sets of 7 fine-grained IE tasks contains 14 datasets to further promote the research. The datasets and code are available at https://github.com/pkuserc/ChatGPT_for_IE.",2023.0,"Bo Li, Gexiang Fang, Yang Yang, Quansen Wang, Wei Ye, Wen Zhao, Shikun Zhang"
bbb2fc6e95d24fb58ab6c25b216b14ac49a32fbe,https://www.semanticscholar.org/paper/bbb2fc6e95d24fb58ab6c25b216b14ac49a32fbe,InstructUIE: Multi-task Instruction Tuning for Unified Information Extraction,"Large language models have unlocked strong multi-task capabilities from reading instructive prompts. However, recent studies have shown that existing large models still have difficulty with information extraction tasks. For example, gpt-3.5-turbo achieved an F1 score of 18.22 on the Ontonotes dataset, which is significantly lower than the state-of-the-art performance. In this paper, we propose InstructUIE, a unified information extraction framework based on instruction tuning, which can uniformly model various information extraction tasks and capture the inter-task dependency. To validate the proposed method, we introduce IE INSTRUCTIONS, a benchmark of 32 diverse information extraction datasets in a unified text-to-text format with expert-written instructions. Experimental results demonstrate that our method achieves comparable performance to Bert in supervised settings and significantly outperforms the state-of-the-art and gpt3.5 in zero-shot settings.",2023.0,"Xiao Wang, Wei Zhou, Can Zu, Han Xia, Tianze Chen, Yuan Zhang, Rui Zheng, Junjie Ye, Qi Zhang, Tao Gui, Jihua Kang, J. Yang, Siyuan Li, Chunsai Du"
351a2d50b4aff8e9754dc7074dd589b10a7465d4,https://www.semanticscholar.org/paper/351a2d50b4aff8e9754dc7074dd589b10a7465d4,Large Language Models for Generative Information Extraction: A Survey,"Information Extraction (IE) aims to extract structural knowledge from plain natural language texts. Recently, generative Large Language Models (LLMs) have demonstrated remarkable capabilities in text understanding and generation. As a result, numerous works have been proposed to integrate LLMs for IE tasks based on a generative paradigm. To conduct a comprehensive systematic review and exploration of LLM efforts for IE tasks, in this study, we survey the most recent advancements in this field. We first present an extensive overview by categorizing these works in terms of various IE subtasks and techniques, and then we empirically analyze the most advanced methods and discover the emerging trend of IE tasks with LLMs. Based on a thorough review conducted, we identify several insights in technique and promising research directions that deserve further exploration in future studies. We maintain a public repository and consistently update related works and resources on GitHub (LLM4IE repository).",2023.0,"Derong Xu, Wei Chen, Wenjun Peng, Chao Zhang, Tong Xu, Xiangyu Zhao, Xian Wu, Yefeng Zheng, Enhong Chen"
f5cc6df1a7d00a5ed93bee3db2e5df00c338a936,https://www.semanticscholar.org/paper/f5cc6df1a7d00a5ed93bee3db2e5df00c338a936,LLMs Accelerate Annotation for Medical Information Extraction,"The unstructured nature of clinical notes within electronic health records often conceals vital patient-related information, making it challenging to access or interpret. To uncover this hidden information, specialized Natural Language Processing (NLP) models are required. However, training these models necessitates large amounts of labeled data, a process that is both time-consuming and costly when relying solely on human experts for annotation. In this paper, we propose an approach that combines Large Language Models (LLMs) with human expertise to create an efficient method for generating ground truth labels for medical text annotation. By utilizing LLMs in conjunction with human annotators, we significantly reduce the human annotation burden, enabling the rapid creation of labeled datasets. We rigorously evaluate our method on a medical information extraction task, demonstrating that our approach not only substantially cuts down on human intervention but also maintains high accuracy. The results highlight the potential of using LLMs to improve the utilization of unstructured clinical data, allowing for the swift deployment of tailored NLP solutions in healthcare.",2023.0,"Akshay Goel, Almog Gueta, Omry Gilon, Chang Liu, Sofia Erell, Lan Huong Nguyen, Xiaohong Hao, Bolous Jaber, Shashir Reddy, Rupesh Kartha, Jean Steiner, Itay Laish, Amir Feder"
278eae3362fd00b1092051240ab31dfa0bf8b581,https://www.semanticscholar.org/paper/278eae3362fd00b1092051240ab31dfa0bf8b581,An Empirical Study on Information Extraction using Large Language Models,"Human-like large language models (LLMs), especially the most powerful and popular ones in OpenAI's GPT family, have proven to be very helpful for many natural language processing (NLP) related tasks. Therefore, various attempts have been made to apply LLMs to information extraction (IE), which is a fundamental NLP task that involves extracting information from unstructured plain text. To demonstrate the latest representative progress in LLMs' information extraction ability, we assess the information extraction ability of GPT-4 (the latest version of GPT at the time of writing this paper) from four perspectives: Performance, Evaluation Criteria, Robustness, and Error Types. Our results suggest a visible performance gap between GPT-4 and state-of-the-art (SOTA) IE methods. To alleviate this problem, considering the LLMs' human-like characteristics, we propose and analyze the effects of a series of simple prompt-based methods, which can be generalized to other LLMs and NLP tasks. Rich experiments show our methods' effectiveness and some of their remaining issues in improving GPT-4's information extraction ability.",2023.0,"Ridong Han, T. Peng, Chaohao Yang, Benyou Wang, Lu Liu, Xiang Wan"
7ccfd5953e75f242536e99cdeda545a3c66ea98f,https://www.semanticscholar.org/paper/7ccfd5953e75f242536e99cdeda545a3c66ea98f,Universal Information Extraction as Unified Semantic Matching,"The challenge of information extraction (IE) lies in the diversity of label schemas and the heterogeneity of structures.
Traditional methods require task-specific model design and rely heavily on expensive supervision, making them difficult to generalize to new schemas.
In this paper, we decouple IE into two basic abilities, structuring and conceptualizing, which are shared by different tasks and schemas.
Based on this paradigm, we propose to universally model various IE tasks with Unified Semantic Matching (USM) framework, which introduces three unified token linking operations to model the abilities of structuring and conceptualizing.
In this way, USM can jointly encode schema and input text, uniformly extract substructures in parallel, and controllably decode target structures on demand.
Empirical evaluation on 4 IE tasks shows that the proposed method achieves state-of-the-art performance under the supervised experiments and shows strong generalization ability in zero/few-shot transfer settings.",2023.0,"Jie Lou, Yaojie Lu, Dai Dai, Wei Jia, Hongyu Lin, Xianpei Han, Le Sun, Hua Wu"
222dcbf5ee19fdfc9cfbd9c75af168a5c2122a4a,https://www.semanticscholar.org/paper/222dcbf5ee19fdfc9cfbd9c75af168a5c2122a4a,A Joint Neural Model for Information Extraction with Global Features,"Most existing joint neural models for Information Extraction (IE) use local task-specific classifiers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a victim of a die event is likely to be a victim of an attack event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, OneIE, that aims to extract the globally optimal IE result as a graph from an input sentence. OneIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classifiers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state of-the-art on all subtasks. In addition, as OneIE does not use any language-specific feature, we prove it can be easily applied to new languages or trained in a multilingual manner.",2020.0,"Ying Lin, Heng Ji, Fei Huang, Lingfei Wu"
4f410ab5c8b12b34b38421241366ee456bbebab9,https://www.semanticscholar.org/paper/4f410ab5c8b12b34b38421241366ee456bbebab9,Incorporating Non-local Information into Information Extraction Systems by Gibbs Sampling,"Most current statistical natural language processing models use only local features so as to permit dynamic programming in inference, but this makes them unable to fully account for the long distance structure that is prevalent in language use. We show how to solve this dilemma with Gibbs sampling, a simple Monte Carlo method used to perform approximate inference in factored probabilistic models. By using simulated annealing in place of Viterbi decoding in sequence models such as HMMs, CMMs, and CRFs, it is possible to incorporate non-local structure while preserving tractable inference. We use this technique to augment an existing CRF-based information extraction system with long-distance dependency models, enforcing label consistency and extraction template consistency constraints. This technique results in an error reduction of up to 9% over state-of-the-art systems on two established information extraction tasks.",2005.0,"J. Finkel, Trond Grenager, Christopher D. Manning"
77365b30336ac46d620d958dc4c108a159c02834,https://www.semanticscholar.org/paper/77365b30336ac46d620d958dc4c108a159c02834,WebFormer: The Web-page Transformer for Structure Information Extraction,"Structure information extraction refers to the task of extracting structured text fields from web pages, such as extracting a product offer from a shopping page including product title, description, brand and price. It is an important research topic which has been widely studied in document understanding and web search. Recent natural language models with sequence modeling have demonstrated state-of-the-art performance on web information extraction. However, effectively serializing tokens from unstructured web pages is challenging in practice due to a variety of web layout patterns. Limited work has focused on modeling the web layout for extracting the text fields. In this paper, we introduce WebFormer, a Web-page transFormer model for structure information extraction from web documents. First, we design HTML tokens for each DOM node in the HTML by embedding representations from their neighboring tokens through graph attention. Second, we construct rich attention patterns between HTML tokens and text tokens, which leverages the web layout for effective attention weight computation. We conduct an extensive set of experiments on SWDE and Common Crawl benchmarks. Experimental results demonstrate the superior performance of the proposed approach over several state-of-the-art methods.",2022.0,"Qifan Wang, Yi Fang, Anirudh Ravula, Fuli Feng, Xiaojun Quan, Dongfang Liu"
f3e31b2216145be47c21ea8a1b4d7fbafe04c7b7,https://www.semanticscholar.org/paper/f3e31b2216145be47c21ea8a1b4d7fbafe04c7b7,A Survey of Information Extraction Based on Deep Learning,"As a core task and an important link in the fields of natural language understanding and information retrieval, information extraction (IE) can structure and semanticize unstructured multi-modal information. In recent years, deep learning (DL) has attracted considerable research attention to IE tasks. Deep learning-based entity relation extraction techniques have gradually surpassed traditional feature- and kernel-function-based methods in terms of the depth of feature extraction and model accuracy. In this paper, we explain the basic concepts of IE and DL, primarily expounding on the research progress and achievements of DL technologies in the field of IE. At the level of IE tasks, it is expounded from entity relationship extraction, event extraction, and multi-modal information extraction three aspects, and creates a comparative analysis of various extraction techniques. We also summarize the prospects and development trends in DL in the field of IE as well as difficulties requiring further study. It is believed that research can be carried out in the direction of multi-model and multi-task joint extraction, information extraction based on knowledge enhancement, and information fusion based on multi-modal at the method level. At the model level, further research should be carried out in the aspects of strengthening theoretical research, model lightweight, and improving model generalization ability.",2022.0,"Yang Yang, Zhilei Wu, Yuexiang Yang, Shuangshuang Lian, Fengjie Guo, Zhiwei Wang"
5054109a7cb78cb7a671274ca8dc02a680c0cc1a,https://www.semanticscholar.org/paper/5054109a7cb78cb7a671274ca8dc02a680c0cc1a,Abstract Meaning Representation Guided Graph Encoding and Decoding for Joint Information Extraction,"The tasks of Rich Semantic Parsing, such as Abstract Meaning Representation (AMR), share similar goals with Information Extraction (IE) to convert natural language texts into structured semantic representations. To take advantage of such similarity, we propose a novel AMR-guided framework for joint information extraction to discover entities, relations, and events with the help of a pre-trained AMR parser. Our framework consists of two novel components: 1) an AMR based semantic graph aggregator to let the candidate entity and event trigger nodes collect neighborhood information from AMR graph for passing message among related knowledge elements; 2) an AMR guided graph decoder to extract knowledge elements based on the order decided by the hierarchical structures in AMR. Experiments on multiple datasets have shown that the AMR graph encoder and decoder have provided significant gains and our approach has achieved new state-of-the-art performance on all IE subtasks.",2021.0,"Zixuan Zhang, Heng Ji"
43c3ccb02ed34b6f38872bc7d75a85d812ac2746,https://www.semanticscholar.org/paper/43c3ccb02ed34b6f38872bc7d75a85d812ac2746,Towards Robust Visual Information Extraction in Real World: New Dataset and Novel Solution,"Visual Information Extraction (VIE) has attracted considerable attention recently owing to its various advanced applications such as document understanding, automatic marking and intelligent education. Most existing works decoupled this problem into several independent sub-tasks of text spotting (text detection and recognition) and information extraction, which completely ignored the high correlation among them during optimization. In this paper, we propose a robust Visual Information Extraction System (VIES) towards real-world scenarios, which is an unified end-to-end trainable framework for simultaneous text detection, recognition and information extraction by taking a single document image as input and outputting the structured information. Specifically, the information extraction branch collects abundant visual and semantic representations from text spotting for multimodal feature fusion and conversely, provides higher-level semantic clues to contribute to the optimization of text spotting. Moreover, regarding the shortage of public benchmarks, we construct a fully-annotated dataset called EPHOIE (https://github.com/HCIILAB/EPHOIE), which is the first Chinese benchmark for both text spotting and visual information extraction. EPHOIE consists of 1,494 images of examination paper head with complex layouts and background, including a total of 15,771 Chinese handwritten or printed text instances. Compared with the state-of-the-art methods, our VIES shows significant superior performance on the EPHOIE dataset and achieves a 9.01% F-score gain on the widely used SROIE dataset under the end-to-end scenario.",2021.0,"Jiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Jiaxin Zhang, Shuaitao Zhang, Qianying Wang, Y. Wu, Mingxiang Cai"
e99a259299d4d555ee4c354f2095ab4401369c82,https://www.semanticscholar.org/paper/e99a259299d4d555ee4c354f2095ab4401369c82,SciREX: A Challenge Dataset for Document-Level Information Extraction,"Extracting information from full documents is an important problem in many domains, but most previous work focus on identifying relationships within a sentence or a paragraph. It is challenging to create a large-scale information extraction (IE) dataset at the document level since it requires an understanding of the whole document to annotate entities and their document-level relationships that usually span beyond sentences or even sections. In this paper, we introduce SciREX, a document level IE dataset that encompasses multiple IE tasks, including salient entity identification and document level N-ary relation identification from scientific articles. We annotate our dataset by integrating automatic and human annotations, leveraging existing scientific knowledge resources. We develop a neural model as a strong baseline that extends previous state-of-the-art IE models to document-level IE. Analyzing the model performance shows a significant gap between human performance and current baselines, inviting the community to use our dataset as a challenge to develop document-level IE models. Our data and code are publicly available at https://github.com/allenai/SciREX .",2020.0,"Sarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, Iz Beltagy"
dbfc17833434243e07c4629e58f3d8ed7112dbfe,https://www.semanticscholar.org/paper/dbfc17833434243e07c4629e58f3d8ed7112dbfe,Learning from Noisy Labels for Entity-Centric Information Extraction,"Recent information extraction approaches have relied on training deep neural models. However, such models can easily overfit noisy labels and suffer from performance degradation. While it is very costly to filter noisy labels in large learning resources, recent studies show that such labels take more training steps to be memorized and are more frequently forgotten than clean labels, therefore are identifiable in training. Motivated by such properties, we propose a simple co-regularization framework for entity-centric information extraction, which consists of several neural models with identical structures but different parameter initialization. These models are jointly optimized with the task-specific losses and are regularized to generate similar predictions based on an agreement loss, which prevents overfitting on noisy labels. Extensive experiments on two widely used but noisy benchmarks for information extraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework. We release our code to the community for future research.",2021.0,"Wenxuan Zhou, Muhao Chen"
df4d715898e04038a3a367b977fe82e479dcb291,https://www.semanticscholar.org/paper/df4d715898e04038a3a367b977fe82e479dcb291,GenIE: Generative Information Extraction,"Structured and grounded representation of text is typically formalized by closed information extraction, the problem of extracting an exhaustive set of (subject, relation, object) triplets that are consistent with a predefined set of entities and relations from a knowledge base schema. Most existing works are pipelines prone to error accumulation, and all approaches are only applicable to unrealistically small numbers of entities and relations. We introduce GenIE (generative information extraction), the first end-to-end autoregressive formulation of closed information extraction. GenIE naturally exploits the language knowledge from the pre-trained transformer by autoregressively generating relations and entities in textual form. Thanks to a new bi-level constrained generation strategy, only triplets consistent with the predefined knowledge base schema are produced. Our experiments show that GenIE is state-of-the-art on closed information extraction, generalizes from fewer training data points than baselines, and scales to a previously unmanageable number of entities and relations. With this work, closed information extraction becomes practical in realistic scenarios, providing new opportunities for downstream tasks. Finally, this work paves the way towards a unified end-to-end approach to the core tasks of information extraction.",2021.0,"Martin Josifoski, Nicola De Cao, Maxime Peyrard, Robert West"
20d0564fd3fdbc24f266ca2076826a2271c3ea08,https://www.semanticscholar.org/paper/20d0564fd3fdbc24f266ca2076826a2271c3ea08,Spatial Dependency Parsing for Semi-Structured Document Information Extraction,"Information Extraction (IE) for semi-structured document images is often approached as a sequence tagging problem by classifying each recognized input token into one of the IOB (Inside, Outside, and Beginning) categories. However, such problem setup has two inherent limitations that (1) it cannot easily handle complex spatial relationships and (2) it is not suitable for highly structured information, which are nevertheless frequently observed in real-world document images. To tackle these issues, we first formulate the IE task as spatial dependency parsing problem that focuses on the relationship among text segment nodes in the documents. Under this setup, we then propose SPADE (SPAtial DEpendency parser) that models highly complex spatial relationships and an arbitrary number of information layers in the documents in an end-to-end manner. We evaluate it on various kinds of documents such as receipts, name cards, forms, and invoices, and show that it achieves a similar or better performance compared to strong baselines including BERT-based IOB taggger, with up to 37.7% improvement.",2020.0,"Wonseok Hwang, Jinyeong Yim, Seunghyun Park, Sohee Yang, Minjoon Seo"
a0af2f149273f7c77d0ff3f068fc15449bb6776b,https://www.semanticscholar.org/paper/a0af2f149273f7c77d0ff3f068fc15449bb6776b,TRIE: End-to-End Text Reading and Information Extraction for Document Understanding,"Since real-world ubiquitous documents (e.g., invoices, tickets, resumes and leaflets) contain rich information, automatic document image understanding has become a hot topic. Most existing works decouple the problem into two separate tasks, (1) text reading for detecting and recognizing texts in images and (2) information extraction for analyzing and extracting key elements from previously extracted plain text.However, they mainly focus on improving information extraction task, while neglecting the fact that text reading and information extraction are mutually correlated. In this paper, we propose a unified end-to-end text reading and information extraction network, where the two tasks can reinforce each other. Specifically, the multimodal visual and textual features of text reading are fused for information extraction and in turn, the semantics in information extraction contribute to the optimization of text reading. On three real-world datasets with diverse document images (from fixed layout to variable layout, from structured text to semi-structured text), our proposed method significantly outperforms the state-of-the-art methods in both efficiency and accuracy.",2020.0,"Peng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, Fei Wu"
eef9d4ce8470e7e0100051900870945b8a4714ac,https://www.semanticscholar.org/paper/eef9d4ce8470e7e0100051900870945b8a4714ac,IMoJIE: Iterative Memory-Based Joint Open Information Extraction,"While traditional systems for Open Information Extraction were statistical and rule-based, recently neural models have been introduced for the task. Our work builds upon CopyAttention, a sequence generation OpenIE model (Cui et. al. 18). Our analysis reveals that CopyAttention produces a constant number of extractions per sentence, and its extracted tuples often express redundant information. We present IMoJIE, an extension to CopyAttention, which produces the next extraction conditioned on all previously extracted tuples. This approach overcomes both shortcomings of CopyAttention, resulting in a variable number of diverse extractions per sentence. We train IMoJIE on training data bootstrapped from extractions of several non-neural systems, which have been automatically filtered to reduce redundancy and noise. IMoJIE outperforms CopyAttention by about 18 F1 pts, and a BERT-based strong baseline by 2 F1 pts, establishing a new state of the art for the task.",2020.0,"Keshav Kolluru, Samarth Aggarwal, Vipul Rathore, Mausam, Soumen Chakrabarti"
039ce73659332c12168de439e3f79e7039b636af,https://www.semanticscholar.org/paper/039ce73659332c12168de439e3f79e7039b636af,RESIN: A Dockerized Schema-Guided Cross-document Cross-lingual Cross-media Information Extraction and Event Tracking System,"We present a new information extraction system that can automatically construct temporal event graphs from a collection of news documents from multiple sources, multiple languages (English and Spanish for our experiment), and multiple data modalities (speech, text, image and video). The system advances state-of-the-art from two aspects: (1) extending from sentence-level event extraction to cross-document cross-lingual cross-media event extraction, coreference resolution and temporal event tracking; (2) using human curated event schema library to match and enhance the extraction output. We have made the dockerlized system publicly available for research purpose at GitHub, with a demo video.",2021.0,"Haoyang Wen, Ying Lin, T. Lai, Xiaoman Pan, Sha Li, Xudong Lin, Ben Zhou, Manling Li, Haoyu Wang, Hongming Zhang, Xiaodong Yu, Alexander Dong, Zhenhailong Wang, Y. Fung, Piyush Mishra, Qing Lyu, Dídac Surís, Brian Chen, S. Brown, Martha Palmer, Chris Callison-Burch, Carl Vondrick, Jiawei Han, D. Roth, Shih-Fu Chang, Heng Ji"
fc93fd5802598d8034056b94f53b6f76c32a3805,https://www.semanticscholar.org/paper/fc93fd5802598d8034056b94f53b6f76c32a3805,Named Entity Recognition and Relation Detection for Biomedical Information Extraction,"The number of scientific publications in the literature is steadily growing, containing our knowledge in the biomedical, health, and clinical sciences. Since there is currently no automatic archiving of the obtained results, much of this information remains buried in textual details not readily available for further usage or analysis. For this reason, natural language processing (NLP) and text mining methods are used for information extraction from such publications. In this paper, we review practices for Named Entity Recognition (NER) and Relation Detection (RD), allowing, e.g., to identify interactions between proteins and drugs or genes and diseases. This information can be integrated into networks to summarize large-scale details on a particular biomedical or clinical problem, which is then amenable for easy data management and further analysis. Furthermore, we survey novel deep learning methods that have recently been introduced for such tasks.",2020.0,"Nadeesha Perera, M. Dehmer, F. Emmert-Streib"
5d4cbdd2172039b84b8628f1a2f77b83ba1fa551,https://www.semanticscholar.org/paper/5d4cbdd2172039b84b8628f1a2f77b83ba1fa551,Enriching contextualized language model from knowledge graph for biomedical information extraction,"Biomedical information extraction (BioIE) is an important task. The aim is to analyze biomedical texts and extract structured information such as named entities and semantic relations between them. In recent years, pre-trained language models have largely improved the performance of BioIE. However, they neglect to incorporate external structural knowledge, which can provide rich factual information to support the underlying understanding and reasoning for biomedical information extraction. In this paper, we first evaluate current extraction methods, including vanilla neural networks, general language models and pre-trained contextualized language models on biomedical information extraction tasks, including named entity recognition, relation extraction and event extraction. We then propose to enrich a contextualized language model by integrating a large scale of biomedical knowledge graphs (namely, BioKGLM). In order to effectively encode knowledge, we explore a three-stage training procedure and introduce different fusion strategies to facilitate knowledge injection. Experimental results on multiple tasks show that BioKGLM consistently outperforms state-of-the-art extraction models. A further analysis proves that BioKGLM can capture the underlying relations between biomedical knowledge concepts, which are crucial for BioIE.",2020.0,"Hao Fei, Yafeng Ren, Yue Zhang, Donghong Ji, Xiaohui Liang"
d00cbb0c05c1dc922126fe72c1078b773d01c688,https://www.semanticscholar.org/paper/d00cbb0c05c1dc922126fe72c1078b773d01c688,ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction,"The ICDAR 2019 Challenge on ""Scanned receipts OCR and key information extraction"" (SROIE) covers important aspects related to the automated analysis of scanned receipts. The SROIE tasks play a key role in many document analysis systems and hold significant commercial potential. Although a lot of work has been published over the years on administrative document analysis, the community has advanced relatively slowly, as most datasets have been kept private. One of the key contributions of SROIE to the document analysis community is to offer a first, standardized dataset of 1000 whole scanned receipt images and annotations, as well as an evaluation procedure for such tasks. The Challenge is structured around three tasks, namely Scanned Receipt Text Localization (Task 1), Scanned Receipt OCR (Task 2) and Key Information Extraction from Scanned Receipts (Task 3). The competition opened on 10th February, 2019 and closed on 5th May, 2019. We received 29, 24 and 18 valid submissions received for the three competition tasks, respectively. This report presents the competition datasets, define the tasks and the evaluation protocols, offer detailed submission statistics, as well as an analysis of the submitted performance. While the tasks of text localization and recognition seem to be relatively easy to tackle, it is interesting to observe the variety of ideas and approaches proposed for the information extraction task. According to the submissions' performance we believe there is still margin for improving information extraction performance, although the current dataset would have to grow substantially in following editions. Given the success of the SROIE competition evidenced by the wide interest generated and the healthy number of submissions from academic, research institutes and industry over different countries, we consider that the SROIE competition can evolve into a useful resource for the community, drawing further attention and promoting research and development efforts in this field.",2019.0,"Zheng Huang, Kai Chen, Jianhua He, X. Bai, Dimosthenis Karatzas, Shijian Lu, C. V. Jawahar"
f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1,https://www.semanticscholar.org/paper/f262ef2f50dfcaf07dc6598f22fb9b2470b37cf1,A general framework for information extraction using dynamic span graphs,"We introduce a general framework for several information extraction tasks that share span representations using dynamically constructed span graphs. The graphs are dynamically constructed by selecting the most confident entity spans and linking these nodes with confidence-weighted relation types and coreferences. The dynamic span graph allow coreference and relation type confidences to propagate through the graph to iteratively refine the span representations. This is unlike previous multi-task frameworks for information extraction in which the only interaction between tasks is in the shared first-layer LSTM. Our framework significantly outperforms state-of-the-art on multiple information extraction tasks across multiple datasets reflecting different domains. We further observe that the span enumeration approach is good at detecting nested span entities, with significant F1 score improvement on the ACE dataset.",2019.0,"Yi Luan, David Wadden, Luheng He, A. Shah, Mari Ostendorf, Hannaneh Hajishirzi"
0578dfb2a28b77abde19b32de777e0365df3020e,https://www.semanticscholar.org/paper/0578dfb2a28b77abde19b32de777e0365df3020e,Data-driven materials research enabled by natural language processing and information extraction,"Given the emergence of data science and machine learning throughout all aspects of society, but particularly in the scientific domain, there is increased importance placed on obtaining data. Data in materials science are particularly heterogeneous, based on the significant range in materials classes that are explored and the variety of materials properties that are of interest. This leads to data that range many orders of magnitude, and these data may manifest as numerical text or image-based information, which requires quantitative interpretation. The ability to automatically consume and codify the scientific literature across domains—enabled by techniques adapted from the field of natural language processing—therefore has immense potential to unlock and generate the rich datasets necessary for data science and machine learning. This review focuses on the progress and practices of natural language processing and text mining of materials science literature and highlights opportunities for extracting additional information beyond text contained in figures and tables in articles. We discuss and provide examples for several reasons for the pursuit of natural language processing for materials, including data compilation, hypothesis development, and understanding the trends within and across fields. Current and emerging natural language processing methods along with their applications to materials science are detailed. We, then, discuss natural language processing and data challenges within the materials science domain where future directions may prove valuable.",2020.0,"E. Olivetti, J. Cole, Edward Kim, O. Kononova, G. Ceder, T. Y. Han, A. Hiszpanski"
6fc991dbc1714b425d11b4de3d9d247d21d77c0b,https://www.semanticscholar.org/paper/6fc991dbc1714b425d11b4de3d9d247d21d77c0b,Supervised Open Information Extraction,"We present data and methods that enable a supervised learning approach to Open Information Extraction (Open IE). Central to the approach is a novel formulation of Open IE as a sequence tagging problem, addressing challenges such as encoding multiple extractions for a predicate. We also develop a bi-LSTM transducer, extending recent deep Semantic Role Labeling models to extract Open IE tuples and provide confidence scores for tuning their precision-recall tradeoff. Furthermore, we show that the recently released Question-Answer Meaning Representation dataset can be automatically converted into an Open IE corpus which significantly increases the amount of available training data. Our supervised model outperforms the existing state-of-the-art Open IE systems on benchmark datasets.",2018.0,"Gabriel Stanovsky, Julian Michael, Luke Zettlemoyer, Ido Dagan"
b2ff13555c3e9264e7d56644b659b9d201a270b0,https://www.semanticscholar.org/paper/b2ff13555c3e9264e7d56644b659b9d201a270b0,Medical Information Extraction in the Age of Deep Learning,"Summary Objectives: We survey recent developments in medical Information Extraction (IE) as reported in the literature from the past three years. Our focus is on the fundamental methodological paradigm shift from standard Machine Learning (ML) techniques to Deep Neural Networks (DNNs). We describe applications of this new paradigm concentrating on two basic IE tasks, named entity recognition and relation extraction, for two selected semantic classes—diseases and drugs (or medications)—and relations between them. Methods: For the time period from 2017 to early 2020, we searched for relevant publications from three major scientific communities: medicine and medical informatics, natural language processing, as well as neural networks and artificial intelligence. Results: In the past decade, the field of Natural Language Processing (NLP) has undergone a profound methodological shift from symbolic to distributed representations based on the paradigm of Deep Learning (DL). Meanwhile, this trend is, although with some delay, also reflected in the medical NLP community. In the reporting period, overwhelming experimental evidence has been gathered, as illustrated in this survey for medical IE, that DL-based approaches outperform non-DL ones by often large margins. Still, small-sized and access-limited corpora create intrinsic problems for data-greedy DL as do special linguistic phenomena of medical sublanguages that have to be overcome by adaptive learning strategies. Conclusions: The paradigm shift from (feature-engineered) ML to DNNs changes the fundamental methodological rules of the game for medical NLP. This change is by no means restricted to medical IE but should also deeply influence other areas of medical informatics, either NLP- or non-NLP-based.",2020.0,"U. Hahn, Michel Oleynik"
58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28,https://www.semanticscholar.org/paper/58877aa9aa2d09585a4ff4881b02cb1c7ff9bc28,Representation Learning for Information Extraction from Form-like Documents,"We propose a novel approach using representation learning for tackling the problem of extracting structured information from form-like document images. We propose an extraction system that uses knowledge of the types of the target fields to generate extraction candidates and a neural network architecture that learns a dense representation of each candidate based on neighboring words in the document. These learned representations are not only useful in solving the extraction task for unseen document templates from two different domains but are also interpretable, as we show using loss cases.",2020.0,"Bodhisattwa Prasad Majumder, Navneet Potti, Sandeep Tata, James Bradley Wendt, Qi Zhao, Marc Najork"
ae8c331e091ba27e2671cdc63c44982b9fe66e98,https://www.semanticscholar.org/paper/ae8c331e091ba27e2671cdc63c44982b9fe66e98,Information extraction meets the Semantic Web: A survey,"Millennium Institute for Foundational Research on Data (IMFD) 
Comision Nacional de Investigacion Cientifica y Tecnologica (CONICYT), CONICYT FONDECYT: 1181896",2020.0,"José-Lázaro Martínez-Rodríguez, Aidan Hogan, I. Lopez-Arevalo"
498bb0efad6ec15dd09d941fb309aa18d6df9f5f,https://www.semanticscholar.org/paper/498bb0efad6ec15dd09d941fb309aa18d6df9f5f,Open Information Extraction from the Web,"Traditionally, Information Extraction (IE) has focused on satisfying precise, narrow, pre-specified requests from small homogeneous corpora (e.g., extract the location and time of seminars from a set of announcements). Shifting to a new domain requires the user to name the target relations and to manually create new extraction rules or hand-tag new training examples. This manual labor scales linearly with the number of target relations. This paper introduces Open IE (OIE), a new extraction paradigm where the system makes a single data-driven pass over its corpus and extracts a large set of relational tuples without requiring any human input. The paper also introduces TEXTRUNNER, a fully implemented, highly scalable OIE system where the tuples are assigned a probability and indexed to support efficient extraction and exploration via user queries. We report on experiments over a 9,000,000 Web page corpus that compare TEXTRUNNER with KNOWITALL, a state-of-the-art Web IE system. TEXTRUNNER achieves an error reduction of 33% on a comparable set of extractions. Furthermore, in the amount of time it takes KNOWITALL to perform extraction for a handful of pre-specified relations, TEXTRUNNER extracts a far broader set of facts reflecting orders of magnitude more relations, discovered on the fly. We report statistics on TEXTRUNNER’s 11,000,000 highest probability tuples, and show that they contain over 1,000,000 concrete facts and over 6,500,000more abstract assertions.",2007.0,"Michele Banko, Michael J. Cafarella, S. Soderland, M. Broadhead, Oren Etzioni"
d2339ae1d60dfa90d24b54b3921c848e4ed8e889,https://www.semanticscholar.org/paper/d2339ae1d60dfa90d24b54b3921c848e4ed8e889,Named Entity Recognition and Normalization Applied to Large-Scale Information Extraction from the Materials Science Literature,"The number of published materials science articles has increased manyfold over the past few decades. Now, a major bottleneck in the materials discovery pipeline arises in connecting new results with the previously established literature. A potential solution to this problem is to map the unstructured raw-text of published articles onto structured database entries that allows for programmatic querying. To this end, we apply text-mining with named entity recognition (NER) for large-scale information extraction from the published materials science literature. The NER model is trained to extract summary-level information from materials science documents, including: inorganic material mentions, sample descriptors, phase labels, material properties and applications, as well as any synthesis and characterization methods used. Our classifier achieves an accuracy (f1) of 87%, and is applied to information extraction from 3.27 million materials science abstracts. We extract more than 80 million materials-science-related named entities, and the content of each abstract is represented as a database entry in a structured format. We demonstrate that simple database queries can be used to answer complex ``meta-questions"" of the published literature that would have previously required laborious, manual literature searches to answer. All of our data and functionality has been made freely available (https://github.com/materialsintelligence/matscholar), and we expect these results to accelerate the pace of future materials science discovery.",2019.0,"Leigh Weston, V. Tshitoyan, John Dagdelen, O. Kononova, Amalie Trewartha, K. Persson, G. Ceder, Anubhav Jain"
04df8c70257b5280b9d303502c9d7ddf946f181b,https://www.semanticscholar.org/paper/04df8c70257b5280b9d303502c9d7ddf946f181b,Graph Convolution for Multimodal Information Extraction from Visually Rich Documents,"Visually rich documents (VRDs) are ubiquitous in daily business and life. Examples are purchase receipts, insurance policy documents, custom declaration forms and so on. In VRDs, visual and layout information is critical for document understanding, and texts in such documents cannot be serialized into the one-dimensional sequence without losing information. Classic information extraction models such as BiLSTM-CRF typically operate on text sequences and do not incorporate visual features. In this paper, we introduce a graph convolution based model to combine textual and visual information presented in VRDs. Graph embeddings are trained to summarize the context of a text segment in the document, and further combined with text embeddings for entity extraction. Extensive experiments have been conducted to show that our method outperforms BiLSTM-CRF baselines by significant margins, on two real-world datasets. Additionally, ablation studies are also performed to evaluate the effectiveness of each component of our model.",2019.0,"Xiaojing Liu, Feiyu Gao, Qiong Zhang, Huasha Zhao"
3238a0d5ffaa89f7e2cb73594985444ef5dff392,https://www.semanticscholar.org/paper/3238a0d5ffaa89f7e2cb73594985444ef5dff392,Free Your CSI: A Channel State Information Extraction Platform For Modern Wi-Fi Chipsets,"Modern wireless transmission systems heavily benefit from knowing the channel response. The evaluation of Channel State Information (CSI) during the reception of a frame preamble is fundamental to properly equalizing the rest of the transmission at the receiver side. Reporting this state information back to the transmitter facilitates mechanisms such as beamforming and MIMO, thus boosting the network performance. While these features are an integral part of standards such as 802.11ac, accessing CSI data on commercial devices is either not possible, limited to outdated chipsets or very inflexible. This hinders the research and development of innovative CSI-dependent techniques including localization, object tracking, and interference evaluation. To help researchers and practitioners, we introduce the nexmon CSI Extractor Tool. It allows per-frame CSI extraction for up to four spatial streams using up to four receive chains on modern Broadcom and Cypress Wi-Fi chips with up to 80MHz bandwidth in both the 2.4 and 5GHz bands. The tool supports devices ranging from the low-cost Raspberry Pi platform, over mobile platforms such as Nexus smartphones to state-of-the-art Wi-Fi APs. We release all tools and Wi-Fi firmware patches as extensible open source project. It includes our user-friendly smartphone application to demonstrate the CSI extraction capabilities in form of a waterfall diagram.",2019.0,"Francesco Gringoli, Matthias Schulz, Jakob Link, M. Hollick"
4653cf7cf9780ec3779c33c32dbb05037fe01ed1,https://www.semanticscholar.org/paper/4653cf7cf9780ec3779c33c32dbb05037fe01ed1,Span Model for Open Information Extraction on Accurate Corpus,"Open information extraction (Open IE) is a challenging task especially due to its brittle data basis. Most of Open IE systems have to be trained on automatically built corpus and evaluated on inaccurate test set. In this work, we first alleviate this difficulty from both sides of training and test sets. For the former, we propose an improved model design to more sufficiently exploit training dataset. For the latter, we present our accurately re-annotated benchmark test set (Re-OIE6) according to a series of linguistic observation and analysis. Then, we introduce a span model instead of previous adopted sequence labeling formulization for n-ary Open IE. Our newly introduced model achieves new state-of-the-art performance on both benchmark evaluation datasets.",2019.0,"Junlang Zhan, Hai Zhao"
c3e51cc7514d1d6e2416ff00564e87914c5588ee,https://www.semanticscholar.org/paper/c3e51cc7514d1d6e2416ff00564e87914c5588ee,Neural Open Information Extraction,"Conventional Open Information Extraction (Open IE) systems are usually built on hand-crafted patterns from other NLP tools such as syntactic parsing, yet they face problems of error propagation. In this paper, we propose a neural Open IE approach with an encoder-decoder framework. Distinct from existing methods, the neural Open IE approach learns highly confident arguments and relation tuples bootstrapped from a state-of-the-art Open IE system. An empirical study on a large benchmark dataset shows that the neural Open IE system significantly outperforms several baselines, while maintaining comparable computational efficiency.",2018.0,"Lei Cui, Furu Wei, M. Zhou"
78983b899635d796cd4cc06271b908c5d9d6984b,https://www.semanticscholar.org/paper/78983b899635d796cd4cc06271b908c5d9d6984b,A Survey on Open Information Extraction,"We provide a detailed overview of the various approaches that were proposed to date to solve the task of Open Information Extraction. We present the major challenges that such systems face, show the evolution of the suggested approaches over time and depict the specific issues they address. In addition, we provide a critique of the commonly applied evaluation procedures for assessing the performance of Open IE systems and highlight some directions for future work.",2018.0,"C. Niklaus, Matthias Cetto, A. Freitas, S. Handschuh"
1da8e1ad1814d81f69433ac877ef70caa950e4e6,https://www.semanticscholar.org/paper/1da8e1ad1814d81f69433ac877ef70caa950e4e6,GraphIE: A Graph-Based Framework for Information Extraction,"Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks — namely textual, social media and visual information extraction — shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin.",2018.0,"Yujie Qian, Enrico Santus, Zhijing Jin, Jiang Guo, R. Barzilay"
e9074a7a43c6e3b133c45960fcec0a9a34e59fdd,https://www.semanticscholar.org/paper/e9074a7a43c6e3b133c45960fcec0a9a34e59fdd,Twenty-five years of information extraction,"Abstract Information extraction is the process of converting unstructured text into a structured data base containing selected information from the text. It is an essential step in making the information content of the text usable for further processing. In this paper, we describe how information extraction has changed over the past 25 years, moving from hand-coded rules to neural networks, with a few stops on the way. We connect these changes to research advances in NLP and to the evaluations organized by the US Government.",2019.0,R. Grishman
3171ec184b5fec0bc7b47356ad74d8598e858ddc,https://www.semanticscholar.org/paper/3171ec184b5fec0bc7b47356ad74d8598e858ddc,Leveraging Linguistic Structure For Open Domain Information Extraction,"Relation triples produced by open domain information extraction (open IE) systems are useful for question answering, inference, and other IE tasks. Traditionally these are extracted using a large set of patterns; however, this approach is brittle on out-of-domain text and long-range dependencies, and gives no insight into the substructure of the arguments. We replace this large pattern set with a few patterns for canonically structured sentences, and shift the focus to a classifier which learns to extract self-contained clauses from longer sentences. We then run natural logic inference over these short clauses to determine the maximally specific arguments for each candidate triple. We show that our approach outperforms a state-of-the-art open IE system on the end-to-end TAC-KBP 2013 Slot Filling task.",2015.0,"Gabor Angeli, Melvin Johnson, Christopher D. Manning"
45f146bdf91198ea93858d0619c2acc5761e540d,https://www.semanticscholar.org/paper/45f146bdf91198ea93858d0619c2acc5761e540d,Open Information Extraction from Conjunctive Sentences,"We develop CALM, a coordination analyzer that improves upon the conjuncts identified from dependency parses. It uses a language model based scoring and several linguistic constraints to search over hierarchical conjunct boundaries (for nested coordination). By splitting a conjunctive sentence around these conjuncts, CALM outputs several simple sentences. We demonstrate the value of our coordination analyzer in the end task of Open Information Extraction (Open IE). State-of-the-art Open IE systems lose substantial yield due to ineffective processing of conjunctive sentences. Our Open IE system, CALMIE, performs extraction over the simple sentences identified by CALM to obtain up to 1.8x yield with a moderate increase in precision compared to extractions from original sentences.",2018.0,"Swarnadeep Saha, Mausam"
3c1e46a8e262af64492f71c25e40d2fe86589c88,https://www.semanticscholar.org/paper/3c1e46a8e262af64492f71c25e40d2fe86589c88,OPIEC: An Open Information Extraction Corpus,"Open information extraction (OIE) systems extract relations and their arguments from natural language text in an unsupervised manner. The resulting extractions are a valuable resource for downstream tasks such as knowledge base construction, open question answering, or event schema induction. In this paper, we release, describe, and analyze an OIE corpus called OPIEC, which was extracted from the text of English Wikipedia. OPIEC complements the available OIE resources: It is the largest OIE corpus publicly available to date (over 340M triples) and contains valuable metadata such as provenance information, confidence scores, linguistic annotations, and semantic annotations including spatial and temporal information. We analyze the OPIEC corpus by comparing its content with knowledge bases such as DBpedia or YAGO, which are also based on Wikipedia. We found that most of the facts between entities present in OPIEC cannot be found in DBpedia and/or YAGO, that OIE facts often differ in the level of specificity compared to knowledge base facts, and that OIE open relations are generally highly polysemous. We believe that the OPIEC corpus is a valuable resource for future research on automated knowledge base construction.",2019.0,"Kiril Gashteovski, Sebastian Wanner, S. Hertling, Samuel Broscheit, Rainer Gemulla"
00b3cb7e22c1e3979555b6e771fb50065bff078e,https://www.semanticscholar.org/paper/00b3cb7e22c1e3979555b6e771fb50065bff078e,Automatic Information Extraction from Piping and Instrumentation Diagrams,"One of the most common modes of representing engineering schematics are Piping and Instrumentation diagrams (P&IDs) that describe the layout of an engineering process flow along with the interconnected process equipment. Over the years, P&ID diagrams have been manually generated, scanned and stored as image files. These files need to be digitized for purposes of inventory management and updation, and easy reference to different components of the schematics. There are several challenging vision problems associated with digitizing real world P&ID diagrams. Real world P&IDs come in several different resolutions, and often contain noisy textual information. Extraction of instrumentation information from these diagrams involves accurate detection of symbols that frequently have minute visual differences between them. Identification of pipelines that may converge and diverge at different points in the image is a further cause for concern. Due to these reasons, to the best of our knowledge, no system has been proposed for end-to-end data extraction from P&ID diagrams. However, with the advent of deep learning and the spectacular successes it has achieved in vision, we hypothesized that it is now possible to re-examine this problem armed with the latest deep learning models. To that end, we present a novel pipeline for information extraction from P&ID sheets via a combination of traditional vision techniques and state-of-the-art deep learning models to identify and isolate pipeline codes, pipelines, inlets and outlets, and for detecting symbols. This is followed by association of the detected components with the appropriate pipeline. The extracted pipeline information is used to populate a tree-like data-structure for capturing the structure of the piping schematics. We evaluated proposed method on a real world dataset of P&ID sheets obtained from an oil firm and have obtained promising results.",2019.0,"Rohit Rahul, S. Paliwal, Monika Sharma, L. Vig"
d4b651d6a904f69f8fa1dcad4ebe972296af3a9a,https://www.semanticscholar.org/paper/d4b651d6a904f69f8fa1dcad4ebe972296af3a9a,Identifying Relations for Open Information Extraction,"Open Information Extraction (IE) is the task of extracting assertions from massive corpora without requiring a pre-specified vocabulary. This paper shows that the output of state-of-the-art Open IE systems is rife with uninformative and incoherent extractions. To overcome these problems, we introduce two simple syntactic and lexical constraints on binary relations expressed by verbs. We implemented the constraints in the ReVerb Open IE system, which more than doubles the area under the precision-recall curve relative to previous extractors such as TextRunner and woepos. More than 30% of ReVerb's extractions are at precision 0.8 or higher---compared to virtually none for earlier systems. The paper concludes with a detailed analysis of ReVerb's errors, suggesting directions for future work.",2011.0,"Anthony Fader, S. Soderland, Oren Etzioni"
16ad94b0ad4c38a629c96889024b165ea83e9440,https://www.semanticscholar.org/paper/16ad94b0ad4c38a629c96889024b165ea83e9440,Survey of Temporal Information Extraction,"Documents contain information that can be used for various applications, such as question answering (QA) system, information retrieval (IR) system, and recommendation system. To use the information, it is necessary to develop a method of extracting such information from the documents written in a form of natural language. There are several kinds of the information (e.g., temporal information, spatial information, semantic role information), where different kinds of information will be extracted with different methods. In this paper, the existing studies about the methods of extracting the temporal information are reported and several related issues are discussed. The issues are about the task boundary of the temporal information extraction, the history of the annotation languages and shared tasks, the research issues, the applications using the temporal information, and evaluation metrics. Although the history of the tasks of temporal information extraction is not long, there have been many studies that tried various methods. This paper gives which approach is known to be the better way of extracting a particular part of the temporal information, and also provides a future research direction.",2019.0,"Chae-Gyun Lim, Young-Seob Jeong, Ho‐Jin Choi"
1e94f1f54597f7f65300712ae97035e50ff3bc9a,https://www.semanticscholar.org/paper/1e94f1f54597f7f65300712ae97035e50ff3bc9a,MinIE: Minimizing Facts in Open Information Extraction,"The goal of Open Information Extraction (OIE) is to extract surface relations and their arguments from natural-language text in an unsupervised, domain-independent manner. In this paper, we propose MinIE, an OIE system that aims to provide useful, compact extractions with high precision and recall. MinIE approaches these goals by (1) representing information about polarity, modality, attribution, and quantities with semantic annotations instead of in the actual extraction, and (2) identifying and removing parts that are considered overly specific. We conducted an experimental study with several real-world datasets and found that MinIE achieves competitive or higher precision and recall than most prior systems, while at the same time producing shorter, semantically enriched extractions.",2017.0,"Kiril Gashteovski, Rainer Gemulla, Luciano Del Corro"
dc4287cada9543941af1ec998400118593b4d6f7,https://www.semanticscholar.org/paper/dc4287cada9543941af1ec998400118593b4d6f7,Natural Language Processing for Information Extraction,"With rise of digital age, there is an explosion of information in the form of news, articles, social media, and so on. Much of this data lies in unstructured form and manually managing and effectively making use of it is tedious, boring and labor intensive. This explosion of information and need for more sophisticated and efficient information handling tools gives rise to Information Extraction(IE) and Information Retrieval(IR) technology. Information Extraction systems takes natural language text as input and produces structured information specified by certain criteria, that is relevant to a particular application. Various sub-tasks of IE such as Named Entity Recognition, Coreference Resolution, Named Entity Linking, Relation Extraction, Knowledge Base reasoning forms the building blocks of various high end Natural Language Processing (NLP) tasks such as Machine Translation, Question-Answering System, Natural Language Understanding, Text Summarization and Digital Assistants like Siri, Cortana and Google Now. This paper introduces Information Extraction technology, its various sub-tasks, highlights state-of-the-art research in various IE subtasks, current challenges and future research directions.",2018.0,Sonit Singh
bc24ba1d209e8eab62895f6b448b0ad74eb5b2ff,https://www.semanticscholar.org/paper/bc24ba1d209e8eab62895f6b448b0ad74eb5b2ff,A Two-Step Resume Information Extraction Algorithm,"With the rapid growth of Internet-based recruiting, there are a great number of personal resumes among recruiting systems. To gain more attention from the recruiters, most resumes are written in diverse formats, including varying font size, font colour, and table cells. However, the diversity of format is harmful to data mining, such as resume information extraction, automatic job matching, and candidates ranking. Supervised methods and rule-based methods have been proposed to extract facts from resumes, but they strongly rely on hierarchical structure information and large amounts of labelled data, which are hard to collect in reality. In this paper, we propose a two-step resume information extraction approach. In the first step, raw text of resume is identified as different resume blocks. To achieve the goal, we design a novel feature, Writing Style, to model sentence syntax information. Besides word index and punctuation index, word lexical attribute and prediction results of classifiers are included in Writing Style. In the second step, multiple classifiers are employed to identify different attributes of fact information in resumes. Experimental results on a real-world dataset show that the algorithm is feasible and effective.",2018.0,"Jie Chen, Chunxia Zhang, Zhendong Niu"
ef33f662563b094fcd258168d93c898c93de5613,https://www.semanticscholar.org/paper/ef33f662563b094fcd258168d93c898c93de5613,Domain Analysis of Information Extraction Techniques,"— In this research, we extant a short outline of Information Extraction, which is also a natural language processing domain that tries to find required information in structured, semi structured and unstructured Data. We draw a taxonomy of information extraction tasks and techniques. The other important thing is that we also extract learning methods like supervised, semi supervised and unsupervised learning and which methods are used in these types of learning. Our domain analysis consists on social media, Biomedical, chemical and unstructured data. There are different tasks included in information extraction which makes this activity more manageable as well as to easy to work in specific domain. We also detect weakness of existing techniques.",2018.0,"Talha Mahboob Alam, M. Awan"
0ff595f0645a3e25a2f37145768985b10ead0509,https://www.semanticscholar.org/paper/0ff595f0645a3e25a2f37145768985b10ead0509,Answering Complex Questions Using Open Information Extraction,"While there has been substantial progress in factoid question-answering (QA), answering complex questions remains challenging, typically requiring both a large body of knowledge and inference techniques. Open Information Extraction (Open IE) provides a way to generate semi-structured knowledge for QA, but to date such knowledge has only been used to answer simple questions with retrieval-based methods. We overcome this limitation by presenting a method for reasoning with Open IE knowledge, allowing more complex questions to be handled. Using a recently proposed support graph optimization framework for QA, we develop a new inference model for Open IE, in particular one that can work effectively with multiple short facts, noise, and the relational structure of tuples. Our model significantly outperforms a state-of-the-art structured solver on complex questions of varying difficulty, while also removing the reliance on manually curated knowledge.",2017.0,"Tushar Khot, Ashish Sabharwal, Peter Clark"
ba72809dc81423b1e38eb76bddf79b0850553e5d,https://www.semanticscholar.org/paper/ba72809dc81423b1e38eb76bddf79b0850553e5d,Hierarchical attention networks for information extraction from cancer pathology reports,"Abstract Objective We explored how a deep learning (DL) approach based on hierarchical attention networks (HANs) can improve model performance for multiple information extraction tasks from unstructured cancer pathology reports compared to conventional methods that do not sufﬁciently capture syntactic and semantic contexts from free-text documents. Materials and Methods Data for our analyses were obtained from 942 deidentiﬁed pathology reports collected by the National Cancer Institute Surveillance, Epidemiology, and End Results program. The HAN was implemented for 2 information extraction tasks: (1) primary site, matched to 12 International Classification of Diseases for Oncology topography codes (7 breast, 5 lung primary sites), and (2) histological grade classiﬁcation, matched to G1–G4. Model performance metrics were compared to conventional machine learning (ML) approaches including naive Bayes, logistic regression, support vector machine, random forest, and extreme gradient boosting, and other DL models, including a recurrent neural network (RNN), a recurrent neural network with attention (RNN w/A), and a convolutional neural network. Results Our results demonstrate that for both information tasks, HAN performed signiﬁcantly better compared to the conventional ML and DL techniques. In particular, across the 2 tasks, the mean micro and macroF-scores for the HAN with pretraining were (0.852,0.708), compared to naive Bayes (0.518, 0.213), logistic regression (0.682, 0.453), support vector machine (0.634, 0.434), random forest (0.698, 0.508), extreme gradient boosting (0.696, 0.522), RNN (0.505, 0.301), RNN w/A (0.637, 0.471), and convolutional neural network (0.714, 0.460). Conclusions HAN-based DL models show promise in information abstraction tasks within unstructured clinical pathology reports.",2017.0,"Shang Gao, M. T. Young, John X. Qiu, Hong-Jun Yoon, J. B. Christian, P. Fearn, G. Tourassi, Arvind Ramanthan"
a32d7aba28ce9f130934b8e892df5bf2cad97e21,https://www.semanticscholar.org/paper/a32d7aba28ce9f130934b8e892df5bf2cad97e21,Creating a Large Benchmark for Open Information Extraction,"Open information extraction (Open IE) was presented as an unrestricted variant of traditional information extraction. It has been gaining substantial attention, manifested by a large number of automatic Open IE extractors and downstream applications. In spite of this broad attention, the Open IE task deﬁnition has been lacking – there are no formal guidelines and no large scale gold standard annotation. Subsequently, the various implementations of Open IE resorted to small scale post-hoc evaluations, inhibiting an objective and re-producible cross-system comparison. In this work, we develop a methodology that leverages the recent QA-SRL annotation to create a ﬁrst independent and large scale Open IE annotation, 1 and use it to automatically compare the most prominent Open IE systems.",2016.0,"Gabriel Stanovsky, Ido Dagan"
5873b77b3d784d9e1ea7447ddcaaa5388f16a4d4,https://www.semanticscholar.org/paper/5873b77b3d784d9e1ea7447ddcaaa5388f16a4d4,Open Information Extraction Systems and Downstream Applications,"Open Information Extraction (Open IE) extracts textual tuples comprising relation phrases and argument phrases from within a sentence, without requiring a pre-specified relation vocabulary. In this paper we first describe a decade of our progress on building Open IE extractors, which results in our latest extractor, OPENIE4, which is computationally efficient, outputs n-ary and nested relations, and also outputs relations mediated by nouns in addition to verbs. We also identify several strengths of the Open IE paradigm, which enable it to be a useful intermediate structure for end tasks. We survey its use in both human-facing applications and downstream NLP tasks, including event schema induction, sentence similarity, text comprehension, learning word vector embeddings, and more.",2016.0,Mausam Mausam
6149eea0b589fddbbeda46a0a92da3c097d2ef39,https://www.semanticscholar.org/paper/6149eea0b589fddbbeda46a0a92da3c097d2ef39,Semantic NLP-Based Information Extraction from Construction Regulatory Documents for Automated Compliance Checking,"AbstractAutomated regulatory compliance checking requires automated extraction of requirements from regulatory textual documents and their formalization in a computer-processable rule representation. Such information extraction (IE) is a challenging task that requires complex analysis and processing of text. Natural language processing (NLP) aims to enable computers to process natural language text in a human-like manner. This paper proposes a semantic, rule-based NLP approach for automated IE from construction regulatory documents. The proposed approach uses a set of pattern-matching-based IE rules and conflict resolution (CR) rules in IE. A variety of syntactic (syntax/grammar-related) and semantic (meaning/context-related) text features are used in the patterns of the IE and CR rules. Phrase structure grammar (PSG)-based phrasal tags and separation and sequencing of semantic information elements are proposed and used to reduce the number of needed patterns. An ontology is used to aid in the recognition...",2016.0,"Jiansong Zhang, N. El-Gohary"
ac50801574f7c97f1cf8f6718a9ff2b3e18c2dc6,https://www.semanticscholar.org/paper/ac50801574f7c97f1cf8f6718a9ff2b3e18c2dc6,Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning,"Most successful information extraction systems operate with access to a large collection of documents. In this work, we explore the task of acquiring and incorporating external evidence to improve extraction accuracy in domains where the amount of training data is scarce. This process entails issuing search queries, extraction from new sources and reconciliation of extracted values, which are repeated until sufficient evidence is collected. We approach the problem using a reinforcement learning framework where our model learns to select optimal actions based on contextual information. We employ a deep Q-network, trained to optimize a reward function that reflects extraction accuracy while penalizing extra effort. Our experiments on two databases -- of shooting incidents, and food adulteration cases -- demonstrate that our system significantly outperforms traditional extractors and a competitive meta-classifier baseline.",2016.0,"Karthik Narasimhan, Adam Yala, R. Barzilay"
88ddea93a98bae235af462d2b93ad6e6a703f742,https://www.semanticscholar.org/paper/88ddea93a98bae235af462d2b93ad6e6a703f742,EliIE: An open-source information extraction system for clinical trial eligibility criteria,"Objective
To develop an open-source information extraction system called Eligibility Criteria Information Extraction (EliIE) for parsing and formalizing free-text clinical research eligibility criteria (EC) following Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) version 5.0.


Materials and Methods
EliIE parses EC in 4 steps: (1) clinical entity and attribute recognition, (2) negation detection, (3) relation extraction, and (4) concept normalization and output structuring. Informaticians and domain experts were recruited to design an annotation guideline and generate a training corpus of annotated EC for 230 Alzheimer's clinical trials, which were represented as queries against the OMOP CDM and included 8008 entities, 3550 attributes, and 3529 relations. A sequence labeling-based method was developed for automatic entity and attribute recognition. Negation detection was supported by NegEx and a set of predefined rules. Relation extraction was achieved by a support vector machine classifier. We further performed terminology-based concept normalization and output structuring.


Results
In task-specific evaluations, the best F1 score for entity recognition was 0.79, and for relation extraction was 0.89. The accuracy of negation detection was 0.94. The overall accuracy for query formalization was 0.71 in an end-to-end evaluation.


Conclusions
This study presents EliIE, an OMOP CDM-based information extraction system for automatic structuring and formalization of free-text EC. According to our evaluation, machine learning-based EliIE outperforms existing systems and shows promise to improve.",2017.0,"Tian Kang, Shaodian Zhang, Youlan Tang, G. Hruby, A. Rusanov, Noémie Elhadad, C. Weng"
b73191adcc938cfcf20ce0327cf5cd1f539f7f81,https://www.semanticscholar.org/paper/b73191adcc938cfcf20ce0327cf5cd1f539f7f81,Scientific Information Extraction with Semi-supervised Neural Tagging,"This paper addresses the problem of extracting keyphrases from scientific articles and categorizing them as corresponding to a task, process, or material. We cast the problem as sequence tagging and introduce semi-supervised methods to a neural tagging model, which builds on recent advances in named entity recognition. Since annotated training data is scarce in this domain, we introduce a graph-based semi-supervised algorithm together with a data selection scheme to leverage unannotated articles. Both inductive and transductive semi-supervised learning strategies outperform state-of-the-art information extraction performance on the 2017 SemEval Task 10 ScienceIE task.",2017.0,"Yi Luan, Mari Ostendorf, Hannaneh Hajishirzi"
8d1b0d9f9788cfac4e8b133ce83638278647d36f,https://www.semanticscholar.org/paper/8d1b0d9f9788cfac4e8b133ce83638278647d36f,Information Extraction in Illicit Web Domains,"Extracting useful entities and attribute values from illicit domains such as human trafficking is a challenging problem with the potential for widespread social impact. Such domains employ atypical language models, have 'long tails' and suffer from the problem of concept drift. In this paper, we propose a lightweight, feature-agnostic Information Extraction (IE) paradigm specifically designed for such domains. Our approach uses raw, unlabeled text from an initial corpus, and a few (12-120) seed annotations per domain-specific attribute, to learn robust IE models for unobserved pages and websites. Empirically, we demonstrate that our approach can outperform feature-centric Conditional Random Field baselines by over 18% F-Measure on five annotated sets of real-world human trafficking datasets in both low-supervision and high-supervision settings. We also show that our approach is demonstrably robust to concept drift, and can be efficiently bootstrapped even in a serial computing environment.",2017.0,"M. Kejriwal, Pedro A. Szekely"
3153caa8f3f756b30824361d861d2ed4afec2b84,https://www.semanticscholar.org/paper/3153caa8f3f756b30824361d861d2ed4afec2b84,Waterbody information extraction from remote-sensing images after disasters based on spectral information and characteristic knowledge,"ABSTRACT This article proposes a post-disaster waterbody information extraction method based on spectral information from remote-sensing images and characteristic knowledge that can resist interference from factors such as changes in water quality, waves caused by accelerated water flow, and varying water levels. The method first analyses the display characteristics of waterbodies from remote-sensing images (their spectral characteristics, geometric features, and textural features), forming a decision tree of rules that represent characteristic knowledge for waterbody information extraction. This rule set is added to the various processing stages of waterbody information extraction after disasters to construct a waterbody information extraction model. Second, an object-oriented method is used for image segmentation. A rough initial waterbody information extraction is performed based on spectral information, and then refined based on the characteristic knowledge. Third, noise is eliminated and holes are filled in the images of the refined waterbody information extraction results. Finally, the accuracy of this new waterbody information extraction method is evaluated from both qualitative and quantitative aspects. Accuracy assessments of the experimental results obtained using remote-sensing images from the Wenchuan earthquake and a 2010 flood in Pakistan show that the proposed method is both efficient and accurate at extracting post-disaster waterbody information even when the background is complex.",2017.0,"Xin Zhao, Ping Wang, Chao Chen, Tao Jiang, Zhigang Yu, Biyun Guo"
bc750544010edfd6141c11aa06eb07508f222e44,https://www.semanticscholar.org/paper/bc750544010edfd6141c11aa06eb07508f222e44,Snorkel: Fast Training Set Generation for Information Extraction,"State-of-the art machine learning methods such as deep learning rely on large sets of hand-labeled training data. Collecting training data is prohibitively slow and expensive, especially when technical domain expertise is required; even the largest technology companies struggle with this challenge. We address this critical bottleneck with Snorkel, a new system for quickly creating, managing, and modeling training sets. Snorkel enables users to generate large volumes of training data by writing labeling functions, which are simple functions that express heuristics and other weak supervision strategies. These user-authored labeling functions may have low accuracies and may overlap and conflict, but Snorkel automatically learns their accuracies and synthesizes their output labels. Experiments and theory show that surprisingly, by modeling the labeling process in this way, we can train high-accuracy machine learning models even using potentially lower-accuracy inputs. Snorkel is currently used in production at top technology and consulting companies, and used by researchers to extract information from electronic health records, after-action combat reports, and the scientific literature. In this demonstration, we focus on the challenging task of information extraction, a common application of Snorkel in practice. Using the task of extracting corporate employment relationships from news articles, we will demonstrate and build intuition for a radically different way of developing machine learning systems which allows us to effectively bypass the bottleneck of hand-labeling training data.",2017.0,"Alexander J. Ratner, Stephen H. Bach, Henry R. Ehrenberg, C. Ré"
0bdb616e15d3d6f17b90e2e5c588bfecac13768a,https://www.semanticscholar.org/paper/0bdb616e15d3d6f17b90e2e5c588bfecac13768a,Odin’s Runes: A Rule Language for Information Extraction,"Odin is an information extraction framework that applies cascades of finite state automata over both surface text and syntactic dependency graphs. Support for syntactic patterns allow us to concisely define relations that are otherwise difficult to express in languages such as Common Pattern Specification Language (CPSL), which are currently limited to shallow linguistic features. The interaction of lexical and syntactic automata provides robustness and flexibility when writing extraction rules. This paper describes Odin’s declarative language for writing these cascaded automata.",2016.0,"M. A. Valenzuela-Escarcega, Gus Hahn-Powell, M. Surdeanu"
9705aee79d97c1f6fdcb36f2dd26ea0bcb601a5d,https://www.semanticscholar.org/paper/9705aee79d97c1f6fdcb36f2dd26ea0bcb601a5d,Applying Information Extraction for Patent Structure Analysis,"Patent engineers are spending significant time analyzing patent claim structures to grasp the range of technology covered or to compare similar patents in the same patent family. Though patent claims are the most important section in a patent, it is hard for a human to examine them. In this paper, we propose an information-extraction-based technique to grasp the patent claim structure. We confirmed that our approach is promising through empirical evaluation of entity mention extraction and the relation extraction method. We also built a preliminary interface to visualize patent structures, compare patents, and search similar patents.",2017.0,"Masayuki Okamoto, Zifei Shan, R. Orihara"
e8f7e63ef52e6c19a0933cf7d920fcc2ad60eab7,https://www.semanticscholar.org/paper/e8f7e63ef52e6c19a0933cf7d920fcc2ad60eab7,Learning for Biomedical Information Extraction: Methodological Review of Recent Advances,"Biomedical information extraction (BioIE) is important to many applications, including clinical decision support, integrative biology, and pharmacovigilance, and therefore it has been an active research. Unlike existing reviews covering a holistic view on BioIE, this review focuses on mainly recent advances in learning based approaches, by systematically summarizing them into different aspects of methodological development. In addition, we dive into open information extraction and deep learning, two emerging and influential techniques and envision next generation of BioIE.",2016.0,"F. Liu, Jinying Chen, Abhyuday N. Jagannatha, Hong Yu"
bf85ac3b7dcb2901526c749e83bf4542042d50ef,https://www.semanticscholar.org/paper/bf85ac3b7dcb2901526c749e83bf4542042d50ef,A hybrid ontology-based information extraction system,"Information Extraction is the process of automatically obtaining knowledge from plain text. Because of the ambiguity of written natural language, Information Extraction is a difficult task. Ontology-based Information Extraction (OBIE) reduces this complexity by including contextual information in the form of a domain ontology. The ontology provides guidance to the extraction process by providing concepts and relationships about the domain. However, OBIE systems have not been widely adopted because of the difficulties in deployment and maintenance. The Ontology-based Components for Information Extraction (OBCIE) architecture has been proposed as a form to encourage the adoption of OBIE by promoting reusability through modularity. In this paper, we propose two orthogonal extensions to OBCIE that allow the construction of hybrid OBIE systems with higher extraction accuracy and a new functionality. The first extension utilizes OBCIE modularity to integrate different types of implementation into one extraction system, producing a more accurate extraction. For each concept or relationship in the ontology, we can select the best implementation for extraction, or we can combine both implementations under an ensemble learning schema. The second extension is a novel ontology-based error detection mechanism. Following a heuristic approach, we can identify sentences that are logically inconsistent with the domain ontology. Because the implementation strategy for the extraction of a concept is independent of the functionality of the extraction, we can design a hybrid OBIE system with concepts utilizing different implementation strategies for extracting correct or incorrect sentences. Our evaluation shows that, in the implementation extension, our proposed method is more accurate in terms of correctness and completeness of the extraction. Moreover, our error detection method can identify incorrect statements with a high accuracy.",2016.0,"Fernando Gutierrez, D. Dou, S. Fickas, Daya C. Wimalasuriya, Hui Zong"
4c187332ba519e50feb6ca1454dd02682a0a3643,https://www.semanticscholar.org/paper/4c187332ba519e50feb6ca1454dd02682a0a3643,Nested Propositions in Open Information Extraction,",",2016.0,"Nikita Bhutani, H. V. Jagadish, Dragomir R. Radev"
98bb75dcb7dfe8e675781fe2008170e8f00a5dee,https://www.semanticscholar.org/paper/98bb75dcb7dfe8e675781fe2008170e8f00a5dee,FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information,"Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.",2021.0,"Rami Aly, Zhijiang Guo, M. Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, O. Cocarascu, Arpit Mittal"
bece46ed303f8eaef2affae2cba4e0aef51fe636,https://www.semanticscholar.org/paper/bece46ed303f8eaef2affae2cba4e0aef51fe636,Maximum Entropy Markov Models for Information Extraction and Segmentation,"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.",2000.0,"A. McCallum, Dayne Freitag, Fernando C Pereira"
112a778b752683897851150570a1e2042d66e610,https://www.semanticscholar.org/paper/112a778b752683897851150570a1e2042d66e610,Information Extraction Under Privacy Constraints,"A privacy-constrained information extraction problem is considered where for a pair of correlated discrete random variables $(X,Y)$ governed by a given joint distribution, an agent observes $Y$ and wants to convey to a potentially public user as much information about $Y$ as possible without compromising the amount of information revealed about $X$. To this end, the so-called {\em rate-privacy function} is introduced to quantify the maximal amount of information (measured in terms of mutual information) that can be extracted from $Y$ under a privacy constraint between $X$ and the extracted information, where privacy is measured using either mutual information or maximal correlation. Properties of the rate-privacy function are analyzed and information-theoretic and estimation-theoretic interpretations of it are presented for both the mutual information and maximal correlation privacy measures. It is also shown that the rate-privacy function admits a closed-form expression for a large family of joint distributions of $(X,Y)$. Finally, the rate-privacy function under the mutual information privacy measure is considered for the case where $(X,Y)$ has a joint probability density function by studying the problem where the extracted information is a uniform quantization of $Y$ corrupted by additive Gaussian noise. The asymptotic behavior of the rate-privacy function is studied as the quantization resolution grows without bound and it is observed that not all of the properties of the rate-privacy function carry over from the discrete to the continuous case.",2015.0,"S. Asoodeh, Mario Díaz, F. Alajaji, T. Linder"
6bc5fb610947f647224aab962d4a13da5afecac5,https://www.semanticscholar.org/paper/6bc5fb610947f647224aab962d4a13da5afecac5,Information Extraction with RapidMiner,"In this paper we present the Information Extraction (IE)- plugin for the open source Data Mining (DM) software RapidMiner 1 (Mierswa et al., 2006). The IE-plugin can be seen as an interface between natural language and IE- or DM-methods, because it converts docu- ments containing natural language texts into machine-readable form in order to extract interesting information like special entities and relations between those. The plugin is very modular and easy to use, which makes it more applicable for dierent domains and tasks.",2015.0,F. Jungermann
5b74a915d19e6dccee613951ff938d762a643c2f,https://www.semanticscholar.org/paper/5b74a915d19e6dccee613951ff938d762a643c2f,"Benchmarking Clinical Speech Recognition and Information Extraction: New Data, Methods, and Evaluations","Background Over a tenth of preventable adverse events in health care are caused by failures in information flow. These failures are tangible in clinical handover; regardless of good verbal handover, from two-thirds to all of this information is lost after 3-5 shifts if notes are taken by hand, or not at all. Speech recognition and information extraction provide a way to fill out a handover form for clinical proofing and sign-off. Objective The objective of the study was to provide a recorded spoken handover, annotated verbatim transcriptions, and evaluations to support research in spoken and written natural language processing for filling out a clinical handover form. This dataset is based on synthetic patient profiles, thereby avoiding ethical and legal restrictions, while maintaining efficacy for research in speech-to-text conversion and information extraction, based on realistic clinical scenarios. We also introduce a Web app to demonstrate the system design and workflow. Methods We experiment with Dragon Medical 11.0 for speech recognition and CRF++ for information extraction. To compute features for information extraction, we also apply CoreNLP, MetaMap, and Ontoserver. Our evaluation uses cross-validation techniques to measure processing correctness. Results The data provided were a simulation of nursing handover, as recorded using a mobile device, built from simulated patient records and handover scripts, spoken by an Australian registered nurse. Speech recognition recognized 5276 of 7277 words in our 100 test documents correctly. We considered 50 mutually exclusive categories in information extraction and achieved the F1 (ie, the harmonic mean of Precision and Recall) of 0.86 in the category for irrelevant text and the macro-averaged F1 of 0.70 over the remaining 35 nonempty categories of the form in our 101 test documents. Conclusions The significance of this study hinges on opening our data, together with the related performance benchmarks and some processing software, to the research and development community for studying clinical documentation and language-processing. The data are used in the CLEFeHealth 2015 evaluation laboratory for a shared task on speech recognition.",2015.0,"H. Suominen, Liyuan Zhou, L. Hanlen, Gabriela Ferraro"
0d9166c50495f0babd4e64de235d3317c8b97b4f,https://www.semanticscholar.org/paper/0d9166c50495f0babd4e64de235d3317c8b97b4f,Automated Road Information Extraction From Mobile Laser Scanning Data,"This paper presents a survey of literature about road feature extraction, giving a detailed description of a Mobile Laser Scanning (MLS) system (RIEGL VMX-450) for transportation-related applications. This paper describes the development of automated algorithms for extracting road features (road surfaces, road markings, and pavement cracks) from MLS point cloud data. The proposed road surface extraction algorithm detects road curbs from a set of profiles that are sliced along vehicle trajectory data. Based on segmented road surface points, we create Geo-Referenced Feature (GRF) images and develop two algorithms, respectively, for extracting the following: 1) road markings with high retroreflectivity and 2) cracks containing low contrast with their surroundings, low signal-to-noise ratio, and poor continuity. A comprehensive comparison illustrates satisfactory performance of the proposed algorithms and concludes that MLS is a reliable and cost-effective alternative for rapid road inspection.",2015.0,"H. Guan, Jonathan Li, Yongtao Yu, M. Chapman, Cheng Wang"
a4544fac5136f972fa4f3b19493d47f34ffe6236,https://www.semanticscholar.org/paper/a4544fac5136f972fa4f3b19493d47f34ffe6236,Information Extraction,"Much of the world's knowledge is recorded in natural language text, but making effective use of it in this form poses a major challenge. Information extraction converts this knowledge to a structured form suitable for computer manipulation, opening up many possibilities for using it. In this review, the author describes the processing pipeline of information extraction, how the pipeline components are trained, and how this training can be made more efficient. He also describes some of the challenges that must be addressed for information extraction to become a more widely used technology.",2015.0,Ralph Grishman
8ad0e78a9619c50bcb3cae4a589ec9a5d38c437c,https://www.semanticscholar.org/paper/8ad0e78a9619c50bcb3cae4a589ec9a5d38c437c,Open Language Learning for Information Extraction,"Open Information Extraction (IE) systems extract relational tuples from text, without requiring a pre-specified vocabulary, by identifying relation phrases and associated arguments in arbitrary sentences. However, state-of-the-art Open IE systems such as ReVerb and woe share two important weaknesses -- (1) they extract only relations that are mediated by verbs, and (2) they ignore context, thus extracting tuples that are not asserted as factual. This paper presents ollie, a substantially improved Open IE system that addresses both these limitations. First, ollie achieves high yield by extracting relations mediated by nouns, adjectives, and more. Second, a context-analysis step increases precision by including contextual information from the sentence in the extractions. ollie obtains 2.7 times the area under precision-yield curve (AUC) compared to ReVerb and 1.9 times the AUC of woeparse.",2012.0,"Mausam, Michael Schmitz, S. Soderland, Robert Bart, Oren Etzioni"
319e572fcddff77513eed8a25effbc7d9ff8ef85,https://www.semanticscholar.org/paper/319e572fcddff77513eed8a25effbc7d9ff8ef85,Information Extraction over Structured Data: Question Answering with Freebase,"Answering natural language questions using the Freebase knowledge base has recently been explored as a platform for advancing the state of the art in open domain semantic parsing. Those efforts map questions to sophisticated meaning representations that are then attempted to be matched against viable answer candidates in the knowledge base. Here we show that relatively modest information extraction techniques, when paired with a webscale corpus, can outperform these sophisticated approaches by roughly 34% relative gain.",2014.0,"Xuchen Yao, Benjamin Van Durme"
d48edf9e81653f4c3da716b037b0b50d54c5b034,https://www.semanticscholar.org/paper/d48edf9e81653f4c3da716b037b0b50d54c5b034,Knowledge-Based Weak Supervision for Information Extraction of Overlapping Relations,"Information extraction (IE) holds the promise of generating a large-scale knowledge base from the Web's natural language text. Knowledge-based weak supervision, using structured data to heuristically label a training corpus, works towards this goal by enabling the automated learning of a potentially unbounded number of relation extractors. Recently, researchers have developed multi-instance learning algorithms to combat the noisy training data that can come from heuristic labeling, but their models assume relations are disjoint --- for example they cannot extract the pair Founded(Jobs, Apple) and CEO-of(Jobs, Apple). 
 
This paper presents a novel approach for multi-instance learning with overlapping relations that combines a sentence-level extraction model with a simple, corpus-level component for aggregating the individual facts. We apply our model to learn extractors for NY Times text using weak supervision from Free-base. Experiments show that the approach runs quickly and yields surprising gains in accuracy, at both the aggregate and sentence level.",2011.0,"Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, Daniel S. Weld"
22f61c79c92fb5cbce57aa71fffc97ada7595551,https://www.semanticscholar.org/paper/22f61c79c92fb5cbce57aa71fffc97ada7595551,Hypothetical Thinking and Information Extraction in the Laboratory,"In several common-value environments (e.g., auctions or elections), players should make informational inferences from opponents' strategies under certain hypothetical events (e.g., winning the auction or being pivotal). We design a voting experiment that identifies whether subjects make these inferences and distinguishes between hypothetical thinking and information extraction. Depending on feedback, between 50 and 80 percent of subjects behave non-optimally. More importantly, these mistakes are driven by difficulty in extracting information from hypothetical, but not from actual, events. Mistakes are robust to experience and hints, and also arise in more general settings where players have no private information.",2014.0,"Ignacio Esponda, Emanuel Vespa"
6f0637f44ac481905d732be7a2e15d8f699aeeb3,https://www.semanticscholar.org/paper/6f0637f44ac481905d732be7a2e15d8f699aeeb3,Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation,"Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",2018.0,"Xiao Liu, Zhunchen Luo, Heyan Huang"
6f0637f44ac481905d732be7a2e15d8f699aeeb3,https://www.semanticscholar.org/paper/6f0637f44ac481905d732be7a2e15d8f699aeeb3,Jointly Multiple Events Extraction via Attention-based Graph Information Aggregation,"Event extraction is of practical utility in natural language processing. In the real world, it is a common phenomenon that multiple events existing in the same sentence, where extracting them are more difficult than extracting a single event. Previous works on modeling the associations between events by sequential modeling methods suffer a lot from the low efficiency in capturing very long-range dependencies. In this paper, we propose a novel Jointly Multiple Events Extraction (JMEE) framework to jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. The experiment results demonstrate that our proposed framework achieves competitive results compared with state-of-the-art methods.",2018.0,"Xiao Liu, Zhunchen Luo, Heyan Huang"
7891d3a91b86190df1756b08e69f879108579f40,https://www.semanticscholar.org/paper/7891d3a91b86190df1756b08e69f879108579f40,UIMA Ruta: Rapid development of rule-based information extraction applications,"Abstract Rule-based information extraction is an important approach for processing the increasingly available amount of unstructured data. The manual creation of rule-based applications is a time-consuming and tedious task, which requires qualified knowledge engineers. The costs of this process can be reduced by providing a suitable rule language and extensive tooling support. This paper presents UIMA Ruta, a tool for rule-based information extraction and text processing applications. The system was designed with focus on rapid development. The rule language and its matching paradigm facilitate the quick specification of comprehensible extraction knowledge. They support a compact representation while still providing a high level of expressiveness. These advantages are supplemented by the development environment UIMA Ruta Workbench. It provides, in addition to extensive editing support, essential assistance for explanation of rule execution, introspection, automatic validation, and rule induction. UIMA Ruta is a useful tool for academia and industry due to its open source license. We compare UIMA Ruta to related rule-based systems especially concerning the compactness of the rule representation, the expressiveness, and the provided tooling support. The competitiveness of the runtime performance is shown in relation to a popular and freely-available system. A selection of case studies implemented with UIMA Ruta illustrates the usefulness of the system in real-world scenarios.",2014.0,"Peter Klügl, Martin Toepfer, P. Beck, G. Fette, F. Puppe"
b75329489baf067e6f7bbb74f16ffd49fba80dfa,https://www.semanticscholar.org/paper/b75329489baf067e6f7bbb74f16ffd49fba80dfa,Freebase QA: Information Extraction or Semantic Parsing?,"We contrast two seemingly distinct approaches to the task of question answering (QA) using Freebase: one based on information extraction techniques, the other on semantic parsing. Results over the same test-set were collected from two state-ofthe-art, open-source systems, then analyzed in consultation with those systems’ creators. We conclude that the differences between these technologies, both in task performance, and in how they get there, is not significant. This suggests that the semantic parsing community should target answering more compositional open-domain questions that are beyond the reach of more direct information extraction methods.",2014.0,"Xuchen Yao, Jonathan Berant, Benjamin Van Durme"
c9aeb7e31b16b7273a80ae748b3ff48105928147,https://www.semanticscholar.org/paper/c9aeb7e31b16b7273a80ae748b3ff48105928147,An Information Bottleneck Approach for Controlling Conciseness in Rationale Extraction,"Decisions of complex language understanding models can be rationalized by limiting their inputs to a relevant subsequence of the original text. A rationale should be as concise as possible without significantly degrading task performance, but this balance can be difficult to achieve in practice. In this paper, we show that it is possible to better manage this trade-off by optimizing a bound on the Information Bottleneck (IB) objective. Our fully unsupervised approach jointly learns an explainer that predicts sparse binary masks over sentences, and an end-task predictor that considers only the extracted rationale. Using IB, we derive a learning objective that allows direct control of mask sparsity levels through a tunable sparse prior. Experiments on ERASER benchmark tasks demonstrate significant gains over norm-minimization techniques for both task performance and agreement with human rationales. Furthermore, we find that in the semi-supervised setting, a modest amount of gold rationales (25% of training examples) closes the gap with a model that uses the full input.",2020.0,"Bhargavi Paranjape, Mandar Joshi, John Thickstun, Hannaneh Hajishirzi, Luke Zettlemoyer"
406f6caa0b091cd9305d69b58e075765728b3802,https://www.semanticscholar.org/paper/406f6caa0b091cd9305d69b58e075765728b3802,ClausIE: clause-based open information extraction,"We propose ClausIE, a novel, clause-based approach to open information extraction, which extracts relations and their arguments from natural language text. ClausIE fundamentally differs from previous approaches in that it separates the detection of ``useful'' pieces of information expressed in a sentence from their representation in terms of extractions. In more detail, ClausIE exploits linguistic knowledge about the grammar of the English language to first detect clauses in an input sentence and to subsequently identify the type of each clause according to the grammatical function of its constituents. Based on this information, ClausIE is able to generate high-precision extractions; the representation of these extractions can be flexibly customized to the underlying application. ClausIE is based on dependency parsing and a small set of domain-independent lexica, operates sentence by sentence without any post-processing, and requires no training data (whether labeled or unlabeled). Our experimental study on various real-world datasets suggests that ClausIE obtains higher recall and higher precision than existing approaches, both on high-quality text as well as on noisy text as found in the web.",2013.0,"Luciano Del Corro, Rainer Gemulla"
674bd5ba8aa35ccba6691788dbb394665e669128,https://www.semanticscholar.org/paper/674bd5ba8aa35ccba6691788dbb394665e669128,Crowdsourcing step-by-step information extraction to enhance existing how-to videos,"Millions of learners today use how-to videos to master new skills in a variety of domains. But browsing such videos is often tedious and inefficient because video player interfaces are not optimized for the unique step-by-step structure of such videos. This research aims to improve the learning experience of existing how-to videos with step-by-step annotations. We first performed a formative study to verify that annotations are actually useful to learners. We created ToolScape, an interactive video player that displays step descriptions and intermediate result thumbnails in the video timeline. Learners in our study performed better and gained more self-efficacy using ToolScape versus a traditional video player. To add the needed step annotations to existing how-to videos at scale, we introduce a novel crowdsourcing workflow. It extracts step-by-step structure from an existing video, including step times, descriptions, and before and after images. We introduce the Find-Verify-Expand design pattern for temporal and visual annotation, which applies clustering, text processing, and visual analysis algorithms to merge crowd output. The workflow does not rely on domain-specific customization, works on top of existing videos, and recruits untrained crowd workers. We evaluated the workflow with Mechanical Turk, using 75 cooking, makeup, and Photoshop videos on YouTube. Results show that our workflow can extract steps with a quality comparable to that of trained annotators across all three domains with 77% precision and 81% recall.",2014.0,"Juho Kim, P. Nguyen, Sarah A. Weir, Philip J. Guo, Rob Miller, Krzysztof Z Gajos"
20aec9c36bd75a69988652ef78e899d720d4e4df,https://www.semanticscholar.org/paper/20aec9c36bd75a69988652ef78e899d720d4e4df,RESIDE: Improving Distantly-Supervised Neural Relation Extraction using Side Information,"Distantly-supervised Relation Extraction (RE) methods train an extractor by automatically aligning relation instances in a Knowledge Base (KB) with unstructured text. In addition to relation instances, KBs often contain other relevant side information, such as aliases of relations (e.g., founded and co-founded are aliases for the relation founderOfCompany). RE models usually ignore such readily available side information. In this paper, we propose RESIDE, a distantly-supervised neural relation extraction method which utilizes additional side information from KBs for improved relation extraction. It uses entity type and relation alias information for imposing soft constraints while predicting relations. RESIDE employs Graph Convolution Networks (GCN) to encode syntactic information from text and improves performance even when limited side information is available. Through extensive experiments on benchmark datasets, we demonstrate RESIDE’s effectiveness. We have made RESIDE’s source code available to encourage reproducible research.",2018.0,"Shikhar Vashishth, Rishabh Joshi, Sai Suman Prayaga, C. Bhattacharyya, P. Talukdar"
db036dbf3b54f631496d83235b3d84dc311322a5,https://www.semanticscholar.org/paper/db036dbf3b54f631496d83235b3d84dc311322a5,ChemDataExtractor: A Toolkit for Automated Extraction of Chemical Information from the Scientific Literature,"The emergence of ""big data"" initiatives has led to the need for tools that can automatically extract valuable chemical information from large volumes of unstructured data, such as the scientific literature. Since chemical information can be present in figures, tables, and textual paragraphs, successful information extraction often depends on the ability to interpret all of these domains simultaneously. We present a complete toolkit for the automated extraction of chemical entities and their associated properties, measurements, and relationships from scientific documents that can be used to populate structured chemical databases. Our system provides an extensible, chemistry-aware, natural language processing pipeline for tokenization, part-of-speech tagging, named entity recognition, and phrase parsing. Within this scope, we report improved performance for chemical named entity recognition through the use of unsupervised word clustering based on a massive corpus of chemistry articles. For phrase parsing and information extraction, we present the novel use of multiple rule-based grammars that are tailored for interpreting specific document domains such as textual paragraphs, captions, and tables. We also describe document-level processing to resolve data interdependencies and show that this is particularly necessary for the autogeneration of chemical databases since captions and tables commonly contain chemical identifiers and references that are defined elsewhere in the text. The performance of the toolkit to correctly extract various types of data was evaluated, affording an F-score of 93.4%, 86.8%, and 91.5% for extracting chemical identifiers, spectroscopic attributes, and chemical property attributes, respectively; set against the CHEMDNER chemical name extraction challenge, ChemDataExtractor yields a competitive F-score of 87.8%. All tools have been released under the MIT license and are available to download from http://www.chemdataextractor.org .",2016.0,"Matthew C. Swain, J. Cole"
b1b36f55d388223f33f81c95df8a99a1b5139404,https://www.semanticscholar.org/paper/b1b36f55d388223f33f81c95df8a99a1b5139404,A Frustratingly Easy Approach for Entity and Relation Extraction,"End-to-end relation extraction aims to identify named entities and extract relations between them. Most recent work models these two subtasks jointly, either by casting them in one structured prediction framework, or performing multi-task learning through shared representations. In this work, we present a simple pipelined approach for entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05 and SciERC), obtaining a 1.7%-2.8% absolute improvement in relation F1 over previous joint models with the same pre-trained encoders. Our approach essentially builds on two independent encoders and merely uses the entity model to construct the input for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information early in the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both entity and relation encoders at inference time, achieving an 8-16× speedup with a slight reduction in accuracy.",2021.0,"Zexuan Zhong, Danqi Chen"
3c29f5244e60cf75ba2f2c52be683107c7c44f93,https://www.semanticscholar.org/paper/3c29f5244e60cf75ba2f2c52be683107c7c44f93,Spatial Information Inference Net: Road Extraction Using Road-Specific Contextual Information,"For road extraction tasks in VHR satellite imagery, a deep neural network may perform well. But a network with certain reasoning ability as human will get a more satisfying result. To this end, we focus on how to effectively model the context information of the road and propose a well-designed spatial information inference structure (SIIS) which can add into any typical semantic segmentation network. The network with SIIS called SII-Net can not only learn the local visual characteristic of the road but also the global spatial structure information (such as the continuity and trend of the road). So, it can effectively solve the challenging occlusion problem in road detection and well preserve the continuity of the extracted road. The experimental results of two datasets show that the proposed method can improve the comprehensive performance of road extraction.",2019.0,"Ji Qi, Chao Tao, Hao Wang, Yuqi Tang, Zhenqi Cui"
91bfc0630f08dd4f9f8117f0c3651d540e1f015c,https://www.semanticscholar.org/paper/91bfc0630f08dd4f9f8117f0c3651d540e1f015c,Partial least-squares methods for spectral analyses. 1. Relation to other quantitative calibration methods and the extraction of qualitative information,"Partial least-squares (PLS) methods for spectral analyses are related to other multivariate calibration methods such as classical least-squares (CLS), inverse least-squares (ILS), and principal component regression (PCR) methods which have been used often in quantitative spectral analyses. The PLS method which analyzes one chemical component at a time is presented, and the basis of each step in the algorithm is explained. PLS calibration is shown to be composed of a series of simplified CLS and ILS steps. This detailed understanding of the PLS algorithm has helped to identify how chemically interpretable qualitative spectral information can be obtained from the intermediate steps of the PLS algorithm. These methods for extracting qualitative information are demonstrated by use of simulated spectral data. The qualitative information directly available from the PLS analysis is superior to that obtained from PCR but is not complete as that which can be generated during CLS analyses. Methods are presented for selecting optimal numbers of loading vectors for both the PLS and PCR models in order to optimize the model while simultaneously reducing the potential for overfitting the calibration data. Outlier detection and methods to evaluate the statistical significance of results obtained from the different calibration methods applied tomore » the same spectral data are also discussed.« less",1988.0,"D. Haaland, E. V. Thomas"
f06ff5f719eb9cd939dde8fc9b199b17adcbc75f,https://www.semanticscholar.org/paper/f06ff5f719eb9cd939dde8fc9b199b17adcbc75f,Road Extraction by Deep Residual U-Net,"Road extraction from aerial images has been a hot research topic in the field of remote sensing image analysis. In this letter, a semantic segmentation neural network, which combines the strengths of residual learning and U-Net, is proposed for road area extraction. The network is built with residual units and has similar architecture to that of U-Net. The benefits of this model are twofold: first, residual units ease training of deep networks. Second, the rich skip connections within the network could facilitate information propagation, allowing us to design networks with fewer parameters, however, better performance. We test our network on a public road data set and compare it with U-Net and other two state-of-the-art deep-learning-based road extraction methods. The proposed approach outperforms all the comparing methods, which demonstrates its superiority over recently developed state of the arts.",2017.0,"Zhengxin Zhang, Qingjie Liu, Yunhong Wang"
0692b9ad39145f57237199f3d4488667d5d9e5e7,https://www.semanticscholar.org/paper/0692b9ad39145f57237199f3d4488667d5d9e5e7,Rule-Based Information Extraction is Dead! Long Live Rule-Based Information Extraction Systems!,"The rise of “Big Data” analytics over unstructured text has led to renewed interest in information extraction (IE). We surveyed the landscape of IE technologies and identified a major disconnect between industry and academia: while rule-based IE dominates the commercial world, it is widely regarded as dead-end technology by the academia. We believe the disconnect stems from the way in which the two communities measure the benefits and costs of IE, as well as academia’s perception that rulebased IE is devoid of research challenges. We make a case for the importance of rule-based IE to industry practitioners. We then lay out a research agenda in advancing the state-of-theart in rule-based IE systems which we believe has the potential to bridge the gap between academic research and industry practice.",2013.0,"Laura Chiticariu, Yunyao Li, Frederick Reiss"
f9e7402ad740b73cc0bb64178f86df3478c3aaf5,https://www.semanticscholar.org/paper/f9e7402ad740b73cc0bb64178f86df3478c3aaf5,Wrapper Induction for Information Extraction,"Many Internet information resources present relational data|telephone directories, product catalogs, etc. Because these sites are formatted for people, mechanically extracting their content is di(cid:14)cult. Systems using such resources typically use hand-coded wrappers, procedures to extract data from information resources. We introduce wrapper induction, a method for automatically constructing wrappers, and identify hlrt , a wrapper class that is e(cid:14)ciently learnable, yet expressive enough to handle 48% of a recently surveyed sample of Internet resources. We use PAC analysis to bound the problem’s sample complexity, and show that the system degrades gracefully with imperfect labeling knowledge.",1997.0,"N. Kushmerick, Daniel S. Weld, Robert B. Doorenbos"
fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8,https://www.semanticscholar.org/paper/fac2368c2ec81ef82fd168d49a0def2f8d1ec7d8,"Entity, Relation, and Event Extraction with Contextualized Span Representations","We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets.",2019.0,"David Wadden, Ulme Wennberg, Yi Luan, Hannaneh Hajishirzi"
f28a5306050828de121349a0e520a998ca8818fe,https://www.semanticscholar.org/paper/f28a5306050828de121349a0e520a998ca8818fe,An Information Extraction Framework for Cohort Identification Using Electronic Health Records,"Information extraction (IE), a natural language processing (NLP) task that automatically extracts structured or semi-structured information from free text, has become popular in the clinical domain for supporting automated systems at point-of-care and enabling secondary use of electronic health records (EHRs) for clinical and translational research. However, a high performance IE system can be very challenging to construct due to the complexity and dynamic nature of human language. In this paper, we report an IE framework for cohort identification using EHRs that is a knowledge-driven framework developed under the Unstructured Information Management Architecture (UIMA). A system to extract specific information can be developed by subject matter experts through expert knowledge engineering of the externalized knowledge resources used in the framework.",2013.0,"Hongfang Liu, S. Bielinski, S. Sohn, Sean P. Murphy, K. Wagholikar, Siddhartha R. Jonnalagadda, K. Ravikumar, S. Wu, I. Kullo, C. Chute"
defaa3ee8276ffebac9bbcd4ddf62ff8b7d591b0,https://www.semanticscholar.org/paper/defaa3ee8276ffebac9bbcd4ddf62ff8b7d591b0,A hybrid system for temporal information extraction from clinical text,"OBJECTIVE
To develop a comprehensive temporal information extraction system that can identify events, temporal expressions, and their temporal relations in clinical text. This project was part of the 2012 i2b2 clinical natural language processing (NLP) challenge on temporal information extraction.


MATERIALS AND METHODS
The 2012 i2b2 NLP challenge organizers manually annotated 310 clinic notes according to a defined annotation guideline: a training set of 190 notes and a test set of 120 notes. All participating systems were developed on the training set and evaluated on the test set. Our system consists of three modules: event extraction, temporal expression extraction, and temporal relation (also called Temporal Link, or 'TLink') extraction. The TLink extraction module contains three individual classifiers for TLinks: (1) between events and section times, (2) within a sentence, and (3) across different sentences. The performance of our system was evaluated using scripts provided by the i2b2 organizers. Primary measures were micro-averaged Precision, Recall, and F-measure.


RESULTS
Our system was among the top ranked. It achieved F-measures of 0.8659 for temporal expression extraction (ranked fourth), 0.6278 for end-to-end TLink track (ranked first), and 0.6932 for TLink-only track (ranked first) in the challenge. We subsequently investigated different strategies for TLink extraction, and were able to marginally improve performance with an F-measure of 0.6943 for TLink-only track.",2013.0,"Buzhou Tang, Yonghui Wu, Min Jiang, Yukun Chen, J. Denny, Hua Xu"
dc7f01616aea5d85e594d6b9d1113ca3bb292d28,https://www.semanticscholar.org/paper/dc7f01616aea5d85e594d6b9d1113ca3bb292d28,"Accurate Proteome-wide Label-free Quantification by Delayed Normalization and Maximal Peptide Ratio Extraction, Termed MaxLFQ *","Protein quantification without isotopic labels has been a long-standing interest in the proteomics field. However, accurate and robust proteome-wide quantification with label-free approaches remains a challenge. We developed a new intensity determination and normalization procedure called MaxLFQ that is fully compatible with any peptide or protein separation prior to LC-MS analysis. Protein abundance profiles are assembled using the maximum possible information from MS signals, given that the presence of quantifiable peptides varies from sample to sample. For a benchmark dataset with two proteomes mixed at known ratios, we accurately detected the mixing ratio over the entire protein expression range, with greater precision for abundant proteins. The significance of individual label-free quantifications was obtained via a t test approach. For a second benchmark dataset, we accurately quantify fold changes over several orders of magnitude, a task that is challenging with label-based methods. MaxLFQ is a generic label-free quantification technology that is readily applicable to many biological questions; it is compatible with standard statistical analysis workflows, and it has been validated in many and diverse biological projects. Our algorithms can handle very large experiments of 500+ samples in a manageable computing time. It is implemented in the freely available MaxQuant computational proteomics platform and works completely seamlessly at the click of a button.",2014.0,"J. Cox, Marco Y. Hein, Christian A. Luber, Igor Paron, Nagarjuna Nagaraj, M. Mann"
2e50fa3360990aad1fc65a89231d16f8d3373836,https://www.semanticscholar.org/paper/2e50fa3360990aad1fc65a89231d16f8d3373836,Modeling Missing Data in Distant Supervision for Information Extraction,"Distant supervision algorithms learn information extraction models given only large readily available databases and text collections. Most previous work has used heuristics for generating labeled data, for example assuming that facts not contained in the database are not mentioned in the text, and facts in the database must be mentioned at least once. In this paper, we propose a new latent-variable approach that models missing data. This provides a natural way to incorporate side information, for instance modeling the intuition that text will often mention rare entities which are likely to be missing in the database. Despite the added complexity introduced by reasoning about missing data, we demonstrate that a carefully designed local search approach to inference is very accurate and scales to large datasets. Experiments demonstrate improved performance for binary and unary relation extraction when compared to learning with heuristic labels, including on average a 27% increase in area under the precision recall curve in the binary case.",2013.0,"Alan Ritter, Luke Zettlemoyer, Mausam, Oren Etzioni"
ec59845da9ee7efd33787b23f2f7c4be532da00e,https://www.semanticscholar.org/paper/ec59845da9ee7efd33787b23f2f7c4be532da00e,Inside YAGO2s: a transparent information extraction architecture,"YAGO [9, 6] is one of the largest public ontologies constructed by information extraction. In a recent refactoring called YAGO2s, the system has been given a modular and completely transparent architecture. In this demo, users can see how more than 30 individual modules of YAGO work in parallel to extract facts, to check facts for their correctness, to deduce facts, and to merge facts from different sources. A GUI allows users to play with different input files, to trace the provenance of individual facts to their sources, to change deduction rules, and to run individual extractors. Users can see step by step how the extractors work together to combine the individual facts to the coherent whole of the YAGO ontology.",2013.0,"J. Biega, Erdal Kuzey, Fabian M. Suchanek"
b04e3d829cec0d98224b73560b2bf27586523e48,https://www.semanticscholar.org/paper/b04e3d829cec0d98224b73560b2bf27586523e48,Open Information Extraction with Tree Kernels,"Traditional relation extraction seeks to identify pre-specified semantic relations within natural language text, while open Information Extraction (Open IE) takes a more general approach, and looks for a variety of relations without restriction to a fixed relation set. With this generalization comes the question, what is a relation? For example, should the more general task be restricted to relations mediated by verbs, nouns, or both? To help answer this question, we propose two levels of subtasks for Open IE. One task is to determine if a sentence potentially contains a relation between two entities? The other task looks to confirm explicit relation words for two entities. We propose multiple SVM models with dependency tree kernels for both tasks. For explicit relation extraction, our system can extract both noun and verb relations. Our results on three datasets show that our system is superior when compared to state-of-the-art systems like REVERB and OLLIE for both tasks. For example, in some experiments our system achieves 33% improvement on nominal relation extraction over OLLIE. In addition we propose an unsupervised rule-based approach which can serve as a strong baseline for Open IE systems.",2013.0,"Ying Xu, Mi-Young Kim, Kevin Quinn, R. Goebel, Denilson Barbosa"
87dee6b4a5fcbab541b45a967c24030df6cee29b,https://www.semanticscholar.org/paper/87dee6b4a5fcbab541b45a967c24030df6cee29b,Intellix -- End-User Trained Information Extraction for Document Archiving,"Automatic information extraction from scanned business documents is especially valuable in the application domain of document archiving. But current systems for automated document processing still require a lot of configuration work that can only be done by experienced users or administrators. We present an approach for information extraction which purely builds on end-user provided training examples and intentionally omits efficient known extraction techniques like rule based extraction that require intense training and/or information extraction expertise. Our evaluation on a large corpus of business documents shows competitive results of above 85% F1-measure on 10 commonly used fields like document type, sender, receiver and date. The system is deployed and used inside the commercial document management system DocuWare.",2013.0,"Daniel Schuster, Klemens Muthmann, D. Esser, A. Schill, Michael Berger, C. Weidling, Kamil Aliyev, A. Hofmeier"
f505dfe7296c695708c6ea2d3e2eb1ecc58f1b9a,https://www.semanticscholar.org/paper/f505dfe7296c695708c6ea2d3e2eb1ecc58f1b9a,Open Information Extraction via Contextual Sentence Decomposition,"We show how contextual sentence decomposition (CSD), a technique originally developed for high-precision semantic search, can be used for open information extraction (OIE). Intuitively, CSD decomposes a sentence into the parts that semantically ""belong together"". By identifying the (implicit or explicit) verb in each such part, we obtain facts like in OIE. We compare our system, called CSD-IE, to three state-of-the-art OIE systems: ReVerb, OLLIE, and ClausIE. We consider the following aspects: accuracy (does the extracted triple express a meaningful fact, which is also expressed in the original sentence), minimality (can the extracted triple be further decomposed into smaller meaningful triples), coverage (percentage of text contained in at least one extracted triple), and number of facts extracted. We show how CSD-IE clearly outperforms ReVerb and OLLIE in terms of coverage and recall, but at comparable accuracy and minimality, and how CSD-IE achieves precision and recall comparable to ClausIE, but at significantly better minimality.",2013.0,"Hannah Bast, Elmar Haussmann"
0f8dda06dc840ba5aaf52729a49d3020283041f2,https://www.semanticscholar.org/paper/0f8dda06dc840ba5aaf52729a49d3020283041f2,Speaker Embedding Extraction with Phonetic Information,"Speaker embeddings achieve promising results on many speaker verification tasks. Phonetic information, as an important component of speech, is rarely considered in the extraction of speaker embeddings. In this paper, we introduce phonetic information to the speaker embedding extraction based on the x-vector architecture. Two methods using phonetic vectors and multi-task learning are proposed. On the Fisher dataset, our best system outperforms the original x-vector approach by 20% in EER, and by 15%, 15% in minDCF08 and minDCF10, respectively. Experiments conducted on NIST SRE10 further demonstrate the effectiveness of the proposed methods.",2018.0,"Yi Liu, Liang He, Jia Liu, Michael T. Johnson"
2b84b1660760e7363b1b64f3e28e393ced83551d,https://www.semanticscholar.org/paper/2b84b1660760e7363b1b64f3e28e393ced83551d,TwitIE: An Open-Source Information Extraction Pipeline for Microblog Text,"Twitter is the largest source of microblog text, responsible for gigabytes of human discourse every day. Processing microblog text is difficult: the genre is noisy, documents have little context, and utterances are very short. As such, conventional NLP tools fail when faced with tweets and other microblog text. We present TwitIE, an open-source NLP pipeline customised to microblog text at every stage. Additionally, it includes Twitter-specific data import and metadata handling. This paper introduces each stage of the TwitIE pipeline, which is a modification of the GATE ANNIE open-source pipeline for news text. An evaluation against some state-of-the-art systems is also presented.",2013.0,"Kalina Bontcheva, Leon Derczynski, Adam Funk, M. Greenwood, D. Maynard, N. Aswani"
6ff8f0151eb787436cd5dee75aba74c44b89ea53,https://www.semanticscholar.org/paper/6ff8f0151eb787436cd5dee75aba74c44b89ea53,Information Extraction,"SYNONYMS NONE DEFINITION Information Extraction (IE) is a task of extracting pre-specified types of facts from written texts or speech transcripts, and converting them into structured representations (e.g., databases). IE terminologies are explained via an example as follows. Media tycoon Barry Diller on Wednesday quit as chief of Vivendi Universal Entertainment, the entertainment unit of French giant Vivendi Universal whose future appears up for grabs.-"" End-Position "" event. The above sentence includes a "" Personnel_End-Position "" event mention, with the trigger word which most clearly expresses the event occurrence, the position, the person who quit the position, the organization, and the time during which the event happened: Trigger Quit Person Barry Diller Media tycoon Organization Vivendi Universal Entertainment the entertainment unit of French giant Vivendi Universal Position Chief Time-within Wednesday Table 1. Event Extraction Example HISTORICAL BACKGROUND The earliest IE system was directed by Naomi Sager of the Linguistic String Project group [1] in the medical domain. However, the specific task of information extraction was formally evaluated through the There were four specific evaluations: Named entity, coreference and template element reflected in the evaluation tasks introduced for MUC-6, and template relation introduced in MUC-7.",,"Byline Heng, Ji"
8e269982523d0a782030dc50a74f5d6fe3b974c5,https://www.semanticscholar.org/paper/8e269982523d0a782030dc50a74f5d6fe3b974c5,Attention-Based Extraction of Structured Information from Street View Imagery,"We present a neural network model — based on Convolutional Neural Networks, Recurrent Neural Networks and a novel attention mechanism — which achieves 84.2% accuracy on the challenging French Street Name Signs (FSNS) dataset, significantly outperforming the previous state of the art (Smith'16), which achieved 72.46%. Furthermore, our new method is much simpler and more general than the previous approach. To demonstrate the generality of our model, we show that it also performs well on an even more challenging dataset derived from Google Street View, in which the goal is to extract business names from store fronts. Finally, we study the speed/accuracy tradeoff that results from using CNN feature extractors of different depths. Surprisingly, we find that deeper is not always better (in terms of accuracy, as well as speed). Our resulting model is simple, accurate and fast, allowing it to be used at scale on a variety of challenging real-world text extraction problems.",2017.0,"Z. Wojna, Alexander N. Gorban, Dar-Shyang Lee, K. Murphy, Qian Yu, Yeqing Li, Julian Ibarz"
ec4b9e560654a923f58ff5e61691ba6ecc9c5c10,https://www.semanticscholar.org/paper/ec4b9e560654a923f58ff5e61691ba6ecc9c5c10,The Extraction of Neural Information from the Surface EMG for the Control of Upper-Limb Prostheses: Emerging Avenues and Challenges,"Despite not recording directly from neural cells, the surface electromyogram (EMG) signal contains information on the neural drive to muscles, i.e, the spike trains of motor neurons. Using this property, myoelectric control consists of the recording of EMG signals for extracting control signals to command external devices, such as hand prostheses. In commercial control systems, the intensity of muscle activity is extracted from the EMG and used for single degrees of freedom activation (direct control). Over the past 60 years, academic research has progressed to more sophisticated approaches but, surprisingly, none of these academic achievements has been implemented in commercial systems so far. We provide an overview of both commercial and academic myoelectric control systems and we analyze their performance with respect to the characteristics of the ideal myocontroller. Classic and relatively novel academic methods are described, including techniques for simultaneous and proportional control of multiple degrees of freedom and the use of individual motor neuron spike trains for direct control. The conclusion is that the gap between industry and academia is due to the relatively small functional improvement in daily situations that academic systems offer, despite the promising laboratory results, at the expense of a substantial reduction in robustness. None of the systems so far proposed in the literature fulfills all the important criteria needed for widespread acceptance by the patients, i.e. intuitive, closed-loop, adaptive, and robust real-time (<;200 ms delay) control, minimal number of recording electrodes with low sensitivity to repositioning, minimal training, limited complexity and low consumption. Nonetheless, in recent years, important efforts have been invested in matching these criteria, with relevant steps forwards.",2014.0,"D. Farina, N. Jiang, Hubertus Rehbaum, A. Holobar, B. Graimann, H. Dietl, O. Aszmann"
29324d2dbe98a97e0647193b39fe8f4aae4edc37,https://www.semanticscholar.org/paper/29324d2dbe98a97e0647193b39fe8f4aae4edc37,ChemDataExtractor: A toolkit for automated extraction of chemical information from the scientific literature,Presentation delivered at Text and Data Mining Symposium held at the University of Cambridge.,2017.0,Callum J Court
4930de1aff4b1948157a15ac9cdb02364bee97bb,https://www.semanticscholar.org/paper/4930de1aff4b1948157a15ac9cdb02364bee97bb,Graph Convolution over Pruned Dependency Trees Improves Relation Extraction,"Dependency trees help relation extraction models capture long-range relations between words. However, existing dependency-based models either neglect crucial information (e.g., negation) by pruning the dependency trees too aggressively, or are computationally inefficient because it is difficult to parallelize over different tree structures. We propose an extension of graph convolutional networks that is tailored for relation extraction, which pools information over arbitrary dependency structures efficiently in parallel. To incorporate relevant information while maximally removing irrelevant content, we further apply a novel pruning strategy to the input trees by keeping words immediately around the shortest path between the two entities among which a relation might hold. The resulting model achieves state-of-the-art performance on the large-scale TACRED dataset, outperforming existing sequence and dependency-based neural models. We also show through detailed analysis that this model has complementary strengths to sequence models, and combining them further improves the state of the art.",2018.0,"Yuhao Zhang, Peng Qi, Christopher D. Manning"
f5d6779073901989f14de984e2236891a801c2bd,https://www.semanticscholar.org/paper/f5d6779073901989f14de984e2236891a801c2bd,Joint Extraction of Entities and Relations Based on a Novel Tagging Scheme,"Joint extraction of entities and relations is an important task in information extraction. To tackle this problem, we firstly propose a novel tagging scheme that can convert the joint extraction task to a tagging problem.. Then, based on our tagging scheme, we study different end-to-end models to extract entities and their relations directly, without identifying entities and relations separately. We conduct experiments on a public dataset produced by distant supervision method and the experimental results show that the tagging based methods are better than most of the existing pipelined and joint learning methods. What’s more, the end-to-end model proposed in this paper, achieves the best results on the public dataset.",2017.0,"Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, P. Zhou, Bo Xu"
707eb268a60d47b18552f8fe2b0629ba6836617d,https://www.semanticscholar.org/paper/707eb268a60d47b18552f8fe2b0629ba6836617d,Event Extraction as Machine Reading Comprehension,"Event extraction (EE) is a crucial information extraction task that aims to extract event information in texts. Previous methods for EE typically model it as a classification task, which are usually prone to the data scarcity problem. In this paper, we propose a new learning paradigm of EE, by explicitly casting it as a machine reading comprehension problem (MRC). Our approach includes an unsupervised question generation process, which can transfer event schema into a set of natural questions, followed by a BERT-based question-answering process to retrieve answers as EE results. This learning paradigm enables us to strengthen the reasoning process of EE, by introducing sophisticated models in MRC, and relieve the data scarcity problem, by introducing the large-scale datasets in MRC. The empirical results show that: i) our approach attains state-of-the-art performance by considerable margins over previous methods. ii) Our model is excelled in the data-scarce scenario, for example, obtaining 49.8\% in F1 for event argument extraction with only 1\% data, compared with 2.2\% of the previous method. iii) Our model also fits with zero-shot scenarios, achieving $37.0\%$ and $16\%$ in F1 on two datasets without using any EE training data.",2020.0,"Jian Liu, Yubo Chen, Kang Liu, Wei Bi, Xiaojiang Liu"
7b66a5dd12d4b262138dc5864e908bc87f2d919a,https://www.semanticscholar.org/paper/7b66a5dd12d4b262138dc5864e908bc87f2d919a,PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction,"Joint extraction of entities and relations from unstructured texts is a crucial task in information extraction. Recent methods achieve considerable performance but still suffer from some inherent limitations, such as redundancy of relation prediction, poor generalization of span-based extraction and inefficiency. In this paper, we decompose this task into three subtasks, Relation Judgement, Entity Extraction and Subject-object Alignment from a novel perspective and then propose a joint relational triple extraction framework based on Potential Relation and Global Correspondence (PRGC). Specifically, we design a component to predict potential relations, which constrains the following entity extraction to the predicted relation subset rather than all relations; then a relation-specific sequence tagging component is applied to handle the overlapping problem between subjects and objects; finally, a global correspondence component is designed to align the subject and object into a triple with low-complexity. Extensive experiments show that PRGC achieves state-of-the-art performance on public benchmarks with higher efficiency and delivers consistent performance gain on complex scenarios of overlapping triples. The source code has been submitted as the supplementary material and will be made publicly available after the blind review.",2021.0,"Heng Zheng, Rui Wen, Xi Chen, Yifan Yang, Yunyan Zhang, Ziheng Zhang, Ningyu Zhang, Bin Qin, Ming Xu, Yefeng Zheng"
e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80,https://www.semanticscholar.org/paper/e4363d077a890c8d5c5e66b82fe69a1bbbdd5c80,Attention Guided Graph Convolutional Networks for Relation Extraction,"Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches.",2019.0,"Zhijiang Guo, Yan Zhang, Wei Lu"
3d852a8aea396f2c110d4aeaec301030de93a8f8,https://www.semanticscholar.org/paper/3d852a8aea396f2c110d4aeaec301030de93a8f8,Feature Extraction for Hyperspectral Imagery: The Evolution From Shallow to Deep: Overview and Toolbox,"Hyperspectral images (HSIs) provide detailed spectral information through hundreds of (narrow) spectral channels (also known as dimensionality or bands), which can be used to accurately classify diverse materials of interest. The increased dimensionality of such data makes it possible to significantly improve data information content but provides a challenge to conventional techniques (the so-called curse of dimensionality) for accurate analysis of HSIs.",2020.0,"Behnood Rasti, D. Hong, Renlong Hang, Pedram Ghamisi, Xudong Kang, J. Chanussot, J. Benediktsson"
3899f87a2031f3434f89beb68c11a1ca6428328a,https://www.semanticscholar.org/paper/3899f87a2031f3434f89beb68c11a1ca6428328a,End-to-End Relation Extraction using LSTMs on Sequences and Tree Structures,"We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional tree-structured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the state-of-the-art feature-based model on end-to-end relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components.",2016.0,"Makoto Miwa, Mohit Bansal"
068b7e7ef91d84652d63b1ae165d747266a19bae,https://www.semanticscholar.org/paper/068b7e7ef91d84652d63b1ae165d747266a19bae,Exploiting syntactic and semantics information for chemical–disease relation extraction,"Identifying chemical–disease relations (CDR) from biomedical literature could improve chemical safety and toxicity studies. This article proposes a novel syntactic and semantic information exploitation method for CDR extraction. The proposed method consists of a feature-based model, a tree kernel-based model and a neural network model. The feature-based model exploits lexical features, the tree kernel-based model captures syntactic structure features, and the neural network model generates semantic representations. The motivation of our method is to fully utilize the nice properties of the three models to explore diverse information for CDR extraction. Experiments on the BioCreative V CDR dataset show that the three models are all effective for CDR extraction, and their combination could further improve extraction performance. Database URL: http://www.biocreative.org/resources/corpora/biocreative-v-cdr-corpus/.",2016.0,"Huiwei Zhou, Huijie Deng, Long Chen, Yunlong Yang, Chen Jia, Degen Huang"
75ae36f8f3462588a498c4159d05793e77c378a3,https://www.semanticscholar.org/paper/75ae36f8f3462588a498c4159d05793e77c378a3,Open Information Extraction: The Second Generation,"How do we scale information extraction to the massive size and unprecedented heterogeneity of the Web corpus? Beginning in 2003, our KnowItAll project has sought to extract high-quality knowledge from the Web. 
 
In 2007, we introduced the Open Information Extraction (Open IE) paradigm which eschews hand-labeled training examples, and avoids domain-specific verbs and nouns, to develop unlexicalized, domain-independent extractors that scale to the Web corpus. Open IE systems have extracted billions of assertions as the basis for both common-sense knowledge and novel question-answering systems. 
 
This paper describes the second generation of Open IE systems, which rely on a novel model of how relations and their arguments are expressed in English sentences to double precision/recall compared with previous systems such as TEXTRUNNER and WOE.",2011.0,"Oren Etzioni, Anthony Fader, Janara Christensen, S. Soderland, Mausam"
7afc4b1b89081d118700801064e251870d05617b,https://www.semanticscholar.org/paper/7afc4b1b89081d118700801064e251870d05617b,Joint Event Extraction via Recurrent Neural Networks,"Event extraction is a particularly challenging problem in information extraction. The state-of-the-art models for this problem have either applied convolutional neural networks in a pipelined framework (Chen et al., 2015) or followed the joint architecture via structured prediction with rich local and global features (Li et al., 2013). The former is able to learn hidden feature representations automatically from data based on the continuous and generalized representations of words. The latter, on the other hand, is capable of mitigating the error propagation problem of the pipelined approach and exploiting the inter-dependencies between event triggers and argument roles via discrete structures. In this work, we propose to do event extraction in a joint framework with bidirectional recurrent neural networks, thereby beneﬁting from the advantages of the two models as well as addressing issues inherent in the existing approaches. We systematically investigate different memory features for the joint model and demonstrate that the proposed model achieves the state-of-the-art performance on the ACE 2005 dataset.",2016.0,"Thien Huu Nguyen, Kyunghyun Cho, R. Grishman"
916ac71109a4c59640ba2de167acab7276d33429,https://www.semanticscholar.org/paper/916ac71109a4c59640ba2de167acab7276d33429,Dependency-Based Open Information Extraction,"Building shallow semantic representations from text corpora is the first step to perform more complex tasks such as text entailment, enrichment of knowledge bases, or question answering. Open Information Extraction (OIE) is a recent unsupervised strategy to extract billions of basic assertions from massive corpora, which can be considered as being a shallow semantic representation of those corpora. In this paper, we propose a new multilingual OIE system based on robust and fast rule-based dependency parsing. It permits to extract more precise assertions (verb-based triples) from text than state of the art OIE systems, keeping a crucial property of those systems: scaling to Web-size document collections.",2012.0,"Pablo Gamallo, Marcos Garcia, Santiago Fernández-Lanza"
659344fe40d46f68445a068e7e8d68ffc379eba4,https://www.semanticscholar.org/paper/659344fe40d46f68445a068e7e8d68ffc379eba4,KrakeN: N-ary Facts in Open Information Extraction,"Current techniques for Open Information Extraction (OIE) focus on the extraction of binary facts and suffer significant quality loss for the task of extracting higher order N-ary facts. This quality loss may not only affect the correctness, but also the completeness of an extracted fact. We present KrakeN, an OIE system specifically designed to capture N-ary facts, as well as the results of an experimental study on extracting facts from Web text in which we examine the issue of fact completeness. Our preliminary experiments indicate that KrakeN is a high precision OIE approach that captures more facts per sentence at greater completeness than existing OIE approaches, but is vulnerable to noisy and ungrammatical text.",2012.0,"A. Akbik, Alexander Löser"
2c0f2a03c3a427cc61359b5e2c31cfefe9850a31,https://www.semanticscholar.org/paper/2c0f2a03c3a427cc61359b5e2c31cfefe9850a31,WebSets: extracting sets of entities from the web using unsupervised information extraction,"We describe a open-domain information extraction method for extracting concept-instance pairs from an HTML corpus. Most earlier approaches to this problem rely on combining clusters of distributionally similar terms and concept-instance pairs obtained with Hearst patterns. In contrast, our method relies on a novel approach for clustering terms found in HTML tables, and then assigning concept names to these clusters using Hearst patterns. The method can be efficiently applied to a large corpus, and experimental results on several datasets show that our method can accurately extract large numbers of concept-instance pairs.",2012.0,"Bhavana Dalvi, William W. Cohen, Jamie Callan"
b84ba99107db64fc2213d02c0473756d7ba0f4a9,https://www.semanticscholar.org/paper/b84ba99107db64fc2213d02c0473756d7ba0f4a9,Incremental Information Extraction Using Relational Databases,"Information extraction systems are traditionally implemented as a pipeline of special-purpose processing modules targeting the extraction of a particular kind of information. A major drawback of such an approach is that whenever a new extraction goal emerges or a module is improved, extraction has to be reapplied from scratch to the entire text corpus even though only a small part of the corpus might be affected. In this paper, we describe a novel approach for information extraction in which extraction needs are expressed in the form of database queries, which are evaluated and optimized by database systems. Using database queries for information extraction enables generic extraction and minimizes reprocessing of data by performing incremental extraction to identify which part of the data is affected by the change of components or goals. Furthermore, our approach provides automated query generation components so that casual users do not have to learn the query language in order to perform extraction. To demonstrate the feasibility of our incremental extraction approach, we performed experiments to highlight two important aspects of an information extraction system: efficiency and quality of extraction results. Our experiments show that in the event of deployment of a new module, our incremental extraction approach reduces the processing time by 89.64 percent as compared to a traditional pipeline approach. By applying our methods to a corpus of 17 million biomedical abstracts, our experiments show that the query performance is efficient for real-time applications. Our experiments also revealed that our approach achieves high quality extraction results.",2012.0,"L. Tari, P. Tu, J. Hakenberg, Yi Chen, Tran Cao Son, Graciela Gonzalez, Chitta Baral"
da252767cc53379eaef56023c54c6dca93735362,https://www.semanticscholar.org/paper/da252767cc53379eaef56023c54c6dca93735362,"Natural Antioxidants in Foods and Medicinal Plants: Extraction, Assessment and Resources","Natural antioxidants are widely distributed in food and medicinal plants. These natural antioxidants, especially polyphenols and carotenoids, exhibit a wide range of biological effects, including anti-inflammatory, anti-aging, anti-atherosclerosis and anticancer. The effective extraction and proper assessment of antioxidants from food and medicinal plants are crucial to explore the potential antioxidant sources and promote the application in functional foods, pharmaceuticals and food additives. The present paper provides comprehensive information on the green extraction technologies of natural antioxidants, assessment of antioxidant activity at chemical and cellular based levels and their main resources from food and medicinal plants.",2017.0,"Dong-ping Xu, Ya Li, Xiao Meng, Tong Zhou, Yue Zhou, Jie Zheng, Jiao-Jiao Zhang, Huabin Li"
540112b5f56f1572c87a4d29ab6d5250795ecf2e,https://www.semanticscholar.org/paper/540112b5f56f1572c87a4d29ab6d5250795ecf2e,Document-level Relation Extraction as Semantic Segmentation,"Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA.",2021.0,"Ningyu Zhang, Xiang Chen, Xin Xie, Shumin Deng, Chuanqi Tan, Mosha Chen, Fei Huang, Luo Si, Huajun Chen"
002f4a997d585028377257f691768789e4a3a030,https://www.semanticscholar.org/paper/002f4a997d585028377257f691768789e4a3a030,Advanced remote sensing : terrestrial information extraction and applications,"""Advanced Remote Sensing"" is an application-based reference that provides a single source of mathematical concepts necessary for remote sensing data gathering and assimilation. It presents state-of-the-art techniques for estimating land surface variables from a variety of data types, including optical sensors such as RADAR and LIDAR. Scientists in a number of different fields including geography, geology, atmospheric science, environmental science, planetary science and ecology will have access to critically-important data extraction techniques and their virtually unlimited applications. While rigorous enough for the most experienced of scientists, the techniques are well designed and integrated, making the book's content intuitive, clearly presented, and practical in its implementation. This title provides a comprehensive overview of various practical methods and algorithms. It provides detailed description of the principles and procedures of the state-of-the-art algorithms. It includes real-world case studies that open several chapters. It presents more than 500 full-color figures and tables. It is edited by top remote sensing experts with contributions from authors across the geosciences.",2012.0,"S. Liang, Xiaowen Li, Jindi Wang"
013faec0400d315935e71a2bdfeb22cc83752b3e,https://www.semanticscholar.org/paper/013faec0400d315935e71a2bdfeb22cc83752b3e,Reasoning with Latent Structure Refinement for Document-Level Relation Extraction,"Document-level relation extraction requires integrating information within and across multiple sentences of a document and capturing complex interactions between inter-sentence entities. However, effective aggregation of relevant information in the document remains a challenging research question. Existing approaches construct static document-level graphs based on syntactic trees, co-references or heuristics from the unstructured text to model the dependencies. Unlike previous methods that may not be able to capture rich non-local interactions for inference, we propose a novel model that empowers the relational reasoning across sentences by automatically inducing the latent document-level graph. We further develop a refinement strategy, which enables the model to incrementally aggregate relevant information for multi-hop reasoning. Specifically, our model achieves an F1 score of 59.05 on a large-scale document-level dataset (DocRED), significantly improving over the previous results, and also yields new state-of-the-art results on the CDR and GDA dataset. Furthermore, extensive analyses show that the model is able to discover more accurate inter-sentence relations.",2020.0,"Guoshun Nan, Zhijiang Guo, Ivan Sekulic, Wei Lu"
233d861338cfcd479b1d21897453fcc66418d5e1,https://www.semanticscholar.org/paper/233d861338cfcd479b1d21897453fcc66418d5e1,Open Information Extraction Using Wikipedia,"Information-extraction (IE) systems seek to distill semantic relations from natural-language text, but most systems use supervised learning of relation-specific examples and are thus limited by the availability of training data. Open IE systems such as TextRunner, on the other hand, aim to handle the unbounded number of relations found on the Web. But how well can these open systems perform? 
 
This paper presents WOE, an open IE system which improves dramatically on TextRunner's precision and recall. The key to WOE's performance is a novel form of self-supervised learning for open extractors -- using heuristic matches between Wikipedia infobox attribute values and corresponding sentences to construct training data. Like TextRunner, WOE's extractor eschews lexicalized features and handles an unbounded set of semantic relations. WOE can operate in two modes: when restricted to POS tag features, it runs as quickly as TextRunner, but when set to use dependency-parse features its precision and recall rise even higher.",2010.0,"Fei Wu, Daniel S. Weld"
d1e9111e0e630898e9653908b129f61232ec6814,https://www.semanticscholar.org/paper/d1e9111e0e630898e9653908b129f61232ec6814,Green Extraction Methods for Polyphenols from Plant Matrices and Their Byproducts: A Review.,"Polyphenols as phytochemicals have gained significant importance owing to several associated health benefits with regard to lifestyle diseases and oxidative stress. To date, the development of a single standard method for efficient and rapid extraction of polyphenols from plant matrices has remained a challenge due to the inherent limitations of various conventional extraction methods. The exploitation of polyphenols as bioactive compounds at various commercial levels has motivated scientists to explore more eco-friendly, efficient, and cost-effective extraction techniques, based on a green extraction approach. The current review aims to provide updated technical information about extraction mechanisms, their advantages and disadvantages, and factors affecting efficiencies, and also presents a comparative overview of applications of the following modern green extraction techniques-supercritical fluid extraction, ultrasound-assisted extraction, microwave-assisted extraction, pressurized liquid extraction, and pressurized hot water extraction-as alternatives to conventional extraction methods for polyphenol extraction. These techniques are proving to be promising for the extraction of thermolabile phenolic compounds due to their advantages over conventional, time-consuming, and laborious extraction techniques, such as reduced solvent use and time and energy consumption and higher recovery rates with lower operational costs. The growing interest in plant-derived polyphenols prompts continual search for green and economically feasible modern extraction techniques. Modern green extraction techniques represent promising approaches by virtue of overcoming current limitations to the exploitation of polyphenols as bioactive compounds to explore their wide-reaching applications on an industrial scale and in emerging global markets. Future research is needed in order to remove the technical barriers to scale-up the processes for industrial needs by increasing our understanding and improving the design of modern extraction operations.",2017.0,"K. Ameer, H. Shahbaz, J. Kwon"
3cc310717f422592c3cc9a046943777468c14358,https://www.semanticscholar.org/paper/3cc310717f422592c3cc9a046943777468c14358,A Survey of Web Information Extraction Systems,"The Internet presents a huge amount of useful information which is usually formatted for its users, which makes it difficult to extract relevant data from various sources. Therefore, the availability of robust, flexible information extraction (IE) systems that transform the Web pages into program-friendly structures such as a relational database will become a great necessity. Although many approaches for data extraction from Web pages have been developed, there has been limited effort to compare such tools. Unfortunately, in only a few cases can the results generated by distinct tools be directly compared since the addressed extraction tasks are different. This paper surveys the major Web data extraction approaches and compares them in three dimensions: the task domain, the automation degree, and the techniques used. The criteria of the first dimension explain why an IE system fails to handle some Web sites of particular structures. The criteria of the second dimension classify IE systems based on the techniques used. The criteria of the third dimension measure the degree of automation for IE systems. We believe these criteria provide qualitatively measures to evaluate various IE approaches",2006.0,"Chia-Hui Chang, Mohammed Kayed, M. Girgis, Khaled Shaalan"
bfe3927c8ce03a641f957cfb7335052d332ff8a2,https://www.semanticscholar.org/paper/bfe3927c8ce03a641f957cfb7335052d332ff8a2,MIKE: Keyphrase Extraction by Integrating Multidimensional Information,"Traditional supervised keyphrase extraction models depend on the features of labelled keyphrases while prevailing unsupervised models mainly rely on structure of the word graph, with candidate words as nodes and edges capturing the co-occurrence information between words. However, systematically integrating all these multidimensional heterogeneous information into a unified model is relatively unexplored. In this paper, we focus on how to effectively exploit multidimensional information to improve the keyphrase extraction performance (MIKE). Specifically, we propose a random-walk parametric model, MIKE, that learns the latent representation for a candidate keyphrase that captures the mutual influences among all information, and simultaneously optimizes the parameters and ranking scores of candidates in the word graph. We use the gradient-descent algorithm to optimize our model and show the comprehensive experiments with two publicly-available WWW and KDD datasets in Computer Science. Experimental results demonstrate that our approach significantly outperforms the state-of-the-art graph-based keyphrase extraction approaches.",2017.0,"Yuxiang Zhang, Yaocheng Chang, Xiaoqing Liu, Sujatha Das Gollapalli, Xiaoli Li, Chunjing Xiao"
98a6b07d51261df9418981c1dddf09ad4a9c48e4,https://www.semanticscholar.org/paper/98a6b07d51261df9418981c1dddf09ad4a9c48e4,Event Extraction via Dynamic Multi-Pooling Convolutional Neural Networks,"Traditional approaches to the task of ACE event extraction primarily rely on elaborately designed features and complicated natural language processing (NLP) tools. These traditional approaches lack generalization, take a large amount of human effort and are prone to error propagation and data sparsity problems. This paper proposes a novel event-extraction method, which aims to automatically extract lexical-level and sentence-level features without using complicated NLP tools. We introduce a word-representation model to capture meaningful semantic regularities for words and adopt a framework based on a convolutional neural network (CNN) to capture sentence-level clues. However, CNN can only capture the most important information in a sentence and may miss valuable facts when considering multiple-event sentences. We propose a dynamic multi-pooling convolutional neural network (DMCNN), which uses a dynamic multi-pooling layer according to event triggers and arguments, to reserve more crucial information. The experimental results show that our approach significantly outperforms other state-of-the-art methods.",2015.0,"Yubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, Jun Zhao"
2e428fd3bfff371b2dcb8fa452bed3b7780d8ee5,https://www.semanticscholar.org/paper/2e428fd3bfff371b2dcb8fa452bed3b7780d8ee5,Template-Based Information Extraction without the Templates,"Standard algorithms for template-based information extraction (IE) require predefined template schemas, and often labeled data, to learn to extract their slot fillers (e.g., an embassy is the Target of a Bombing template). This paper describes an approach to template-based IE that removes this requirement and performs extraction without knowing the template structure in advance. Our algorithm instead learns the template structure automatically from raw text, inducing template schemas as sets of linked events (e.g., bombings include detonate, set off, and destroy events) associated with semantic roles. We also solve the standard IE task, using the induced syntactic patterns to extract role fillers from specific documents. We evaluate on the MUC-4 terrorism dataset and show that we induce template structure very similar to hand-created gold structure, and we extract role fillers with an F1 score of .40, approaching the performance of algorithms that require full knowledge of the templates.",2011.0,"Nathanael Chambers, Dan Jurafsky"
c7d2a4132d30f55ad22523262ce1ebd9c9f88504,https://www.semanticscholar.org/paper/c7d2a4132d30f55ad22523262ce1ebd9c9f88504,Machine-learned solutions for three stages of clinical information extraction: the state of the art at i2b2 2010,"Objective As clinical text mining continues to mature, its potential as an enabling technology for innovations in patient care and clinical research is becoming a reality. A critical part of that process is rigid benchmark testing of natural language processing methods on realistic clinical narrative. In this paper, the authors describe the design and performance of three state-of-the-art text-mining applications from the National Research Council of Canada on evaluations within the 2010 i2b2 challenge. Design The three systems perform three key steps in clinical information extraction: (1) extraction of medical problems, tests, and treatments, from discharge summaries and progress notes; (2) classification of assertions made on the medical problems; (3) classification of relations between medical concepts. Machine learning systems performed these tasks using large-dimensional bags of features, as derived from both the text itself and from external sources: UMLS, cTAKES, and Medline. Measurements Performance was measured per subtask, using micro-averaged F-scores, as calculated by comparing system annotations with ground-truth annotations on a test set. Results The systems ranked high among all submitted systems in the competition, with the following F-scores: concept extraction 0.8523 (ranked first); assertion detection 0.9362 (ranked first); relationship detection 0.7313 (ranked second). Conclusion For all tasks, we found that the introduction of a wide range of features was crucial to success. Importantly, our choice of machine learning algorithms allowed us to be versatile in our feature design, and to introduce a large number of features without overfitting and without encountering computing-resource bottlenecks.",2011.0,"Berry de Bruijn, Colin Cherry, S. Kiritchenko, Joel D. Martin, Xiao-Dan Zhu"
53955e1959b8a7d8d356c2c04ceaec8ce1194850,https://www.semanticscholar.org/paper/53955e1959b8a7d8d356c2c04ceaec8ce1194850,Cross-Domain WiFi Sensing with Channel State Information: A Survey,"The past years have witnessed the rapid conceptualization and development of wireless sensing based on Channel State Information (CSI) with commodity WiFi devices. Recent studies have demonstrated the vast potential of WiFi sensing in detection, recognition, and estimation applications. However, the widespread deployment of WiFi sensing systems still faces a significant challenge: how to ensure the sensing performance when exposing a pre-trained sensing system to new domains, such as new environments, different configurations, and unseen users, without data collection and system retraining. This survey provides a comprehensive review of recent research efforts on cross-domain WiFi Sensing. We first introduce the mathematical model of CSI and explore the impact of different domains on CSI. Then we present a general workflow of cross-domain WiFi sensing systems, which consists of signal processing and cross-domain sensing. Five cross-domain sensing algorithms, including domain-invariant feature extraction, virtual sample generation, transfer learning, few-shot learning and big data solution, are summarized to show how they achieve high sensing accuracy when encountering new domains. The advantages and limitations of each algorithm are also summarized and the performance comparison is made based on different applications. Finally, we discuss the remaining challenges to further promote the practical usability of cross-domain WiFi sensing systems.",2022.0,"Chen Chen, Gang Zhou, Youfang Lin"
2f6f4b0789b54a115bf8ef67a6eeaa1e632dc6b3,https://www.semanticscholar.org/paper/2f6f4b0789b54a115bf8ef67a6eeaa1e632dc6b3,"A Brief Survey of Text Mining: Classification, Clustering and Extraction Techniques","The amount of text that is generated every day is increasing dramatically. This tremendous volume of mostly unstructured text cannot be simply processed and perceived by computers. Therefore, efficient and effective techniques and algorithms are required to discover useful patterns. Text mining is the task of extracting meaningful information from text, which has gained significant attentions in recent years. In this paper, we describe several of the most fundamental text mining tasks and techniques including text pre-processing, classification and clustering. Additionally, we briefly explain text mining in biomedical and health care domains.",2017.0,"M. Allahyari, Seyedamin Pouriyeh, Mehdi Assefi, Saied Safaei, Elizabeth D. Trippe, Juan B. Gutiérrez, K. Kochut"
2c5ec74fb56fbfbceaa4cd5c8312ada4e2e19503,https://www.semanticscholar.org/paper/2c5ec74fb56fbfbceaa4cd5c8312ada4e2e19503,Entity-Relation Extraction as Multi-Turn Question Answering,"In this paper, we propose a new paradigm for the task of entity-relation extraction. We cast the task as a multi-turn question answering problem, i.e., the extraction of entities and elations is transformed to the task of identifying answer spans from the context. This multi-turn QA formalization comes with several key advantages: firstly, the question query encodes important information for the entity/relation class we want to identify; secondly, QA provides a natural way of jointly modeling entity and relation; and thirdly, it allows us to exploit the well developed machine reading comprehension (MRC) models. Experiments on the ACE and the CoNLL04 corpora demonstrate that the proposed paradigm significantly outperforms previous best models. We are able to obtain the state-of-the-art results on all of the ACE04, ACE05 and CoNLL04 datasets, increasing the SOTA results on the three datasets to 49.6 (+1.2), 60.3 (+0.7) and 69.2 (+1.4), respectively. Additionally, we construct and will release a newly developed dataset RESUME, which requires multi-step reasoning to construct entity dependencies, as opposed to the single-step dependency extraction in the triplet exaction in previous datasets. The proposed multi-turn QA model also achieves the best performance on the RESUME dataset.",2019.0,"Xiaoya Li, Fan Yin, Zijun Sun, Xiayu Li, Arianna Yuan, Duo Chai, Mingxin Zhou, Jiwei Li"
9ef07373873cc0f0b940512dcdde4e7b54b0cfb0,https://www.semanticscholar.org/paper/9ef07373873cc0f0b940512dcdde4e7b54b0cfb0,Web-scale information extraction in knowitall: (preliminary results),"Manually querying search engines in order to accumulate a large bodyof factual information is a tedious, error-prone process of piecemealsearch. Search engines retrieve and rank potentially relevantdocuments for human perusal, but do not extract facts, assessconfidence, or fuse information from multiple documents. This paperintroduces KnowItAll, a system that aims to automate the tedious process ofextracting large collections of facts from the web in an autonomous,domain-independent, and scalable manner.The paper describes preliminary experiments in which an instance of KnowItAll, running for four days on a single machine, was able to automatically extract 54,753 facts. KnowItAll associates a probability with each fact enabling it to trade off precision and recall. The paper analyzes KnowItAll's architecture and reports on lessons learned for the design of large-scale information extraction systems.",2004.0,"Oren Etzioni, Michael J. Cafarella, Doug Downey, Stanley Kok, Ana-Maria Popescu, T. Shaked, S. Soderland, Daniel S. Weld, A. Yates"
30d226088c4f00b24ded6367757ad88320cc40aa,https://www.semanticscholar.org/paper/30d226088c4f00b24ded6367757ad88320cc40aa,An analysis of open information extraction based on semantic role labeling,"Open Information Extraction extracts relations from text without requiring a pre-specified domain or vocabulary. While existing techniques have used only shallow syntactic features, we investigate the use of semantic role labeling techniques for the task of Open IE. Semantic role labeling (SRL) and Open IE, although developed mostly in isolation, are quite related. We compare SRL-based open extractors, which perform computationally expensive, deep syntactic analysis, with TextRunner, an open extractor, which uses shallow syntactic analysis but is able to analyze many more sentences in a fixed amount of time and thus exploit corpus-level statistics. Our evaluation answers questions regarding these systems, including, can SRL extractors, which are trained on PropBank, cope with heterogeneous text found on the Web? Which extractor attains better precision, recall, f-measure, or running time? How does extractor performance vary for binary, n-ary and nested relations? How much do we gain by running multiple extractors? How do we select the optimal extractor given amount of data, available time, types of extractions desired?",2011.0,"Janara Christensen, Mausam, S. Soderland, Oren Etzioni"
6a5608e6fee3ecc65361525906b0d092ad9952bb,https://www.semanticscholar.org/paper/6a5608e6fee3ecc65361525906b0d092ad9952bb,Learning from Context or Names? An Empirical Study on Neural Relation Extraction,"Neural models have achieved remarkable success on relation extraction (RE) benchmarks. However, there is no clear understanding which type of information affects existing RE models to make decisions and how to further improve the performance of these models. To this end, we empirically study the effect of two main information sources in text: textual context and entity mentions (names). We find that (i) while context is the main source to support the predictions, RE models also heavily rely on the information from entity mentions, most of which is type information, and (ii) existing datasets may leak shallow heuristics via entity mentions and thus contribute to the high performance on RE benchmarks. Based on the analyses, we propose an entity-masked contrastive pre-training framework for RE to gain a deeper understanding on both textual context and type information while avoiding rote memorization of entities or use of superficial cues in mentions. We carry out extensive experiments to support our views, and show that our framework can improve the effectiveness and robustness of neural models in different RE scenarios. All the code and datasets are released at this https URL.",2020.0,"Hao Peng, Tianyu Gao, Xu Han, Yankai Lin, Peng Li, Zhiyuan Liu, Maosong Sun, Jie Zhou"
912829c0f865874f24e591a94c557f9c734ed0d7,https://www.semanticscholar.org/paper/912829c0f865874f24e591a94c557f9c734ed0d7,Open Information Extraction,"The research on information extraction is being developed into open information extraction,i.e.extracting open categories of entities,relations and events from open domain text resources.The methods used are also transferred from pure statistical machine learning model based on human annotated corpora into statistical learning model incorporated with knowledge bases mined from large-scaled and heterogeneous Web resources.This paper firstly reviews the history of the researches on information extraction,then detailedly introduces the task definitions,difficulties,typical methods,evaluations,performances and the challenges of three main open domain information extraction tasks,i.e.entity extraction,entity disambiguation and relation extraction.Finally,based on our researches on this field,we analyze and discuss the development directions of open information extraction research and its applications in large-scaled knowledge engineering,question answering,etc.",2011.0,Cai Li
68a025280dedcd53388e94c7069483641f7b09b9,https://www.semanticscholar.org/paper/68a025280dedcd53388e94c7069483641f7b09b9,Information extraction from pathology reports in a hospital setting,"As more health data becomes available, information extraction aims to make an impact on the workflows of hospitals and care centers. One of the targeted areas is the management of pathology reports, which are employed for cancer diagnosis and staging. In this work we integrate text mining tools in the workflow of the Royal Melbourne Hospital, to extract information from pathology reports with minimal expert intervention. Our framework relies on coarse-grained annotation (at document level), making it highly portable. Our evaluation shows that the kind of language used in these reports makes it feasible to extract information with high precision and recall, by means of state-of-the-art classification methods, and feature engineering.",2011.0,"David Martínez, Yue Li"
a3ead91f63ca244359cfb4dbb9a22ab149d577eb,https://www.semanticscholar.org/paper/a3ead91f63ca244359cfb4dbb9a22ab149d577eb,"Information Extraction Using Web Usage Mining, Web Scrapping and Semantic Annotation","Extracting useful information from the web is the most significant issue of concern for the realization of semantic web. This may be achieved by several ways among which Web Usage Mining, Web Scrapping and Semantic Annotation plays an important role. Web mining enables to find out the relevant results from the web and is used to extract meaningful information from the discovery patterns kept back in the servers. Web usage mining is a type of web mining which mines the information of access routes/manners of users visiting the web sites. Web scraping, another technique, is a process of extracting useful information from HTML pages which may be implemented using a scripting language known as Prolog Server Pages(PSP) based on Prolog. Third, Semantic annotation is a technique which makes it possible to add semantics and a formal structure to unstructured textual documents, an important aspect in semantic information extraction which may be performed by a tool known as KIM(Knowledge Information Management). In this paper, we revisit, explore and discuss some information extraction techniques on web like web usage mining, web scrapping and semantic annotation for a better or efficient information extraction on the web illustrated with examples.",2011.0,"S. K. Malik, S. Rizvi"
6b450f61c46c1bf4309a430e3902ba28ef28738d,https://www.semanticscholar.org/paper/6b450f61c46c1bf4309a430e3902ba28ef28738d,SystemT: A Declarative Information Extraction System,"Emerging text-intensive enterprise applications such as social analytics and semantic search pose new challenges of scalability and usability to Information Extraction (IE) systems. This paper presents SystemT, a declarative IE system that addresses these challenges and has been deployed in a wide range of enterprise applications. SystemT facilitates the development of high quality complex annotators by providing a highly expressive language and an advanced development environment. It also includes a cost-based optimizer and a high-performance, flexible runtime with minimum memory footprint. We present SystemT as a useful resource that is freely available, and as an opportunity to promote research in building scalable and usable IE systems.",2011.0,"Yunyao Li, Frederick Reiss, Laura Chiticariu"
7a12502ba5b9686e37b0ec9d86a2dc7f4b7022ac,https://www.semanticscholar.org/paper/7a12502ba5b9686e37b0ec9d86a2dc7f4b7022ac,Web-scale information extraction with vertex,"Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale.",2011.0,"P. Gulhane, Amit Madaan, Rupesh R. Mehta, J. Ramamirtham, R. Rastogi, Sandeepkumar Satpal, Srinivasan H. Sengamedu, Ashwin Tengli, Charu Tiwari"
4c078c7c69f429c4adc260c228a3e6bf99b74f62,https://www.semanticscholar.org/paper/4c078c7c69f429c4adc260c228a3e6bf99b74f62,Customizing an Information Extraction System to a New Domain,"We introduce several ideas that improve the performance of supervised information extraction systems with a pipeline architecture, when they are customized for new domains. We show that: (a) a combination of a sequence tagger with a rule-based approach for entity mention extraction yields better performance for both entity and relation mention extraction; (b) improving the identification of syntactic heads of entity mentions helps relation extraction; and (c) a deterministic inference engine captures some of the joint domain structure, even when introduced as a postprocessing step to a pipeline system. All in all, our contributions yield a 20% relative increase in F1 score in a domain significantly different from the domains used during the development of our information extraction system.",2011.0,"M. Surdeanu, David McClosky, Mason Smith, Andrey Gusev, Christopher D. Manning"
0066950f64280142f2704647376064e84b5ee40d,https://www.semanticscholar.org/paper/0066950f64280142f2704647376064e84b5ee40d,Filtering and clustering relations for unsupervised information extraction in open domain,"Information Extraction has recently been extended to new areas by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing a priori unknown relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus. We propose in this paper an approach for the filtering of such candidate relations based on heuristics and machine learning models. More precisely, we show that the best model for achieving this task is a Conditional Random Field model according to evaluations performed on a manually annotated corpus of about one thousand relations. We also tackle the problem of identifying semantically similar relations by clustering large sets of them. Such clustering is achieved by combining a classical clustering algorithm and a method for the efficient identification of highly similar relation pairs. Finally, we evaluate the impact of our filtering of relations on this semantic clustering with both internal measures and external measures. Results show that the filtering procedure doubles the recall of the clustering while keeping the same precision.",2011.0,"Wei Wang, Romaric Besançon, Olivier Ferret, Brigitte Grau"
42dacd3ec1f8714083f1285fc43008b78a5035bb,https://www.semanticscholar.org/paper/42dacd3ec1f8714083f1285fc43008b78a5035bb,Joint inference for cross-document information extraction,"Previous information extraction (IE) systems are typically organized as a pipeline architecture of separated stages which make independent local decisions. When the data grows beyond some certain size, the extracted facts become inter-dependent and thus we can take advantage of information redundancy to conduct reasoning across documents and improve the performance of IE. We describe a joint inference approach based on information network structure to conduct cross-fact reasoning with an integer linear programming framework. Without using any additional labeled data this new method obtained 13.7%-24.4% user browsing cost reduction over a state-of-the-art IE system which extracts various types of facts independently.",2011.0,"Qi Li, S. Anzaroot, Wen-Pin Lin, Xiang Li, Heng Ji"
41e936981f5a2d55bfec0143e9a15e23ad96436b,https://www.semanticscholar.org/paper/41e936981f5a2d55bfec0143e9a15e23ad96436b,Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping,"Information extraction systems usually require two dictionaries: a semantic lexicon and a dictionary of extraction patterns for the domain. We present a multilevel bootstrapping algorithm that generates both the semantic lexicon and extraction patterns simultaneously. As input, our technique requires only unannotated training texts and a handful of seed words for a category. We use a mutual bootstrapping technique to alternately select the best extraction pattern for the category and bootstrap its extractions into the semantic lexicon, which is the basis for selecting the next extraction pattern. To make this approach more robust, we add a second level of bootstrapping (metabootstrapping) that retains only the most reliable lexicon entries produced by mutual bootstrapping and then restarts the process. We evaluated this multilevel bootstrapping technique on a collection of corporate web pages and a corpus of terrorism news articles. The algorithm produced high-quality dictionaries for several semantic categories.",1999.0,"E. Riloff, R. Jones"
686d9ee744fa013cc21cdd86acd864c936e9e456,https://www.semanticscholar.org/paper/686d9ee744fa013cc21cdd86acd864c936e9e456,Large language models are few-shot clinical information extractors,"A long-running goal of the clinical NLP community is the extraction of important variables trapped in clinical notes. However, roadblocks have included dataset shift from the general domain and a lack of public clinical corpora and annotations. In this work, we show that large language models, such as InstructGPT (Ouyang et al., 2022), perform well at zero- and few-shot information extraction from clinical text despite not being trained specifically for the clinical domain. Whereas text classification and generation performance have already been studied extensively in such models, here we additionally demonstrate how to leverage them to tackle a diverse set of NLP tasks which require more structured outputs, including span identification, token-level sequence classification, and relation extraction. Further, due to the dearth of available data to evaluate these systems, we introduce new datasets for benchmarking few-shot clinical information extraction based on a manual re-annotation of the CASI dataset (Moon et al., 2014) for new tasks. On the clinical extraction tasks we studied, the GPT-3 systems significantly outperform existing zero- and few-shot baselines.",2022.0,"Monica Agrawal, S. Hegselmann, Hunter Lang, Yoon Kim, D. Sontag"
06b36e744dca445863c9f9aefe76aea95ba95999,https://www.semanticscholar.org/paper/06b36e744dca445863c9f9aefe76aea95ba95999,Enhancing Clinical Concept Extraction with Contextual Embedding,"OBJECTIVE
Neural network-based representations (""embeddings"") have dramatically advanced natural language processing (NLP) tasks, including clinical NLP tasks such as concept extraction. Recently, however, more advanced embedding methods and representations (eg, ELMo, BERT) have further pushed the state of the art in NLP, yet there are no common best practices for how to integrate these representations into clinical tasks. The purpose of this study, then, is to explore the space of possible options in utilizing these new models for clinical concept extraction, including comparing these to traditional word embedding methods (word2vec, GloVe, fastText).


MATERIALS AND METHODS
Both off-the-shelf, open-domain embeddings and pretrained clinical embeddings from MIMIC-III (Medical Information Mart for Intensive Care III) are evaluated. We explore a battery of embedding methods consisting of traditional word embeddings and contextual embeddings and compare these on 4 concept extraction corpora: i2b2 2010, i2b2 2012, SemEval 2014, and SemEval 2015. We also analyze the impact of the pretraining time of a large language model like ELMo or BERT on the extraction performance. Last, we present an intuitive way to understand the semantic information encoded by contextual embeddings.


RESULTS
Contextual embeddings pretrained on a large clinical corpus achieves new state-of-the-art performances across all concept extraction tasks. The best-performing model outperforms all state-of-the-art methods with respective F1-measures of 90.25, 93.18 (partial), 80.74, and 81.65.


CONCLUSIONS
We demonstrate the potential of contextual embeddings through the state-of-the-art performance these methods achieve on clinical concept extraction. Additionally, we demonstrate that contextual embeddings encode valuable semantic information not accounted for in traditional word representations.",2019.0,"Yuqi Si, Jingqi Wang, Hua Xu, Kirk Roberts"
a3086e3346c2f0f8cc9d65b91055eb6e61dc11cb,https://www.semanticscholar.org/paper/a3086e3346c2f0f8cc9d65b91055eb6e61dc11cb,Social-based traffic information extraction and classification,"Social networks such as Twitter and Facebook are popular, personal, and real-time in nature. We found that there exists a significant number of traffic information such as traffic congestion, incidents, and weather in Twitter. However, an algorithm is needed to extract and classify the traffic information before publishing (re-tweeting) and becoming useful for others. Traffic information was extracted from Twitter using syntactic analysis and then further classified into two categories: point and link. This method can classify 2,942 traffic tweets into the point category with 76.85% accuracy and classify 331 traffic tweets into the link category with 93.23% accuracy. Our system can report traffic information real-time.",2011.0,"Napong Wanichayapong, Wasawat Pruthipunyaskul, W. Pattara-Atikom, Pimwadee Chaovalit"
f2786451bc0a02835e6890a76f71e04e75ce55e8,https://www.semanticscholar.org/paper/f2786451bc0a02835e6890a76f71e04e75ce55e8,Ontology-based information extraction: An introduction and a survey of current approaches,"Information extraction (IE) aims to retrieve certain types of information from natural language text by processing them automatically. For example, an IE system might retrieve information about geopolitical indicators of countries from a set of web pages while ignoring other types of information. Ontology-based information extraction (OBIE) has recently emerged as a subfield of information extraction. Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the IE process. Because of the use of ontologies, this field is related to knowledge representation and has the potential to assist the development of the Semantic Web. In this paper, we provide an introduction to ontology-based information extraction and review the details of different OBIE systems developed so far. We attempt to identify a common architecture among these systems and classify them based on different factors, which leads to a better understanding on their operation. We also discuss the implementation details of these systems including the tools used by them and the metrics used to measure their performance. In addition, we attempt to identify the possible future directions for this field.",2010.0,"Daya C. Wimalasuriya, D. Dou"
15218d9c029cbb903ae7c729b2c644c24994c201,https://www.semanticscholar.org/paper/15218d9c029cbb903ae7c729b2c644c24994c201,Normalized (pointwise) mutual information in collocation extraction,". In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.",2009.0,G. Bouma
20033971c228a982513cd2f8fe454288de481d5f,https://www.semanticscholar.org/paper/20033971c228a982513cd2f8fe454288de481d5f,Contrastive Triple Extraction with Generative Transformer,"Triple extraction is an essential task in information extraction for natural language processing and knowledge graph construction. In this paper, we revisit the end-to-end triple extraction task for sequence generation. Since generative triple extraction may struggle to capture long-term dependencies and generate unfaithful triples, we introduce a novel model, contrastive triple extraction with a generative transformer. Specifically, we introduce a single shared transformer module for encoder-decoder-based generation. To generate faithful results, we propose a novel triplet contrastive training object. Moreover, we introduce two mechanisms to further improve model performance (i.e., batch-wise dynamic attention-masking and triple-wise calibration). Experimental results on three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves better performance than that of baselines.",2020.0,"Hongbin Ye, Ningyu Zhang, Shumin Deng, Mosha Chen, Chuanqi Tan, Fei Huang, Huajun Chen"
e930e75fc639dacc04a0dae5d929b31367bda122,https://www.semanticscholar.org/paper/e930e75fc639dacc04a0dae5d929b31367bda122,Enabling information extraction by inference of regular expressions from sample entities,"Regular expressions are the dominant technique to extract business relevant entities (e.g., invoice numbers or product names) from text data (e.g., invoices), since these entity types often follow a strict underlying syntactical pattern. However, the manual construction of regular expressions that guarantee a high recall and precision is a tedious manual task and requires expert knowledge. In this paper, we propose an approach that automatically infers regular expressions from a set of (positive) sample entities, which in turn can be derived either from enterprise databases (e.g., a product catalog) or annotated documents (e.g., historical invoices). The main innovation of our approach is that it learns effective regular expressions that can be easily interpreted and modified by a user. The effectiveness is obtained by a novel method that weights dependent entity features of different granularity (i.e. on character and token level) against each other and selects the most suitable ones to form a regular expression.",2011.0,"Falk Brauer, Robert Rieger, Adrian Mocan, Wojciech M. Barczynski"
df3196bd230e23a86fbf8254daa78c7be9df028a,https://www.semanticscholar.org/paper/df3196bd230e23a86fbf8254daa78c7be9df028a,Coupled semi-supervised learning for information extraction,"We consider the problem of semi-supervised learning to extract categories (e.g., academic fields, athletes) and relations (e.g., PlaysSport(athlete, sport)) from web pages, starting with a handful of labeled training examples of each category or relation, plus hundreds of millions of unlabeled web documents. Semi-supervised training using only a few labeled examples is typically unreliable because the learning task is underconstrained. This paper pursues the thesis that much greater accuracy can be achieved by further constraining the learning task, by coupling the semi-supervised training of many extractors for different categories and relations. We characterize several ways in which the training of category and relation extractors can be coupled, and present experimental results demonstrating significantly improved accuracy as a result.",2010.0,"Andrew Carlson, J. Betteridge, Richard C. Wang, Estevam Hruschka, Tom Michael Mitchell"
17ff074ae373cb95575ce8ca579f35b4088f02d6,https://www.semanticscholar.org/paper/17ff074ae373cb95575ce8ca579f35b4088f02d6,SystemT: An Algebraic Approach to Declarative Information Extraction,"As information extraction (IE) becomes more central to enterprise applications, rule-based IE engines have become increasingly important. In this paper, we describe SystemT, a rule-based IE system whose basic design removes the expressivity and performance limitations of current systems based on cascading grammars. SystemT uses a declarative rule language, AQL, and an optimizer that generates high-performance algebraic execution plans for AQL rules. We compare SystemT's approach against cascading grammars, both theoretically and with a thorough experimental evaluation. Our results show that SystemT can deliver result quality comparable to the state-of-the-art and an order of magnitude higher annotation throughput.",2010.0,"Laura Chiticariu, R. Krishnamurthy, Yunyao Li, S. Raghavan, Frederick Reiss, Shivakumar Vaithyanathan"
8d8536e5f330d9aca2e9ad608563b7a2cacd5f91,https://www.semanticscholar.org/paper/8d8536e5f330d9aca2e9ad608563b7a2cacd5f91,Temporal Information Extraction,"
 
 Research on information extraction (IE) seeks to distill relational tuples from natural language text, such as the contents of the WWW. Most IE work has focussed on identifying static facts, encoding them as binary relations. This is unfortunate, because the vast majority of facts are fluents, only holding true during an interval of time. It is less helpful to extract PresidentOf(Bill-Clinton, USA) without the temporal scope 1/20/93 — 1/20/01. This paper presents TIE, a novel, information-extraction system, which distills facts from text while inducing as much temporal information as possible. In addition to recognizing temporal relations between times and events, TIE performs global inference, enforcing transitivity to bound the start and ending times for each event. We introduce the notion of temporal entropy as a way to evaluate the performance of temporal IE systems and present experiments showing that TIE outperforms three alternative approaches.
 
",2010.0,"Xiao Ling, Daniel S. Weld"
786d1ca11bfd95d8c845ef4a90b2d55092c2fb2f,https://www.semanticscholar.org/paper/786d1ca11bfd95d8c845ef4a90b2d55092c2fb2f,Coupled Multi-Layer Attentions for Co-Extraction of Aspect and Opinion Terms,"
 
 The task of aspect and opinion terms co-extraction aims to explicitly extract aspect terms describing features of an entity and opinion terms expressing emotions from user-generated texts. To achieve this task, one effective approach is to exploit relations between aspect terms and opinion terms by parsing syntactic structure for each sentence. However, this approach requires expensive effort for parsing and highly depends on the quality of the parsing results. In this paper, we offer a novel deep learning model, named coupled multi-layer attentions. The proposed model provides an end-to-end solution and does not require any parsers or other linguistic resources for preprocessing. Specifically, the proposed model is a multi-layer attention network, where each layer consists of a couple of attentions with tensor operators. One attention is for extracting aspect terms, while the other is for extracting opinion terms. They are learned interactively to dually propagate information between aspect terms and opinion terms. Through multiple layers, the model can further exploit indirect relations between terms for more precise information extraction. Experimental results on three benchmark datasets in SemEval Challenge 2014 and 2015 show that our model achieves state-of-the-art performances compared with several baselines.
 
",2017.0,"Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, Xiaokui Xiao"
389a994a5dff386650fb10328933fa7c58ce332f,https://www.semanticscholar.org/paper/389a994a5dff386650fb10328933fa7c58ce332f,Enterprise information extraction: recent developments and open challenges,"Information extraction (IE) - the problem of extracting structured information from unstructured text - has become an increasingly important topic in recent years. A SIGMOD 2006 tutorial [3] outlined challenges and opportunities for the database community to advance the state of the art in information extraction, and posed the following grand challenge: ""Can we build a System R for information extraction? Our tutorial gives an overview of progress the database community has made towards meeting this challenge. In particular, we start by discussing design requirements in building an enterprise IE system. We then survey recent technological advances towards addressing these requirements, broadly categorized as: (1) Languages for specifying extraction programs in a declarative way, thus allowing database-style performance optimizations; (2) Infrastructure needed to ensure scalability, and (3) Development support for enterprise IE systems. Finally, we outline several open challenges and opportunities for the database community to further advance the state of the art in enterprise IE systems. The tutorial is intended for students and researchers interested in information extraction and its applications, and assumes no prior knowledge of the area.",2010.0,"Laura Chiticariu, Yunyao Li, S. Raghavan, Frederick Reiss"
697b17b9db921618282dc299715a6f3b2d3074f1,https://www.semanticscholar.org/paper/697b17b9db921618282dc299715a6f3b2d3074f1,Automatic rule refinement for information extraction,"Rule-based information extraction from text is increasingly being used to populate databases and to support structured queries on unstructured text. Specification of suitable information extraction rules requires considerable skill and standard practice is to refine rules iteratively, with substantial effort. In this paper, we show that techniques developed in the context of data provenance, to determine the lineage of a tuple in a database, can be leveraged to assist in rule refinement. Specifically, given a set of extraction rules and correct and incorrect extracted data, we have developed a technique to suggest a ranked list of rule modifications that an expert rule specifier can consider. We implemented our technique in the SystemT information extraction system developed at IBM Research -- Almaden and experimentally demonstrate its effectiveness.",2010.0,"B. Liu, Laura Chiticariu, Vivian Chu, H. V. Jagadish, Frederick Reiss"
cf53bda1fbaf6a70da4dd541423caab72267cf47,https://www.semanticscholar.org/paper/cf53bda1fbaf6a70da4dd541423caab72267cf47,Semantic Role Labeling for Open Information Extraction,"Open Information Extraction is a recent paradigm for machine reading from arbitrary text. In contrast to existing techniques, which have used only shallow syntactic features, we investigate the use of semantic features (semantic roles) for the task of Open IE. We compare TextRunner (Banko et al., 2007), a state of the art open extractor, with our novel extractor SRL-IE, which is based on UIUC's SRL system (Punyakanok et al., 2008). We find that SRL-IE is robust to noisy heterogeneous Web data and outperforms TextRunner on extraction quality. On the other hand, TextRunner performs over 2 orders of magnitude faster and achieves good precision in high locality and high redundancy extractions. These observations enable the construction of hybrid extractors that output higher quality results than TextRunner and similar quality as SRL-IE in much less time.",2010.0,"Janara Christensen, Mausam, S. Soderland, Oren Etzioni"
2533e6ef309f625f891d506d013af47715f8b95f,https://www.semanticscholar.org/paper/2533e6ef309f625f891d506d013af47715f8b95f,Components for information extraction: ontology-based information extractors and generic platforms,"Information Extraction (IE) has existed as a field for several decades and has produced some impressive systems in the recent past. Despite its success, widespread usage and commercialization remain elusive goals for this field. We identify the lack of effective mechanisms for reuse as one major reason behind this situation. Here, we mean not only the reuse of the same IE technique in different situations but also the reuse of information related to the application of IE techniques (e.g., features used for classification). We have developed a comprehensive component-based approach for information extraction that promotes reuse to address this situation. We designed this approach starting from our previous work on the use of multiple ontologies in information extraction [24]. The key ideas of our approach are ""information extractors,"" which are components of an IE system that make extractions with respect to particular components of an ontology and ""platforms for IE,"" which are domain and corpus independent implementations of IE techniques. A case study has shown that this component-based approach can be successfully applied in practical situations.",2010.0,"Daya C. Wimalasuriya, D. Dou"
38daea2f58b6d96a630f77bdfd38645817d6093d,https://www.semanticscholar.org/paper/38daea2f58b6d96a630f77bdfd38645817d6093d,Adapting Open Information Extraction to Domain-Specific Relations,"Information extraction (IE) can identify a set of relations from free text to support question answering (QA). Until recently, IE systems were domain-specific and needed a combination of manual engineering and supervised learning to adapt to each target domain. A new paradigm, Open IE operates on large text corpora without any manual tagging of relations, and indeed without any pre-specified relations. Due to its open-domain and open-relation nature, Open IE is purely textual and is unable to relate the surface forms to an ontology, if known in advance. We explore the steps needed to adapt Open IE to a domain-specific ontology and demonstrate our approach of mapping domain-independent tuples to an ontology using domains from DARPA’s Machine Reading Project. Our system achieves precision over 0.90 from as few as 8 training examples for an NFL-scoring domain.",2010.0,"S. Soderland, Brendan Roof, Bo Qin, Shi Xu, Mausam, Oren Etzioni"
135ace829b6ad2ec9db040d8e5fd137034e83665,https://www.semanticscholar.org/paper/135ace829b6ad2ec9db040d8e5fd137034e83665,Semi-Markov Conditional Random Fields for Information Extraction,"We describe semi-Markov conditional random fields (semi-CRFs), a conditionally trained version of semi-Markov chains. Intuitively, a semi-CRF on an input sequence x outputs a ""segmentation"" of x, in which labels are assigned to segments (i.e., subsequences) of x rather than to individual elements xi of x. Importantly, features for semi-CRFs can measure properties of segments, and transitions within a segment can be non-Markovian. In spite of this additional power, exact learning and inference algorithms for semi-CRFs are polynomial-time—often only a small constant factor slower than conventional CRFs. In experiments on five named entity recognition problems, semi-CRFs generally outperform conventional CRFs.",2004.0,"Sunita Sarawagi, William W. Cohen"
f6c115bb2a9a0e7e4ce24d848fdd7334d0e731e4,https://www.semanticscholar.org/paper/f6c115bb2a9a0e7e4ce24d848fdd7334d0e731e4,Probabilistic declarative information extraction,"Unstructured text represents a large fraction of the world's data. It often contains snippets of structured information (e.g., people's names and zip codes). Information Extraction (IE) techniques identify such structured information in text. In recent years, database research has pursued IE on two fronts: declarative languages and systems for managing IE tasks, and probabilistic databases for querying the output of IE. In this paper, we make the first step to merge these two directions, without loss of statistical robustness, by implementing a state-of-the-art statistical IE model - Conditional Random Fields (CRF) - in the setting of a Probabilistic Database that treats statistical models as first-class data objects. We show that the Viterbi algorithm for CRF inference can be specified declaratively in recursive SQL. We also show the performance benefits relative to a standalone open-source Viterbi implementation. This work opens up the optimization opportunities for queries involving both inference and relational operators over IE models.",2010.0,"D. Wang, E. Michelakis, M. Franklin, Minos N. Garofalakis, J. Hellerstein"
b0d555a9ea67285fccd2ef8d887907bcc811f67a,https://www.semanticscholar.org/paper/b0d555a9ea67285fccd2ef8d887907bcc811f67a,PositionRank: An Unsupervised Approach to Keyphrase Extraction from Scholarly Documents,"The large and growing amounts of online scholarly data present both challenges and opportunities to enhance knowledge discovery. One such challenge is to automatically extract a small set of keyphrases from a document that can accurately describe the document’s content and can facilitate fast information processing. In this paper, we propose PositionRank, an unsupervised model for keyphrase extraction from scholarly documents that incorporates information from all positions of a word’s occurrences into a biased PageRank. Our model obtains remarkable improvements in performance over PageRank models that do not take into account word positions as well as over strong baselines for this task. Specifically, on several datasets of research papers, PositionRank achieves improvements as high as 29.09%.",2017.0,"C. Florescu, Cornelia Caragea"
b8da823ad81e3b8e5b80d82f86129fdb1d9132e7,https://www.semanticscholar.org/paper/b8da823ad81e3b8e5b80d82f86129fdb1d9132e7,Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions,"
 
 Distant supervision for relation extraction is an efficient method to scale relation extraction to very large corpora which contains thousands of relations. However, the existing approaches have flaws on selecting valid instances and lack of background knowledge about the entities. In this paper, we propose a sentence-level attention model to select the valid instances, which makes full use of the supervision information from knowledge bases. And we extract entity descriptions from Freebase and Wikipedia pages to supplement background knowledge for our task. The background knowledge not only provides more information for predicting relations, but also brings better entity representations for the attention module. We conduct three experiments on a widely used dataset and the experimental results show that our approach outperforms all the baseline systems significantly.
 
",2017.0,"Guoliang Ji, Kang Liu, Shizhu He, Jun Zhao"
451b61b390b86ae5629a21461d4c619ea34046e0,https://www.semanticscholar.org/paper/451b61b390b86ae5629a21461d4c619ea34046e0,PERFORMANCE MEASURES FOR INFORMATION EXTRACTION,"While precision and recall have served the information extraction community well as two separate measures of system performance, we show that the F -measure, the weighted harmonic mean of precision and recall, exhibits certain undesirable behaviors. To overcome these limitations, we define an error measure, the slot error rate, which combines the different types of error directly, without having to resort to precision and recall as preliminary measures. The slot error rate is analogous to the word error rate that is used for measuring speech recognition performance; it is intended to be a measure of the cost to the user for the system to make the different types of errors.",2007.0,"J. Makhoul, F. Kubala, R. Schwartz, R. Weischedel"
07a79d667c99d7d6ed1fdfd97695f8f6d3c2eab2,https://www.semanticscholar.org/paper/07a79d667c99d7d6ed1fdfd97695f8f6d3c2eab2,Feature extraction for hyperspectral image classification: a review,"ABSTRACT Hyperspectral image sensors capture surface reflectance over a range of wavelengths. The fine spectral information is recorded in terms of hundreds of bands. Hyperspectral image classification has observed a great interest among researchers in remote sensing community. High dimensionality provides rich spectral information for the classification process. But due to dense sampling, some of the bands may contain redundant information. Sometimes, spectral information alone may not be sufficient to obtain desired accuracy of results. Therefore, often spatial and spectral information is integrated for better accuracy. However, unlike spectral information, the spatial information is not directly available with the image. Additional efforts are needed to extract spatial information. Feature extraction is an important step in a classification framework. It has following major objectives: redundancy reduction, dimensionality reduction (usually but not always), enhancing discriminative information, and modelling of spatial features. The spectral feature extraction process transforms the original data to a new space of a different dimension, enhancing the class separability without significant loss of information. Various mathematical techniques are applied for modelling spatial features based on pixel spatial neighbourhood relations. In this paper, a review of the major feature extraction techniques is presented. Experimental results are presented for two benchmark hyperspectral images to evaluate different feature extraction techniques for various parameters.",2020.0,"B. Kumar, O. Dikshit, Ashwani Gupta, M. Singh"
8f684080d2b81d3178d681d6917cb077c082a9e1,https://www.semanticscholar.org/paper/8f684080d2b81d3178d681d6917cb077c082a9e1,Fast and Accurate Single Image Super-Resolution via Information Distillation Network,"Recently, deep convolutional neural networks (CNNs) have been demonstrated remarkable progress on single image super-resolution. However, as the depth and width of the networks increase, CNN-based super-resolution methods have been faced with the challenges of computational complexity and memory consumption in practice. In order to solve the above questions, we propose a deep but compact convolutional network to directly reconstruct the high resolution image from the original low resolution image. In general, the proposed model consists of three parts, which are feature extraction block, stacked information distillation blocks and reconstruction block respectively. By combining an enhancement unit with a compression unit into a distillation block, the local long and short-path features can be effectively extracted. Specifically, the proposed enhancement unit mixes together two different types of features and the compression unit distills more useful information for the sequential blocks. In addition, the proposed network has the advantage of fast execution due to the comparatively few numbers of filters per layer and the use of group convolution. Experimental results demonstrate that the proposed method is superior to the state-of-the-art methods, especially in terms of time performance. Code is available at https://github.com/Zheng222/IDN-Caffe.",2018.0,"Zheng Hui, Xiumei Wang, Xinbo Gao"
358ca777d9992bdc06fdcc1940e3b18a8da68878,https://www.semanticscholar.org/paper/358ca777d9992bdc06fdcc1940e3b18a8da68878,Inter-sentence Relation Extraction with Document-level Graph Convolutional Neural Network,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction.",2019.0,"Sunil Kumar Sahu, Fenia Christopoulou, Makoto Miwa, S. Ananiadou"
e7df04b716c71e2029742e03ef7fa1e7fe4fd977,https://www.semanticscholar.org/paper/e7df04b716c71e2029742e03ef7fa1e7fe4fd977,High accuracy information extraction of medication information from clinical notes: 2009 i2b2 medication extraction challenge,"OBJECTIVE
Medication information comprises a most valuable source of data in clinical records. This paper describes use of a cascade of machine learners that automatically extract medication information from clinical records.


DESIGN
Authors developed a novel supervised learning model that incorporates two machine learning algorithms and several rule-based engines.


MEASUREMENTS
Evaluation of each step included precision, recall and F-measure metrics. The final outputs of the system were scored using the i2b2 workshop evaluation metrics, including strict and relaxed matching with a gold standard.


RESULTS
Evaluation results showed greater than 90% accuracy on five out of seven entities in the name entity recognition task, and an F-measure greater than 95% on the relationship classification task. The strict micro averaged F-measure for the system output achieved best submitted performance of the competition, at 85.65%.


LIMITATIONS
Clinical staff will only use practical processing systems if they have confidence in their reliability. Authors estimate that an acceptable accuracy for a such a working system should be approximately 95%. This leaves a significant performance gap of 5 to 10% from the current processing capabilities.


CONCLUSION
A multistage method with mixed computational strategies using a combination of rule-based classifiers and statistical classifiers seems to provide a near-optimal strategy for automated extraction of medication information from clinical records.",2010.0,"J. Patrick, Min Li"
623233996bd67ff80910c61a9ba771069772c0dc,https://www.semanticscholar.org/paper/623233996bd67ff80910c61a9ba771069772c0dc,An Information-Extraction System for Urdu---A Resource-Poor Language,"There has been an increase in the amount of multilingual text on the Internet due to the proliferation of news sources and blogs. The Urdu language, in particular, has experienced explosive growth on the Web. Text mining for information discovery, which includes tasks such as identifying topics, relationships and events, and sentiment analysis, requires sophisticated natural language processing (NLP). NLP systems begin with modules such as word segmentation, part-of-speech tagging, and morphological analysis and progress to modules such as shallow parsing and named entity tagging. While there have been considerable advances in developing such comprehensive NLP systems for English, the work for Urdu is still in its infancy. The tasks of interest in Urdu NLP includes analyzing data sources such as blogs and comments to news articles to provide insight into social and human behavior. All of this requires a robust NLP system. The objective of this work is to develop an NLP infrastructure for Urdu that is customizable and capable of providing basic analysis on which more advanced information extraction tools can be built. This system assimilates resources from various online sources to facilitate improved named entity tagging and Urdu-to-English transliteration. The annotated data required to train the learning models used here is acquired by standardizing the currently limited resources available for Urdu. Techniques such as bootstrap learning and resource sharing from a syntactically similar language, Hindi, are explored to augment the available annotated Urdu data. Each of the new Urdu text processing modules has been integrated into a general text-mining platform. The evaluations performed demonstrate that the accuracies have either met or exceeded the state of the art.",2010.0,"S. Mukund, R. Srihari, Erik Peterson"
338a891907dce447da9a0fa2f27221bd35164163,https://www.semanticscholar.org/paper/338a891907dce447da9a0fa2f27221bd35164163,Mining the peanut gallery: opinion extraction and semantic classification of product reviews,"The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.",2003.0,"Kushal Dave, S. Lawrence, David M. Pennock"
378f0a62471ef232c7730d8a67717afa5104ab21,https://www.semanticscholar.org/paper/378f0a62471ef232c7730d8a67717afa5104ab21,Heterogeneous Information Network Embedding for Recommendation,"Due to the flexibility in modelling data heterogeneity, heterogeneous information network (HIN) has been adopted to characterize complex and heterogeneous auxiliary data in recommender systems, called HIN based recommendation. It is challenging to develop effective methods for HIN based recommendation in both extraction and exploitation of the information from HINs. Most of HIN based recommendation methods rely on path based similarity, which cannot fully mine latent structure features of users and items. In this paper, we propose a novel heterogeneous network embedding based approach for HIN based recommendation, called HERec. To embed HINs, we design a meta-path based random walk strategy to generate meaningful node sequences for network embedding. The learned node embeddings are first transformed by a set of fusion functions, and subsequently integrated into an extended matrix factorization (MF) model. The extended MF model together with fusion functions are jointly optimized for the rating prediction task. Extensive experiments on three real-world datasets demonstrate the effectiveness of the HERec model. Moreover, we show the capability of the HERec model for the cold-start problem, and reveal that the transformed embedding information from HINs can improve the recommendation performance.",2017.0,"C. Shi, Binbin Hu, Wayne Xin Zhao, Philip S. Yu"
05c728cb38f06dccd587beb9b0399157eebfbce8,https://www.semanticscholar.org/paper/05c728cb38f06dccd587beb9b0399157eebfbce8,Unsupervised Keyphrase Extraction with Multipartite Graphs,We propose an unsupervised keyphrase extraction model that encodes topical information within a multipartite graph structure. Our model represents keyphrase candidates and topics in a single graph and exploits their mutually reinforcing relationship to improve candidate ranking. We further introduce a novel mechanism to incorporate keyphrase selection preferences into the model. Experiments conducted on three widely used datasets show significant improvements over state-of-the-art graph-based models.,2018.0,Florian Boudin
1158fab369af68a867a5f8ba6260ef56c97aafc3,https://www.semanticscholar.org/paper/1158fab369af68a867a5f8ba6260ef56c97aafc3,A review of keyphrase extraction,"Keyphrase extraction is a textual information processing task concerned with the automatic extraction of representative and characteristic phrases from a document that express all the key aspects of its content. Keyphrases constitute a succinct conceptual summary of a document, which is very useful in digital information management systems for semantic indexing, faceted search, document clustering and classification. This article introduces keyphrase extraction, provides a well‐structured review of the existing work, offers interesting insights on the different evaluation approaches, highlights open issues and presents a comparative experimental study of popular unsupervised techniques on five datasets.",2019.0,"Eirini Papagiannopoulou, Grigorios Tsoumakas"
5e077918a979537f27b9a0820672f23a434a98ee,https://www.semanticscholar.org/paper/5e077918a979537f27b9a0820672f23a434a98ee,"Mayo clinical Text Analysis and Knowledge Extraction System (cTAKES): architecture, component evaluation and applications","We aim to build and evaluate an open-source natural language processing system for information extraction from electronic medical record clinical free-text. We describe and evaluate our system, the clinical Text Analysis and Knowledge Extraction System (cTAKES), released open-source at http://www.ohnlp.org. The cTAKES builds on existing open-source technologies-the Unstructured Information Management Architecture framework and OpenNLP natural language processing toolkit. Its components, specifically trained for the clinical domain, create rich linguistic and semantic annotations. Performance of individual components: sentence boundary detector accuracy=0.949; tokenizer accuracy=0.949; part-of-speech tagger accuracy=0.936; shallow parser F-score=0.924; named entity recognizer and system-level evaluation F-score=0.715 for exact and 0.824 for overlapping spans, and accuracy for concept mapping, negation, and status attributes for exact and overlapping spans of 0.957, 0.943, 0.859, and 0.580, 0.939, and 0.839, respectively. Overall performance is discussed against five applications. The cTAKES annotations are the foundation for methods and modules for higher-level semantic processing of clinical free-text.",2010.0,"G. Savova, James J. Masanz, P. Ogren, Jiaping Zheng, S. Sohn, Karin Kipper Schuler, C. Chute"
c345a4e5e224523a29c9d5c4774927eb5eea8132,https://www.semanticscholar.org/paper/c345a4e5e224523a29c9d5c4774927eb5eea8132,Information Extraction,"Information Extraction (IE) techniques aim to extract the names of entities and objects from text and to identify the roles that they play in event descriptions. IE systems generally focus on a specific domain or topic, searching only for information that is relevant to a user's interests. In this chapter, we first give historical background on information extraction and discuss several kinds of information extraction tasks that have emerged in recent years. Next, we outline the series of steps that are involved in creating a typical information extraction system, which can be encoded as a cascaded finite-state transducer. Along the way, we present examples to illustrate what each step does. Finally, we present an overview of different learning-based methods for information extraction, including supervised learning approaches, weakly supervised and bootstrapping techniques, and discourse-oriented approaches. Information extraction (IE) is the process of scanning text for information relevant to some interest, including extracting entities, relations, and, most challenging, events–or who did what to whom when and where. It requires deeper analysis than key word searches, but its aims fall short of the very hard and long-term problem of text understanding, where we seek to capture all the information in a text, along with the speaker's or writer's intention.",2010.0,Jerry R. Hobbs
f7372b28ccfbd4464c247af2d1058d0922532584,https://www.semanticscholar.org/paper/f7372b28ccfbd4464c247af2d1058d0922532584,SystemT: a system for declarative information extraction,"As applications within and outside the enterprise encounter increasing volumes of unstructured data, there has been renewed interest in the area of information extraction (IE) -- the discipline concerned with extracting structured information from unstructured text. Classical IE techniques developed by the NLP community were based on cascading grammars and regular expressions. However, due to the inherent limitations of grammarbased extraction, these techniques are unable to: (i) scale to large data sets, and (ii) support the expressivity requirements of complex information tasks. At the IBM Almaden Research Center, we are developing SystemT, an IE system that addresses these limitations by adopting an algebraic approach. By leveraging well-understood database concepts such as declarative queries and costbased optimization, SystemT enables scalable execution of complex information extraction tasks. In this paper, we motivate the SystemT approach to information extraction. We describe our extraction algebra and demonstrate the effectiveness of our optimization techniques in providing orders of magnitude reduction in the running time of complex extraction tasks.",2009.0,"R. Krishnamurthy, Yunyao Li, S. Raghavan, Frederick Reiss, Shivakumar Vaithyanathan, Huaiyu Zhu"
c558e2b5dcab8d89f957f3045a9bbd43fd6a28ed,https://www.semanticscholar.org/paper/c558e2b5dcab8d89f957f3045a9bbd43fd6a28ed,Joint Extraction of Events and Entities within a Document Context,"Events and entities are closely related; entities are often actors or participants in events and events without entities are uncommon. The interpretation of events and entities is highly contextually dependent. Existing work in information extraction typically models events separately from entities, and performs inference at the sentence level, ignoring the rest of the document. In this paper, we propose a novel approach that models the dependencies among variables of events, entities, and their relations, and performs joint inference of these variables across a document. The goal is to enable access to document-level contextual information and facilitate context-aware predictions. We demonstrate that our approach substantially outperforms the state-of-the-art methods for event extraction as well as a strong baseline for entity extraction.",2016.0,"Bishan Yang, Tom Michael Mitchell"
179deeb7efba5aa1443724088f030a542dcdfad0,https://www.semanticscholar.org/paper/179deeb7efba5aa1443724088f030a542dcdfad0,A Study on Information Extraction of Water Body with the Modified Normalized Difference Water Index （MNDWI）,"A modified normalized difference water index(MNDWI) has been proposed in this paper based on the normalized difference water index(NDWI) of Mcfeeters (1966), which uses MIR(TM5) instead of NIR(TM4) to construct the MNDWI. The MNDWI has been tested in the ocean, lake and river areas with the background of built-up lands and/or vegetated lands, and with both clean and polluted water bodies using Landsat TM/ETM+ imagery. This reveals that the MNDWI can significantly enhance the water information, especially in the area mainly with built-up land as background. The MNDWI can depress the built-up land information effectively while highlighting water information, and accurately extract the water body information from the study areas. While the enhanced water information using the NDWI always has been mixed with built-up land noise and the area of a water body extracted based on the index is thus overestimated. Therefore, the NDWI is not suitable for enhancing and extracting water information in built-up land-dominated areas. Furthermore, the MNDWI can reveal subtle features of water more efficiently than the NDWI or other visible spectral bands do due largely to its wider dynamic data range. The application of the MNDWI in the Xiamen image has achieved an excellent result. The MNDWI image successfully reveals significant non-point pollution of the water surrounding the Xiamen Island due to agricultural activities. In addition, taking the advantage of the ratio computation, the MNDWI can remove shadow noise from water information without using sophisticated procedures, which is otherwise difficult to be removed.",2005.0,Han-qiu Xu
8ff2052500f5f020411fdaeb1c77be482c014689,https://www.semanticscholar.org/paper/8ff2052500f5f020411fdaeb1c77be482c014689,A Unified Model of Phrasal and Sentential Evidence for Information Extraction,"Information Extraction (IE) systems that extract role fillers for events typically look at the local context surrounding a phrase when deciding whether to extract it. Often, however, role fillers occur in clauses that are not directly linked to an event word. We present a new model for event extraction that jointly considers both the local context around a phrase along with the wider sentential context in a probabilistic framework. Our approach uses a sentential event recognizer and a plausible role-filler recognizer that is conditioned on event sentences. We evaluate our system on two IE data sets and show that our model performs well in comparison to existing IE systems that rely on local phrasal context.",2009.0,"Siddharth Patwardhan, E. Riloff"
34ef34bb95b640a3d61b10f3bf02baec603e53d2,https://www.semanticscholar.org/paper/34ef34bb95b640a3d61b10f3bf02baec603e53d2,A quality-aware optimizer for information extraction,"A large amount of structured information is buried in unstructured text. Information extraction systems can extract structured relations from the documents and enable sophisticated, SQL-like queries over unstructured text. Information extraction systems are not perfect and their output has imperfect precision and recall (i.e., contains spurious tuples and misses good tuples). Typically, an extraction system has a set of parameters that can be used as “knobs” to tune the system to be either precision- or recall-oriented. Furthermore, the choice of documents processed by the extraction system also affects the quality of the extracted relation. So far, estimating the output quality of an information extraction task has been an ad hoc procedure, based mainly on heuristics. In this article, we show how to use Receiver Operating Characteristic (ROC) curves to estimate the extraction quality in a statistically robust way and show how to use ROC analysis to select the extraction parameters in a principled manner. Furthermore, we present analytic models that reveal how different document retrieval strategies affect the quality of the extracted relation. Finally, we present our maximum likelihood approach for estimating, on the fly, the parameters required by our analytic models to predict the runtime and the output quality of each execution plan. Our experimental evaluation demonstrates that our optimization approach predicts accurately the output quality and selects the fastest execution plan that satisfies the output quality restrictions.",2009.0,"Alpa Jain, Panagiotis G. Ipeirotis"
eb8192d6439aedb59bb57104e90b1f82382b249a,https://www.semanticscholar.org/paper/eb8192d6439aedb59bb57104e90b1f82382b249a,Amplifying community content creation with mixed initiative information extraction,"Although existing work has explored both information extraction and community content creation, most research has focused on them in isolation. In contrast, we see the greatest leverage in the synergistic pairing of these methods as two interlocking feedback cycles. This paper explores the potential synergy promised if these cycles can be made to accelerate each other by exploiting the same edits to advance both community content creation and learning-based information extraction. We examine our proposed synergy in the context of Wikipedia infoboxes and the Kylin information extraction system. After developing and refining a set of interfaces to present the verification of Kylin extractions as a non primary task in the context of Wikipedia articles, we develop an innovative use of Web search advertising services to study people engaged in some other primary task. We demonstrate our proposed synergy by analyzing our deployment from two complementary perspectives: (1) we show we accelerate community content creation by using Kylin's information extraction to significantly increase the likelihood that a person visiting a Wikipedia article as a part of some other primary task will spontaneously choose to help improve the article's infobox, and (2) we show we accelerate information extraction by using contributions collected from people interacting with our designs to significantly improve Kylin's extraction performance.",2009.0,"Raphael Hoffmann, Saleema Amershi, Kayur Patel, Fei Wu, J. Fogarty, Daniel S. Weld"
7c78bf7f88ae684f241551567170433a6cb8639a,https://www.semanticscholar.org/paper/7c78bf7f88ae684f241551567170433a6cb8639a,Information extraction challenges in managing unstructured data,"Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community.",2009.0,"A. Doan, J. Naughton, R. Ramakrishnan, A. Baid, Xiaoyong Chai, Fei Chen, Ting Chen, E. Chu, Pedro DeRose, Byron J. Gao, Chaitanya S. Gokhale, Jiansheng Huang, Warren Shen, Ba-Quy Vuong"
51910432af30042f69afa60542a87c056b129c52,https://www.semanticscholar.org/paper/51910432af30042f69afa60542a87c056b129c52,Using multiple ontologies in information extraction,"Ontology-Based Information Extraction (OBIE) has recently emerged as a subfield of Information Extraction (IE). Here, ontologies - which provide formal and explicit specifications of conceptualizations - play a crucial role in the information extraction process. Several OBIE systems have been implemented previously but all of them use a single ontology although multiple ontologies have been designed for many domains. We have studied the theoretical basis for using multiple ontologies in information extraction and have developed information extraction systems that use them. These systems investigate the two major scenarios for having multiple ontologies for the same domain: specializing in sub-domains and providing different perspectives. The domain of universities has been used for the former scenario through a corpus collected from university websites. For the latter, the domain of terrorist attacks and a corpus used by a previous Message Understanding Conference (MUC) have been used. The results from these two case studies indicate that using multiple ontologies in information extraction has led to a clear improvement in performance measures.",2009.0,"Daya C. Wimalasuriya, D. Dou"
9b194c186134a794b0c282e4cdd51d7217d9b0e3,https://www.semanticscholar.org/paper/9b194c186134a794b0c282e4cdd51d7217d9b0e3,Uncertainty management in rule-based information extraction systems,"Rule-based information extraction is a process by which structured objects are extracted from text based on user-defined rules. The compositional nature of rule-based information extraction also allows rules to be expressed over previously extracted objects. Such extraction is inherently uncertain, due to the varying precision associated with the rules used in a specific extraction task. Quantifying this uncertainty is crucial for querying the extracted objects in probabilistic databases, and for improving the recall of extraction tasks that use compositional rules. In this paper, we provide a probabilistic framework for handling the uncertainty in rule-based information extraction. Specifically, for each extraction task, we build a parametric exponential model of uncertainty that captures the interaction between the different rules, as well as the compositional nature of the rules; the exponential form of our model follows from maximum-entropy considerations. We also give model-decomposition techniques that make the learning algorithms scalable to large numbers of rules and constraints. Experiments over multiple real-world extraction tasks confirm that our approach yields accurate probability estimates with only a small performance overhead. Moreover, our framework supports incremental pay-as-you-go improvements in the accuracy of probability estimates as new rules, data, or constraints are added.",2009.0,"E. Michelakis, R. Krishnamurthy, P. Haas, Shivakumar Vaithyanathan"
59c57e64af5615a7190269dcfb6c0cc2675367b3,https://www.semanticscholar.org/paper/59c57e64af5615a7190269dcfb6c0cc2675367b3,PolyUHK: A Robust Information Extraction System for Web PersonalNames,"Personal information extraction is an important component of advanced information retrieval. There are two problems needed to be solved in this practical task: personal name ambiguity and extraction of personal information for a specific person. For personal name ambiguity, which is a very common phenomenon in the fast growing Web resource, we propose a robust system which extracts features with a totally unsupervised approach from resources beyond the given Web corpus. The experiments show that these broad features not only can improve performances, but also increase the robustness of a disambiguation system. For personal information extraction, a rule-based information extraction system is introduced, which is able to re-use current well-developed tools effectively and identify the properties of Web data. The experiments show that the system can achieve state-of-the-art performances, especially the high precision.",2009.0,"Ying Chen, Sophia Yat-Mei Lee, Chu-Ren Huang"
ff5f10af72bc93d641651e64b988ee3db8b530f9,https://www.semanticscholar.org/paper/ff5f10af72bc93d641651e64b988ee3db8b530f9,TextMarker : A Tool for Rule-Based Information Extraction,This paper presents TEXTMARKER– a powerful toolkit for rule-based information extraction. TEXTMARKER is based on UIMA and provides versatile information processing and advanced extraction techniques. We thoroughly describe the system and its capabilities for human-like information processing and rapid prototyping of information extraction applications.,2009.0,"Peter Kluegl, M. Atzmueller, F. Puppe"
6a4ecdf39d4597ff22cc5f8710cdfffb67d1ae51,https://www.semanticscholar.org/paper/6a4ecdf39d4597ff22cc5f8710cdfffb67d1ae51,Static Relations: a Piece in the Biomedical Information Extraction Puzzle,"We propose a static relation extraction task to complement biomedical information extraction approaches. We argue that static relations such as part-whole are implicitly involved in many common extraction settings, define a task setting making them explicit, and discuss their integration into previously proposed tasks and extraction methods. We further identify a specific static relation extraction task motivated by the BioNLP'09 shared task on event extraction, introduce an annotated corpus for the task, and demonstrate the feasibility of the task by experiments showing that the defined relations can be reliably extracted. The task setting and corpus can serve to support several forms of domain information extraction.",2009.0,"S. Pyysalo, Tomoko Ohta, Jin-Dong Kim, Junichi Tsujii"
5008604ec9cd2f54b97e7a7dcfa6c9118900cd61,https://www.semanticscholar.org/paper/5008604ec9cd2f54b97e7a7dcfa6c9118900cd61,Patent claim decomposition for improved information extraction,"In several application domains research in natural language processing and information extraction has spawned valuable tools that support humans in structuring, aggregating and managing large amounts of information available as text. Patent claims, although subject to a number of rigid constraints and therefore forced into foreseeable structures, are written in a language even good parsing algorithms tend to fail miserably at. This is primarily caused by long and complex sentences that are a concatenation of a multitude of descriptive elements. We present an approach to split patent claims into several parts in order to improve parsing performance for further automatic processing.",2009.0,"Peter Parapatics, Michael Dittenbach"
83cce573fa388662b0c2b9394db60bc8d6f0dfe7,https://www.semanticscholar.org/paper/83cce573fa388662b0c2b9394db60bc8d6f0dfe7,Information Extraction in Images and Video : A Survey,"Text data present in images and video contain useful information for automatic annotation, indexing, and structuring of images. Extraction of this information involves detection, localization, tracking, extraction, enhancement, and recognition of the text from a given image. However, variations of text due to differences in size, style, orientation, and alignment, as well as low image contrast and complex background make the problem of automatic text extraction extremely challenging. While comprehensive surveys of related problems such as face detection, document analysis, and image & video indexing can be found, the problem of text information extraction is not well surveyed. A large number of techniques have been proposed to address this problem, and the purpose of this paper is to classify and review these algorithms, discuss benchmark data and performance evaluation, and to point out promising directions for future research.",2003.0,"K. Jung, K. Kim, Anil K. Jain"
16bd1fbe3694173eda4ad4338a85f8288d19bf02,https://www.semanticscholar.org/paper/16bd1fbe3694173eda4ad4338a85f8288d19bf02,Relational Learning of Pattern-Match Rules for Information Extraction,"Information extraction is a form of shallow text processing that locates a specified set of relevant items in a natural-language document. Systems for this task require significant domain-specific knowledge and are time-consuming and difficult to build by hand, making them a good application for machine learning. We present a system, RAPIER, that uses pairs of sample documents and filled templates to induce pattern-match rules that directly extract fillers for the slots in the template. RAPIER employs a bottom-up learning algorithm which incorporates techniques from several inductive logic programming systems and acquires unbounded patterns that include constraints on the words, part-of-speech tags, and semantic classes present in the filler and the surrounding text. We present encouraging experimental results on two domains.",1999.0,"Mary Elaine Califf, R. Mooney"
b114a7e586f1b7281efe4501dd90a60f127eb1f7,https://www.semanticscholar.org/paper/b114a7e586f1b7281efe4501dd90a60f127eb1f7,SOFIE: a self-organizing framework for information extraction,"This paper presents SOFIE, a system for automated ontology extension. SOFIE can parse natural language documents, extract ontological facts from them and link the facts into an ontology. SOFIE uses logical reasoning on the existing knowledge and on the new knowledge in order to disambiguate words to their most probable meaning, to reason on the meaning of text patterns and to take into account world knowledge axioms. This allows SOFIE to check the plausibility of hypotheses and to avoid inconsistencies with the ontology. The framework of SOFIE unites the paradigms of pattern matching, word sense disambiguation and ontological reasoning in one unified model. Our experiments show that SOFIE delivers high-quality output, even from unstructured Internet documents.",2009.0,"Fabian M. Suchanek, Mauro Sozio, G. Weikum"
1b38f6b1c2a3fbf0718f6319f7bbcdfa0e239568,https://www.semanticscholar.org/paper/1b38f6b1c2a3fbf0718f6319f7bbcdfa0e239568,Regular Expression Learning for Information Extraction,"Regular expressions have served as the dominant workhorse of practical information extraction for several years. However, there has been little work on reducing the manual effort involved in building high-quality, complex regular expressions for information extraction tasks. In this paper, we propose ReLIE, a novel transformation-based algorithm for learning such complex regular expressions. We evaluate the performance of our algorithm on multiple datasets and compare it against the CRF algorithm. We show that ReLIE, in addition to being an order of magnitude faster, outperforms CRF under conditions of limited training data and cross-domain data. Finally, we show how the accuracy of CRF can be improved by using features extracted by ReLIE.",2008.0,"Yunyao Li, R. Krishnamurthy, S. Raghavan, Shivakumar Vaithyanathan, H. V. Jagadish"
c4bd8e9c69c5270905a1c0eb1a87fca7a92944cb,https://www.semanticscholar.org/paper/c4bd8e9c69c5270905a1c0eb1a87fca7a92944cb,Information extraction from Wikipedia: moving down the long tail,"Not only is Wikipedia a comprehensive source of quality information, it has several kinds of internal structure (e.g., relational summaries known as infoboxes), which enable self-supervised information extraction. While previous efforts at extraction from Wikipedia achieve high precision and recall on well-populated classes of articles, they fail in a larger number of cases, largely because incomplete articles and infrequent use of infoboxes lead to insufficient training data. This paper presents three novel techniques for increasing recall from Wikipedia's long tail of sparse classes: (1) shrinkage over an automatically-learned subsumption taxonomy, (2) a retraining technique for improving the training data, and (3) supplementing results by extracting from the broader Web. Our experiments compare design variations and show that, used in concert, these techniques increase recall by a factor of 1.76 to 8.71 while maintaining or increasing precision.",2008.0,"Fei Wu, Raphael Hoffmann, Daniel S. Weld"
35a38da0b7079bb61ef29bb27915ada2c4665e0a,https://www.semanticscholar.org/paper/35a38da0b7079bb61ef29bb27915ada2c4665e0a,Visual Web Information Extraction with Lixto,"We present new techniques for supervised wrapper generation and automated web information extraction, and a system called Lixto implementing these techniques. Our system can generate wrappers which translate relevant pieces of HTML pages into XML. Lixto, of which a working prototype has been implemented, assists the user to semi-automatically create wrapper programs by providing a fully visual and interactive user interface. In this convenient user-interface very expressive extraction programs can be created. Internally, this functionality is reected by the new logicbased declarative language Elog. Users never have to deal with Elog and even familiarity with HTML is not required. Lixto can be used to create an \XML-Companion"" for an HTML web page with changing content, containing the continually updated XML translation of the relevant information.",2001.0,"Robert Baumgartner, Sergio Flesca, G. Gottlob"
a6a95c5a75b496c396b60c79be96c40ed2e230a1,https://www.semanticscholar.org/paper/a6a95c5a75b496c396b60c79be96c40ed2e230a1,Efficient Information Extraction over Evolving Text Data,"Most current information extraction (IE) approaches have considered only static text corpora, over which we typically have to apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and to keep extracted information up to date, we often must apply IE repeatedly, to consecutive corpus snapshots. We describe Cyclex, an approach that efficiently executes such repeated IE, by recycling previous IE efforts. Specifically, given a current corpus snapshot U, Cyclex identifies text portions of U that also appear in the previous corpus snapshot V. Since Cyclex has already executed IE over V, it can now recycle the IE results of these parts, by combining these results with the results of executing IE over the remaining parts of U, to produce the complete IE results for U. Realizing Cyclex raises many challenges, including modeling information extractors, exploring the trade-off between runtime and completeness in identifying overlapping text, and making informed, cost-based decisions between redoing IE from scratch and recycling previous IE results. We describe initial solutions to these challenges, and experiments over two real-world data sets that demonstrate the utility of our approach.",2008.0,"Fei Chen, A. Doan, Jun Yang, R. Ramakrishnan"
ea285ecff6985d78449ce7433b26a3d7307e0558,https://www.semanticscholar.org/paper/ea285ecff6985d78449ce7433b26a3d7307e0558,An Algebraic Approach to Rule-Based Information Extraction,"Traditional approaches to rule-based information extraction (IE) have primarily been based on regular expression grammars. However, these grammar-based systems have difficulty scaling to large data sets and large numbers of rules. Inspired by traditional database research, we propose an algebraic approach to rule-based IE that addresses these scalability issues through query optimization. The operators of our algebra are motivated by our experience in building several rule-based extraction programs over diverse data sets. We present the operators of our algebra and propose several optimization strategies motivated by the text-specific characteristics of our operators. Finally we validate the potential benefits of our approach by extensive experiments over real-world blog data.",2008.0,"Frederick Reiss, S. Raghavan, R. Krishnamurthy, Huaiyu Zhu, Shivakumar Vaithyanathan"
15a8f2b51b5a8db739818c63a2d54c18abda92b0,https://www.semanticscholar.org/paper/15a8f2b51b5a8db739818c63a2d54c18abda92b0,Rule-Based Information Extraction for Structured Data Acquisition using TextMarker,"Information extraction is concerned with the location of specific items in (unstructured) textual documents, e.g., being applied for the acquisition of structured data. Then, the acquired data can be applied for mining methods requiring structured input data, in contrast to other text mining methods that utilize a bag-of-words approach. This paper presents a semi-automatic approach for structured data acquisition using a rule-based information extraction system. We propose a semi-automatic process model that includes the TEXTMARKER system for information extraction and data acquisition from textual documents. TEXTMARKER applies simple rules for extracting blocks from a given (semi-structured) document, which can be further analyzed using domain-specific rules. Thus, both low-level and higher-level information extraction is supported. We demonstrate the applicability and benefit of the approach with two case studies of two realworld applications.",2008.0,"M. Atzmüller, Peter Klügl, F. Puppe"
c98aa28295c3ee30e73e261285ee9b557aa742f6,https://www.semanticscholar.org/paper/c98aa28295c3ee30e73e261285ee9b557aa742f6,Information Extraction: Methodologies and Applications,"This chapter is concerned with the methodologies and applications of information extraction. Information is hidden in the large volume of web pages and thus it is necessary to extract useful information from the web content, called Information Extraction. In information extraction, given a sequence of instances, we identify and pull out a sub-sequence of the input that represents information we are interested in. In the past years, there was a rapid expansion of activities in the information extraction area. Many methods have been proposed for automating the process of extraction. However, due to the heterogeneity and the lack of structure of Web data, automated discovery of targeted or unexpected knowledge information still presents many challenging research problems. In this chapter, we will investigate the problems of information extraction and survey existing methodologies for solving these problems. Several real-world applications of information extraction will be introduced. Emerging challenges will be discussed.",2008.0,"Jie Tang, MingCai Hong, Duo Zhang, Juan-Zi Li"
ab0d31b96154e844c8a928216c39e0468e55213d,https://www.semanticscholar.org/paper/ab0d31b96154e844c8a928216c39e0468e55213d,Punctuating speech for information extraction,"This paper studies the effect of automatic sentence boundary detection and comma prediction on entity and relation extraction in speech. We show that punctuating the machine generated transcript according to maximum F-measure of period and comma annotation results in suboptimal information extraction. Precisely, period and comma decision thresholds can be chosen in order to improve the entity value score and the relation value score by 4% relative. Error analysis shows that preventing noun-phrase splitting by generating longer sentences and fewer commas can be harmful for IE performance. Indeed, it seems that missed punctuation allows syntactic parsers to merge noun-phrases and prevent the extraction of correct information.",2008.0,"Benoit Favre, R. Grishman, D. Hillard, Heng Ji, Dilek Z. Hakkani-Tür, Mari Ostendorf"
db8f75747cf3587fd2182f9f5ae468ec54788bb1,https://www.semanticscholar.org/paper/db8f75747cf3587fd2182f9f5ae468ec54788bb1,Toward best-effort information extraction,"Current approaches to develop information extraction (IE) programs have largely focused on producing precise IE results. As such, they suffer from three major limitations. First, it is often difficult to execute partially specified IE programs and obtain meaningful results, thereby producing a long ""debug loop"". Second, it often takes a long time before we can obtain the first meaningful result (by finishing and running a precise IE program), thereby rendering these approaches impractical for time-sensitive IE applications. Finally, by trying to write precise IE programs we may also waste a significant amount of effort, because an approximate result -- one that can be produced quickly -- may already be satisfactory in many IE settings.
 To address these limitations, we propose iFlex, an IE approach that relaxes the precise IE requirement to enable best-effort IE. In iFlex, a developer U uses a declarative language to quickly write an initial approximate IE program P with a possible-worlds semantics. Then iFlex evaluates P using an approximate query processor to quickly extract an approximate result. Next, U examines the result, and further refines P if necessary, to obtain increasingly more precise results. To refine P, U can enlist a next-effort assistant, which suggests refinements based on the data and the current version of P. Extensive experiments on real-world domains demonstrate the utility of the iFlex approach.",2008.0,"Warren Shen, Pedro DeRose, R. McCann, A. Doan, R. Ramakrishnan"
1d7140f7491966341261c14c3584a435412a3f18,https://www.semanticscholar.org/paper/1d7140f7491966341261c14c3584a435412a3f18,Airborne laser data for stand delineation and information extraction,"A literature review of new publications in the field of 3D data for forest applications shows that the application of airborne laser scanner data (ALS) is in the focus of research today due to its great potential for practical applications. While there is a lot of research carried out to derive forest management parameters based on laser metrics deduced from a single tree assessment or a statistical area based assessment, the delineation of stand or sub‐stand units derived from laser metrics itself is a rather new approach. In order to describe stand characteristics statistical grid cell approaches or single tree approaches have been developed. The LIDAR based segmentation of stand or sub‐stand units is rarely documented. This article provides information on enhanced processes to delineate stand or sub‐stand units and to extract different forest information based on airborne laser derived parameters. For the stand delineation an automatic process was developed which provides a stand or sub‐stand unit delineation which is according to the first results sufficiently uniform within stands and sufficiently different in species, age class, height class, structure and composition between stands in order to be distinguishable from adjacent areas. With a combined method the stand boundaries as they are established by the mapping units today, as well as sub‐stand units which have in common physical characteristics indicating the same management disposition, were assessed. Finally a first validation of the forest stand unit delineation is provided, indicating the high potential of ALS data for separating stand units.",2009.0,"B. Koch, C. Straub, M. Dees, Yang Wang, H. Weinacker"
0b2a199151d67aa1770950ef1b60ecb917027260,https://www.semanticscholar.org/paper/0b2a199151d67aa1770950ef1b60ecb917027260,Information Extraction Agents for Service-Oriented Architecture Using Web Service Systems: A Framework,"In some business domains, such as financial investment services, maintaining current information is a serious challenge, as markets evolve by the hour. This creates a demand for continually-updated information systems to support business decision-makers. Many sources of domain information are online documents containing unstructured text. For the information encoded in these natural language texts to be usable by systems, it must be extracted and reshaped into forms which those systems can recognize. The goal of this research is to investigate ways to exploit information-rich, online source texts in order to automatically update information in web service (WS) systems implemented in service-oriented architecture (SOA) environments. We propose a general framework for service-oriented web service systems that incorporates information extraction (IE) components capable of handling unstructured text. Since IE systems are sophisticated and difficult to implement, performance and economic considerations suggest that firms with special expertise should perform these tasks and sell the services to service customers. Our prototype information extraction system, which can be embedded in a WS system, is described to illustrate the framework.",2008.0,"Sumali J. Conlon, Jason G. Hale, Susan Lukose, J. Strong"
23c91e18520f04f470d9820908bf9c2088f7b8a4,https://www.semanticscholar.org/paper/23c91e18520f04f470d9820908bf9c2088f7b8a4,Single Channel Target Speaker Extraction and Recognition with Speaker Beam,"This paper addresses the problem of single channel speech recognition of a target speaker in a mixture of speech signals. We propose to exploit auxiliary speaker information provided by an adaptation utterance from the target speaker to extract and recognize only that speaker. Using such auxiliary information, we can build a speaker extraction neural network (NN) that is independent of the number of sources in the mixture, and that can track speakers across different utterances, which are two challenging issues occurring with conventional approaches for speech recognition of mixtures. We call such an informed speaker extraction scheme “SpeakerBeam”. SpeakerBeam exploits a recently developed context adaptive deep NN (CADNN) that allows tracking speech from a target speaker using a speaker adaptation layer, whose parameters are adjusted depending on auxiliary features representing the target speaker characteristics. SpeakerBeam was previously investigated for speaker extraction using a microphone array. In this paper, we demonstrate that it is also efficient for single channel speaker extraction. The speaker adaptation layer can be employed either to build a speaker adaptive acoustic model that recognizes only the target speaker or a mask-based speaker extraction network that extracts the target speech from the speech mixture signal prior to recognition. We also show that the latter speaker extraction network can be optimized jointly with an acoustic model to further improve ASR performance.",2018.0,"Marc Delcroix, Kateřina Žmolíková, K. Kinoshita, A. Ogawa, T. Nakatani"
d0d60c2bfca32b3e6d8feee815a3a132a2f47499,https://www.semanticscholar.org/paper/d0d60c2bfca32b3e6d8feee815a3a132a2f47499,Practical extraction of disaster-relevant information from social media,"During times of disasters online users generate a significant amount of data, some of which are extremely valuable for relief efforts. In this paper, we study the nature of social-media content generated during two different natural disasters. We also train a model based on conditional random fields to extract valuable information from such content. We evaluate our techniques over our two datasets through a set of carefully designed experiments. We also test our methods over a non-disaster dataset to show that our extraction model is useful for extracting information from socially-generated content in general.",2013.0,"Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz, P. Meier"
335e8b19ea50d3af6fcefe6f8421e2c9c8936f3f,https://www.semanticscholar.org/paper/335e8b19ea50d3af6fcefe6f8421e2c9c8936f3f,Toward information extraction: identifying protein names from biological papers.,"To solve the mystery of the life phenomenon, we must clarify when genes are expressed and how their products interact with each other. But since the amount of continuously updated knowledge on these interactions is massive and is only available in the form of published articles, an intelligent information extraction (IE) system is needed. To extract these information directly from articles, the system must firstly identify the material names. However, medical and biological documents often include proper nouns newly made by the authors, and conventional methods based on domain specific dictionaries cannot detect such unknown words or coinages. In this study, we propose a new method of extracting material names, PROPER, using surface clue on character strings. It extracts material names in the sentence with 94.70% precision and 98.84% recall, regardless of whether it is already known or newly defined.",1998.0,"Ken Fukuda, A. Tamura, Tatsuhiko Tsunoda, T. Takagi"
ad10607412e196279bf056d13c8b6fa27fd61f26,https://www.semanticscholar.org/paper/ad10607412e196279bf056d13c8b6fa27fd61f26,TextRunner: Open Information Extraction on the Web,"Traditional information extraction systems have focused on satisfying precise, narrow, pre-specified requests from small, homogeneous corpora. In contrast, the TextRunner system demonstrates a new kind of information extraction, called Open Information Extraction (OIE), in which the system makes a single, data-driven pass over the entire corpus and extracts a large set of relational tuples, without requiring any human input. (Banko et al., 2007) TextRunner is a fully-implemented, highly scalable example of OIE. TextRunner's extractions are indexed, allowing a fast query mechanism.",2007.0,"A. Yates, Michele Banko, M. Broadhead, Michael J. Cafarella, Oren Etzioni, S. Soderland"
c22e82083b78e4022d5b9d2cd4c2a2f42f73151a,https://www.semanticscholar.org/paper/c22e82083b78e4022d5b9d2cd4c2a2f42f73151a,IEPAD: information extraction based on pattern discovery,"research in information extraction (IE) regards the generation of wrappers that can extract particular information from semi- structured Web documents. Similar to compiler generation, the extractor is actually a driver program, which is accompanied with the generated extraction rule. Previous work in this field aims to learn extraction rules from users' training example. In this paper, we propose IEPAD, a system that automatically discovers extraction rules from Web pages. The system can automatically identify record boundary by repeated pattern mining and multiple sequence alignment. The discovery of repeated patterns are realized through a data structure call PAT trees. Additionally, repeated patterns are further extended by pattern alignment to comprehend all record instances. This new track to IE involves no human effort and content-dependent heuristics. Experimental results show that the constructed extraction rules can achieve 97 percent extraction over fourteen popular search engines.",2001.0,"Chia-Hui Chang, S. Lui"
93e7e3d02d16f72cd071f69ccb302859ec441d3f,https://www.semanticscholar.org/paper/93e7e3d02d16f72cd071f69ccb302859ec441d3f,"Combining Lexical, Syntactic, and Semantic Features with Maximum Entropy Models for Information Extraction","Extracting semantic relationships between entities is challenging because of a paucity of annotated data and the errors induced by entity detection modules. We employ Maximum Entropy models to combine diverse lexical, syntactic and semantic features derived from the text. Our system obtained competitive results in the Automatic Content Extraction (ACE) evaluation. Here we present our general approach and describe our ACE results.",2004.0,N. Kambhatla
9afc2cb61a4da0aa29fa9f40889d21ff67157c7a,https://www.semanticscholar.org/paper/9afc2cb61a4da0aa29fa9f40889d21ff67157c7a,Joint Inference in Information Extraction,"The goal of information extraction is to extract database records from text or semi-structured sources. Traditionally, information extraction proceeds by first segmenting each candidate record separately, and then merging records that refer to the same entities. While computationally efficient, this approach is suboptimal, because it ignores the fact that segmenting one candidate record can help to segment similar ones. For example, resolving a well-segmented field with a less-clear one can disambiguate the latter's boundaries. In this paper we propose a joint approach to information extraction, where segmentation of all records and entity resolution are performed together in a single integrated inference process. While a number of previous authors have taken steps in this direction (eg., Pasula et al. (2003), Wellner et al. (2004)), to our knowledge this is the first fully joint approach. In experiments on the CiteSeer and Cora citation matching datasets, joint inference improved accuracy, and our approach outperformed previous ones. Further, by using Markov logic and the existing algorithms for it, our solution consisted mainly of writing the appropriate logical formulas, and required much less engineering than previous ones.",2007.0,"Hoifung Poon, Pedro M. Domingos"
0832f8d85916e08cf4d8431fc694fdcf97dd234c,https://www.semanticscholar.org/paper/0832f8d85916e08cf4d8431fc694fdcf97dd234c,Declarative Information Extraction Using Datalog with Embedded Extraction Predicates,"In this paper we argue that developing information extraction (IE) programs using Datalog with embedded procedural extraction predicates is a good way to proceed. First, compared to current ad-hoc composition using, e.g., Perl or C++, Datalog provides a cleaner and more powerful way to compose small extraction modules into larger programs. Thus, writing IE programs this way retains and enhances the important advantages of current approaches: programs are easy to understand, debug, and modify. Second, once we write IE programs in this framework, we can apply query optimization techniques to them. This gives programs that, when run over a variety of data sets, are more efficient than any monolithic program because they are optimized based on the statistics of the data on which they are invoked. We show how optimizing such programs raises challenges specific to text data that cannot be accommodated in the current relational optimization framework, then provide initial solutions. Extensive experiments over real-world data demonstrate that optimization is indeed vital for IE programs and that we can effectively optimize IE programs written in this proposed framework.",2007.0,"Warren Shen, A. Doan, J. Naughton, R. Ramakrishnan"
95c9961c73db64837fd6b8dbda2f0b246fed6812,https://www.semanticscholar.org/paper/95c9961c73db64837fd6b8dbda2f0b246fed6812,Towards domain-independent information extraction from web tables,"Traditionally, information extraction from web tables has focused on small, more or less homogeneous corpora, often based on assumptions about the use of <table> tags. A multitude of different HTML implementations of web tables make these approaches difficult to scale. In this paper, we approach the problem of domain-independent information extraction from web tables by shifting our attention from the tree-based representation of webpages to a variation of the two-dimensional visual box model used by web browsers to display the information on the screen. The there by obtained topological and style information allows us to fill the gap created by missing domain-specific knowledge about content and table templates. We believe that, in a future step, this approach can become the basis for a new way of large-scale knowledge acquisition from the current ""Visual Web.",2007.0,"Wolfgang Gatterbauer, Paul Bohunsky, M. Herzog, Bernhard Krüpl, Bernhard Pollak"
a36835241b44cda9253d86ddaf67f84ffc1d9a89,https://www.semanticscholar.org/paper/a36835241b44cda9253d86ddaf67f84ffc1d9a89,Learning Hidden Markov Model Structure for Information Extraction,"Statistical machine learning techniques, while well proven in fields such as speech recognition, are just beginning to be applied to the information extraction domain. We explore the use of hidden Markov models for information extraction tasks, specifically focusing on how to learn model structure from data and how to make the best use of labeled and unlabeled data. We show that a manually-constructed model that contains multiple states per extraction field outperforms a model with one state per field, and discuss strategies for learning the model structure automatically from data. We also demonstrate that the use of distantly-labeled data to set model parameters provides a significant improvement in extraction accuracy. Our models are applied to the task of extracting important fields from the headers of computer science research papers, and achieve an extraction accuracy of 92.9%.",1999.0,"K. Seymore, Roni Rosenfeld"
84364e5660d0db7fe4654febb1e8aba0399835b5,https://www.semanticscholar.org/paper/84364e5660d0db7fe4654febb1e8aba0399835b5,Preemptive Information Extraction using Unrestricted Relation Discovery,We are trying to extend the boundary of Information Extraction (IE) systems. Existing IE systems require a lot of time and human effort to tune for a new scenario. Preemptive Information Extraction is an attempt to automatically create all feasible IE systems in advance without human intervention. We propose a technique called Unrestricted Relation Discovery that discovers all possible relations from texts and presents them as tables. We present a preliminary system that obtains reasonably good results.,2006.0,"Yusuke Shinyama, S. Sekine"
e83501ba08950912ab2c81d2a974898de2913b85,https://www.semanticscholar.org/paper/e83501ba08950912ab2c81d2a974898de2913b85,Broad-Coverage Sense Disambiguation and Information Extraction with a Supersense Sequence Tagger,"In this paper we approach word sense disambiguation and information extraction as a unified tagging problem. The task consists of annotating text with the tagset defined by the 41 Wordnet supersense classes for nouns and verbs. Since the tagset is directly related to Wordnet synsets, the tagger returns partial word sense disambiguation. Furthermore, since the noun tags include the standard named entity detection classes -- person, location, organization, time, etc. -- the tagger, as a by-product, returns extended named entity information. We cast the problem of supersense tagging as a sequential labeling task and investigate it empirically with a discriminatively-trained Hidden Markov Model. Experimental evaluation on the main sense-annotated datasets available, i.e., Semcor and Senseval, shows considerable improvements over the best known ""first-sense"" baseline.",2006.0,"Massimiliano Ciaramita, Y. Altun"
37c9408d511cbc1122b5b570694eed52b04e9636,https://www.semanticscholar.org/paper/37c9408d511cbc1122b5b570694eed52b04e9636,Active Learning for Natural Language Parsing and Information Extraction,"In natural language acquisition, it is dicult to gather the annotated data needed for supervised learning; however, unannotated data is fairly plentiful. Active learning methods attempt to select for annotation and training only the most informative examples, and therefore are potentially very useful in natural language applications. However, existing results for active learning have only considered standard classication tasks. To reduce annotation eort while maintaining accuracy, we apply active learning to two non-classication tasks in natural language processing: semantic parsing and information extraction. We show that active learning can signicantly reduce the number of annotated examples required to achieve a given level of performance for these complex tasks.",1999.0,"Cynthia A. Thompson, Mary Elaine Califf, R. Mooney"
41a441ce0081c8a829a982dc19c60131bfbde046,https://www.semanticscholar.org/paper/41a441ce0081c8a829a982dc19c60131bfbde046,Mining knowledge from text using information extraction,"An important approach to text mining involves the use of natural-language information extraction. Information extraction (IE) distills structured data or knowledge from unstructured text by identifying references to named entities as well as stated relationships between such entities. IE systems can be used to directly extricate abstract knowledge from a text corpus, or to extract concrete data from a set of documents which can then be further analyzed with traditional data-mining techniques to discover more general patterns. We discuss methods and implemented systems for both of these approaches and summarize results on mining real text corpora of biomedical abstracts, job announcements, and product descriptions. We also discuss challenges that arise when employing current information extraction technology to discover knowledge in text.",2005.0,"R. Mooney, Razvan C. Bunescu"
ef76646912d675c987af5c0fe6bd06f40886ff0d,https://www.semanticscholar.org/paper/ef76646912d675c987af5c0fe6bd06f40886ff0d,Adaptive information extraction,"The growing availability of online textual sources and the potential number of applications of knowledge acquisition from textual data has lead to an increase in Information Extraction (IE) research. Some examples of these applications are the generation of data bases from documents, as well as the acquisition of knowledge useful for emerging technologies like question answering, information integration, and others related to text mining. However, one of the main drawbacks of the application of IE refers to its intrinsic domain dependence. For the sake of reducing the high cost of manually adapting IE applications to new domains, experiments with different Machine Learning (ML) techniques have been carried out by the research community. This survey describes and compares the main approaches to IE and the different ML techniques used to achieve Adaptive IE technology.",2006.0,"J. Turmo, A. Ageno, Neus Català"
fe2b08135fbfe3d3a8846f5b328afb8a0ab1567d,https://www.semanticscholar.org/paper/fe2b08135fbfe3d3a8846f5b328afb8a0ab1567d,KIM – a semantic platform for information extraction and retrieval,"The KIM platform provides a novel Knowledge and Information Management framework and services for automatic semantic annotation, indexing, and retrieval of documents. It provides a mature and semantically enabled infrastructure for scalable and customizable information extraction (IE) as well as annotation and document management, based on GATE.General Architecture for Text Engineering (GATE) (http://gate.ac.uk), leading NLP and IE platform developed at the University of Sheffield. Our understanding is that a system for semantic annotation should be based upon a simple model of real-world entity concepts, complemented with quasi-exhaustive instance knowledge. To ensure efficiency, easy sharing, and reusability of the metadata we introduce an upper-level ontology. Based on the ontology, a large-scale instance base of entity descriptions is maintained. The knowledge resources involved are handled by use of state-of-the-art Semantic Web technology and standards, including RDF(S) repositories, ontology middleware and reasoning. From a technical point of view, the platform allows KIM-based applications to use it for automatic semantic annotation, for content retrieval based on semantic queries, and for semantic repository access. As a framework, KIM also allows various IE modules, semantic repositories and information retrieval engines to be plugged into it. This paper presents the KIM platform, with an emphasis on its architecture, interfaces, front-ends, and other technical issues.",2004.0,"Borislav Popov, A. Kiryakov, Damyan Ognyanoff, D. Manov, A. Kirilov"
436087083293ca8728fb96d2e05c011fff2c7751,https://www.semanticscholar.org/paper/436087083293ca8728fb96d2e05c011fff2c7751,Adaptive Information Extraction from Text by Rule Induction and Generalisation,"(LP)2 is a covering algorithm for adaptive Information Extraction from text (IE). It induces symbolic rules that insert SGML tags into texts by learning from examples found in a user-defined tagged corpus. Training is performed in two steps: initially a set of tagging rules is learned; then additional rules are induced to correct mistakes and imprecision in tagging. Induction is performed by bottom-up generalization of examples in the training corpus. Shallow knowledge about Natural Language Processing (NLP) is used in the generalization process. The algorithm has a considerable success story. From a scientific point of view, experiments report excellent results with respect to the current state of the art on two publicly available corpora. From an application point of view, a successful industrial IE tool has been based on (LP)2. Real world applications have been developed and licenses have been released to external companies for building other applications. This paper presents (LP)2, experimental results and applications, and discusses the role of shallow NLP in rule induction.",2001.0,F. Ciravegna
02c8a0bc8bab9920e6615cfacf1df2ab3f2b1f68,https://www.semanticscholar.org/paper/02c8a0bc8bab9920e6615cfacf1df2ab3f2b1f68,Information Extraction with HMM Structures Learned by Stochastic Optimization,"Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction—the task of populating database slots with corresponding phrases from text documents. A remaining problem, however, is the selection of state-transition structure for the model. This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization. Our algorithm begins with a simple model and then performs hill-climbing in the space of possible structures by splitting states and gauging performance on a validation set. Experimental results show that this technique finds HMM models that almost always out-perform a fixed model, and have superior average performance across tasks.",2000.0,"Dayne Freitag, A. McCallum"
85d0e249aa2ae16d152ac2ada51a04bf2f4dfbf2,https://www.semanticscholar.org/paper/85d0e249aa2ae16d152ac2ada51a04bf2f4dfbf2,A review of polarimetry in the context of synthetic aperture radar: concepts and information extraction,"This study provides an update of the polarimetric tools currently being used for optimum information extraction from polarimetric synthetic aperture radar (SAR) images. The basics of polarimetric theory are summarized and discussed in the context of SAR. Calibration of polarimetric SAR, which is an important issue for the extraction of meaningful polarization information, is reviewed. Information extraction using the scattered and received wave parameters and target decomposition theory is considered. In particular, the use of coherent versus incoherent target decomposition is discussed and the practical limitations of these target decompositions are outlined. Speckle filtering and classification of polarimetric SAR images are also thoroughly analyzed, and the important directions for future research are outlined.",2004.0,"R. Touzi, W. Boerner, J. S. Lee, E. Lueneburg"
75288ecdeb29f093190c1a0130be2d24619238ed,https://www.semanticscholar.org/paper/75288ecdeb29f093190c1a0130be2d24619238ed,FASTUS: A Finite-state Processor for Information Extraction from Real-world Text,"Approaches to text processing that rely on parsing the text with a context-free grammar tend to be slow and error-prone because of the massive ambiguity of long sentences. In contrast, FASTUS employs a nondeterministic finite-state language model that produces a phrasal decomposition of a sentence into noun groups, verb groups and particles. Another finite-state machine recognizes domain-specific phrases based on combinations of the heads of the constituents found in the first pass. FASTUS has been evaluated on several blind tests that demonstrate that state-of-the-art performance on information-extraction tasks is obtainable with surprisingly little computational effort.",1993.0,"D. Appelt, Jerry R. Hobbs, J. Bear, David J. Israel, M. Tyson"
96c80f4abfd8efef422f870b34697cee70374391,https://www.semanticscholar.org/paper/96c80f4abfd8efef422f870b34697cee70374391,Exploiting Subjectivity Classification to Improve Information Extraction,"Information extraction (IE) systems are prone to false hits for a variety of reasons and we observed that many of these false hi ts occur in sentences that contain subjective language (e.g., opinions, emotions, and sentiments). Motivated by these observations, we explore the idea of using subjectivity analysis to improve the precision of information extraction systems. In this paper, we describe an IE system that uses a subjective sentence classifier to filter its extractions. We experimented with several different strategies for using the subjectivity classifications, including an aggressive strategy that discards all extractions found in subjective sentences and more complex strategies that selectively discard extractions. We evaluated the performance of these different approaches on the MUC-4 terrorism data set. We found that indiscriminately filtering extractions from subjective sentences was overly aggressive, but more selective filtering strategies improved IE precision with minimal recall loss.",2005.0,"E. Riloff, Janyce Wiebe, W. Phillips"
310cd6a39b0539193561148cd9897b1953fa8b28,https://www.semanticscholar.org/paper/310cd6a39b0539193561148cd9897b1953fa8b28,A Probabilistic Model of Redundancy in Information Extraction,"Unsupervised Information Extraction (UIE) is the task of extracting knowledge from text without using hand-tagged training examples. A fundamental problem for both UIE and supervised IE is assessing the probability that extracted information is correct. In massive corpora such as the Web, the same extraction is found repeatedly in different documents. How does this redundancy impact the probability of correctness? 
 
This paper introduces a combinatorial ""balls-andurns"" model that computes the impact of sample size, redundancy, and corroboration from multiple distinct extraction rules on the probability that an extraction is correct. We describe methods for estimating the model's parameters in practice and demonstrate experimentally that for UIE the model's log likelihoods are 15 times better, on average, than those obtained by Pointwise Mutual Information (PMI) and the noisy-or model used in previous work. For supervised IE, the model's performance is comparable to that of Support Vector Machines, and Logistic Regression.",2005.0,"Doug Downey, Oren Etzioni, S. Soderland"
dd7cee21074ea6b346011d7463f7387ad9bfcc2a,https://www.semanticscholar.org/paper/dd7cee21074ea6b346011d7463f7387ad9bfcc2a,Information Extraction from HTML: Application of a General Machine Learning Approach,"Because the World Wide Web consists primarily of text, information extraction is central to any effort that would use the Web as a resource for knowledge discovery. We show how information extraction can be cast as a standard machine learning problem, and argue for the suitability of relational learning in solving it. The implementation of a general-purpose relational learner for information extraction, SRV, is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document structure and the kinds of information available for use in learning extraction patterns. Instead, structural and other information is supplied as input in the form of an extensible token-oriented feature set. We demonstrate the effectiveness of this approach by adapting SRV for use in learning extraction rules for a domain consisting of university course and research project pages sampled from the Web. Making SRV Web-ready only involves adding several simple HTML-specific features to its basic feature set.",1998.0,Dayne Freitag
6df333b6e4dc95a19fb5dcfa49dbd3ac11db967b,https://www.semanticscholar.org/paper/6df333b6e4dc95a19fb5dcfa49dbd3ac11db967b,Empirical Methods in Information Extraction,"This article surveys the use of empirical, machine-learning methods for a particular natural language-understanding task-information extraction. The author presents a generic architecture for information-extraction systems and then surveys the learning algorithms that have been developed to address the problems of accuracy, portability, and knowledge acquisition for each component of the architecture.",1997.0,Claire Cardie
79398502f4dcc812cefcb944fc748b32998aec5c,https://www.semanticscholar.org/paper/79398502f4dcc812cefcb944fc748b32998aec5c,Automatic Acquisition of Domain Knowledge for Information Extraction,"In developing an Information Extraction (IE) system for a new class of events or relations, one of the major tasks is identifying the many ways in which these events or relations may be expressed in text. This has generally involved the manual analysis and, in some cases, the annotation of large quantities of text involving these events. This paper presents an alternative approach, based on an automatic discovery procedure, EXDISCO, which identifies a set of relevant documents and a set of event patterns from un-annotaled text, starting from a small set of ""seed patterns."" We evaluate EXDISCO by comparing the performance of discovered patterns against that of manually constructed systems on actual extraction tasks.",2000.0,"R. Yangarber, R. Grishman, P. Tapanainen, Silja Huttunen"
c82f650d298004f9f7d68bb88e69e6d80b808cae,https://www.semanticscholar.org/paper/c82f650d298004f9f7d68bb88e69e6d80b808cae,Information extraction,"here may be more text data in electronic form than ever before, but much of it is ignored. No human can read, understand, and synthesize megabytes of text on an everyday basis. Missed information— and lost opportunities—has spurred researchers to explore various information management strategies to establish order in the text wilderness. The most common strategies are information retrieval (IR) and information filtering [4]. A relatively new development—information extraction (IE)—is the subject of this article. We can view IR systems as combine harvesters that bring back useful material from vast fields of raw material. With large amounts of potentially useful information in hand, an IE system can then transform the raw material, refining and reducing it to a germ of the original text (see Figure 1). Suppose financial analysts are investigating production of semiconductor devices (see Figure 2). They might want to know several things:",1996.0,Heng Ji
016d34269a505a74d1f481314b30c13049d993bb,https://www.semanticscholar.org/paper/016d34269a505a74d1f481314b30c13049d993bb,Bottom-Up Relational Learning of Pattern Matching Rules for Information Extraction,"Information extraction is a form of shallow text processing that locates a speciﬁed set of relevant items in a natural-language document. Systems for this task require signiﬁcant domain-speciﬁc knowledge and are time-consuming and difﬁcult to build by hand, making them a good application for machine learning. We present an algorithm, R APIER , that uses pairs of sample documents and ﬁlled templates to induce pattern-match rules that directly extract ﬁllers for the slots in the template. R APIER is a bottom-up learning algorithm that incorporates techniques from several inductive logic programming systems. We have implemented the algorithm in a system that allows patterns to have constraints on the words, part-of-speech tags, and semantic classes present in the ﬁller and the surrounding text. We present encouraging experimental results on two domains.",2003.0,"Mary Elaine Califf, R. Mooney"
3b9217ac8d4fdd9528442389425b792b1ef0ad93,https://www.semanticscholar.org/paper/3b9217ac8d4fdd9528442389425b792b1ef0ad93,Information Extraction with HMMs and Shrinkage,"Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling time series data, and have been applied with success to many language-related tasks such as part of speech tagging, speech recognition, text segmentation and topic detection. This paper describes the application of HMMs to another language related task--information extraction--the problem of locating textual sub-segments that answer a particular information need. In our work, the HMM state transition probabilities and word emission probabilities are learned from labeled training data. As in many machine learning problems, however, the lack of sufficient labeled training data hinders the reliability of the model. The key contribution of this paper is the use of a statistical technique called ""shrinkage"" that significantly improves parameter estimation of the HMM emission probabilities in the face of sparse training data. In experiments on seminar announcements and Reuters acquisitions articles, shrinkage is shown to reduce error by up to 40%, and the resulting HMM outperforms a state-of-the-art rule-learning system.",1999.0,"Dayne Freitag, A. McCallum"
b214871c7780f1e1030da595af8715bd2962d811,https://www.semanticscholar.org/paper/b214871c7780f1e1030da595af8715bd2962d811,Sparse Information Extraction: Unsupervised Language Models to the Rescue,"Even in a massive corpus such as the Web, a substantial fraction of extractions appear infrequently. This paper shows how to assess the correctness of sparse extractions by utilizing unsupervised language models. The REALM system, which combines HMMbased and n-gram-based language models, ranks candidate extractions by the likelihood that they are correct. Our experiments show that REALM reduces extraction error by 39%, on average, when compared with previous work. Because REALM pre-computes language models based on its corpus and does not require any hand-tagged seeds, it is far more scalable than approaches that learn models for each individual relation from handtagged data. Thus, REALM is ideally suited for open information extraction where the relations of interest are not specified in advance and their number is potentially vast.",2007.0,"Doug Downey, Stefan Schoenmackers, Oren Etzioni"
5d2ddf59f4abd2980bb14d768be0f96cdd3c5929,https://www.semanticscholar.org/paper/5d2ddf59f4abd2980bb14d768be0f96cdd3c5929,A Multi-resolution Framework for Information Extraction from Free Text,"Extraction of relations between entities is an important part of Information Extraction on free text. Previous methods are mostly based on statistical correlation and dependency relations between entities. This paper re-examines the problem at the multiresolution layers of phrase, clause and sentence using dependency and discourse relations. Our multi-resolution framework ARE (Anchor and Relation) uses clausal relations in 2 ways: 1) to filter noisy dependency paths; and 2) to increase reliability of dependency path extraction. The resulting system outperforms the previous approaches by 3%, 7%, 4% on MUC4, MUC6 and ACE RDC domains respectively.",2007.0,"M. Maslennikov, Tat-Seng Chua"
b336e1c2320b3213aa507931b77271b730133775,https://www.semanticscholar.org/paper/b336e1c2320b3213aa507931b77271b730133775,"Hierarchical, perceptron-like learning for ontology-based information extraction","Recent work on ontology-based Information Extraction (IE) has tried to make use of knowledge from the target ontology in order to improve semantic annotation results. However, very few approaches exploit the ontology structure itself, and those that do so, have some limitations. This paper introduces a hierarchical learning approach for IE, which uses the target ontology as an essential part of the extraction process, by taking into account the relations between concepts. The approach is evaluated on the largest available semantically annotated corpus. The results demonstrate clearly the benefits of using knowledge from the ontology as input to the information extraction process. We also demonstrate the advantages of our approach over other state-of-the-art learning systems on a commonly used benchmark dataset.",2007.0,"Yaoyong Li, Kalina Bontcheva"
c6c7f37fae4376473cc9502a7ee094a4bd7437c1,https://www.semanticscholar.org/paper/c6c7f37fae4376473cc9502a7ee094a4bd7437c1,Effective Information Extraction with Semantic Affinity Patterns and Relevant Regions,"We present an information extraction system that decouples the tasks of finding relevant regions of text and applying extraction patterns. We create a self-trained relevant sentence classifier to identify relevant regions, and use a semantic affinity measure to automatically learn domain-relevant extraction patterns. We then distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training.",2007.0,"Siddharth Patwardhan, E. Riloff"
1644fc9d0e7005bd740552d4e70b733ea613ca0b,https://www.semanticscholar.org/paper/1644fc9d0e7005bd740552d4e70b733ea613ca0b,Web Information Extraction by HTML Tree Edit Distance Matching,"The main issue for effective Web information extraction is how to recognize similar patterns in a Web page. Traditionally, it has been shown that pattern matching by using the HTML DOM tree is more efficient than the simple string matching approach. Nonetheless, previous tree-based pattern matching methods have problems by assuming that all HTML tags have the same values, assigning the same weight to each node in HTML trees. This paper proposes an enhanced tree matching algorithm that improves the tree edit distance method by considering the characteristics of HTML features. We assign different values to different HTML tree nodes according to their weights for displaying the corresponding data objects in the browser. Pattern matching of HTML patterns is done by obtaining the maximum mapping values of two HTML trees that are constructed with weighted node values from HTML data objects. Experiments are done over several Web commerce sites to evaluate the effectiveness of the proposed HTML tree matching algorithm.",2007.0,"Yeonjung Kim, Jeahyun Park, Taehwan Kim, Joongmin Choi"
18d04fa815cbc9e7a2eeab189a4d7388cb43b0cb,https://www.semanticscholar.org/paper/18d04fa815cbc9e7a2eeab189a4d7388cb43b0cb,Using Predicate-Argument Structures for Information Extraction,"In this paper we present a novel, customizable IE paradigm that takes advantage of predicate-argument structures. We also introduce a new way of automatically identifying predicate argument structures, which is central to our IE paradigm. It is based on: (1) an extended set of features; and (2) inductive decision tree learning. The experimental results prove our claim that accurate predicate-argument structures enable high quality IE results.",2003.0,"M. Surdeanu, S. Harabagiu, John Williams, Paul Aarseth"
0ffc0c15abdaa31a7ff4e68ebca922359fad142d,https://www.semanticscholar.org/paper/0ffc0c15abdaa31a7ff4e68ebca922359fad142d,Information Extraction: Beyond Document Retrieval,"In this paper we give a synoptic view of the growth of the text processing technology of information extraction (IE) whose function is to extract information about a pre‐specified set of entities, relations or events from natural language texts and to record this information in structured representations called templates. Here we describe the nature of the IE task, review the history of the area from its origins in AI work in the 1960s and 70s till the present, discuss the techniques being used to carry out the task, describe application areas where IE systems are or are about to be at work, and conclude with a discussion of the challenges facing the area. What emerges is a picture of an exciting new text processing technology with a host of new applications, both on its own and in conjunction with other technologies, such as information retrieval, machine translation and data mining.",1998.0,"R. Gaizauskas, Yorick Wilks"
ac5fde6006f9aada3e98adfda001e92343912f13,https://www.semanticscholar.org/paper/ac5fde6006f9aada3e98adfda001e92343912f13,Integrated Annotation for Biomedical Information Extraction,"We describe an approach to two areas of biomedical information extraction, drug development and cancer genomics. We have developed a framework which includes corpus annotation integrated at multiple levels: a Treebank containing syntactic structure, a Propbank containing predicate-argument structure, and annotation of entities and relations among the entities. Crucial to this approach is the proper characterization of entities as relation components, which allows the integration of the entity annotation with the syntactic structure while retaining the capacity to annotate and extract more complex events. We are training statistical taggers using this annotation for such extraction as well as using them for improving the annotation process.",2004.0,"S. Kulick, Ann Bies, M. Liberman, Mark A. Mandel, Ryan T. McDonald, Martha Palmer, A. Schein, L. Ungar, S. Winters, Peter S. White"
54aa030597d0211d1a2f27fe1711cc27ab0b0349,https://www.semanticscholar.org/paper/54aa030597d0211d1a2f27fe1711cc27ab0b0349,Information extraction as a basis for high-precision text classification,"We describe an approach to text classification that represents a compromise between traditional word-based techniques and in-depth natural language processing. Our approach uses a natural language processing task called “information extraction” as a basis for high-precision text classification. We present three algorithms that use varying amounts of extracted information to classify texts. The relevancy signatures algorithm uses linguistic phrases; the augmented relevancy signatures algorithm uses phrases and local context; and the case-based text classification algorithm uses larger pieces of context. Relevant phrases and contexts are acquired automatically using a training corpus. We evaluate the algorithms on the basis of two test sets from the MUC-4 corpus. All three algorithms achieved high precision on both test sets, with the augmented relevancy signatures algorithm and the case-based algorithm reaching 100% precision with over 60% recall on one set. Additionally, we compare the algorithms on a larger collection of 1700 texts and describe an automated method for empirically deriving appropriate threshold values. The results suggest that information extraction techniques can support high-precision text classification and, in general, that using more extracted information improves performance. As a practical matter, we also explain how the text classification system can be easily ported across domains.",1994.0,"E. Riloff, W. Lehnert"
6b836debb8d22e6b97df7730b5d4b8ef88961347,https://www.semanticscholar.org/paper/6b836debb8d22e6b97df7730b5d4b8ef88961347,Mining with Information Extraction,"The popularity of the Web and the large number of documents available in electronic form has motivated the search for hidden knowledge in text collections. Consequently, there is growing research interest in the general topic of text mining. In this paper, we develop a text-mining system by integrating methods from Information Extraction (IE) and Data Mining (Knowledge Discovery from Databases or KDD). By utilizing existing IE and KDD techniques, text-mining systems can be developed relatively rapidly and evaluated on existing text corpora for testing IE systems. We present a general text-mining framework called DiscoTEX which employs an IE module for transforming natural-language documents into structured data and a KDD module for discovering prediction rules from the extracted data. When discovering patterns in extracted text, strict matching of strings is inadequate because textual database entries generally exhibit variations due to typographical errors, misspellings, abbreviations, and other sources. We introduce the notion of discovering “soft-matching” rules from text and present two new learning algorithms. TextRISE is an inductive method for learning soft-matching prediction rules that integrates rule-based and instance-based learning methods. Simple, interpretable rules are discovered using rule induction, while a nearest-neighbor algorithm provides soft matching. SoftApriori is a text-mining algorithm for discovering association rules from texts that uses a similarity measure to allow flexible matching to variable database items. We present experimental results on inducing prediction and association rules from natural-language texts demonstrating that TextRISE and SoftApriori learn more accurate rules than previous methods for these tasks. We also present an approach to using rules mined from extracted data to improve the accuracy of information extraction. Experimental results demonstate that such discovered patterns can be used to effectively improve the underlying IE method.",2002.0,Un Yong Nahm
48e94dbedf4924b537d4a0713b7dad88648b2228,https://www.semanticscholar.org/paper/48e94dbedf4924b537d4a0713b7dad88648b2228,Monadic datalog and the expressive power of languages for web information extraction,"Research on information extraction from Web pages (wrapping) has seen much activity in recent times (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping.In this paper, we first study monadic datalog as a wrapping language (over ranked or unranked tree structures). Using previous work by Neven and Schwentick, we show that this simple language is equivalent to full monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and thus propose MSO as a yardstick for evaluating and comparing wrappers.Using the above result, we study the kernel fragment Elog- of the Elog wrapping language used in the Lixto system (a visual wrapper generator). The striking fact here is that Elog- exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified. We also formally compare Elog to other wrapping languages proposed in the literature.",2002.0,"G. Gottlob, Christoph E. Koch"
5ac3ba473ac5193255d4dabd50b65c77ba4d0399,https://www.semanticscholar.org/paper/5ac3ba473ac5193255d4dabd50b65c77ba4d0399,Unsupervised Information Extraction Approach Using Graph Mutual Reinforcement,"Information Extraction (IE) is the task of extracting knowledge from unstructured text. We present a novel unsupervised approach for information extraction based on graph mutual reinforcement. The proposed approach does not require any seed patterns or examples. Instead, it depends on redundancy in large data sets and graph based mutual reinforcement to induce generalized ""extraction patterns"". The proposed approach has been used to acquire extraction patterns for the ACE (Automatic Content Extraction) Relation Detection and Characterization (RDC) task. ACE RDC is considered a hard task in information extraction due to the absence of large amounts of training data and inconsistencies in the available data. The proposed approach achieves superior performance which could be compared to supervised techniques with reasonable training data.",2006.0,"Hany Hassan, Ahmed Hassan Awadallah, O. Emam"
5a4d81c67da140f88de5d8625906a9be17b3af40,https://www.semanticscholar.org/paper/5a4d81c67da140f88de5d8625906a9be17b3af40,On-Demand Information Extraction,"At present, adapting an Information Extraction system to new topics is an expensive and slow process, requiring some knowledge engineering for each new topic. We propose a new paradigm of Information Extraction which operates 'on demand' in response to a user's query. On-demand Information Extraction (ODIE) aims to completely eliminate the customization effort. Given a user's query, the system will automatically create patterns to extract salient relations in the text of the topic, and build tables from the extracted information using paraphrase discovery technology. It relies on recent advances in pattern discovery, paraphrase discovery, and extended named entity tagging. We report on experimental results in which the system created useful tables for many topics, demonstrating the feasibility of this approach.",2006.0,S. Sekine
65f69c8c4f52a069cbcdc88c1242b018041cb8ad,https://www.semanticscholar.org/paper/65f69c8c4f52a069cbcdc88c1242b018041cb8ad,Managing information extraction: state of the art and research directions,"This tutorial makes the case for developing a unified framework that manages information extraction from unstructured data (focusing in particular on text). We first survey research on information extraction in the database, AI, NLP, IR, and Web communities in recent years. Then we discuss why this is the right time for the database community to actively participate and address the problem of managing information extraction (including in particular the challenges of maintaining and querying the extracted information, and accounting for the imprecision and uncertainty inherent in the extraction process). Finally, we show how interested researchers can take the next step, by pointing to open problems, available datasets, applicable standards, and software tools. We do not assume prior knowledge of text management, NLP, extraction techniques, or machine learning.",2006.0,"A. Doan, R. Ramakrishnan, Shivakumar Vaithyanathan"
4fd223b585bab9e0cc1fc156f3cea9fd85f325a5,https://www.semanticscholar.org/paper/4fd223b585bab9e0cc1fc156f3cea9fd85f325a5,Comparing Information Extraction Pattern Models,"Several recently reported techniques for the automatic acquisition of Information Extraction (IE) systems have used dependency trees as the basis of their extraction pattern representation. These approaches have used a variety of pattern models (schemes for representing IE patterns based on particular parts of the dependency analysis). An appropriate model should be expressive enough to represent the information which is to be extracted from text without being overly complicated. Four previously reported pattern models are evaluated using existing IE evaluation corpora and three dependency parsers. It was found that one model, linked chains, could represent around 95% of the information of interest without generating an unwieldy number of possible patterns.",2006.0,"Mark Stevenson, M. Greenwood"
5ac3ba473ac5193255d4dabd50b65c77ba4d0399,https://www.semanticscholar.org/paper/5ac3ba473ac5193255d4dabd50b65c77ba4d0399,Unsupervised Information Extraction Approach Using Graph Mutual Reinforcement,"Information Extraction (IE) is the task of extracting knowledge from unstructured text. We present a novel unsupervised approach for information extraction based on graph mutual reinforcement. The proposed approach does not require any seed patterns or examples. Instead, it depends on redundancy in large data sets and graph based mutual reinforcement to induce generalized ""extraction patterns"". The proposed approach has been used to acquire extraction patterns for the ACE (Automatic Content Extraction) Relation Detection and Characterization (RDC) task. ACE RDC is considered a hard task in information extraction due to the absence of large amounts of training data and inconsistencies in the available data. The proposed approach achieves superior performance which could be compared to supervised techniques with reasonable training data.",2006.0,"Hany Hassan, Ahmed Hassan Awadallah, O. Emam"
bfd0a4567793843ea738d56e0fbfc11db590cec2,https://www.semanticscholar.org/paper/bfd0a4567793843ea738d56e0fbfc11db590cec2,Book Reviews: Information Extraction: Algorithms and Prospects in a Retrieval Context by Marie-Francine Moens,"Information extraction regards the processes of structuring and combining content that is explicitly stated or implied in one or multiple unstructured information sources. It involves a semantic classification and linking of certain pieces of information and is considered as a light form of content understanding by the machine. Currently, there is a considerable interest in integrating the results of information extraction in retrieval systems, because of the growing demand for search engines that return precise answers to flexible information queries. Advanced retrieval models satisfy that need and they rely on tools that automatically build a probabilistic model of the content of a (multi-media) document. The book focuses on content recognition in text. It elaborates on the past and current most successful algorithms and their application in a variety of domains (e.g., news filtering, mining of biomedical text, intelligence gathering, competitive intelligence, legal information searching, and processing of informal text). An important part discusses current statistical and machine learning algorithms for information detection and classification and integrates their results in probabilistic retrieval models. The book also reveals a number of ideas towards an advanced understanding and synthesis of textual content. The book is aimed at researchers and software developers interested in information extraction and retrieval, but the many illustrations and real world examples make it also suitable as a handbook for students.",2006.0,D. Maynard
513331cf8380815acc931e29346fe60e2c437c75,https://www.semanticscholar.org/paper/513331cf8380815acc931e29346fe60e2c437c75,Ontology-based Information Extraction with SOBA,"In this paper we describe SOBA, a sub-component of the SmartWeb multi-modal dialog system. SOBA is a component for ontologybased information extraction from soccer web pages for automatic population of a knowledge base that can be used for domainspecific question answering. SOBA realizes a tight connection between the ontology, knowledge base and the information extraction component. The originality of SOBA is in the fact that it extracts information from heterogeneous sources such as tabular structures, text and image captions in a semantically integrated way. In particular, it stores extracted information in a knowledge base, and in turn uses the knowledge base to interpret and link newly extracted information with respect to already existing entities.",2006.0,"P. Buitelaar, P. Cimiano, Stefania Racioppa, Melanie Siegel"
aa1688f979263e984ff007d5415d0759b424f75a,https://www.semanticscholar.org/paper/aa1688f979263e984ff007d5415d0759b424f75a,Avatar Information Extraction System,"TheAVATAR Information Extraction System ( IES) at the IBM Almaden Research Center enables highprecision, rule-based, information extraction from text-documents. Draw ing from our experience we propose the use of probabilistic database techniques as the formal under pi nings of information extraction systems so as to maintain high precision while increasing recall. This involve s building a framework where rule-based annotators can be mapped to queries in a databas e system. We use examples from AVATAR IES to describe the challenges in achieving this goal. Finally, we show that derivin g precision estimates in such a database system presents a significant challe nge for probabilistic database systems.",2006.0,"T. S. Jayram, R. Krishnamurthy, S. Raghavan, Shivakumar Vaithyanathan, Huaiyu Zhu"
b6c913b3c852bae4f568bd30d4c26cb3762a1248,https://www.semanticscholar.org/paper/b6c913b3c852bae4f568bd30d4c26cb3762a1248,Combining Information Extraction Systems Using Voting and Stacked Generalization,"This article investigates the effectiveness of voting and stacked generalization -also known as stacking- in the context of information extraction (IE). A new stacking framework is proposed that accommodates well-known approaches for IE. The key idea is to perform cross-validation on the base-level data set, which consists of text documents annotated with relevant information, in order to create a meta-level data set that consists of feature vectors. A classifier is then trained using the new vectors. Therefore, base-level IE systems are combined with a common classifier at the meta-level. Various voting schemes are presented for comparing against stacking in various IE domains. Well known IE systems are employed at the base-level, together with a variety of classifiers at the meta-level. Results show that both voting and stacking work better when relying on probabilistic estimates by the base-level systems. Voting proved to be effective in most domains in the experiments. Stacking, on the other hand, proved to be consistently effective over all domains, doing comparably or better than voting and always better than the best base-level systems. Particular emphasis is also given to explaining the results obtained by voting and stacking at the meta-level, with respect to the varying degree of similarity in the output of the base-level systems.",2005.0,"Georgios Sigletos, G. Paliouras, C. Spyropoulos, M. Hatzopoulos"
f0928181b94c965a2fbdc98a8b741c91c35e41bd,https://www.semanticscholar.org/paper/f0928181b94c965a2fbdc98a8b741c91c35e41bd,Biomedical Information Extraction with Predicate-Argument Structure Patterns,"Due to the ever growing amount of publications, Information Extraction (IE) from text is increasingly is recognized as one of crucial technologies in bioinformatics. However, for IE to be practically applicable, adaptability/portability of a system is crucial, considering extremely diverse demands in biomedical IE application. We should be able to construct a set of “extraction rules” adapted for a specific application at low",2005.0,"Akane Yakushiji, Yusuke Miyao, Yuka Tateisi, Junichi Tsujii"
f1926ba0277ef84942683a07d50f61ed3582b76f,https://www.semanticscholar.org/paper/f1926ba0277ef84942683a07d50f61ed3582b76f,2D Conditional Random Fields for Web information extraction,"The Web contains an abundance of useful semistructured information about real world objects, and our empirical study shows that strong sequence characteristics exist for Web information about objects of the same type across different Web sites. Conditional Random Fields (CRFs) are the state of the art approaches taking the sequence characteristics to do better labeling. However, as the information on a Web page is two-dimensionally laid out, previous linear-chain CRFs have their limitations for Web information extraction. To better incorporate the two-dimensional neighborhood interactions, this paper presents a two-dimensional CRF model to automatically extract object information from the Web. We empirically compare the proposed model with existing linear-chain CRF models for product information extraction, and the results show the effectiveness of our model.",2005.0,"Jun Zhu, Zaiqing Nie, Ji-Rong Wen, Bo Zhang, Wei-Ying Ma"
1aeee8b379e6fcf10d33c15df78e135b16b24d5e,https://www.semanticscholar.org/paper/1aeee8b379e6fcf10d33c15df78e135b16b24d5e,Unsupervised Learning of Field Segmentation Models for Information Extraction,"The applicability of many current information extraction techniques is severely limited by the need for supervised training data. We demonstrate that for certain field structured extraction tasks, such as classified advertisements and bibliographic citations, small amounts of prior knowledge can be used to learn effective models in a primarily unsupervised fashion. Although hidden Markov models (HMMs) provide a suitable generative model for field structured text, general unsupervised HMM learning fails to learn useful structure in either of our domains. However, one can dramatically improve the quality of the learned structure by exploiting simple prior knowledge of the desired solutions. In both domains, we found that unsupervised methods can attain accuracies with 400 unlabeled examples comparable to those attained by supervised methods on 50 labeled examples, and that semi-supervised methods can make good use of small amounts of labeled data.",2005.0,"Trond Grenager, D. Klein, Christopher D. Manning"
ff304304fddfb4de368b45894c20f44fdab817d5,https://www.semanticscholar.org/paper/ff304304fddfb4de368b45894c20f44fdab817d5,Evaluating machine learning for information extraction,"Comparative evaluation of Machine Learning (ML) systems used for Information Extraction (IE) has suffered from various inconsistencies in experimental procedures. This paper reports on the results of the Pascal Challenge on Evaluating Machine Learning for Information Extraction, which provides a standardised corpus, set of tasks, and evaluation methodology. The challenge is described and the systems submitted by the ten participants are briefly introduced and their performance is analysed.",2005.0,"N. Ireson, F. Ciravegna, Mary Elaine Califf, Dayne Freitag, N. Kushmerick, A. Lavelli"
b6542febae6ad33d1d2c558869999d41454a5335,https://www.semanticscholar.org/paper/b6542febae6ad33d1d2c558869999d41454a5335,Scaling Information Extraction to Large Document Collections,"Information extraction and text mining applications are just beginning to tap the immense amounts of valuable textual information available online. In order to extract information from millions, and in some cases, billions of documents, different solutions to scalability emerged. We review key approaches for scaling up information extraction, including using general-purpose search engines as well as indexing techniques specialized for information extraction applications. Scalable information extraction is an active area of research, and we highlight some of the opportunities and challenges in this area that are relevant to the database community.",2005.0,Eugene Agichtein
672ed5bc836039cd105f09724212bdc596bbf895,https://www.semanticscholar.org/paper/672ed5bc836039cd105f09724212bdc596bbf895,Using Uneven Margins SVM and Perceptron for Information Extraction,"The classification problem derived from information extraction (IE) has an imbalanced training set. This is particularly true when learning from smaller datasets which often have a few positive training examples and many negative ones. This paper takes two popular IE algorithms -- SVM and Perceptron -- and demonstrates how the introduction of an uneven margins parameter can improve the results on imbalanced training data in IE. Our experiments demonstrate that the uneven margin was indeed helpful, especially when learning from few examples. Essentially, the smaller the training set is, the more beneficial the uneven margin can be. We also compare our systems to other state-of-the-art algorithms on several benchmarking corpora for IE.",2005.0,"Yaoyong Li, Kalina Bontcheva, H. Cunningham"
fe8dc921ebe4f85969f4181c50959fa0dc552476,https://www.semanticscholar.org/paper/fe8dc921ebe4f85969f4181c50959fa0dc552476,"KnowItNow: Fast, Scalable Information Extraction from the Web","Numerous NLP applications rely on search-engine queries, both to extract information from and to compute statistics over the Web corpus. But search engines often limit the number of available queries. As a result, query-intensive NLP applications such as Information Extraction (IE) distribute their query load over several days, making IE a slow, offline process.This paper introduces a novel architecture for IE that obviates queries to commercial search engines. The architecture is embodied in a system called KnowItNow that performs high-precision IE in minutes instead of days. We compare KnowItNow experimentally with the previously-published KnowItAll system, and quantify the tradeoff between recall and speed. KnowItNow's extraction rate is two to three orders of magnitude higher than KnowItAll's.",2005.0,"Michael J. Cafarella, Doug Downey, S. Soderland, Oren Etzioni"
8ae9fbffb11338e033233946917cd41e4cf0959c,https://www.semanticscholar.org/paper/8ae9fbffb11338e033233946917cd41e4cf0959c,Resume Information Extraction with Cascaded Hybrid Model,"This paper presents an effective approach for resume information extraction to support automatic resume management and routing. A cascaded information extraction (IE) framework is designed. In the first pass, a resume is segmented into a consecutive blocks attached with labels indicating the information types. Then in the second pass, the detailed information, such as Name and Address, are identified in certain blocks (e.g. blocks labelled with Personal Information), instead of searching globally in the entire resume. The most appropriate model is selected through experiments for each IE task in different passes. The experimental results show that this cascaded hybrid model achieves better F-score than flat models that do not apply the hierarchical structure of resumes. It also shows that applying different IE models in different passes according to the contextual structure is effective.",2005.0,"Kun Yu, Gang Guan, M. Zhou"
9d87caf162c239927c85c8a4a9c3e3c1ecd65ba3,https://www.semanticscholar.org/paper/9d87caf162c239927c85c8a4a9c3e3c1ecd65ba3,Information Extraction from speech,"Summary form only given. The state of the art in automatic speech recognition has reached the point that searching for and extracting information from large speech repositories or streaming audio has become a growing reality. This paper summarizes the technologies that have been instrumental in making audio as searchable as text, including speech recognition, speaker clustering, segmentation, and identification; topic classification; and story segmentation. Once speech is turned into text, information extraction methods can then be applied, such as named entity extraction, finding relationships between named entities, and resolution of anaphoric references. Examples of deployed systems for information extraction from speech, which incorporate some of the aforementioned technologies, will be given.",2006.0,J. Makhoul
3fcc00daa35d90b0a9a81721f1a855de0dd8a4c1,https://www.semanticscholar.org/paper/3fcc00daa35d90b0a9a81721f1a855de0dd8a4c1,Using Gazetteers in Discriminative Information Extraction,"Much work on information extraction has successfully used gazetteers to recognise uncommon entities that cannot be reliably identified from local context alone. Approaches to such tasks often involve the use of maximum entropy-style models, where gazetteers usually appear as highly informative features in the model. Although such features can improve model accuracy, they can also introduce hidden negative effects. In this paper we describe and analyse these effects and suggest ways in which they may be overcome. In particular, we show that by quarantining gazetteer features and training them in a separate model, then decoding using a logarithmic opinion pool (Smith et al., 2005), we may achieve much higher accuracy. Finally, we suggest ways in which other features with gazetteer feature-like behaviour may be identified.",2006.0,"Andrew D. M. Smith, M. Osborne"
3b9356b2156c94dbb8f2d00c8436efa3a3b9e9b6,https://www.semanticscholar.org/paper/3b9356b2156c94dbb8f2d00c8436efa3a3b9e9b6,Multimedia Information Extraction,"Abstract : This paper describes the need for information extraction technologies within the military, some of the current technologies available, and the problems associated with them. It also looks at some of the ongoing research projects in areas of multimedia information extraction. Finally, it looks at the StreamSage audio extraction software and the demonstration of this software, explains how to run the software and the demo, and evaluates them.",2007.0,Emilly Budlong
e45fbdb0a98ce7101e933811f3d219f4a23bd09e,https://www.semanticscholar.org/paper/e45fbdb0a98ce7101e933811f3d219f4a23bd09e,A statistical information extraction system for Turkish,"This paper presents the results of a study on information extraction from unrestricted Turkish text using statistical language processing methods. In languages like English, there is a very small number of possible word forms with a given root word. However, languages like Turkish have very productive agglutinative morphology. Thus, it is an issue to build statistical models for specific tasks using the surface forms of the words, mainly because of the data sparseness problem. In order to alleviate this problem, we used additional syntactic information, i.e. the morphological structure of the words. We have successfully applied statistical methods using both the lexical and morphological information to sentence segmentation, topic segmentation, and name tagging tasks. For sentence segmentation, we have modeled the final inflectional groups of the words and combined it with the lexical model, and decreased the error rate to 4.34%, which is 21% better than the result obtained using only the surface forms of the words. For topic segmentation, stems of the words (especially nouns) have been found to be more effective than using the surface forms of the words and we have achieved 10.90% segmentation error rate on our test set according to the weighted TDT-2 segmentation cost metric. This is 32% better than the word-based baseline model. For name tagging, we used four different information sources to model names. Our first information source is based on the surface forms of the words. Then we combined the contextual cues with the lexical model, and obtained some improvement. After this, we modeled the morphological analyses of the words, and finally we modeled the tag sequence, and reached an F-Measure of 91.56%, according to the MUC evaluation criteria. Our results are important in the sense that, using linguistic information, i.e. morphological analyses of the words, and a corpus large enough to train a statistical model significantly improves these basic information extraction tasks for Turkish.",2003.0,"Gökhan Tür, Dilek Z. Hakkani-Tür, Kemal Oflazer"
12e4f1efb13a7e46efb233f7b8f1a20fe9bd6006,https://www.semanticscholar.org/paper/12e4f1efb13a7e46efb233f7b8f1a20fe9bd6006,Acquisition of Linguistic Patterns for Knowledge-based Information Extraction,"In this paper we present a new method of automatic acquisition of linguistic patterns for Information Extraction, as implemented in the CICERO system. Our approach combines lexico-semantic information available from the WordNet database with collocating data extracted from training corpora. Due to the open-domain nature of the WordNet information and the immediate availability of large collections of texts, our method can be easily ported to open-domain Information Extraction.",2000.0,"S. Harabagiu, Steven J. Maiorano"
f2788de735f564d00a301e32152f04384b1fca7a,https://www.semanticscholar.org/paper/f2788de735f564d00a301e32152f04384b1fca7a,Open Domain Information Extraction via Automatic Semantic Labeling,This paper presents a semantic labeling technique based on information encoded in FrameNet. Sentences labeled for frames relevant to any new Information Extraction domain enable the automatic acquisition of extraction rules for the new domain. The experimental results show that both the semantic labeling and the extraction rules enabled by the labels are generated automatically with a high precision.,2003.0,"Alessandro Moschitti, Paul Morarescu, S. Harabagiu"
902fcbc820e3b2f07c0325576a02e64232d55806,https://www.semanticscholar.org/paper/902fcbc820e3b2f07c0325576a02e64232d55806,A Mutually Beneficial Integration of Data Mining and Information Extraction,"Text mining concerns applying data mining techniques to unstructured text. Information extraction(IE) is a form of shallow text understanding that locates specific pieces of data in natural language documents, transforming unstructured text into a structured database. This paper describes a system called DISCOTEX, that combines IE and data mining methodologies to perform text mining as well as improve the performance of the underlying extraction system. Rules mined from a database extracted from a corpus of texts are used to predict additional information to extract from future documents, thereby improving the recall of IE. Encouraging results are presented on applying these techniques to a corpus of computer job announcement postings from an Internet newsgroup.",2000.0,"Un Yong Nahm, R. Mooney"
b356ed82bdf2461564fcb1294bf6035f46c641ce,https://www.semanticscholar.org/paper/b356ed82bdf2461564fcb1294bf6035f46c641ce,Adaptive Information Extraction and Sublanguage Analysis,"Information extraction (IE) has made significant progress in the last decade. We have developed practical, efficient approaches to IE which have yielded modest levels of performance on general texts and quite good performance on restricted, ‘semi-structured’ texts. More notably, over the last few years there has been a blossoming of work in adaptive IE — the topic of this and other recent workshops — IE systems which can be rapidly and automatically (or semi-automatically) moved to new extraction tasks.",2001.0,R. Grishman
e343b8c52f5f9cceae8811cc2e20db4d7f5fe1d4,https://www.semanticscholar.org/paper/e343b8c52f5f9cceae8811cc2e20db4d7f5fe1d4,"(LP) 2 , an Adaptive Algorithm for Information Extraction from Web-related Texts","(LP) is an algorithm for adaptive Information Extraction from Web-related text that induces symbolic rules by learning from a corpus tagged with SGML tags. Induction is performed by bottom-up generalisation of examples in a training corpus. Training is performed in two steps: initially a set of tagging rules is learned; then additional rules are induced to correct mistakes and imprecision in tagging. Shallow NLP is used to generalise rules beyond the flat word structure. Generalization allows a better coverage on unseen texts, as it limits data sparseness and overfitting in the training phase. In experiments on publicly available corpora the algorithm outperforms any other algorithm presented in literature and tested on the same corpora. Experiments also show a significant gain in using NLP in terms of (1) effectiveness (2) reduction of training time and (3) training corpus size. In this paper we present the machine learning algorithm for rule induction. In particular we focus on the NLP-based generalisation and the strategy for pruning both the search space and the final rule set.",2001.0,F. Ciravegna
0356d97fdd0126dd5a0223a47cfeb9e99a2ee023,https://www.semanticscholar.org/paper/0356d97fdd0126dd5a0223a47cfeb9e99a2ee023,Information Extraction - A User Guide,"This technical memo describes Information Extraction from the point-of-view of a potential user of the technology. No knowledge of language processing is assumed. Information Extraction is a process which takes unseen texts as input and produces fixed-format, unambiguous data as output. This data may be used directly for display to users, or may be stored in a database or spreadsheet for later analysis, or may be used for indexing purposes in Information Retrieval applications. See also this http URL",1997.0,H. Cunningham
c44893379f112e35a98df7acad6caf32ecddd5d8,https://www.semanticscholar.org/paper/c44893379f112e35a98df7acad6caf32ecddd5d8,Information Extraction for Question Answering: Improving Recall Through Syntactic Patterns,"We investigate the impact of the precision/recall trade-off of information extraction on the performance of an offline corpus-based question answering (QA) system. One of our findings is that, because of the robust final answer selection mechanism of the QA system, recall is more important. We show that the recall of the extraction component can be improved using syntactic parsing instead of more common surface text patterns, substantially increasing the number of factoid questions answered by the QA system.",2004.0,"V. Jijkoun, Jori Mur, M. de Rijke"
13c98a57653f521a9fd61ed47f7fa9d711f2467f,https://www.semanticscholar.org/paper/13c98a57653f521a9fd61ed47f7fa9d711f2467f,Two applications of information extraction to biological science journal articles: enzyme interactions and protein structures.,"Information extraction technology, as defined and developed through the U.S. DARPA Message Understanding Conferences (MUCs), has proved successful at extracting information primarily from newswire texts and primarily in domains concerned with human activity. In this paper we consider the application of this technology to the extraction of information from scientific journal papers in the area of molecular biology. In particular, we describe how an information extraction system designed to participate in the MUC exercises has been modified for two bioinformatics applications: EMPathIE, concerned with enzyme and metabolic pathways; and PASTA, concerned with protein structure. Progress to date provides convincing grounds for believing that IE techniques will deliver novel and effective ways for scientists to make use of the core literature which defines their disciplines.",1999.0,"K. Humphreys, G. Demetriou, R. Gaizauskas"
9e0d54f45145512e68a44fb7b40fc16bcbd87425,https://www.semanticscholar.org/paper/9e0d54f45145512e68a44fb7b40fc16bcbd87425,SUMMARISATION OF SPOKEN AUDIO THROUGH INFORMATION EXTRACTION,"Automatic summarisation of spoken audio is a fairly new research pursuit, in large part due to the relative novelty of technology for accurately decoding audio into text. Techniques that account for the peculiarities and potential ambiguities of decoded audio (high error rates, lack of syntactic boundaries) appear promising for culling summary information from audio for content-based browsing and skimming. This paper combines acoustic con-ﬁdence measures with simple information retrieval and extraction techniques in order to obtain accurate, read-able summaries of broadcast news programs. It also demonstrates how extracted summaries, full-text speech recogniser output and audio ﬁles can be usefully linked together through an audio-visual interface. The results suggest that information extraction based on statistical information can produce viable summaries of decoded audio.",1999.0,Robert J. Valenza
60e75b3482c0cc46adee2ace4c7cede6774bf147,https://www.semanticscholar.org/paper/60e75b3482c0cc46adee2ace4c7cede6774bf147,Paraphrase Acquisition for Information Extraction,"We are trying to find paraphrases from Japanese news articles which can be used for Information Extraction. We focused on the fact that a single event can be reported in more than one article in different ways. However, certain kinds of noun phrases such as names, dates and numbers behave as ""anchors"" which are unlikely to change across articles. Our key idea is to identify these anchors among comparable articles and extract portions of expressions which share the anchors. This way we can extract expressions which convey the same information. Obtained paraphrases are generalized as templates and stored for future use.In this paper, first we describe our basic idea of paraphrase acquisition. Our method is divided into roughly four steps, each of which is explained in turn. Then we illustrate several issues which we encounter in real texts. To solve these problems, we introduce two techniques: coreference resolution and structural restriction of possible portions of expressions. Finally we discuss the experimental results and conclusions.",2003.0,"Yusuke Shinyama, S. Sekine"
0e20f8ee65e4737583b98dc1a03bc13de4d77697,https://www.semanticscholar.org/paper/0e20f8ee65e4737583b98dc1a03bc13de4d77697,Recognizing referential links: an information extraction prespective,"We present an efficient and robust reference resolution algorithm in an end-to-end state-of-the-art information extraction system, which must work with a considerably impoverished syntactic analysis of the input sentences. Considering this disadvantage, the basic setup to collect, filter, then order by salience does remarkably well with third-person pronouns, but needs more semantic and discourse information to improve the treatments of other expression types.",1997.0,M. Kameyama
7b6b3df70c9be3b49bd1d0c23ac61c13ca9f6ffd,https://www.semanticscholar.org/paper/7b6b3df70c9be3b49bd1d0c23ac61c13ca9f6ffd,Weakly-supervised relation classification for information extraction,"This paper approaches the relation classification problem in information extraction framework with bootstrapping on top of Support Vector Machines. A new bootstrapping algorithm is proposed and empirically evaluated on the ACE corpus. We show that the supervised SVM classifier using various lexical and syntactic features can achieve promising classification accuracy. More importantly, the proposed <i>BootProject</i> algorithm based on random feature projection can significantly reduce the need for labeled training data with only limited sacrifice of performance.",2004.0,Zhu Zhang
d4fbd2ff775ea631ff1b3cdddbacf3211deb940b,https://www.semanticscholar.org/paper/d4fbd2ff775ea631ff1b3cdddbacf3211deb940b,The Frame-Based Module of the SUISEKI Information Extraction System,"SUISEKI, an information extraction system, uses morphological, syntactical, and contextual information to detect gene and protein names and interactions in scientific texts. This article describes the system's rules (called frames) used to detect and analyze interaction networks described in the molecular biology literature.",2002.0,"C. Blaschke, A. Valencia"
f93a372ee266a1417f5fdd07a1aef6ce9b91e810,https://www.semanticscholar.org/paper/f93a372ee266a1417f5fdd07a1aef6ce9b91e810,An architecture for biological information extraction and representation,"Technological advances in biomedical research are generating a plethora of heterogeneous data at a high rate. There is a critical need for extraction, integration and management tools for information discovery and synthesis from these heterogeneous data. In this paper, we present a general architecture, called ALFA, for information extraction and representation from diverse biological data. The ALFA architecture consists of: (i) a networked, hierarchical object model for representing information from heterogeneous data sources in a standardized, structured format; and (ii) a suite of integrated, interactive software tools for information extraction and representation from diverse biological data sources. As part of our research efforts to explore this space, we have currently prototyped the ALFA object model and a set of interactive software tools for searching, filtering, and extracting information from scientific text. In particular, we describe BioFerret, a meta-search tool for searching and filtering relevant information from the web, and ALFA Text Viewer, an interactive tool for user-guided extraction, disambiguation, and representation of information from scientific text. We further demonstrate the potential of our tools in integrating the extracted information with experimental data and diagrammatic biological models via the common underlying ALFA representation.",2004.0,"A. Vailaya, P. Bluvas, R. Kincaid, A. Kuchinsky, Michael L. Creech, Annette Adler"
042b49ddd96847a17d72fdfec7e9189c7af0d06b,https://www.semanticscholar.org/paper/042b49ddd96847a17d72fdfec7e9189c7af0d06b,An Information Extraction Core System for Real World German Text Processing,"This paper describes SMES, an information extraction core system for real world German text processing. The basic design criterion of the system is of providing a set of basic powerful, robust, and efficient natural language components and generic linguistic knowledge sources which can easily be customized for processing different tasks in a flexible manner.",1997.0,"G. Neumann, R. Backofen, Judith Baur, Markus Becker, Christian Braun"
9267978246ac58eded4a2af81648dc2806bf1e62,https://www.semanticscholar.org/paper/9267978246ac58eded4a2af81648dc2806bf1e62,A maximum entropy approach to information extraction from semi-structured and free text,"In this paper, we present a classification-based approach towards single-slot as well as multi-slot information extraction (IE). For single-slot IE, we worked on the domain of Seminar Announcements, where each document contains information on only one seminar. For multi-slot IE, we worked on the domain of Management Succession. For this domain, we restrict ourselves to extracting information sentence by sentence, in the same way as (Soderland 1999). Each sentence can contain information on several management succession events. By using a classification approach based on a maximum entropy framework, our system achieves higher accuracy than the best previously published results in both domains.",2002.0,"Hai Leong Chieu, H. Ng"
4afabe374560baccf19bb626b339d44d937f2dd9,https://www.semanticscholar.org/paper/4afabe374560baccf19bb626b339d44d937f2dd9,"Information extraction from very high resolution satellite imagery over Lukole refugee camp, Tanzania","This paper addresses information extraction from IKONOS imagery over the Lukole refugee camp in Tanzania. More specific, it describes automatic image analysis procedures for a rapid and reliable identification of refugee tents as well as their spatial extent. From the identified tents, the number of refugees can be derived and a map of the camp can be generated, which can be used for improving refugee camp management. Four information extraction methods have been tested and compared: supervised classification, unsupervised classification, multi-resolution segmentation and mathematical morphology analysis. The latter two procedures based on object-oriented classifiers perform best with a spatial accuracy above 85% and a statistical accuracy above 97%. These methods could be used for refugee camp information extraction in other geographical settings and on imagery with different spatial and spectral resolutions.",2003.0,"S. Giada, T. Groeve, Daniele Ehrlich, Pierre Soille"
9eacd2b8226e6124eec8241996ce86b9dfaa393b,https://www.semanticscholar.org/paper/9eacd2b8226e6124eec8241996ce86b9dfaa393b,Unsupervised Discovery of Scenario-Level Patterns for Information Extraction,"Information Extraction (IE) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base---a time-consuming and expensive process. We have implemented a system for finding patterns automatically from un-annotated text. Starting with a small initial set of seed patterns proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall.",2000.0,"R. Yangarber, R. Grishman, P. Tapanainen"
544a0fa1107df1423db05070182f213053003040,https://www.semanticscholar.org/paper/544a0fa1107df1423db05070182f213053003040,Closing the Gap: Learning-Based Information Extraction Rivaling Knowledge-Engineering Methods,"In this paper, we present a learning approach to the scenario template task of information extraction, where information filling one template could come from multiple sentences. When tested on the MUC-4 task, our learning approach achieves accuracy competitive to the best of the MUC-4 systems, which were all built with manually engineered rules. Our analysis reveals that our use of full parsing and state-of-the-art learning algorithms have contributed to the good performance. To our knowledge, this is the first research to have demonstrated that a learning approach to the full-scale information extraction task could achieve performance rivaling that of the knowledge engineering approach.",2003.0,"Hai Leong Chieu, H. Ng, Yoong Keok Lee"
f0a1dfcef6506b12a35cd1e1cd1ddcee42e9e8a9,https://www.semanticscholar.org/paper/f0a1dfcef6506b12a35cd1e1cd1ddcee42e9e8a9,Probabilistic Coreference in Information Extraction,"Certain applications require that the output of an information extraction system be probabilistic, so that a downstream system can reliably fuse the output with possibly contradictory information from other sources. In this paper we consider the problem of assigning a probability distribution to alternative sets of coreference relationships among entity descriptions. We present the results of initial experiments with several approaches to estimating such distributions in an application using SRI’s FASTUS information extraction system.",1997.0,A. Kehler
c11fb8460b4e2e8cf76cc3abe1dc3eaf153b67d9,https://www.semanticscholar.org/paper/c11fb8460b4e2e8cf76cc3abe1dc3eaf153b67d9,Information Extraction Using Hidden Markov Models,"OF THE THESIS Information Extraction Using Hidden Markov Models by Timothy Robert Leek Master of Science in Computer Science University of California, San Diego, 1997 Professor Charles Peter Elkan, Chair This thesis shows how to design and tune a hidden Markov model to extract factual information from a corpus of machine-readable English prose. In particular, the thesis presents a HMM that classi es and parses natural language assertions about genes being located at particular positions on chromosomes. The facts extracted by this HMM can be inserted into biological databases. The HMM is trained on a small set of sentence fragments chosen from the collected scienti c abstracts in the OMIM (On-Line Mendelian Inheritance in Man) database and judged to contain the target binary relationship between gene names and gene locations. Given a novel sentence, all contiguous fragments are ranked by log-odds score, i.e. the log of the ratio of the probability of the fragment according to the target HMM to that according to a \null"" HMM trained on all OMIM sentences. The most probable path through the HMM gives bindings for the annotations with precision as high as 80%. In contrast with traditional natural language processing methods, this stochastic approach makes no use either of part-of-speech taggers or dictionaries, instead employing non-emitting states to assemble modules roughly corresponding to noun, verb, and prepostional phrases. Algorithms for reestimating parameters for HMMs with non-emitting states are presented in detail. The ability to tolerate new words and recognize a wide variety of syntactic forms arises from the judicious use of \gap"" states. v Chapter I Good Facts Are Hard to Find Finding facts in English prose is a task that humans are good at and computers are bad at. However, humans cannot stand to spend more than a few minutes at a time occupied with such drudgery. In this respect, nding facts is unlike a host of the other jobs computers are currently hopeless at, like telling a joke, riding a bike, and cooking a dinner. While there is no pressing need for computers to be good at those things, it is already of paramount importance that computers be pro cient at nding information with precision in the proliferating archives of electronic text available on the Internet and elsewhere. The state of the art in information retrieval technology is of limited use in this application. Standard boolean searching, vectorbased approaches and latent semantic indexing are geared more toward open-ended exploration than toward the targeted, detailed subsentence processing necessary for the fact nding or information extraction task. Since these approaches discard syntax, a large class of targets, in which the relationships between groups of words are important, must be fundamentally beyond them. The critical noun and verb groups of a fact can only be found by doing some kind of parsing. Information extraction is in most cases what people really want to do when they rst set about searching text, i.e. before they lower their sights to correspond to available tools. But this does not mean that nothing less than full-blown NLP (natural language processing) will satisfy. There are many real-world text searching 1 2 tasks that absolutely require syntactic information and yet are restricted enough to be tractable. An historian might want to locate passages in the Virginia colony records mentioning the \event"" of a slave running away. The words slave, run, and away, all very common words, and their various synonyms used in an unconstrained search would return much dross. To nd this fact with precision we need to place constraints upon the arrangement of the words in the sentence; we need to limit the search with syntax. For instance, one might require that when two groups of words corresponding to slave and run appear in a sentence, that the slave is in fact the one doing the running. Similar examples of what we call fact searching are commonplace in most domains. A market analyst might want to scan the Wall Street Journal and pick out all mentions of corporate management changes. And a geneticist would be thrilled to be able to tease out of scienti c abstracts facts mapping genes to speci c locations on chromosomes. Historically, the eld of information extraction has employed discrete manipulations in order to process sentences into the critical noun and verb groups. An incoming sentence is tagged for part-of-speech and then handed o to a scaled-down parser or DFA (deterministic nite automaton) which uses local syntax to decide if the elements of a fact are present and to divide the sentence up into logical elements. Recent advances in statistical natural language processing have been applied to this problem but typically only in an ancillary role, e.g. in constructing dictionaries [17] and tagging words for part-of-speech [4]. The main processing engine remains combinatorial in avor. Systems like FASTUS [8] and CIRCUS [14] do surprisingly well, considering the di culty of the task, achieving precision and recall of better than 80%. But they require hand-built grammars or dictionaries of extraction patterns in order to attain this level of performance. A notable exception is the LIEP [9] system which learns to generalize extraction patterns from training examples. We have chosen to pursue a uni ed stochastic approach to the information extraction task, modeling sentence fragments containing the target fact with a hidden Markov model (HMM) which we use both to decide if a candidate sentence fragment 3 contains the fact and also to identify the important elements or slot llers in the fact. An HMM trained to recognize a small set of representative sentence fragments di ers radically from a DFA or discrete pattern matcher designed for the same task in that it outputs a probability. Unlike a DFA, an HMM will accept any sequence of words with non-zero probability. The probability it computes (after some corrections for sentence length and background frequencies of words) varies gracefully between the extremes of predicting extremely low probability for sequences that tend not to contain the fact to predicting high probability for ones that tend to contain it. There is no need, if we use an HMM to nd and process facts, to employ heuristics in order to rank and choose between competing explanations for a sentence; symbolic approaches often do so [9]. The probability the HMM computes is meaningful information we can use directly to reason about candidate facts in principled ways that submit to analysis. The HMM is a very compact and exible representation for the information extraction task which seems to be less reliant upon human engineering and prior knowledge than non-probabilistic approaches. This thesis will discuss our e orts to construct a model for a binary relationship between gene names and gene locations, as found in a variety of syntactic forms in scienti c abstracts. The model is structured hierarchically: at the top level states are collected into modules corresponding to noun or verb groups, whereas at the bottom level, in some cases, states function entirely deterministically, employing DFAs to recognize commonly occurring patterns. The HMM consists of only 64 states with an average of 3 transitions each, and explicitly mentions less than 150 words. When deploying the model to nd facts in novel sentences, no attempt is made to tag for part-of-speech. \Gap"" states, which assign emission probability according to word frequency in the entire corpus, permit the HMM to recognize disconnected segments of a fact and tolerate new words. Unknown words, if they appear in the right local context, are accepted by the HMM essentially without penalty. So while the list of words likely to participate in forming a gene name or gene location is long and populated by words both common and rare to the corpus our approach is competent at correctly identifying even unknown words as 4 long as they appear anked by other words that serve to index the fact well. The accuracy of this HMM approach to information extraction, in the context of the gene name|location fact, is on par with symbolic approaches. This thesis is organized as follows. We begin with a description of the gene name|location information extraction task. Next, we present the modular HMM architecture constructed for this task, motivating our choice of null or background model and demonstrating the discriminatory power it adds to this approach. A brief technical discussion comes next, of the precise formulae used to reestimate parameters for an HMM with non-emitting states. Then we provide implementation and optimization details, followed by training and testing performance. We conclude with some remarks on the use of prior knowledge and ideas for future work. Chapter II Automatic Annotation Generation We consider the question of nding facts in unrestricted prose in the context of lling in slots in a database of facts about genes. The slots in the database correspond to biological entities. These are described by single words or simple phrases, three examples of which might be the name of a gene, some speci cation of its location, and some list of diseases in which it is known to be involved. An example pair of acceptable entries is SLOT ENTRY Gene Name: (The gene encoding BARK2) Gene Location: (mouse chromosome 5) which we might nd buried in a sentence like The gene encoding BARK2 mapped to mouse chromosome 5, whereas that encoding BARK1 was localized to mouse chromosome 19. This is valuable information that is available nowhere except in the published literature. Specialized databases like SwissProt and GenBank do not contain these kinds of associations. So there is interest in developing automated systems for lling in these slots. In order to populate these slots, we must locate and correctly analyze binary (or perhaps even ternary and higher) relations between likely ele",1997.0,T. Leek
13d7795f4dc74070ddfe95187ce5e50d852702c5,https://www.semanticscholar.org/paper/13d7795f4dc74070ddfe95187ce5e50d852702c5,A Question Answering System Supported by Information Extraction,"This paper discusses an information extraction (IE) system, Textract, in natural language (NL) question answering (QA) and examines the role of IE in QA application. It shows: (i) Named Entity tagging is an important component for QA, (ii) an NL shallow parser provides a structural basis for questions, and (iii) high-level domain independent IE can result in a QA breakthrough.",2000.0,"R. Srihari, W. Li"
4f1db9a1d579bf5906a356801526613eebe464e1,https://www.semanticscholar.org/paper/4f1db9a1d579bf5906a356801526613eebe464e1,Bayesian Information Extraction Network,"Dynamic Bayesian networks (DBNs) offer an elegant way to integrate various aspects of language in one model. Many existing algorithms developed for learning and inference in DBNs are applicable to probabilistic language modeling. To demonstrate the potential of DBNs for natural language processing, we employ a DBN in an information extraction task. We show how to assemble wealth of emerging linguistic instruments for shallow parsing, syntactic and semantic tagging, morphological decomposition, named entity recognition etc. in order to incrementally build a robust information extraction system. Our method outperforms previously published results on an established benchmark domain.",2003.0,"L. Peshkin, Avi Pfeffer"
078ee28e36f9bbcba59a2786985c1d440e09766e,https://www.semanticscholar.org/paper/078ee28e36f9bbcba59a2786985c1d440e09766e,Automatic semantic annotation using unsupervised information extraction and integration,"In this paper we propose a methodology to learn to automatically annotate domain-specific information from large repositories (e.g. Web sites) with minimum user intervention. The methodology is based on a combination of information extraction, information integration and machine learning techniques. Learning is seeded by extracting information from structured sources (e.g. databases and digital libraries). Retrieved information is then used to partially annotate documents. These annotated documents are used to bootstrap learning for simple Information Extraction (IE) methodologies, which in turn will produce more annotations used to annotate more documents. It will be used to train more complex IE engines and the cycle will keep on repeating itself until the required information is obtained. The user intervention is limited to providing an initial URL and to correct information if it is the case when the computation is finished. The revised annotation can then be reused to provide further training and therefore getting more information and/or more precision.",2003.0,"A. Dingli, F. Ciravegna, Y. Wilks"
29c99d263b5e05aae6bb96f004f025dcc9b5caae,https://www.semanticscholar.org/paper/29c99d263b5e05aae6bb96f004f025dcc9b5caae,Multistrategy Learning for Information Extraction,Information extraction IE is the problem of lling out pre de ned structured sum maries from text documents We are in terested in performing IE in non traditional domains where much of the text is often ungrammatical such as electronic bulletin board posts and Web pages We suggest that the best approach is one that takes into ac count many di erent kinds of information and argue for the suitability of a multistrat egy approach We describe learners for IE drawn from three separate machine learning paradigms rote memorization term space text classi cation and relational rule induc tion By building regression models mapping from learner con dence to probability of cor rectness and combining probabilities appro priately it is possible to improve extraction accuracy over that achieved by any individ ual learner We describe three di erent mul tistrategy approaches Experiments on two IE domains a collection of electronic seminar announcements from a university computer science department and a set of newswire ar ticles describing corporate acquisitions from the Reuters collection demonstrate the e ec tiveness of all three approaches,1998.0,Dayne Freitag
6958e58fcaf3f45572bc4e7cf7d45798f0cad175,https://www.semanticscholar.org/paper/6958e58fcaf3f45572bc4e7cf7d45798f0cad175,Relational learning techniques for natural language information extraction,"The recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves speci c types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain speci c information, making them di cult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This paper presents a novel rule representation speci c to natural language and a learning system, Rapier, which learns information extraction rules. Rapier takes pairs of documents and lled templates indicating the information to be extracted and learns patterns to extract llers for the slots in the template. This proposal presents initial results on a small corpus of computer-related job postings with a preliminary version of Rapier. Future research will involve several enhancements to Rapier as well as more thorough testing on several domains and extension to additional natural language processing tasks. We intend to extend the rule representation and algorithm to allow for more types of constraints than are currently supported. We also plan to incorporate active learning, or sample selection, methods, speci cally query by committee, into Rapier. These methods have the potential to substantially reduce the amount of annotation required. We will explore the issue of distinguishing relevant and irrelevant messages, since currently Rapier only extracts from the any messages given to it, assuming that all are relevant. We also intend to run much larger tests with Rapier on multiple domains including the terrorism domain from the third and fourth Message Uncderstanding Conferences, which will allow comparison against other systems. Finally, we plan to demonstrate the generality of Rapier`s representation and algorithm by applying it to other natural language processing tasks such as word sense disambiguation.",1998.0,"Mary Elaine Cali, R. Mooney, R. Miikkulainen, B. Porter, B. Kuipers, E. Riloff, Ben Kuipers, W. Cohen, R. V. D. Geijn"
f326c0fc1791d66bdd93d87201c5b0b166102aa5,https://www.semanticscholar.org/paper/f326c0fc1791d66bdd93d87201c5b0b166102aa5,Toward General-Purpose Learning for Information Extraction,"Two trends are evident in the recent evolution of the field of information extraction: a preference for simple, often corpus-driven techniques over linguistically sophisticated ones; and a broadening of the central problem definition to include many non-traditional text domains. This development calls for information extraction systems which are as retargetable and general as possible. Here, we describe SRV, a learning architecture for information extraction which is designed for maximum generality and flexibility. SRV can exploit domain-specific information, including linguistic syntax and lexical information, in the form of features provided to the system explicitly as input for training. This process is illustrated using a domain created from Reuters corporate acquisitions articles. Features are derived from two general-purpose NLP systems, Sleator and Temperly's link grammar parser and Wordnet. Experiments compare the learner's performance with and without such linguistic information. Surprisingly, in many cases, the system performs as well without this information as with it.",1998.0,Dayne Freitag
dda99e75f4c54db13bb891060c63cb796cb44466,https://www.semanticscholar.org/paper/dda99e75f4c54db13bb891060c63cb796cb44466,Querying text databases for efficient information extraction,"A wealth of information is hidden within unstructured text. This information is often best exploited in structured or relational form, which is suited for sophisticated query processing, for integration with relational databases, and for data mining. Current information extraction techniques extract relations from a text database by examining every document in the database, or use filters to select promising documents for extraction. The exhaustive scanning approach is not practical or even feasible for large databases, and the current filtering techniques require human involvement to maintain and to adapt to new databases and domains. We develop an automatic query-based technique to retrieve documents useful for the extraction of user-defined relations from large text databases, which can be adapted to new domains, databases, or target relations with minimal human effort. We report a thorough experimental evaluation over a large newspaper archive that shows that we significantly improve the efficiency of the extraction process by focusing only on promising documents.",2003.0,"Eugene Agichtein, L. Gravano"
4b619809b48a2ae6185c4454c3edb28fb53adfa8,https://www.semanticscholar.org/paper/4b619809b48a2ae6185c4454c3edb28fb53adfa8,"A Note on the Unification of Information Extraction and Data Mining using Conditional-Probability, Relational Models","Although information extraction and data mining appear together in many applications, their interface in most current systems would better be described as serial juxtaposition than as tight integration. Information extraction populates slots in a database by identifying relevant subsequences of text, but is usually not aware of the emerging patterns and regularities in the database. Data mining methods begin from a populated database, and are often unaware of where the data came from, or its inherent uncertainties. The result is that the accuracy of both suffers, and significant mining of complex text sources is beyond reach. This position paper proposes the use of unified, relational, undirected graphical models for information extraction and data mining, in which extraction decisions and data-mining decisions are made in the same probabilistic “currency,” with a common inference procedure—each component thus being able to make up for the weaknesses of the other and therefore improving the performance of both. For example, data mining run on a partiallyfilled database can find patterns that provide “topdown” accuracy-improving constraints to information extraction. Information extraction can provide a much richer set of “bottom-up” hypotheses to data mining if the mining is set up to handle additional uncertainty information from extraction. We outline an approach and describe several models, but provide no experimental results.",2003.0,"A. McCallum, David D. Jensen"
2e704a33f74c2da4b65cb5db01b26f305f434bd4,https://www.semanticscholar.org/paper/2e704a33f74c2da4b65cb5db01b26f305f434bd4,Hierarchical Hidden Markov Models for Information Extraction,Information extraction can be defined as the task of automatically extracting instances of specified classes or relations from text. We consider the case of using machine learning methods to induce models for extracting relation instances from biomedical articles. We propose and evaluate an approach that is based on using hierarchical hidden Markov models to represent the grammatical structure of the sentences being processed. Our approach first uses a shallow parser to construct a multi-level representation of each sentence being processed. Then we train hierarchical HMMs to capture the regularities of the parses for both positive and negative sentences. We evaluate our method by inducing models to extract binary relations in three biomedical domains. Our experiments indicate that our approach results in more accurate models than several baseline HMM approaches.,2003.0,"Marios Skounakis, M. Craven, Soumya Ray"
f5bb9f82479da98b835e6d00d8d45df1df7ce823,https://www.semanticscholar.org/paper/f5bb9f82479da98b835e6d00d8d45df1df7ce823,Combining information extraction with genetic algorithms for text mining,"An evolutionary approach that combines information extraction technology and genetic algorithms can produce a new, integrated model for text mining. Text mining discovers unseen patterns in textual databases. We've brought together the benefits of GAs for data mining and IE technology to propose a new approach for high-level knowledge discovery. Unlike previous KDT approaches, our model doesn't rely on external resources or conceptual descriptions. Instead, it performs the discovery using only information from the original corpus of text documents and from training data computed from them. The GA that produces the hypotheses is strongly guided by semantic constraints, which means that several specifically defined metrics evaluate the quality and plausibility.",2004.0,"John A. Atkinson-Abutridy, C. Mellish, Stuart Aitken"
610da54c43bfc34da9b555cd2627d7b7be3e1727,https://www.semanticscholar.org/paper/610da54c43bfc34da9b555cd2627d7b7be3e1727,Scenario Customization for Information Extraction,"Information Extraction (IE) is an emerging NLP technology, whose function is to process unstructured, natural language text, to locate specific pieces of information, or facts, in the text, and to use these facts to fill a database. IE systems today are commonly based on pattern matching. The core IE engine uses a cascade of sets of patterns of increasing linguistic complexity. Each pattern consists of a regular expression and an associated mapping from syntactic to logical form. The pattern sets are customized for each new topic, as defined by the set of facts to be extracted. 
Construction of a pattern base for a new topic is recognized as a time-consuming and expensive process—a principal roadblock to wider use of IE technology in the large. An effective pattern base must be precise and have wide coverage. This thesis addresses the portability problem in two stages. First, we introduce a set of tools for building patterns manually from examples. To adapt the IE system to a new subject domain quickly, the user chooses a set of example sentences from a training text, and specifies how each example maps to the extracted event—its logical form. The system then applies meta-rules to transform the example automatically into a general set of patterns. This effectively shifts the portability bottleneck from building patterns to finding good examples. Second, we propose a novel methodology for discovering good examples automatically from a large un-annotated corpus of text. The system is initially seeded with a small set of good patterns given by the user. An incremental learning procedure then identifies new patterns and classes of related terms on successive iterations. We present experimental results, which confirm that the discovered patterns exhibit high quality, as measured in terms of precision and recall.",2000.0,R. Yangarber
1492e291349fafac0100322e3f2979d6fe09f4a1,https://www.semanticscholar.org/paper/1492e291349fafac0100322e3f2979d6fe09f4a1,Acquisition of Linguistic Patterns for Knowledge-Based Information Extraction,"The paper presents an automatic acquisition of linguistic patterns that can be used for knowledge based information extraction from texts. In knowledge based information extraction, linguistic patterns play a central role in the recognition and classification of input texts. Although the knowledge based approach has been proved effective for information extraction on limited domains, there are difficulties in construction of a large number of domain specific linguistic patterns. Manual creation of patterns is time consuming and error prone, even for a small application domain. To solve the scalability and the portability problem, an automatic acquisition of patterns must be provided. We present the PALKA (Parallel Automatic Linguistic Knowledge Acquisition) system that acquires linguistic patterns from a set of domain specific training texts and their desired outputs. A specialized representation of patterns called FP structures has been defined. Patterns are constructed in the form of FP structures from training texts, and the acquired patterns are tuned further through the generalization of semantic constraints. Inductive learning mechanism is applied in the generalization step. The PALKA system has been used to generate patterns for our information extraction system developed for the fourth Message Understanding Conference (MUC-4). >",1995.0,"Jun-Tae Kim, D. Moldovan"
4870c57ef91f70491d7e515bb1949ed6f2ff040b,https://www.semanticscholar.org/paper/4870c57ef91f70491d7e515bb1949ed6f2ff040b,InfoXtract: A Customizable Intermediate Level Information Extraction Engine,"Abstract Information Extraction (IE) systems assist analysts to assimilate information from electronic documents. This paper focuses on IE tasks designed to support information discovery applications. Since information discovery implies examining large volumes of heterogeneous documents for situations that cannot be anticipated a priori, they require IE systems to have breadth as well as depth. This implies the need for a domain-independent IE system that can easily be customized for specific domains: end users must be given tools to customize the system on their own. It also implies the need for defining new intermediate level IE tasks that are richer than the subject-verb-object (SVO) triples produced by shallow systems, yet not as complex as the domain-specific scenarios defined by the Message Understanding Conference (MUC). This paper describes InfoXtract, a robust, scalable, intermediate-level IE engine that can be ported to various domains. It describes new IE tasks such as synthesis of entity profiles, and extraction of concept-based general events which represent realistic near-term goals focused on deriving useful, actionable information. Entity profiles consolidate information about a person/organization/location etc. within a document and across documents into a single template; this takes into account aliases and anaphoric references as well as key relationships and events pertaining to that entity. Concept-based events attempt to normalize information such as time expressions (e.g., yesterday) as well as ambiguous location references (e.g., Buffalo). These new tasks facilitate the correlation of output from an IE engine with structured data to enable text mining. InfoXtract's hybrid architecture comprised of grammatical processing and machine learning is described in detail. Benchmarking results for the core engine and applications utilizing the engine are presented.",2003.0,"R. Srihari, Wei Li, Thomas L. Cornell, Cheng Niu"
a27b19e0d38ff0199377ea08143e8ad9e7bfb14b,https://www.semanticscholar.org/paper/a27b19e0d38ff0199377ea08143e8ad9e7bfb14b,"Information Extraction: Towards Scalable, Adaptable Systems",Can We Make Information Extraction More Adaptive?.- Natural Language Processing and Digital Libraries.- Natural Language Processing and Information Retrieval.- From Speech to Knowledge.- Relating Templates to Language and Logic.- Inferential Information Extraction.- Knowledge Extraction from Bilingual Corpora.- Engineering of IE Systems: An Object-Oriented Approach.,1999.0,M. Pazienza
41d3bca9e168759799ab36bfcb04153dd095dca7,https://www.semanticscholar.org/paper/41d3bca9e168759799ab36bfcb04153dd095dca7,Information Extraction Principles and Methods for Multispectral and Hyperspectral Image Data,"However, permission to reprint/republish this material for advertising or promotional purposes or for creating other new collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from David Landgrebe and the World Scientific Publishing Company. This work was originally prepared for and appears in Informat ion Process ing for Remote Sens ing , edited by C. H. Chen, published by the World Scientific Publishing Co., Inc., 1060 Main Street, River Edge, NJ 07661, USA (Spring, 1999). In addition to general works such as this one, this book contains sections on Pattern Recognition, SAR Image Processing and Segmentation, Parameter Extraction, Neural Network and Fuzzy Logic Methods, Change Detection, Knowledge-based Methods and Data Fusion, Image Processing Algorithms including wavelet analysis techniques, Image Compression, and Discrimination of Buried Objects. Information Extraction Principles and Methods for Multispectral and Hyperspectral Image Data David Landgrebe School of Electrical & Computer Engineering Purdue University, West Lafayette, IN 47907-1285 Voice: 765-494-3486 Fax: 765-494-3358 landgreb@ecn.purdue.edu",1999.0,D. Landgrebe
7354a886178466d65867eba6a8beb3a430d918d5,https://www.semanticscholar.org/paper/7354a886178466d65867eba6a8beb3a430d918d5,Relational Learning via Propositional Algorithms: An Information Extraction Case Study,"This paper develops a new paradigm for relational learning which allows for the representation and learning of relational information using propositional means. This paradigm suggests different tradeoffs than those in the traditional approach to this problem - the ILP approach - and as a result it enjoys several significant advantages over it. In particular, the new paradigm is more flexible and allows the use of any propositional algorithm, including probabilistic algorithms, within it. 
 
We evaluate the new approach on an important and relation-intensive task - Information Extraction - and show that it outperforms existing methods while being orders of magnitude more efficient.",2001.0,"D. Roth, Wen-tau Yih"
735bcbba68b1f7ec40c14a49d83bf4bc7c3c8736,https://www.semanticscholar.org/paper/735bcbba68b1f7ec40c14a49d83bf4bc7c3c8736,"Information Extraction, SNR Improvement, and Data Compression in Multispectral Imagery","The Karhunen-Loeve transformation is applied to multispectral data for information extraction, SNR improvement, and data compression. When applied in the spectral dimension, the transform provides a set of uncorrelated principal component images very useful in automatic classification and human interpretation. Significant improvements in SNR and estimates of the noise variance are also shown to be possible in the spectral dimension. Data compression results using the transform on one-, two-, and three-dimensional blocks over three general types of terrain are presented.",1973.0,"P. Ready, P. Wintz"
fa442cfd2ccc11475d007c5e77d784ad2b05b27d,https://www.semanticscholar.org/paper/fa442cfd2ccc11475d007c5e77d784ad2b05b27d,Integrating Information to Bootstrap Information Extraction from Web Sites,"In this paper we propose a methodology to learn to extract domain-specific information from large repositories (e.g. the Web) with minimum user intervention. Learning is seeded by integrating information from structured sources (e.g. databases and digital libraries). Retrieved information is then used to bootstrap learning for simple Information Extraction (IE) methodologies, which in turn will produce more annotation to train more complex IE engines. All the corpora for training the IE engines are produced automatically by integrating information from different sources such as available corpora and services (e.g. databases or digital libraries, etc.). User intervention is limited to providing an initial URL and adding information missed by the different modules when the computation has finished. The information added or delete by the user can then be reused providing further training and therefore getting more information (recall) and/or more precision. We are currently applying this methodology to mining web sites of Computer Science departments.",2003.0,"F. Ciravegna, A. Dingli, David Guthrie, Y. Wilks"
39df62f0f1b87ec1685cf28ce795dc5c26e112fe,https://www.semanticscholar.org/paper/39df62f0f1b87ec1685cf28ce795dc5c26e112fe,Using Collocation Statistics in Information Extraction,"Our main objective in participating MUC-7 is to investigate and experiment with the use of collocation statistics in information extraction. A collocation is a habitual word combination, such as \weather a storm"", \ le a lawsuit"", and \the falling yen"". Collocation statistics refers to the frequency counts of the collocational relations extracted from a parsed corpus. For example, out of 6577 instances of \addition"" in a corpus, 5190 was used as the object of \in"". Out of 3214 instances of \hire"", 12 of them take \alien"" as the object.",1998.0,Dekang Lin
87906ba455ad9379c238dd306594d22d8161fa7c,https://www.semanticscholar.org/paper/87906ba455ad9379c238dd306594d22d8161fa7c,Towards Semantic Understanding -- An Approach Based on Information Extraction Ontologies,"Information is ubiquitous, and we are flooded with more than we can process. Somehow, we must rely less on visual processing, point-and-click navigation, and manual decision making and more on computer sifting and organization of information and automated negotiation and decision making. A resolution of these problems requires software with semantic understanding---a grand challenge of our time.More particularly, we must solve problems of automated interoperability, integration, and knowledge sharing, and we must build information agents and process agents that we can trust to give us the information we want and need and to negotiate on our behalf in harmony with our beliefs and goals.This paper proffers the use of information-extraction ontologies as an approach that may lead to semantic understanding.",2004.0,D. Embley
303bf8cd04ed165180bb06d49561e251e57a87e0,https://www.semanticscholar.org/paper/303bf8cd04ed165180bb06d49561e251e57a87e0,Protein Structures and Information Extraction from Biological Texts: The PASTA System,"MOTIVATION
The rapid increase in volume of protein structure literature means useful information may be hidden or lost in the published literature and the process of finding relevant material, sometimes the rate-determining factor in new research, may be arduous and slow.


RESULTS
We describe the Protein Active Site Template Acquisition (PASTA) system, which addresses these problems by performing automatic extraction of information relating to the roles of specific amino acid residues in protein molecules from online scientific articles and abstracts. Both the terminology recognition and extraction capabilities of the system have been extensively evaluated against manually annotated data and the results compare favourably with state-of-the-art results obtained in less challenging domains. PASTA is the first information extraction (IE) system developed for the protein structure domain and one of the most thoroughly evaluated IE system operating on biological scientific text to date.


AVAILABILITY
PASTA makes its extraction results available via a browser-based front end: http://www.dcs.shef.ac.uk/nlp/pasta/. The evaluation resources (manually annotated corpora) are also available through the website: http://www.dcs.shef.ac.uk/nlp/pasta/results.html.",2003.0,"R. Gaizauskas, G. Demetriou, P. Artymiuk, P. Willett"
2c67b238299fc33aba2ecab2622eeedc7c64f5e2,https://www.semanticscholar.org/paper/2c67b238299fc33aba2ecab2622eeedc7c64f5e2,AeroDAML: Applying Information Extraction to Generate DAML Annotations from Web Pages,"The DARPA Agent Markup Language (DAML) is an emerging knowledge representation for the Semantic Web. DAML can encode the semantics of a document for use by agents on the web. However, DAML annotation of documents and web pages is a tedious and time consuming task. AeroDAML is a knowledge markup tool that applies natural language information extraction techniques to automatically generate DAML annotations from web pages. AeroDAML links most proper nouns and common relationships with classes and properties in DAML ontologies. This paper discusses the design of AeroDAML including linguistic and practical issues related to semantic annotation.",2001.0,"P. Kogut, William S. Holmes"
118683d8ef0dbd14e6b41eed9a97a32e762c8b2b,https://www.semanticscholar.org/paper/118683d8ef0dbd14e6b41eed9a97a32e762c8b2b,Towards Semantic Web Information Extraction,"The approach towards Semantic Web Information Extraction (IE) presented here is implemented in KIM – a platform for semantic indexing, annotation, and retrieval. It combines IE based on the mature text engineering platform (GATE1) with Semantic Web-compliant knowledge representation and management. The cornerstone is automatic generation of named-entity (NE) annotations with class and instance references to a semantic repository. Simplistic upper-level ontology, providing detailed coverage of the most popular entity types (Person, Organization, Location, etc.; more than 250 classes) is designed and used. A knowledge base (KB) with de-facto exhaustive coverage of real-world entities of general importance is maintained, used, and constantly enriched. Extensions of the ontology and KB take care of handling all the lexical resources used for IE, most notable, instead of gazetteer lists, aliases of specific entities are kept together with them in the KB. A Semantic Gazetteer uses the KB to generate lookup annotations. Ontologyaware pattern-matching grammars allow precise class information to be handled via rules at the optimal level of generality. The grammars are used to recognize NE, with class and instance information referring to the KIM ontology and KB. Recognition of identity relations between the entities is used to unify their references to the KB. Based on the recognized NE, template relation construction is performed via grammar rules. As a result of the latter, the KB is being enriched with the recognized relations between entities. At the final phase of the IE process, previously unknown aliases and entities are being added to the KB with their specific types.",2003.0,"Borislav Popov, A. Kiryakov, D. Manov, A. Kirilov, Damyan Ognyanoff, Miroslav Goranov"
f18aa6fd43b3de04c54d69f13b917fd6adab4b28,https://www.semanticscholar.org/paper/f18aa6fd43b3de04c54d69f13b917fd6adab4b28,First experiences of using semantic knowledge learned by ASIUM for information extraction task using INTEX,"Our aim in this article is to show how semantic knowledge learned for a specific domain can help the creating of a powerful information extraction system. We describe a first experiment of coupling an information extraction system based and the machine learning system ASIUM. We will show how semantic knowledge learned by ASIUM helps the user to write an information extraction system more efficiently, in reducing the time spent on the development of resources. Our approach will be compared to the European ECRAN project, that aims at the same result, regarding development time and performances.",2000.0,"David Faure, T. Poibeau"
063b4fcf59444df073144675ac2c5fe1a63c233a,https://www.semanticscholar.org/paper/063b4fcf59444df073144675ac2c5fe1a63c233a,Extraction Patterns for Information Extraction Tasks : A Survey,"Information Extraction systems rely on a set of extraction patternsthat they use in order to retrieve from each document the relevant information. In this paper we survey the various types of extraction patterns that are generated by machine learning algorithms. We identify three main categories of patterns, which cover a variety of application domains, and we compare and contrast the patterns from each category.",1999.0,knoblock minton
881006073f308f12e4afdd8c68b8dd61be942365,https://www.semanticscholar.org/paper/881006073f308f12e4afdd8c68b8dd61be942365,Syntactic Analysis in Information Extraction Systems,"We have prepared a set of notes incorporating the visual aids used during the Information Extraction Tu-torial for the IJCAI-99 tuto-rial series. This document also contains additional information , such as the URLs of stes on the World Wide Web containing additional information likely to be of interest. If you are reading this document using an appropriately configured Acrobat Reader (available free from Adobe at http:// w w w. a d o b e. c o m / p r o d i n d e x / a c r o b a t / readstep.html) is appropriately configured, you can go directly to these URLs in your web browser by clicking them. This tutorial is designed to introduce you to the fundamental concepts of information extraction (IE) technology, and to give you an idea of what the state of the art performance in extraction technology is, what is involved in building IE systems, and various approaches taken to their design and implementation, and the kinds of resources and tools that are available to assist in constructing information extraction systems, including linguistic resources such as lexicons and name lists, as well as tools for annotating training data for automatically trained systems. Most IE systems process texts in sequential steps (or "" phases "") ranging from lexical and morphological processing, recognition and typing of proper names, parsing of larger syntactic constituents, resolution of anaphora and coreference, and the ultimate extraction of domain-relevent events and relationships from the text. We discuss each of these system components and various approaches to their design. 2 In addition to these tutorial notes, the authors have prepared several other resources related to information extraction of which you may wish to avail yourself. We have created a web page for this tutorial at the URL mentioned in the Power Point slide in the next illustration. This page provides many links of interest to anyone wanting more information about the field of information extraction, including pointers to research sites, commercial sites, and system development tools. We felt that providing this resource would be appreciated by those taking the tutorial, however, we subject ourselves to the risk that some interesting and relevant information has been inadvertently omitted during our preparations. Please do not interpret the presence or absence of a link to any system or research paper to be a positive or negative evaluation of the system or …",1999.0,"D. Appelt, David J. Israel"
f01732c9be8ccb70b41be008e043187d40be0fdf,https://www.semanticscholar.org/paper/f01732c9be8ccb70b41be008e043187d40be0fdf,Extraction Patterns for Information Extraction Tasks: A Survey,"Information Extraction systems rely on a set of extraction patterns that they use in order to retrieve from each document the relevant information. In this paper we survey the various types of extraction patterns that are generated by machine learning algorithms. We identify three main categories of patterns, which cover a variety of application domains, and we compare and contrast the patterns from each category.",1999.0,Ion Muslea
8010439238109fab8d23b86feff31b55dd94a80c,https://www.semanticscholar.org/paper/8010439238109fab8d23b86feff31b55dd94a80c,Multidocument Summarization via Information Extraction,"We present and evaluate the initial version of RIPTIDES, a system that combines information extraction, extraction-based summarization, and natural language generation to support user-directed multidocument summarization.",2001.0,"Michael White, Tanya Korelsky, Claire Cardie, Vincent Ng, D. Pierce, K. Wagstaff"
3f13a3739c550b99f392e4be14686afe9d7ed74b,https://www.semanticscholar.org/paper/3f13a3739c550b99f392e4be14686afe9d7ed74b,Introduction to Information Extraction,"In recent years, analysts have been confronted with the increasing availability of ondline sources of information in the form of naturaldlanguage texts. This increased accessibility of textual information has led to a corresponding interest in technology for processing this text automatically to extract taskdrelevant information. This demand for a technological solution to the need to deal with the oftendoverwhelming quantity of available information has stimulated the development of the field of Information Extraction. This article provides an overview of the problems addressed, current approaches toward solutions, and assesses the state of the art and its potential for future progress.",1999.0,D. Appelt
bdbbc063489be3db1d533fcb0ef18152cf30b374,https://www.semanticscholar.org/paper/bdbbc063489be3db1d533fcb0ef18152cf30b374,"Information extraction and integration from heterogeneous, distributed, autonomous information sources - a federated ontology-driven query-centric approach","This paper motivates and describes the data integration component of INDUS (intelligent data understanding system) environment for data-driven information extraction and integration from heterogeneous, distributed, autonomous information sources. The design of INDUS is motivated by the requirements of applications such as scientific discovery, in which it is desirable for users to be able to access, flexibly interpret, and analyze data from diverse sources from different perspectives in different contexts. INDUS implements a federated, query-centric approach to data integration using user-specified ontologies.",2003.0,"Jaime Reinoso, A. Silvescu, Doina Caragea, Jyotishman Pathak, Vasant G Honavar"
e2ca689c4eb4284ec10f27cc8c6e5bae73569140,https://www.semanticscholar.org/paper/e2ca689c4eb4284ec10f27cc8c6e5bae73569140,Confidence Estimation for Information Extraction,"Information extraction techniques automatically create structured databases from unstructured data sources, such as the Web or newswire documents. Despite the successes of these systems, accuracy will always be imperfect. For many reasons, it is highly desirable to accurately estimate the confidence the system has in the correctness of each extracted field. The information extraction system we evaluate is based on a linear-chain conditional random field (CRF), a probabilistic model which has performed well on information extraction tasks because of its ability to capture arbitrary, overlapping features of the input in a Markov model. We implement several techniques to estimate the confidence of both extracted fields and entire multi-field records, obtaining an average precision of 98% for retrieving correct fields and 87% for multi-field records.",2004.0,"A. Culotta, A. McCallum"
a47888c0243cac0b173c2748d8ed1b0a2a15fdd8,https://www.semanticscholar.org/paper/a47888c0243cac0b173c2748d8ed1b0a2a15fdd8,Automatic information extraction from large websites,"Information extraction from websites is nowadays a relevant problem, usually performed by software modules called wrappers. A key requirement is that the wrapper generation process should be automated to the largest extent, in order to allow for large-scale extraction tasks even in presence of changes in the underlying sites. So far, however, only semi-automatic proposals have appeared in the literature.We present a novel approach to information extraction from websites, which reconciles recent proposals for supervised wrapper induction with the more traditional field of grammar inference. Grammar inference provides a promising theoretical framework for the study of unsupervised---that is, fully automatic---wrapper generation algorithms. However, due to some unrealistic assumptions on the input, these algorithms are not practically applicable to Web information extraction tasks.The main contributions of the article stand in the definition of a class of regular languages, called the prefix mark-up languages, that abstract the structures usually found in HTML pages, and in the definition of a polynomial-time unsupervised learning algorithm for this class. The article shows that, differently from other known classes, prefix mark-up languages and the associated algorithm can be practically used for information extraction purposes.A system based on the techniques described in the article has been implemented in a working prototype. We present some experimental results on known Websites, and discuss opportunities and limitations of the proposed approach.",2004.0,"Valter Crescenzi, G. Mecca"
6b3066689c273856df4719cb420d85793f74c6f1,https://www.semanticscholar.org/paper/6b3066689c273856df4719cb420d85793f74c6f1,Learning Information Extraction Rules: An Inductive Logic Programming approach,"The objective of this work is to learn information extraction rules by applying Inductive Logic Programming (ILP) techniques to natural language data. The approach is ontology-based, which means that the extraction rules conclude with specific ontology relations that characterise the meaning of sentences in the text. An existing ILP system, FOIL, is used to learn attribute-value relations. This enables instances of these relations to be identified in the text. In specific, we explore the linguistic preprocessing of the data, the use of background knowledge in the learning process, and the practical considerations of applying a supervised learning approach to rule induction, i.e. in terms of the human effort in creating the data set, and in the inherent biases in the use of small data sets.",2002.0,J. S. Aitken
444471fdeb54a87202a20101503ec52c2e16e512,https://www.semanticscholar.org/paper/444471fdeb54a87202a20101503ec52c2e16e512,Information Extraction Supported Question Answering,"Abstract : This paper discusses the use of our information extraction (IE) system, Textract, in the question-answering (QA) track of the recently held TREC-8 tests. One of our major objectives is to examine how IE can help IR (Information Retrieval) in applications like QA. Our study shows: (1) IE can provide solid support for QA; (2) low-level IE like Named Entity tagging is often a necessary component in handling most types of questions; (3) a robust natural language shallow parser provides a structural basis for handling questions; (4) high-level domain independent IE, i.e. extraction of multiple relationships and general events, is expected to bring about a breakthrough in QA.",1999.0,"R. Srihari, W. Li"
e6a439ec029efaa370ac287f4ff9fe576ef0e925,https://www.semanticscholar.org/paper/e6a439ec029efaa370ac287f4ff9fe576ef0e925,The Generic Information Extraction System,"An information extraction system is a cascade of transducers or modules that at each step add structure and often lose information, hopefully irrelevant, by applying rules that are acquired manually and/or automatically.",1993.0,Jerry R. Hobbs
89789c61fa76ff34d5b265f04b4fb04b04b1413d,https://www.semanticscholar.org/paper/89789c61fa76ff34d5b265f04b4fb04b04b1413d,Event coreference for information extraction,"We propose a general approach for performing event coreference and for constructing complex event representations, such as those required for information extraction tasks. Our approach is based on a representation which allows a tight coupling between world or conceptual modelling and discourse modelling. The representation and the coreference mechanism are fully implemented within the LaSIE information extraction system where the mechanism is used for both object (noun phrase) and event coreference resolution. Indirect evaluation of the approach shows small, but significant benefit, for information extraction tasks.",1997.0,"K. Humphreys, R. Gaizauskas, Saliha Azzam"
66940556ee65e9ce9eebce34e106cfd193a1f119,https://www.semanticscholar.org/paper/66940556ee65e9ce9eebce34e106cfd193a1f119,Interactive Information Extraction with Constrained Conditional Random Fields,"Information Extraction methods can be used to automatically ""fill-in"" database forms from unstructured data such as Web documents or email. State-of-the-art methods have achieved low error rates but invariably make a number of errors. The goal of an interactive information extraction system is to assist the user in filling in database fields while giving the user confidence in the integrity of the data. The user is presented with an interactive interface that allows both the rapid verification of automatic field assignments and the correction of errors. In cases where there are multiple errors, our system takes into account user corrections, and immediately propagates these constraints such that other fields are often corrected automatically. 
 
Linear-chain conditional random fields (CRFs) have been shown to perform well for information extraction and other language modelling tasks due to their ability to capture arbitrary, overlapping features of the input in a Markov model. We apply this framework with two extensions: a constrained Viterbi decoding which finds the optimal field assignments consistent with the fields explicitly specified or corrected by the user; and a mechanism for estimating the confidence of each extracted field, so that low-confidence extractions can be highlighted. Both of these mechanisms are incorporated in a novel user interface for form filling that is intuitive and speeds the entry of data--providing a 23% reduction in error due to automated corrections.",2004.0,"T. Kristjansson, A. Culotta, Paul A. Viola, A. McCallum"
2e1fb4b197a7aa5f6fe7d6663ac218fc3da6bff4,https://www.semanticscholar.org/paper/2e1fb4b197a7aa5f6fe7d6663ac218fc3da6bff4,Optimal information extraction in energy-limited wireless sensor networks,"The current practice in wireless sensor networks (WSNs) is to develop functional system designs and protocols for information extraction using intuition and heuristics, and validate them through simulations and implementations. We address the need for a complementary formal methodology by developing nonlinear optimization models of static WSN that yield fundamental performance bounds and optimal designs. We present models both for maximizing the total information gathered subject to energy constraints (on sensing, transmission, and reception), and for minimizing the energy usage subject to information constraints. Other constraints in these models correspond to fairness and channel capacity (assuming noise but no interference). We also discuss extensions of these models that can handle data aggregation, interference, and even node mobility. We present results and illustrations from computational experiments using these models that show how the optimal solution varies as a function of the energy/information constraints, network size, fairness constraints, and reception power. We also compare the performance of some simple heuristics with respect to the optimal solutions.",2004.0,"F. Ordóñez, B. Krishnamachari"
4488e3309517d119080e143f3729106e4bb6d444,https://www.semanticscholar.org/paper/4488e3309517d119080e143f3729106e4bb6d444,Collective Segmentation and Labeling of Distant Entities in Information Extraction,"In information extraction, we often wish to identify all mentions of an entity, such as a person or organization. Traditionally, a group of words is labeled as an entity based only on local information. But information from throughout a document can be useful; for example, if the same word is used multiple times, it is likely to have the same label each time. We present a CRF that explicitly represents dependencies between the labels of pairs of similar words in a document. On a standard information extraction data set, we show that learning these dependencies leads to a 13.7% reduction in error on the field that had caused the most repetition errors.",2004.0,"Charles Sutton, A. McCallum"
f6f9cfb31ab8178456dbc897c812b2d7ad2986f7,https://www.semanticscholar.org/paper/f6f9cfb31ab8178456dbc897c812b2d7ad2986f7,Collective Information Extraction with Relational Markov Networks,"Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks (a generalization of CRFs), which can represent arbitrary dependencies between extractions. This allows for ""collective information extraction"" that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.",2004.0,"Razvan C. Bunescu, R. Mooney"
cf934420cdff8beac7205648502d0becf90500c9,https://www.semanticscholar.org/paper/cf934420cdff8beac7205648502d0becf90500c9,Information Extraction via Double Classification,"Information Extraction is concerned with extracting relevant information from a (collection of) documents. We propose an approach consisting of two classification-based machine learning loops. In a first loop we look for the relevant sentences in a document. In the second loop, we perform a word-level classification. We test the system on the Software Jobs corpus and we do an extensive evaluation in which we discuss the influence of the different parameters. Furthermore we show that the type of evaluation method has an important influence on the results.",2003.0,"A. D. Sitter, Walter Daelemans"
66bf0fe5feb48594ca69608905ec69de6225ea52,https://www.semanticscholar.org/paper/66bf0fe5feb48594ca69608905ec69de6225ea52,Information extraction in molecular biology,"Information extraction has become a very active field in bioinformatics recently and a number of interesting papers have been published. Most of the efforts have been concentrated on a few specific problems, such as the detection of protein-protein interactions and the analysis of DNA expression arrays, although it is obvious that there are many other interesting areas of potential application (document retrieval, protein functional description, and detection of disease-related genes to name a few). Paradoxically, these exciting developments have not yet crystallised into general agreement on a set of standard evaluation criteria, such as the ones developed in fields such as protein structure prediction, which makes it very difficult to compare performance across these different systems. In this review we introduce the general field of information extraction, we outline the status of the applications in molecular biology, and we then discuss some ideas about possible standards for evaluation that are needed for the future development of the field.",2002.0,"C. Blaschke, L. Hirschman, A. Valencia"
91dbb9bc3081a32b257ec7d6afe5b0259f2b7f85,https://www.semanticscholar.org/paper/91dbb9bc3081a32b257ec7d6afe5b0259f2b7f85,Automatic Pattern Acquisition for Japanese Information Extraction,"One of the central issues for information extraction is the cost of customization from one scenario to another. Research on the automated acquisition of patterns is important for portability and scalability. In this paper, we introduce Tree-Based Pattern representation where a pattern is denoted as a path in the dependency tree of a sentence. We outline the procedure to acquire Tree-Based Patterns in Japanese from un-annotated text. The system extracts the relevant sentences from the training data based on TF/IDF scoring and the common paths in the parse tree of relevant sentences are taken as extracted patterns.",2001.0,"Kiyoshi Sudo, S. Sekine, R. Grishman"
b7205c22dd674f9b3edd184cc3c7047b7a1187d6,https://www.semanticscholar.org/paper/b7205c22dd674f9b3edd184cc3c7047b7a1187d6,Testbed for information extraction from deep web,"Search results generated by searchable databases are served dynamically and far larger than the static documents on the Web. These results pages have been referred to as the Deep Web. We need to extract the target data in results pages to integrate them on different searchable databases. We propose a test bed for information extraction from search results. We chose 100 databases randomly from 114,540 pages with search forms. Therefore, these databases have a good variety. We selected 51 databases which include URLs in a results pageand manually identify target information to be extracted. We also suggest evaluation measures for comparing extraction methods and methods for extending the target data.",2004.0,"Yasuhiro Yamada, Nick Craswell, Tetsuya Nakatoh, S. Hirokawa"
0f883074a672f48dc47d53cc1b961c3c283430fd,https://www.semanticscholar.org/paper/0f883074a672f48dc47d53cc1b961c3c283430fd,InfoXtract location normalization: a hybrid approach to geographic references in information extraction,"Ambiguity is very high for location names. For example, there are 23 cities named 'Buffalo' in the U.S. Based on our previous work, this paper presents a refined hybrid approach to geographic references using our information extraction engine InfoXtract. The InfoXtract location normalization module consists of local pattern matching and discourse co-occurrence analysis as well as default senses. Multiple knowledge sources are used in a number of ways: (i) pattern matching driven by local context, (ii) maximum spanning tree search for discourse analysis, and (iii) applying default sense heuristics and extracting default senses from the web. The results are benchmarked with 96% accuracy on our test collections that consist of both news articles and tourist guides. The performance contribution for each component of the module is also benchmarked and discussed.",2003.0,"Huifeng Li, K. Srihari, Cheng Niu, W. Li"
244234efb93afa5a87167571cdb7df64aa26e72b,https://www.semanticscholar.org/paper/244234efb93afa5a87167571cdb7df64aa26e72b,Zone Identification in Biology Articles as a Basis for Information Extraction,"Information extraction (IE) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections. We aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results. In this paper, we enhance the idea proposed in (Mizuta and Collier, 2004); annotating articles in terms of rhetorical zones with shallow nesting. We give a qualitative analysis of the zone identification (ZI) process in biology articles. Specifically, we illustrate the linguistic and other features of each zone based on our investigation of articles selected from four major online journals. We also discuss controversial cases and nested zones, and ZI using multiple features. In doing so, we provide a stronger theoretical and practical support for our framework toward automatic ZI.",2004.0,"Y. Mizuta, Nigel Collier"
be04b5820056ea1216face453bb84c69af6deac0,https://www.semanticscholar.org/paper/be04b5820056ea1216face453bb84c69af6deac0,Exploiting ASP for Semantic Information Extraction,"The paper describes HiLeX, a new ASP-based system for the extraction of information from unstructured documents. Unlike previous systems, which are mainly syntactic, HiLeX combines both semantic and syntactic knowledge for a powerful information extraction. In particular, the exploitation of background knowledge, stored in a domain ontology, allows to empower significantly the information extraction mechanisms. HiLeX is founded on a new two-dimensional representation of documents, and heavily exploits DLP– an extension of disjunctive logic programming for ontology representation and reasoning which has been recently implemented on top of DLV . The domain ontology is represented in DLP, and the extraction patterns are encoded by DLP reasoning modules, whose execution yields the actual extraction of information from the input document. HiLeX allows to extract information from both HTML and flat text documents.",2005.0,"M. Ruffolo, N. Leone, M. Manna, D. Saccá, Amedeo Zavatto"
a9db289b7ba5d9858c30f10001a07fb37dc35ae4,https://www.semanticscholar.org/paper/a9db289b7ba5d9858c30f10001a07fb37dc35ae4,A new approach to intranet search based on information extraction,"This paper is concerned with 'intranet search'. By intranet search, we mean searching for information on an intranet within an organization. We have found that search needs on an intranet can be categorized into types, through an analysis of survey results and an analysis of search log data. The types include searching for definitions, persons, experts, and homepages. Traditional information retrieval only focuses on search of relevant documents, but not on search of special types of information. We propose a new approach to intranet search in which we search for information in each of the special types, in addition to the traditional relevance search. Information extraction technologies can play key roles in such kind of 'search by type' approach, because we must first extract from the documents the necessary information in each type. We have developed an intranet search system called 'Information Desk'. In the system, we try to address the most important types of search first - finding term definitions, homepages of groups or topics, employees' personal information and experts on topics. For each type of search, we use information extraction technologies to extract, fuse, and summarize information in advance. The system is in operation on the intranet of Microsoft and receives accesses from about 500 employees per month. Feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information. This paper describes the architecture, features, component technologies, and evaluation results of the system.",2005.0,"Hang Li, Yunbo Cao, Jun Xu, Yunhua Hu, Shenjie Li, Dmitriy Meyerzon"
37539e25be615837ceb76ceee7af9220a093ba4b,https://www.semanticscholar.org/paper/37539e25be615837ceb76ceee7af9220a093ba4b,The effects of speech recognition and punctuation on information extraction performance,"We report on experiments to measure the effect of speech recognition errors and automatic punctuation insertion errors on the performance of information extraction (entity and relation extraction). The outputs of several recognition systems with a range of word error rates (WER), along with punctuation insertion, were fed into a system that extracts entities and relations from the recognized text. Entity and relation value scores were measured as a function of WER and types of punctuation used. The results of the experiments showed that both entity and relation value scores degrade linearly with increasing WER, with a relative reduction in scores of about twice the WER. The information extraction modules require the inclusion of sentence boundaries, at a minimum; however, the experiments showed that the exact locations of these boundaries are not important for entity and relation extraction. In contrast, when comparing the effects of full punctuation to just automatic sentence boundary insertion, there was a loss in entity value scores of 13.5% and in relation value scores of 25%. Further, commas play a significantly greater role in entity and relation extraction than other types of punctuation.",2005.0,"J. Makhoul, Alex Baron, I. Bulyko, L. Nguyen, L. Ramshaw, D. Stallard, R. Schwartz, Bing Xiang"
a3969a6778427b2c32e57e6f9ee38e7eaea44ee9,https://www.semanticscholar.org/paper/a3969a6778427b2c32e57e6f9ee38e7eaea44ee9,Information Extraction from Voicemail,"In this paper we address the problem of extracting key pieces of information from voicemail messages, such as the identity and phone number of the caller. This task differs from the named entity task in that the information we are interested in is a subset of the named entities in the message, and consequently, the need to pick the correct subset makes the problem more difficult. Also, the caller's identity may include information that is not typically associated with a named entity. In this work, we present three information extraction methods, one based on hand-crafted rules, one based on maximum entropy tagging, and one based on probabilistic transducer induction. We evaluate their performance on both manually transcribed messages and on the output of a speech recognition system.",2001.0,"Jing Huang, G. Zweig, M. Padmanabhan"
aacc2a481a8ae3b1f80bd863db3d525d740cd6d0,https://www.semanticscholar.org/paper/aacc2a481a8ae3b1f80bd863db3d525d740cd6d0,Relational Markov Networks for Collective Information Extraction,"Most information extraction (IE) systems treat separate potential extractions as independent. However, in many cases, considering influences between different potential extractions could improve overall accuracy. Statistical methods based on undirected graphical models, such as conditional random fields (CRFs), have been shown to be an effective approach to learning accurate IE systems. We present a new IE method that employs Relational Markov Networks, which can represent arbitrary dependencies between extractions. This allows for “collective information extraction” that exploits the mutual influence between possible extractions. Experiments on learning to extract protein names from biomedical text demonstrate the advantages of this approach.",2004.0,Razvan Bunescu and Raymond J. Mooney
1dcf0be04808e96e27f99abac52cd222151dcf1d,https://www.semanticscholar.org/paper/1dcf0be04808e96e27f99abac52cd222151dcf1d,Text mining with information extraction,"The popularity of the Web and the large number of documents available in electronic form has motivated the search for hidden knowledge in text collections. Consequently, there is growing research interest in the general topic of text mining. In this dissertation, we develop a text-mining system by integrating methods from Information Extraction (IE) and Data Mining (Knowledge Discovery from Databases or KDD). By utilizing existing IE and KDD techniques, text-mining systems can be developed relatively rapidly and evaluated on existing text corpora for testing IE systems. 
We present a general text-mining framework called DISCOTEX which employs an IE module for transforming natural-language documents into structured data and a KDD module for discovering prediction rules from the extracted data. When discovering patterns in extracted text, strict matching of strings is inadequate because textual database entries generally exhibit variations due to typographical errors, misspellings, abbreviations, and other sources. We introduce the notion of discovering “soft-matching” rules from text and present two new learning algorithms. TEXTRISE is an inductive method for learning soft-matching prediction rules that integrates rule-based and instance-based learning methods. Simple, interpretable rules are discovered using rule induction, while a nearest-neighbor algorithm provides soft matching. SOFTAPRIORI is a text-mining algorithm for discovering association rules from texts that uses a similarity measure to allow flexible matching to variable database items. We present experimental results on inducing prediction and association rules from natural-language texts demonstrating that TEXTRISE and SOFTA PRIORI learn more accurate rules than previous methods for these tasks. We also present an approach to using rules mined from extracted data to improve the accuracy of information extraction. Experimental results demonstrate that such discovered patterns can be used to effectively improve the underlying IE method.",2004.0,"Un Yong Nahm, Raymond J. Mooney"
41be61fd94bdced0d728eb9c122f168c73007aa7,https://www.semanticscholar.org/paper/41be61fd94bdced0d728eb9c122f168c73007aa7,A Statistical Information Extraction System for Turkish,"Information Extraction (IE) is the process of analyzing natural language text or speech, and collecting information about speciied types of entities, relationships, or events, such as marking person, location, and organization names or determining the topic changes. Although in recent years there have been numerous studies in processing Turk-ish text, we are not aware of any studies of developing an IE system for Turkish. Furthermore, neither of these studies on processing Turk-ish text employ statistical techniques due to the problems in nding a training corpus and adapting these techniques to Turkish. As a doctoral thesis, I propose to investigate and develop a statistical information extraction system for Turkish.",1999.0,"Gökhan Tür, Dilek Z. Hakkani-Tür, Kemal Oflazer"
bc2aef3cd344cbbf32b340e2a97f6de089700101,https://www.semanticscholar.org/paper/bc2aef3cd344cbbf32b340e2a97f6de089700101,Infrastructure for open-domain information extraction,"The problem of performing open-domain Information Extraction (IE) was historically tied to the problem of ad-hoc acquisition of extraction patterns. In this paper we show that this requirement is not sufficient and that we also need to build new IE architectures that combine the role of linguistic patterns with coreference knowledge and ambiguous syntactic and semantic information. We present the implementation of a novel IE architecture, namely the CICERO system and show how (1) both high precision and high recall results were obtained for a variety of extraction domains; and (2) how textual information can be extracted for virtually any domain in a precise and reliable way. The evaluation of CICERO's performance shows a significant improvement over MUC IE systems.",2002.0,"M. Surdeanu, S. Harabagiu"
3da732ce917e0a22a5a3555fd0922ab9338d7edf,https://www.semanticscholar.org/paper/3da732ce917e0a22a5a3555fd0922ab9338d7edf,Information extraction by text classification,"Information extraction and text classification are usually seen as complementary forms of shallow text processing, in that they are aimed at very different tasks. In this paper, we describe two simple but real-world domains in which text classification techniques can be used directly for information extraction. Specifically, we describe systems for extracting information from business cards, and for automatically processing “change of address” email messages, that are based primarily on text classification techniques. Our main technical contribution is a novel integration of hidden Markov models and text classifiers.",2001.0,"S. Mermelstein, Park Street, Newton"
494030430c01b6b407c4f1d831b4a74d40a23fda,https://www.semanticscholar.org/paper/494030430c01b6b407c4f1d831b4a74d40a23fda,Relational learning techniques for natural language information extraction,"The recent growth of online information available in the form of natural language documents creates a greater need for computing systems with the ability to process those documents to simplify access to the information. One type of processing appropriate for many tasks is information extraction, a type of text skimming that retrieves specific types of information from text. Although information extraction systems have existed for two decades, these systems have generally been built by hand and contain domain specific information, making them difficult to port to other domains. A few researchers have begun to apply machine learning to information extraction tasks, but most of this work has involved applying learning to pieces of a much larger system. This dissertation presents a novel rule representation specific to natural language and a relational learning system, R scAPIER, which learns information extraction rules. R scAPIER takes pairs of documents and filled templates indicating the information to be extracted and learns pattern-matching rules to extract fillers for the slots in the template. The system is tested on several domains, showing its ability to learn rules for different tasks. R scAPIER's performance is compared to a propositional learning system for information extraction, demonstrating the superiority of relational learning for some information extraction tasks. 
Because one difficulty in using machine learning to develop natural language processing systems is the necessity of providing annotated examples to supervised learning systems, this dissertation also describes an attempt to reduce the number of examples R scAPIER requires by employing a form of active learning. Experimental results show that the number of examples required to achieve a given level of performance can be significantly reduced by this method.",1998.0,"Mary Elaine Califf, R. Mooney"
efcd89cc9aa54be0f7b7b401f1a38496b2b80705,https://www.semanticscholar.org/paper/efcd89cc9aa54be0f7b7b401f1a38496b2b80705,Using a semantic network for information extraction,"This paper describes the approach to knowledge representation taken in the LaSIE Information Extraction (IE) system. Unlike many IE systems that skim texts and use large collections of shallow, domain-specific patterns and heuristics to fill in templates, LaSIE attempts a fuller text analysis, first translating individual sentences to a quasi-logical form, and then constructing a weak discourse model of the entire text from which template fills are finally derived. Underpinning the system is a general ‘world model’, represented as a semantic net, which is extended during the processing of a text by adding the classes and instances described in that text. In the paper we describe the system's knowledge representation formalisms, their use in the IE task, and how the knowledge represented in them is acquired, including experiments to extend the system's coverage using the WordNet general purpose semantic network. Preliminary evaluations of our approach, through the Sixth DARPA Message Understanding Conference, indicate comparable performance to shallower approaches. However, we believe its generality and extensibility offer a route towards the higher precision that is required of IE systems if they are to become genuinely usable technologies.",1997.0,"R. Gaizauskas, K. Humphreys"
8192cb9090844dc369959dc08d9f06b9323d59d7,https://www.semanticscholar.org/paper/8192cb9090844dc369959dc08d9f06b9323d59d7,Quantitative evaluation of coreference algorithms in an information extraction system,"Algorithms for performing coreference resolution can only be precisely evaluated given a benchmark corpus of coreference-annotated texts, together with techniques for evaluating the algorithms' output against the corpus. Such a corpus and such techniques have become available for the rst time as part of the Message Understanding Conference 6 (MUC-6) evaluations of information extraction systems. In this paper we describe the MUC-6 coreference task and the approach to taken to it by the Large Scale Information Extraction (LaSIE) system developed at the University of Sheeeld. The basic coreference algorithm used by this system is described in detail, as well as a set of variants, which allow us to experiment with diierent constraints such as restrictions to certain classes of anaphor, distance restrictions between anaphor and antecedent, and weighting factors in assessing semantic similarity of potential coreferents. Quantitative evaluation results are presented for these variants, demonstrating both the utility of quantative analysis for assessing coreference algorithms and the exibility of our approach to coreference which provides a framework that facilitates experimentation with alternative techniques.",2000.0,"R. Gaizauskas, K. Humphreys"
feb120cb4774fe93f020aafc5fa426aa6c8ab036,https://www.semanticscholar.org/paper/feb120cb4774fe93f020aafc5fa426aa6c8ab036,Recommendation system using feature extraction and pattern recognition in clinical care systems,"ABSTRACT Medical information systems have been increasingly facilitating and improving the quality of health monitoring, disease-trend modelling and early intervention with evidence-based medical treatment by data mining and feature extraction. Such systems are part of the enterprise information system of the healthcare organisations. We proposed a new algorithm fb-kNN towards recommendation algorithms based on analysis of the patterns of diseases with patterns in human body, which was then implemented in Healthcare 4.0 for the recommendation of diagnosis and treatment. Our developed tool is a complete package solution for the Enterprise Management System (ERP) which shows improvement in healthcare, reducing chronic diseases and mortality rates.",2018.0,"U. Bhatti, Mengxing Huang, Di Wu, Yu Zhang, Anum Mehmood, Huirui Han"
8a9ca89285620840ae770f8e90438a1df919ef22,https://www.semanticscholar.org/paper/8a9ca89285620840ae770f8e90438a1df919ef22,Template Driven Information Extraction for Populating Ontologies,"We address the integration of information extraction (IE) and ontologies. In particular, using an ontology to aid the IE process, and using the IE results to help populate the ontology. We perform IE by means of domain specific templates and the lightweight use of Natural Languages Processing techniques (NLP). Our main goal is to learn information from text by the use of templates and in this way to alleviate the main bottleneck in creating knowledge-base systems that is ""the extraction of knowledge"". Our domain of study is ""KMi Planet"", a Web-based news server for communication of stories between members in our institute. The main goals of our system are to classify an incoming story, obtain the relevant objects within the story, deduce the relationships between them, and to populate the ontology. Furthermore, we aim to do this with minimal help from the user.",2001.0,"M. Vargas-Vera, J. Domingue, Y. Kalfoglou, E. Motta, S. B. Shum"
61831713ee0c1ff6ab33997f8dc0ccb906ea0961,https://www.semanticscholar.org/paper/61831713ee0c1ff6ab33997f8dc0ccb906ea0961,Experiments with geographic knowledge for information extraction,"Here we present work on using spatial knowledge in conjunction with information extraction (IE). Considerable volume of location data was imported in a knowledge base (KB) with entities of general importance used for semantic annotation, indexing, and retrieval of text. The Semantic Web knowledge representation standards are used, namely RDF(S). An extensive upper-level ontology with more than two hundred classes is designed. With respect to the locations, the goal was to include the most important categories considering public and tasks not specially related to geography or related areas. The locations data is derived from number of publicly available resources and combined to assure best performance for domain-independent named-entity recognition in text. An evaluation and comparison to high performance IE application is given.",2003.0,"D. Manov, A. Kiryakov, Borislav Popov, Kalina Bontcheva, D. Maynard, H. Cunningham"
309ac2ea358e710bf7b637f6f15eed85f3ea0f86,https://www.semanticscholar.org/paper/309ac2ea358e710bf7b637f6f15eed85f3ea0f86,A Pragmatic Information Extraction Strategy for Gathering Data on Genetic Interactions,"We present in this paper a pragmatic strategy to perform information extraction from biologic texts. Since the emergence of the information extraction field, techniques have evolved, become more robust and proved their efficiency on specific domains. We are using a combination of existing linguistic and knowledge processing tools to automatically extract information about gene interactions in the literature. Our ultimate goal is to build a network of gene interactions. The methodologies used and the current results are discussed in this paper.",2000.0,"Denys Proux, F. Rechenmann, L. Julliard"
c94824423c2aab1899681a3235bab68020ca8294,https://www.semanticscholar.org/paper/c94824423c2aab1899681a3235bab68020ca8294,An Overview of Temporal Information Extraction,"Research of temporal Information Extraction was regarded as a subtask of named entity recognition in 1990's. To date, the scope of this research is broadened, ranging from temporal expression extraction and annotation to temporal reasoning and understanding. This area of research is now a hot NLP topic and the results are applicable to question answering, information extraction, text summarization, etc. This paper presents the past, present and future research development in temporal information extraction.",2005.0,"Kam-Fai Wong, Yunqing Xia, Wenjie Li, C. Yuan"
dab48886932ffb7a3e2c3f2803005bb34345cdee,https://www.semanticscholar.org/paper/dab48886932ffb7a3e2c3f2803005bb34345cdee,Wrap-Up: a Trainable Discourse Module for Information Extraction,"The vast amounts of on-line text now available have led to renewed interest in information extraction (IE) systems that analyze unrestricted text, producing a structured representation of selected information from the text. This paper presents a novel approach that uses machine learning to acquire knowledge for some of the higher level IE processing. Wrap-Up is a trainable IE discourse component that makes intersentential inferences and identifies logical relations among information extracted from the text. Previous corpus-based approaches were limited to lower level processing such as part-of-speech tagging, lexical disambiguation, and dictionary construction. Wrap-Up is fully trainable, and not only automatically decides what classifiers are needed, but even derives the feature set for each classifier automatically. Performance equals that of a partially trainable discourse module requiring manual customization for each domain.",1994.0,"S. Soderland, W. Lehnert"
928b9e8f51c42e9a38b9121a3d74d45c02beb262,https://www.semanticscholar.org/paper/928b9e8f51c42e9a38b9121a3d74d45c02beb262,TEG: a hybrid approach to information extraction,"This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labor by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (Trainable Extraction Grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (Stochastic Context Free Grammar) based extraction language, and training them using an annotated corpus. The system does not contain any purely linguistic components, such as PoS tagger or parser. We demonstrate the performance of the system on several named entity extraction and relation extraction tasks. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amount of training data. The improvement in accuracy is slight for named entity extraction task and more pronounced for relation extraction.",2004.0,"Binyamin Rosenfeld, Ronen Feldman, Moshe Fresko, Jonathan Schler, Y. Aumann"
ebbe52030e2b2f16c23f7d11a7415f1f4d221a03,https://www.semanticscholar.org/paper/ebbe52030e2b2f16c23f7d11a7415f1f4d221a03,Diversity of Scenarios in Information extraction,"This paper presents problems of template structure for Information Extraction. We investigate these problems in the context of two new Information Extraction scenarios which are linguistically and structurally more challenging than the traditional MUC scenarios. By a scenario we mean a predeﬁned set of facts to be extracted from text. Traditional views on event structure and template design are not adequate for the more complex scenarios. We identify two structural factors that contribute to the complexity of a scenario: ﬁrst, the scattering of events in text, and second, inclusion relationship between events. These factors cause difﬁculty in representing the facts in an unambiguous way. Traditional views on event structure and template design are not adequate for the more complex scenarios. We propose that these kinds of event relationships can be better described with a modular, hierarchical model.",2002.0,"Silja Huttunen, R. Yangarber, R. Grishman"
0756a23132ad6ff98eaf5e7d72aa3b2363dc7313,https://www.semanticscholar.org/paper/0756a23132ad6ff98eaf5e7d72aa3b2363dc7313,A formal framework for evaluation of information extraction,"An important problem in the field of Information Extraction (IE) is the lack of clear guidelines for evaluating the correctness of the output generated by an extraction algorithm. This paper tries to handle this problem by providing a formal framework for IE and its evaluation. We define IE in two different, but frequently used approaches: the “All Occurrences” and the “One Best per Document” settings, and we give a formal approach for evaluating an IE system in both settings. Our approach is based on the observation that most commonly used evaluation measures use the confusion matrix as a basis for their computation. We also shortly discuss the most frequently used evaluation measures.",2004.0,"A. D. Sitter, T. Calders, Walter Daelemans"
bdc08721414c972ab451f8ef3ef39d63c741b324,https://www.semanticscholar.org/paper/bdc08721414c972ab451f8ef3ef39d63c741b324,Automatically Constructing a Dictionary for Information Extraction Tasks,"Knowledge-based natural language processing systems have achieved good success with certain tasks but they are often criticized because they depend on a domain-specific dictionary that requires a great deal of manual knowledge engineering. This knowledge engineering bottleneck makes knowledge-based NLP systems impractical for real-world applications because they cannot be easily scaled up or ported to new domains. In response to this problem, we developed a system called AutoSlog that automatically builds a domain-specific dictionary of concepts for extracting information from text. Using AutoSlog, we constructed a dictionary for the domain of terrorist event descriptions in only 5 person-hours. We then compared the AutoSlog dictionary with a hand-crafted dictionary that was built by two highly skilled graduate students and required approximately 1500 person-hours of effort. We evaluated the two dictionaries using two blind test sets of 100 texts each. Overall, the AutoSlog dictionary achieved 98% of the performance of the hand-crafted dictionary. On the first test set, the AutoSlog dictionary obtained 96.3% of the performance of the hand-crafted dictionary. On the second test set, the overall scores were virtually indistinguishable with the AutoSlog dictionary achieving 99.7% of the performance of the handcrafted dictionary.",1993.0,E. Riloff
eca8d0c78af7917cd774bbe1b56e62d61bd74cc0,https://www.semanticscholar.org/paper/eca8d0c78af7917cd774bbe1b56e62d61bd74cc0,ViPER: augmenting automatic information extraction with visual perceptions,"In this paper we address the problem of unsupervised Web data extraction. We show that unsupervised Web data extraction becomes feasible when supposing pages that are made up of repetitive patterns, as it is the case, e.g., for search engine result pages. Hereby the extraction rules are generated automatically without any training or human interaction, by means of operating on the DOM tree respectively the flat tag token sequence of a single page.Our contribution to automatic data extraction through this paper is twofold. First, we identify and rank potential repetitive patterns with respect to the user's visual perception of the Web page, well aware that location and size of matching elements within a Web page constitute important criteria for defining relevance. Second, matching sub-sequences of the pattern with the highest weightiness are aligned with global multiple sequence alignment techniques. Experimental results show that our system is able to achieve high accuracy in distilling and aligning regularly structured objects inside complex Web pages.",2005.0,"Kai Simon, G. Lausen"
71522cad10a2843568073ce42c4f18ffd4107cd2,https://www.semanticscholar.org/paper/71522cad10a2843568073ce42c4f18ffd4107cd2,Argumentative Zoning : Information Extraction from Scientific Text,"We present a new type of analysis for scientific text which we call Argumentative Zoning. We demonstrate that this type of text analysis can be used for generating usertailored and task-tailored summaries and for performing more informative citation analyses. We also demonstrate that our type of analysis can be applied to unrestricted text, both automatically and by humans. The corpus we use for the analysis (80 conference papers in computational linguistics) is a difficult test bed; it shows great variation with respect to subdomain, writing style, register and linguistic expression. We present reliability studies which we performed on this corpus and for which we use two unrelated trained annotators. The definition of our seven categories (argumentative zones) is not specific to the domain, only to the text type; it is based on the typical argumentation to be found in scientific articles. It reflects the attribution of intellectual ownership in scientific articles, expressions of authors’ stance towards other work, and typical statements about problem-solving processes. On the basis of sentential features, we use two statistical models (a Naive Bayesian model and an ngram model operating over sentences) to estimate a sentence’s argumentative status, taking the hand-annotated corpus as training material. An alternative, symbolic system uses the features in a rule-based way. The general working hypothesis of this thesis is that empirical discourse studies can contribute to practical document management problems: the analysis of a significant amount of naturally occurring text is essential for discourse linguistic theories, and the application of a robust discourse and argumentation analysis can make text understanding techniques for practical document management more robust.",1999.0,Simone Teufel
f725d7eadeb312f07035017682c88297271e3188,https://www.semanticscholar.org/paper/f725d7eadeb312f07035017682c88297271e3188,Visual sign information extraction and identification by deformable models for intelligent vehicles,"This paper deals with the extraction of part of the visual information presented in streets, roads, and motorways. This information, provided by either traffic or road signs and route-guidance signs, is extremely important for safe and successful driving. An automatic system that is capable of extracting and identifying these signs automatically would help human drivers enormously; navigation would be easier and would allow him or her to concentrate on driving the vehicle. The system would indicate to the driver the presence of a sign in advance, so that some incorrect human decisions could be avoided. A deformable model scheme allows us to include the knowledge used while designing the signs in the algorithm and is used for their detection and identification. Two techniques to find the minimum in the energy function are shown: simulated annealing and genetic algorithms. Some problems are addressed, such as uncontrolled lighting conditions; occlusions; and variations in shape, size, and color.",2004.0,"A. D. L. Escalera, Jose M. Armingol, J. M. Pastor, F. Rodríguez"
9b270077df7338bdbbcf23e26f89555fbe1f3914,https://www.semanticscholar.org/paper/9b270077df7338bdbbcf23e26f89555fbe1f3914,Robust Information Extraction with Perceptrons,"We present a system for the extraction of entity and relation mentions. Our work focused on robustness and simplicity: all system components are modeled using variants of the Perceptron algorithm (Rosemblatt, 1858) and only partial syntactic information is used for feature extraction. Our approach has two novel ideas. First, we define a new large-margin Perceptron algorithm tailored for classunbalanced data which dynamically adjusts its margins, according to the generalization performance of the model. Second, we propose a novel architecture that lets classification ambiguities flow through the system and solves them only at the end. The system achieves competitive accuracy on the ACE English EMD and RMD tasks.",2007.0,"M. Surdeanu, Massimiliano Ciaramita"
1ed5f7ff2ea4bfae8f93b17546a71631880277d0,https://www.semanticscholar.org/paper/1ed5f7ff2ea4bfae8f93b17546a71631880277d0,Accurate Information Extraction from Research Papers using Conditional Random Fields,"With the increasing use of research paper search engines, such as CiteSeer, for both literature search and hiring decisions, the accuracy of such systems is of paramount importance. This paper employs Conditional Random Fields (CRFs) for the task of extracting various common fields from the headers and citation of research papers. The basic theory of CRFs is becoming well-understood, but best-practices for applying them to real-world data requires additional exploration. This paper makes an empirical exploration of several factors, including variations on Gaussian, exponential and hyperbolic-L1 priors for improved regularization, and several classes of features and Markov order. On a standard benchmark data set, we achieve new state-of-the-art performance, reducing error in average F1 by 36%, and word error rate by 78% in comparison with the previous best SVM results. Accuracy compares even more favorably against HMMs.",2004.0,"Fuchun Peng, A. McCallum"
c556f2349d19c9f41247951da2b93ef47841b877,https://www.semanticscholar.org/paper/c556f2349d19c9f41247951da2b93ef47841b877,Ontology-based design information extraction and retrieval,"Because of the increasing complexity of products and the design process, as well as the popularity of computer-aided documentation tools, the number of electronic and textual design documents being generated has exploded. The availability of such extensive document resources has created new challenges and opportunities for research. These include improving design information retrieval to achieve a more coherent environment for design exploration, learning, and reuse. One critical issue is related to the construction of a structured representation for indexing design documents that record engineers' ideas and reasoning processes for a specific design. This representation should explicitly and accurately capture the important design concepts as well as the relationships between these concepts so that engineers can locate their documents of interest with less effort. For design information retrieval, we propose to use shallow natural language processing and domain-specific design ontology to automatically construct a structured and semantics-based representation from unstructured design documents. The design concepts and relationships of the representation are recognized from the document based on the identified linguistic patterns. The recognized concepts and relationships are joined to form a concept graph. The integration of these concept graphs builds an application-specific design ontology, which can be seen as the structured representation of the content of the corporate document repository, as well as an automatically populated knowledge base from previous designs. To improve the performance of design information retrieval, we have developed ontology-based query processing, where users' requests are interpreted based on their domain-specific meanings. Our approach contrasts with the traditionally used keyword-based search. An experiment to test the retrieval performance is conducted by using the design documents from a product design scenario. The results demonstrate that our method outperforms the keyword-based search techniques. This research contributes to the development and use of engineering ontology for design information retrieval.",2007.0,"Zhanjun Li, K. Ramani"
9103083854bd077c4c6422a4fdce1dd07bebdcbe,https://www.semanticscholar.org/paper/9103083854bd077c4c6422a4fdce1dd07bebdcbe,Creating probabilistic databases from information extraction models,"Many real-life applications depend on databases automatically curated from unstructured sources through imperfect structure extraction tools. Such databases are best treated as imprecise representations of multiple extraction possibli-ties. State-of-the-art statistical models of extraction provide a sound probability distribution over extractions but are not easy to represent and query in a relational framework. In this paper we address the challenge of approximating such distributions as imprecise data models. In particular, we investigate a model that captures both row-level and column-level uncertainty and show that this representation provides significantly better approximation compared to models that use only row or only column level uncertainty. We present efficient algorithms for finding the best approximating parameters for such a model: our algorithm exploits the structure of the model to avoid enumerating the exponential number of extraction possibilities.",2006.0,"Rahul Gupta, Sunita Sarawagi"
b0edb04e3e4336bf309c817a5eaff378747a1942,https://www.semanticscholar.org/paper/b0edb04e3e4336bf309c817a5eaff378747a1942,"Web Table Extraction, Retrieval and Augmentation","This tutorial synthesizes and presents research on web tables over the past two decades. We group the tasks into six main categories of information access tasks: (i) table extraction, (ii) table interpretation, (iii) table search, (iv) question answering on tables, (v) knowledge base augmentation, and (vi) table completion. For each category, we identify and introduce seminal approaches, present relevant resources, and point out interdependencies among the different tasks.",2019.0,"Shuo Zhang, K. Balog"
400cf0a4a689f65681a4c618471387ea61598283,https://www.semanticscholar.org/paper/400cf0a4a689f65681a4c618471387ea61598283,Methods for Domain-Independent Information Extraction from the Web: An Experimental Comparison,"Our KNOWITALL system aims to automate the tedious process of extracting large collections of facts (e.g., names of scientists or politicians) from the Web in an autonomous, domain-independent, and scalable manner. In its first major run, KNOWITALL extracted over 50,000 facts with high precision, but suggested a challenge: How can we improve KNOWITALL's recall and extraction rate without sacrificing precision? 
 
This paper presents three distinct ways to address this challenge and evaluates their performance. Rule Learning learns domain-specific extraction rules. Subclass Extraction automatically identifies sub-classes in order to boost recall. List Extraction locates lists of class instances, learns a ""wrapper"" for each list, and extracts elements of each list. Since each method bootstraps from KNOWITALL's domain-independent methods, no hand-labeled training examples are required. Experiments show the relative coverage of each method and demonstrate their synergy. In concert, our methods gave KNOWITALL a 4-fold to 19-fold increase in recall, while maintaining high precision, and discovered 10,300 cities missing from the Tipster Gazetteer.",2004.0,"Oren Etzioni, Michael J. Cafarella, Doug Downey, Ana-Maria Popescu, T. Shaked, S. Soderland, Daniel S. Weld, A. Yates"
2719f7bb6512f647cfdf5258a3795edc55995de4,https://www.semanticscholar.org/paper/2719f7bb6512f647cfdf5258a3795edc55995de4,Multi-Field Information Extraction and Cross-Document Fusion,"In this paper, we examine the task of extracting a set of biographic facts about target individuals from a collection of Web pages. We automatically annotate training text with positive and negative examples of fact extractions and train Rote, Naive Bayes, and Conditional Random Field extraction models for fact extraction from individual Web pages. We then propose and evaluate methods for fusing the extracted information across documents to return a consensus answer. A novel cross-field bootstrapping method leverages data interdependencies to yield improved performance.",2005.0,"Gideon S. Mann, David Yarowsky"
d845f95f9f4f5f6930864b4571598197187f2344,https://www.semanticscholar.org/paper/d845f95f9f4f5f6930864b4571598197187f2344,"Multimedia document presentation, information extraction, and document formation in MINOS: a model and a system","MINOS is an object-oriented multimedia information system that provides integrated facilities for creating and managing complex multimedia objects. In this paper the model for multimedia documents supported by MINOS and its implementation is described. Described in particular are functions provided in MINOS that exploit the capabilities of a modern workstation equipped with image and voice input-output devices to accomplish an active multimedia document presentation and browsing within documents. These functions are powerful enough to support a variety of office applications. Also described are functions provided for the extraction of information from multimedia documents that exist in a large repository of information (multimedia document archiver) and functions that select and transform this information. Facilities for information sharing among objects of the archiver are described; an interactive multimedia editor that is used for the extraction and interactive creation of new information is outlined; finally, a multimedia document formatter that is used to synthesize a new multimedia document from extracted and interactively generated information is presented.
This prototype system runs on a SUN-3 workstation running UNIX'"". An Instavox, directly addressable, analog device is used to store voice segments.",1986.0,"S. Christodoulakis, M. Theodoridou, F. Ho, M. Papa, A. Pathria"
708077c0ae713e1b022d6cb1230502d0b6c9030f,https://www.semanticscholar.org/paper/708077c0ae713e1b022d6cb1230502d0b6c9030f,Information Extraction,"In 2001 the U.S. Department of Labor was tasked with building a Web site that would help people find continuing education opportunities at community colleges, universities, and organizations across the country. The department wanted its Web site to support fielded Boolean searches over locations, dates, times, prerequisites, instructors, topic areas, and course descriptions. Ultimately it was also interested in mining its new database for patterns and educational trends. This was a major data-integration project, aiming to automatically gather detailed, structured information from tens of thousands of individual institutions every three months.",2005.0,A. McCallum
2256993ba4543b860e1dbfa362667147d97f5591,https://www.semanticscholar.org/paper/2256993ba4543b860e1dbfa362667147d97f5591,An Approach to Text Mining using Information Extraction,"In this paper we describe our approach to Text Mining by introducing TextMiner. We perform term and event extraction on each document to find features that are likely to have meaning in the domain, and then apply mining on the extracted features labelling each document. The system consists of two major components, the Text Analysis component and the Data Mining component. The Text Analysis component converts semi structured data such as documents into structured data stored in a database. The second component applies data mining techniques on the output of the first component. We apply our approach in the financial domain (financial documents collection) and our main targets are: a) To manage all the available information, for example classify documents in appropriate categories and b) To “mine” the data in order to “discover” useful knowledge. This work is designed to primarily support two languages, i.e. English and Greek.",2000.0,"H. Karanikas, Christos Tjortjis"
5d2d6034c5afd4ab047fe4687d47559722142c90,https://www.semanticscholar.org/paper/5d2d6034c5afd4ab047fe4687d47559722142c90,Representing Sentence Structure in Hidden Markov Models for Information Extraction,We study the application of Hidden Markov Models (HMMs) to learning information extractors for -ary relations from free text. We propose an approach to representing the grammatical structure of sentences in the states of the model. We also investigate using an objective function during HMM training which maximizes the ability of the learned models to identify the phrases of interest. We evaluate our methods by deriving extractors for two binary relations in biomedical domains. Our experiments indicate that our approach learns more accurate models than several baseline approaches.,2001.0,"Soumya Ray, M. Craven"
0200c5721c0ac448587a3099ade12d44d292dd31,https://www.semanticscholar.org/paper/0200c5721c0ac448587a3099ade12d44d292dd31,Information Extraction and Integration with Florid: The MONDIAL Case Study,For accessing and processing this information provided on the Web there is a need for integration of data from di erent heterogeneous sources Languages for this purpose have to serve for querying the web extracting information from semistructured data and restructuring the results In LHL we argue that languages supporting deduction and object orientation are particularly suited in this context we proposed a formal model for querying structure and contents of Web data A main advantage of our approach is that it brings together the above mentioned issues in a uni ed formal framework The approach is implemented in the Florid system HLLS which is an implementation of the deductive objectorientes database language F Logic KLW This report substantiates the above claims by a case study using Florid We show how several information sources on the Web containing political and geographical data are integrated to a geographical database using Florid The case study illustrates the trade o gained from an integrated Web querying and data manipulation language supporting a concise and elegant programming style Using a deductive language a process of rapid prototyping and re ne ment of the program implementing both a wrapper and a mediator can be easily followed the program consists of a skeleton of generic wrapping rules MHLL augmented by re ning rules and application speci c rules Homepage of the Mondial Case Study http www informatik uni freiburg de may Mondial,1999.0,Wolfgang May
0b550e93b6126e47a3374c0085a5af83f99ce803,https://www.semanticscholar.org/paper/0b550e93b6126e47a3374c0085a5af83f99ce803,Metrics for Evaluation of Ontology-based Information Extraction,"The evaluation of the quality of ontological classification is an important part of semantic web technology. Because this area is under constant development, it requires improvement and standardisation. This paper discusses existing evaluation metrics, and proposes a new method for evaluating the ontology population task, which is general enough to be used in a variety of situations, yet more precise than many current metrics. The paper further describes our first eorts in operationalising the evaluation procedure, including the creation of a semantically annotated corpus that will function as a test bed for the proposed evaluation mechanism, and comparison of dierent evaluation metrics. We conclude that for ontology-based evaluation, a more complex mechanism than is traditionally used is preferable. This mechanism aims to drive a benchmarking assessment tool for the current state-of-the-art of ontology population, and to set a standard for best practice for future evaluation of human language technology for the semantic web.",2006.0,"D. Maynard, Wim Peters, Yaoyong Li"
ad18943eecdfef24f7636dcfec698388a02e4229,https://www.semanticscholar.org/paper/ad18943eecdfef24f7636dcfec698388a02e4229,Information extraction from sound for medical telemonitoring,"Today, the growth of the aging population in Europe needs an increasing number of health care professionals and facilities for aged persons. Medical telemonitoring at home (and, more generally, telemedicine) improves the patient's comfort and reduces hospitalization costs. Using sound surveillance as an alternative solution to video telemonitoring, this paper deals with the detection and classification of alarming sounds in a noisy environment. The proposed sound analysis system can detect distress or everyday sounds everywhere in the monitored apartment, and is connected to classical medical telemonitoring sensors through a data fusion process. The sound analysis system is divided in two stages: sound detection and classification. The first analysis stage (sound detection) must extract significant sounds from a continuous signal flow. A new detection algorithm based on discrete wavelet transform is proposed in this paper, which leads to accurate results when applied to nonstationary signals (such as impulsive sounds). The algorithm presented in this paper was evaluated in a noisy environment and is favorably compared to the state of the art algorithms in the field. The second stage of the system is sound classification, which uses a statistical approach to identify unknown sounds. A statistical study was done to find out the most discriminant acoustical parameters in the input of the classification module. New wavelet based parameters, better adapted to noise, are proposed in this paper. The telemonitoring system validation is presented through various real and simulated test sets. The global sound based system leads to a 3% missed alarm rate and could be fused with other medical sensors to improve performance",2006.0,"D. Istrate, E. Castelli, Michel Vacher, L. Besacier, J. Serignat"
f141e683ee03e8fa3cf458907f77ca6d5b476aa3,https://www.semanticscholar.org/paper/f141e683ee03e8fa3cf458907f77ca6d5b476aa3,Ontologies and Information Extraction,"This report argues that, even in the simplest cases, IE is an ontology-driven process. It is not a mere text filtering method based on simple pattern matching and keywords, because the extracted pieces of texts are interpreted with respect to a predefined partial domain model. This report shows that depending on the nature and the depth of the interpretation to be done for extracting the information, more or less knowledge must be involved. This report is mainly illustrated in biology, a domain in which there are critical needs for content-based exploration of the scientific literature and which becomes a major application domain for IE.",2006.0,"C. Nédellec, A. Nazarenko"
1a0ef2286995c3a381f4e73336fecc0eb82ee2f9,https://www.semanticscholar.org/paper/1a0ef2286995c3a381f4e73336fecc0eb82ee2f9,Information extraction for semi-structured documents,"The number of unstructured or semi-structured documents produced in all types of organizations continues to increase rapidly. Cost-effective ways of finding the relevant ones and extracting useful information from them are increasingly important to a large number of enterprises for operational and decision-support applications. The approach discussed in this paper constitutes a suitable basis for building an effective solution to extracting information from semi-structured documents for two principal reasons. First, it provides an extensible architecture basis for: extracting structured information from semistructured documents; providing fast and accurate selective access to this information; performing selective dissemination of relevant documents depending on filtering criteria. Second, it is simple in terms of: the complexity of the algorithms used for structure recognition and document filtering; the number and size of data structures required to perform the three functions mentioned above; the amount and complexity of the metadata required to handle a given collection of documents. The work described here is part of the Dyade Médiation project, which aims to provide integrated software components for accessing heterogeneous data sources in Internet/Intranet environments.",1997.0,"Dan J. Smith, Mauricio Lopez"
271ee8dd74b5c5e6142f76d6e3bb41c3a7d2ecca,https://www.semanticscholar.org/paper/271ee8dd74b5c5e6142f76d6e3bb41c3a7d2ecca,Recent developments in temporal information extraction,"The growing interest in practical NLP applications such as text summarization and question-answering places increasing demands on the processing of temporal information in natural languages. To support this, several new capabilities have emerged. These include the ability to tag events and time expressions, to temporally anchor and order events, and to build models of the temporal structure of discourse. This paper describes some of the techniques and the further challenges that arise.",2003.0,I. Mani
e80b34a55aa56578f9a4f27ea207f8c42c93a378,https://www.semanticscholar.org/paper/e80b34a55aa56578f9a4f27ea207f8c42c93a378,Acquisition of semantic patterns for information extraction from corpora,"A knowledge acquisition tool to extract semantic patterns for a memory-based information retrieval system is presented. The major goal of this tool is to facilitate the construction of a large knowledge base of semantic patterns. The system acquires semantic patterns from texts with a small amount of user interaction. It acquires new phrasal patterns from the input text, maps each element of the pattern to a meaning frame, generalizes the acquired pattern, and merges it into the current knowledge base. Interaction with the user is introduced at some decision points, where the ambiguity cannot be resolved automatically without other pieces of predefined knowledge. The acquisition process is described in detail, and a preliminary experimental result is discussed.<<ETX>>",1993.0,"Jun-Tae Kim, D. Moldovan"
1bab419a7f837580163c64d30abf42107c4c9615,https://www.semanticscholar.org/paper/1bab419a7f837580163c64d30abf42107c4c9615,Picture perception: effects of luminance on available information and information-extraction rate.,"In each of four experiments, complex visual stimuli--pictures and digit arrays--were remembered better when shown at high luminance than when shown at low luminance. Why does this occur? Two possibilities were considered: first that lowering luminance reduces the amount of available information in the stimulus, and second that lowering luminance reduces the rate at which the information is extracted from the stimulus. Evidence was found for both possibilities. When stimuli were presented at durations short enough to permit only a single eye fixation, luminance affected only the rate at which information is extracted: decreasing luminance by a factor of 100 caused information to be extracted more slowly by a factor that ranged, over experiments, from 1.4 to 2.0. When pictures were presented at durations long enough to permit multiple fixations, however, luminance affected the total amount of extractable information. In a fifth experiment, converging evidence was sought for the proposition that within the first eye fixation on a picture, luminance affects the rate of information extraction. If this proposition is correct and, in addition, the first eye fixation lasts until some criterion amount of information is extracted, then fixation duration should increase with decreasing luminance. This prediction was confirmed.",1985.0,G. Loftus
a54e1212eb46921039c2516905f37b591182751a,https://www.semanticscholar.org/paper/a54e1212eb46921039c2516905f37b591182751a,Information Extraction A Survey,1,2005.0,"K. Kaiser, S. Miksch"
3a58d9b2f82b86bbf65d8a20911e73b5c296ac30,https://www.semanticscholar.org/paper/3a58d9b2f82b86bbf65d8a20911e73b5c296ac30,"An Integrated, Conditional Model of Information Extraction and Coreference with Appli",In a method and apparatus for making a pile-surfaced thermoplastic material by a tack-spin technique the laminate of backing web and adhered pile is hauled off from the heated drawing surface over a rod or bar which fixes the point at which the laminate separates from the heated drawing surface and which directs cooling fluid from an aperture in the rod or bar and onto the back of the backing web and into the fibril-forming area.,2004.0,"Ben Wellner, A. McCallum, Fuchun Peng, Michael Hay"
62db931d3af45eaffdf674b9d0a9ac5e91e0c5ea,https://www.semanticscholar.org/paper/62db931d3af45eaffdf674b9d0a9ac5e91e0c5ea,Integrating the document object model with hyperlinks for enhanced topic distillation and information extraction,"Topic distillation is the process of finding authoritative Web pages and comprehensive “hubs” which reciprocally endorse each other and are relevant to a given query. Hyperlinkbased topic distillation has been traditionally applied to a macroscopic Web model where documents are nodes in a directed graph and hyperlinks are edges. Macroscopic models miss valuable clues such as banners, navigation panels, and template-based inclusions, which are embedded in HTML pages using markup tags. Consequently, results of macroscopic distillation algorithms have been deteriorating in quality as Web pages are becoming more complex. We propose a uniform fine-grained model for the Web in which pages are represented by their tag trees (also called their Document Object Models or DOMs) and these DOM trees are interconnected by ordinary hyperlinks. Surprisingly, macroscopic distillation algorithms do not work in the finegrained scenario. We present a new algorithm suitable for the fine-grained model. It can dis-aggregate hubs into coherent regions by segmenting their DOMtrees. M utual endorsement between hubs and authorities involve these regions, rather than single nodes representing complete hubs. Anecdotes and measurements using a 28-query, 366000-document benchmark suite, used in earlier topic distillation research, reveal two benefits from the new algorithm: distillation quality improves, and a by-product of distillation is the ability to extract relevant snippets from hubs which are only partially relevant to the query.",2001.0,Soumen Chakrabarti
76f9f52c0375b469880208465816d5dd7abeb381,https://www.semanticscholar.org/paper/76f9f52c0375b469880208465816d5dd7abeb381,Location Normalization for Information Extraction,"Ambiguity is very high for location names. For example, there are 23 cities named 'Buffalo' in the U.S. Country names such as 'Canada', 'Brazil' and 'China' are also city names in the USA. Almost every city has a Main Street or Broadway. Such ambiguity needs to be handled before we can refer to location names for visualization of related extracted events. This paper presents a hybrid approach for location normalization which combines (i) lexical grammar driven by local context constraints, (ii) graph search for maximum spanning tree and (iii) integration of semi-automatically derived default senses. The focus is on resolving ambiguities for the following types of location names: island, town, city, province, and country. The results are promising with 93.8% accuracy on our test collections.",2002.0,"Huifeng Li, R. Srihari, Cheng Niu, W. Li"
3b718ed2e50a34911b6ae75ec507eb23b4cb5bc8,https://www.semanticscholar.org/paper/3b718ed2e50a34911b6ae75ec507eb23b4cb5bc8,Grounding spatial named entities for information extraction and question answering,"The task of named entity annotation of unseen text has recently been successfully automated with near-human performance.But the full task involves more than annotation, i.e. identifying the scope of each (continuous) text span and its class (such as place name). It also involves grounding the named entity (i.e. establishing its denotation with respect to the world or a model). The latter aspect has so far been neglected.In this paper, we show how geo-spatial named entities can be grounded using geographic coordinates, and how the results can be visualized using off-the-shelf software. We use this to compare a ""textual surrogate"" of a newspaper story, with a ""visual surrogate"" based on geographic coordinates.",2003.0,"Jochen L. Leidner, Gail Sinclair, B. Webber"
eeacfebe8b281533b00a645fa80a32c502643d09,https://www.semanticscholar.org/paper/eeacfebe8b281533b00a645fa80a32c502643d09,Model-based despeckling and information extraction from SAR images,"Basic textures as they appear, especially in high resolution SAR images, are affected by multiplicative speckle noise and should be preserved by despeckling algorithms. Sharp edges between different regions and strong scatterers also must be preserved. To despeckle images, we use a maximum a posteriori (MAP) estimation of the cross section, choosing between different prior models. The proposed approach uses a Gauss Markov random field (GMRF) model for textured areas and allows an adaptive neighborhood system for edge preservation between uniform areas. In order to obtain the best possible texture reconstruction, an expectation maximization algorithm is used to estimate the texture parameters that provide the highest evidence. Borders between homogeneous areas are detected with a stochastic region-growing algorithm, locally determining the neighborhood system of the Gauss Markov prior. Smoothed strong scatterers are found in the ratio image of the data and the filtering result and are replaced in the image. In this way, texture, edges between homogeneous regions, and strong scatterers are well reconstructed and preserved. Additionally, the estimated model parameters can be used for further image interpretation methods.",2000.0,"M. Walessa, M. Datcu"
f6cf03d3873c7710d57fd081f8772e301b8bb117,https://www.semanticscholar.org/paper/f6cf03d3873c7710d57fd081f8772e301b8bb117,On Information Extraction Principles for Hyperspectral Data,"Means for optimally analyzing hyperspectral data has been a topic of study for some years. Our work has specifically focused on this topic since 19861. The point of departure for our study has been that of signal theory and the signal processing principles that have grown primarily from the communication sciences area over the last half century. The basic approach has been to seek a more fundamental understanding of high dimensional signal spaces in the context of multispectral remote sensing, and then to use this knowledge to extend the methods of conventional multispectral analysis to the hyperspectral domain in an optimal or near optimal fashion. The purpose of this paper is to outline what has been learned so far in this effort. The introduction of hyperspectral sensors that produce much more detailed spectral data than those previously, provide much enhanced abilities to extract useful information from the data stream that they produce. In theory, it is possible to discriminate successfully between any specified set of classes of data by increasing the dimensionality of the data far enough. In fact, current hyperspectral data, which may have from a few 10’s to several hundred of bands, essentially make this possible. However, it is also the case that this more detailed data requires more sophisticated data analysis procedures if their full potential is to be achieved. Much of what has been learned about the necessary procedures is not particularly intuitive, and indeed, in many cases is counter-intuitive. In this paper, we shall attempt not only to illuminate some of these counter-intuitive aspects, but to point the direction for practical methods to make optimal analysis procedures possible.",1997.0,D. Landgrebe
4fdd054955240cbbc469e9437be6898816f41501,https://www.semanticscholar.org/paper/4fdd054955240cbbc469e9437be6898816f41501,Information Extraction from Voicemail Transcripts,Voicemail is not like email. Even such basic information as the name of the caller/sender or a phone number for returning calls is not represented explicitly and must be obtained from message transcripts or other sources. We discuss techniques for doing this and the challenges these tasks present.,2002.0,"Martin Jansche, S. Abney"
eddbf722086a0ba71a3271be9b9e9790d0c25593,https://www.semanticscholar.org/paper/eddbf722086a0ba71a3271be9b9e9790d0c25593,Road Structure Refined CNN for Road Extraction in Aerial Image,"In this letter, we propose a road structure refined convolutional neural network (RSRCNN) approach for road extraction in aerial images. In order to obtain structured output of road extraction, both deconvolutional and fusion layers are designed in the architecture of RSRCNN. For training RSRCNN, a new loss function is proposed to incorporate the geometric information of road structure in cross-entropy loss, thus called road-structure-based loss function. Experimental results demonstrate that the trained RSRCNN model is able to advance the state-of-the-art road extraction for aerial images, in terms of precision, recall, F-score, and accuracy.",2017.0,"Yanan Wei, Zulin Wang, Mai Xu"
8e322aa56d72716cb691136ff8e2dec509fb173f,https://www.semanticscholar.org/paper/8e322aa56d72716cb691136ff8e2dec509fb173f,"Information extraction from biomedical literature: methodology, evaluation and an application","Journals and conference proceedings represent the dominant mechanisms of reporting new biomedical results. The unstructured nature of such publications makes it difficult to utilize data mining or automated knowledge discovery techniques. Annotation (or markup) of these unstructured documents represents the first step in making these documents machine analyzable. In this paper we first present a system called BioAnnotator for identifying and annotating biological terms in documents. BioAnnotator uses domain based dictionary look-up for recognizing known terms and a rule engine for discovering new terms. The combination and dictionary look-up and rules result in good performance (87% precision and 94% recall on the GENIA 1.1 corpus for extracting general biological terms based on an approximate matching criterion). To demonstrate the subsequent mining and knowledge discovery activities that are made feasible by BioAnnotator, we also present a system called MedSummarizer that uses the extracted terms to identify the common concepts in a given group of genes.",2003.0,"L. V. Subramaniam, Sougata Mukherjea, P. Kankar, B. Srivastava, Vishal S. Batra, Pasumarti V. Kamesam, R. Kothari"
5d6fd88fa5a4648e37bacc3b68bd7752543c1e95,https://www.semanticscholar.org/paper/5d6fd88fa5a4648e37bacc3b68bd7752543c1e95,Information extraction as a stepping stone toward story understanding,"Historically, story understanding systems have depended on a great deal of hand-crafted knowledge. Natural language understanding systems that use conceptual knowledge structures [SA77, Cul78, Wil78, Car79, Leh81, Kol83] typically rely on enormous amounts of manual knowledge engineering. While much of the work on conceptual knowledge structures has been hailed as pioneering research in cognitive modeling and narrative understanding, from a practical perspective it has also been viewed with skepticism because of the underlying knowledge engineering bottleneck. The thought of building a large-scale conceptual natural language processing (NLP) system that can understand open-ended text is daunting even to the most ardent enthusiasts. So must we grit our collective teeth and assume that story understanding will be limited to prototype systems in the foreseeable future? Or will conceptual natural language processing ultimately depend on a massive, broad-scale manual knowledge engineering effort, such as CYC [LPS86]?",1999.0,E. Riloff
081e441e8eaea644a6e2ce90b75c520f527bf677,https://www.semanticscholar.org/paper/081e441e8eaea644a6e2ce90b75c520f527bf677,Adaptive web information extraction,The Amorphic system works to extract Web information for use in business intelligence applications.,2006.0,"Dawn G. Gregg, S. Walczak"
84f66ff00634cbdf8f122080b61dfc6c18805e3e,https://www.semanticscholar.org/paper/84f66ff00634cbdf8f122080b61dfc6c18805e3e,The complexity of information extraction,"How difficult are decision problems based on natural data, such as pattern recognition? To answer this question, decision problems are characterized by introducing four measures defined on a Boolean function f of N variables: the implementation cost C(f) , the randomness R(f) , the deterministic entropy H(f) , and the complexity K(f) . The highlights and main results are roughly as follows, l) C(f) \approx R(f) H(f) \approx K(f) , all measured in bits. 2) Decision problems based on natural data are partially random (in the Kolmogorov sense) and have low entropy with respect to their dimensionality, and the relations between the four measures translate to lower and upper bounds on the cost of solving these problems. 3) Allowing small errors in the implementation of f saves a lot in the iow entropy case but saves nothing in the high-entropy case. If f is partially structured, the implementation cost is reduced substantially.",1986.0,Y. Abu-Mostafa
fa06e9acc611ef6013be68fe5155a23b3e6a7b0f,https://www.semanticscholar.org/paper/fa06e9acc611ef6013be68fe5155a23b3e6a7b0f,Information extraction,"Information extraction (IE) maps a language stream into database records that capture part of its meaning. Name, entity, and relation extraction are common subtasks, and an event extraction subtask has also been proposed. Various specific target sets have been defined, with results compared in government-sponsored evaluations, but defining 'meaning' for a broad spectrum of applications remains a challenge. Researchers are exploring a wide variety of learning techniques for these tasks. Initial tests have also been performed measuring IE performance on speech recognition output, but current work has only scratched the surface of this important area.",2005.0,"L. Ramshaw, R. Weischedel"
506bc17fed205254d39add80c6d510315f410672,https://www.semanticscholar.org/paper/506bc17fed205254d39add80c6d510315f410672,A Survey of Deep Learning Methods for Relation Extraction,"Relation Extraction is an important sub-task of Information Extraction which has the potential of employing deep learning (DL) models with the creation of large datasets using distant supervision. In this review, we compare the contributions and pitfalls of the various DL models that have been used for the task, to help guide the path ahead.",2017.0,Shantanu Kumar
e9c3fad3993b9942cc0523e6d064fe3d420ff07e,https://www.semanticscholar.org/paper/e9c3fad3993b9942cc0523e6d064fe3d420ff07e,Ontology-Based Partial Building Information Model Extraction,"AbstractThe current application of building information modeling (BIM) in the construction industry is generally focused on using the complete building information model during the life cycle of the project. With more information being added to the model, the size of the model file and the difficulty to manipulate the model increase. However, different use scenarios may only require access to certain specific information stored in the model. In contrast with the ample research of ontology applications in construction knowledge management, research of ontology in construction modeling has been limited. Hence, the purpose of this study is to use ontology in the extraction of a partial building information model from the original complete model. The building information models covered in this study are in the Industry Foundation Classes (IFC) format, which is a widely supported open BIM standard. An ontology TBox is developed according to the existing IFC schema specifications. For each specific IFC model, a...",2013.0,"Le Zhang, R. Issa"
f9b54d083727de371a3a43562e09c84799685ad7,https://www.semanticscholar.org/paper/f9b54d083727de371a3a43562e09c84799685ad7,Unsupervised Word and Dependency Path Embeddings for Aspect Term Extraction,"In this paper, we develop a novel approach to aspect term extraction based on unsupervised learning of distributed representations of words and dependency paths. The basic idea is to connect two words (w1 and w2) with the dependency path (r) between them in the embedding space. Specifically, our method optimizes the objective w1 + r = w2 in the low-dimensional space, where the multi-hop dependency paths are treated as a sequence of grammatical relations and modeled by a recurrent neural network. Then, we design the embedding features that consider linear context and dependency context information, for the conditional random field (CRF) based aspect term extraction. Experimental results on the SemEval datasets show that, (1) with only embedding features, we can achieve state-of-the-art results; (2) our embedding method which incorporates the syntactic information among words yields better performance than other representative ones in aspect term extraction.",2016.0,"Yichun Yin, Furu Wei, Li Dong, Kaimeng Xu, Ming Zhang, M. Zhou"
8bdda72ced8919386101d06fef86721086f8de0e,https://www.semanticscholar.org/paper/8bdda72ced8919386101d06fef86721086f8de0e,Information Extraction Information Extraction,"In this paper we present a new approach to extract relevant information by knowledge graphs from natural language text. We give a multiple level model based on knowledge graphs for describing template information, and investigate the concept of partial structural parsing. Moreover, we point out that expansion of concepts plays an important role in thinking, so we study the expansion of knowledge graphs to use context information for reasoning and merging of templates.",2002.0,"L. Zhang, C. Hoede"
115b3e2e3b9a8f74eb7ee860bd381cf2344e9e74,https://www.semanticscholar.org/paper/115b3e2e3b9a8f74eb7ee860bd381cf2344e9e74,Hybrid Compression of Hyperspectral Images Based on PCA With Pre-Encoding Discriminant Information,"It has been shown that image compression based on principal component analysis (PCA) provides good compression efficiency for hyperspectral images. However, PCA might fail to capture all the discriminant information of hyperspectral images, since features that are important for classification tasks may not be high in signal energy. To deal with this problem, we propose a hybrid compression method for hyperspectral images with pre-encoding discriminant information. A feature extraction method is first applied to the original images, producing a set of feature vectors that are used to generate feature images and then residual images by subtracting the feature-reconstructed images from the original ones. Both feature images and residual images are compressed and transmitted. Experiments on data from the Airborne Visible/Infrared Imaging Spectrometer sensor indicate that the proposed method provides better compression efficiency with improved classification accuracy than conventional compression methods.",2015.0,"Chulhee Lee, S. Youn, T. Jeong, Eunjae Lee, Joan Serra-Sagristà"
df64be1e1c486038380199d124fc2b15536adfcc,https://www.semanticscholar.org/paper/df64be1e1c486038380199d124fc2b15536adfcc,Keyword extraction from a single document using word co-occurrence statistical information,"We present a new keyword extraction algorithm that applies to a single document without using a corpus. Frequent terms are extracted first, then a set of cooccurrence between each term and the frequent terms, i.e., occurrences in the same sentences, is generated. Co-occurrence distribution shows importance of a term in the documentas follows. If probability distribution of co-occurrence between term a and the frequent terms is biased to a particular subset of frequent terms, then term a is likely to be a keyword. The degree of biases of distribution is measured by the χ 2 -measure. Our algorithm shows comparable performance to tfidf without using a corpus.",2004.0,"Y. Matsuo, M. Ishizuka"
7088a6dd846f58270211eca080a1ea2b07c00ca5,https://www.semanticscholar.org/paper/7088a6dd846f58270211eca080a1ea2b07c00ca5,Robust Extraction of Tomographic Information via Randomized Benchmarking,"Quantum processing tomography typically reconstructs an unknown quantum dynamical operation by measuring its effects on known states of a quantum device. Taking a different approach of comparing the operation of interest to a set of finite and easily implementable reference operations, a new method can reconstruct any quantum operation reliably.",2013.0,"S. Kimmel, M. Silva, C. Ryan, Blake R. Johnson, T. Ohki"
43df0e843b2cede393bb64ed953c7c41391246aa,https://www.semanticscholar.org/paper/43df0e843b2cede393bb64ed953c7c41391246aa,Relation Extraction : A Survey,"With the advent of the Internet, large amount of digital text is generated everyday in the form of news articles, research publications, blogs, question answering forums and social media. It is important to develop techniques for extracting information automatically from these documents, as lot of important information is hidden within them. This extracted information can be used to improve access and management of knowledge hidden in large text corpora. Several applications such as Question Answering, Information Retrieval would benefit from this information. Entities like persons and organizations, form the most basic unit of the information. Occurrences of entities in a sentence are often linked through well-defined relations; e.g., occurrences of person and organization in a sentence may be linked through relations such as employed at. The task of Relation Extraction (RE) is to identify such relations automatically. In this paper, we survey several important supervised, semi-supervised and unsupervised RE techniques. We also cover the paradigms of Open Information Extraction (OIE) and Distant Supervision. Finally, we describe some of the recent trends in the RE techniques and possible future research directions. This survey would be useful for three kinds of readers - i) Newcomers in the field who want to quickly learn about RE; ii) Researchers who want to know how the various RE techniques evolved over time and what are possible future research directions and iii) Practitioners who just need to know which RE technique works best in various settings.",2017.0,"Sachin Sharad Pawar, Girish Keshav Palshikar, P. Bhattacharyya"
c6803d82b0c94607028264b2c3f257700c29f5dd,https://www.semanticscholar.org/paper/c6803d82b0c94607028264b2c3f257700c29f5dd,DNA extraction for streamlined metagenomics of diverse environmental samples.,"A major bottleneck for metagenomic sequencing is rapid and efficient DNA extraction. Here, we compare the extraction efficiencies of three magnetic bead-based platforms (KingFisher, epMotion, and Tecan) to a standardized column-based extraction platform across a variety of sample types, including feces, oral, skin, soil, and water. Replicate sample plates were extracted and prepared for 16S rRNA gene amplicon sequencing in parallel to assess extraction bias and DNA quality. The data demonstrate that any effect of extraction method on sequencing results was small compared with the variability across samples; however, the KingFisher platform produced the largest number of high-quality reads in the shortest amount of time. Based on these results, we have identified an extraction pipeline that dramatically reduces sample processing time without sacrificing bacterial taxonomic or abundance information.",2017.0,"C. Marotz, Amnon Amir, G. Humphrey, James Gaffney, Grant Gogul, R. Knight"
e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da,https://www.semanticscholar.org/paper/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da,Deep Private-Feature Extraction,"We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information.",2018.0,"S. A. Ossia, A. Taheri, A. Shamsabadi, Kleomenis Katevas, H. Haddadi, H. Rabiee"
1162c416c4f0ac5d20cb2d636b1424435ca4bced,https://www.semanticscholar.org/paper/1162c416c4f0ac5d20cb2d636b1424435ca4bced,A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations,"Evidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations.",2017.0,"T. Eftimov, B. Koroušić Seljak, P. Korošec"
c6803d82b0c94607028264b2c3f257700c29f5dd,https://www.semanticscholar.org/paper/c6803d82b0c94607028264b2c3f257700c29f5dd,DNA extraction for streamlined metagenomics of diverse environmental samples.,"A major bottleneck for metagenomic sequencing is rapid and efficient DNA extraction. Here, we compare the extraction efficiencies of three magnetic bead-based platforms (KingFisher, epMotion, and Tecan) to a standardized column-based extraction platform across a variety of sample types, including feces, oral, skin, soil, and water. Replicate sample plates were extracted and prepared for 16S rRNA gene amplicon sequencing in parallel to assess extraction bias and DNA quality. The data demonstrate that any effect of extraction method on sequencing results was small compared with the variability across samples; however, the KingFisher platform produced the largest number of high-quality reads in the shortest amount of time. Based on these results, we have identified an extraction pipeline that dramatically reduces sample processing time without sacrificing bacterial taxonomic or abundance information.",2017.0,"C. Marotz, Amnon Amir, G. Humphrey, James Gaffney, Grant Gogul, R. Knight"
e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da,https://www.semanticscholar.org/paper/e1dcd7fd049ae2ae3b93295d8ea360cafc00f9da,Deep Private-Feature Extraction,"We present and evaluate Deep Private-Feature Extractor (DPFE), a deep model which is trained and evaluated based on information theoretic constraints. Using the selective exchange of information between a user's device and a service provider, DPFE enables the user to prevent certain sensitive information from being shared with a service provider, while allowing them to extract approved information using their model. We introduce and utilize the log-rank privacy, a novel measure to assess the effectiveness of DPFE in removing sensitive information and compare different models based on their accuracy-privacy trade-off. We then implement and evaluate the performance of DPFE on smartphones to understand its complexity, resource demands, and efficiency trade-offs. Our results on benchmark image datasets demonstrate that under moderate resource utilization, DPFE can achieve high accuracy for primary tasks while preserving the privacy of sensitive information.",2018.0,"S. A. Ossia, A. Taheri, A. Shamsabadi, Kleomenis Katevas, H. Haddadi, H. Rabiee"
1162c416c4f0ac5d20cb2d636b1424435ca4bced,https://www.semanticscholar.org/paper/1162c416c4f0ac5d20cb2d636b1424435ca4bced,A rule-based named-entity recognition method for knowledge extraction of evidence-based dietary recommendations,"Evidence-based dietary information represented as unstructured text is a crucial information that needs to be accessed in order to help dietitians follow the new knowledge arrives daily with newly published scientific reports. Different named-entity recognition (NER) methods have been introduced previously to extract useful information from the biomedical literature. They are focused on, for example extracting gene mentions, proteins mentions, relationships between genes and proteins, chemical concepts and relationships between drugs and diseases. In this paper, we present a novel NER method, called drNER, for knowledge extraction of evidence-based dietary information. To the best of our knowledge this is the first attempt at extracting dietary concepts. DrNER is a rule-based NER that consists of two phases. The first one involves the detection and determination of the entities mention, and the second one involves the selection and extraction of the entities. We evaluate the method by using text corpora from heterogeneous sources, including text from several scientifically validated web sites and text from scientific publications. Evaluation of the method showed that drNER gives good results and can be used for knowledge extraction of evidence-based dietary recommendations.",2017.0,"T. Eftimov, B. Koroušić Seljak, P. Korošec"
17ff6fb495872abe1b09d3330d5c4676bd190568,https://www.semanticscholar.org/paper/17ff6fb495872abe1b09d3330d5c4676bd190568,Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach,"Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHession, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHession over the state-of-the-art.",2017.0,"Liyuan Liu, Xiang Ren, Qi Zhu, Shi Zhi, Huan Gui, Heng Ji, Jiawei Han"
b8f791c6449e69d3f7468fbec879c99569c2b3a4,https://www.semanticscholar.org/paper/b8f791c6449e69d3f7468fbec879c99569c2b3a4,Electronic Nose Feature Extraction Methods: A Review,"Many research groups in academia and industry are focusing on the performance improvement of electronic nose (E-nose) systems mainly involving three optimizations, which are sensitive material selection and sensor array optimization, enhanced feature extraction methods and pattern recognition method selection. For a specific application, the feature extraction method is a basic part of these three optimizations and a key point in E-nose system performance improvement. The aim of a feature extraction method is to extract robust information from the sensor response with less redundancy to ensure the effectiveness of the subsequent pattern recognition algorithm. Many kinds of feature extraction methods have been used in E-nose applications, such as extraction from the original response curves, curve fitting parameters, transform domains, phase space (PS) and dynamic moments (DM), parallel factor analysis (PARAFAC), energy vector (EV), power density spectrum (PSD), window time slicing (WTS) and moving window time slicing (MWTS), moving window function capture (MWFC), etc. The object of this review is to provide a summary of the various feature extraction methods used in E-noses in recent years, as well as to give some suggestions and new inspiration to propose more effective feature extraction methods for the development of E-nose technology.",2015.0,"Jia Yan, Xiuzhen Guo, Shukai Duan, Pengfei Jia, Lidan Wang, Chao Peng, Songlin Zhang"
4fe594f1f0358a00b19e2bb950e45988decab9b7,https://www.semanticscholar.org/paper/4fe594f1f0358a00b19e2bb950e45988decab9b7,Opinion Word Expansion and Target Extraction through Double Propagation,"Analysis of opinions, known as opinion mining or sentiment analysis, has attracted a great deal of attention recently due to many practical applications and challenging research problems. In this article, we study two important problems, namely, opinion lexicon expansion and opinion target extraction. Opinion targets (targets, for short) are entities and their attributes on which opinions have been expressed. To perform the tasks, we found that there are several syntactic relations that link opinion words and targets. These relations can be identified using a dependency parser and then utilized to expand the initial opinion lexicon and to extract targets. This proposed method is based on bootstrapping. We call it double propagation as it propagates information between opinion words and targets. A key advantage of the proposed method is that it only needs an initial opinion lexicon to start the bootstrapping process. Thus, the method is semi-supervised due to the use of opinion word seeds. In evaluation, we compare the proposed method with several state-of-the-art methods using a standard product review test collection. The results show that our approach outperforms these existing methods significantly.",2011.0,"Guang Qiu, B. Liu, Jiajun Bu, Chun Chen"
c15aeaa48974807d2ff61b94af7a318a48d156f7,https://www.semanticscholar.org/paper/c15aeaa48974807d2ff61b94af7a318a48d156f7,Formal Ontology and Information Systems,"Research on ontology is becoming increasingly widespread in the computer science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction. We shall use the generic term “information systems”, in its broadest sense, to collectively refer to these application perspectives. We argue in this paper that so-called ontologies present their own methodological and architectural peculiarities: on the methodological side, their main peculiarity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an information system, leading to the perspective of ontology-driven information systems.",1998.0,Nicola Guarino
e5b2e466953bf0675077518fb5f2ab821a88500a,https://www.semanticscholar.org/paper/e5b2e466953bf0675077518fb5f2ab821a88500a,Deficits in communication and information transfer between hospital-based and primary care physicians: implications for patient safety and continuity of care.,"CONTEXT
Delayed or inaccurate communication between hospital-based and primary care physicians at hospital discharge may negatively affect continuity of care and contribute to adverse events.


OBJECTIVES
To characterize the prevalence of deficits in communication and information transfer at hospital discharge and to identify interventions to improve this process.


DATA SOURCES
MEDLINE (through November 2006), Cochrane Database of Systematic Reviews, and hand search of article bibliographies.


STUDY SELECTION
Observational studies investigating communication and information transfer at hospital discharge (n = 55) and controlled studies evaluating the efficacy of interventions to improve information transfer (n = 18).


DATA EXTRACTION
Data from observational studies were extracted on the availability, timeliness, content, and format of discharge communications, as well as primary care physician satisfaction. Results of interventions were summarized by their effect on timeliness, accuracy, completeness, and overall quality of the information transfer.


DATA SYNTHESIS
Direct communication between hospital physicians and primary care physicians occurred infrequently (3%-20%). The availability of a discharge summary at the first postdischarge visit was low (12%-34%) and remained poor at 4 weeks (51%-77%), affecting the quality of care in approximately 25% of follow-up visits and contributing to primary care physician dissatisfaction. Discharge summaries often lacked important information such as diagnostic test results (missing from 33%-63%), treatment or hospital course (7%-22%), discharge medications (2%-40%), test results pending at discharge (65%), patient or family counseling (90%-92%), and follow-up plans (2%-43%). Several interventions, including computer-generated discharge summaries and using patients as couriers, shortened the delivery time of discharge communications. Use of standardized formats to highlight the most pertinent information improved the perceived quality of documents.


CONCLUSIONS
Deficits in communication and information transfer at hospital discharge are common and may adversely affect patient care. Interventions such as computer-generated summaries and standardized formats may facilitate more timely transfer of pertinent patient information to primary care physicians and make discharge summaries more consistently available during follow-up care.",2007.0,"S. Kripalani, F. Lefevre, C. Phillips, Mark V. Williams, P. Basaviah, D. Baker"
672c8bf27fd6c04d34496e359844f1f9d95ed1e4,https://www.semanticscholar.org/paper/672c8bf27fd6c04d34496e359844f1f9d95ed1e4,Neural Relation Extraction with Multi-lingual Attention,"Relation extraction has been widely used for finding unknown relational facts from plain text. Most existing methods focus on exploiting mono-lingual data for relation extraction, ignoring massive information from the texts in various languages. To address this issue, we introduce a multi-lingual neural relation extraction framework, which employs mono-lingual attention to utilize the information within mono-lingual texts and further proposes cross-lingual attention to consider the information consistency and complementarity among cross-lingual texts. Experimental results on real-world datasets show that, our model can take advantage of multi-lingual texts and consistently achieve significant improvements on relation extraction as compared with baselines.",2017.0,"Yankai Lin, Zhiyuan Liu, Maosong Sun"
bbd1fcf20db62f496b2676eda0c38fbc1590d1a8,https://www.semanticscholar.org/paper/bbd1fcf20db62f496b2676eda0c38fbc1590d1a8,Extracting information from the text of electronic medical records to improve case detection: a systematic review,"Abstract Background Electronic medical records (EMRs) are revolutionizing health-related research. One key issue for study quality is the accurate identification of patients with the condition of interest. Information in EMRs can be entered as structured codes or unstructured free text. The majority of research studies have used only coded parts of EMRs for case-detection, which may bias findings, miss cases, and reduce study quality. This review examines whether incorporating information from text into case-detection algorithms can improve research quality. Methods A systematic search returned 9659 papers, 67 of which reported on the extraction of information from free text of EMRs with the stated purpose of detecting cases of a named clinical condition. Methods for extracting information from text and the technical accuracy of case-detection algorithms were reviewed. Results Studies mainly used US hospital-based EMRs, and extracted information from text for 41 conditions using keyword searches, rule-based algorithms, and machine learning methods. There was no clear difference in case-detection algorithm accuracy between rule-based and machine learning methods of extraction. Inclusion of information from text resulted in a significant improvement in algorithm sensitivity and area under the receiver operating characteristic in comparison to codes alone (median sensitivity 78% (codes + text) vs 62% (codes), P  = .03; median area under the receiver operating characteristic 95% (codes + text) vs 88% (codes), P  = .025). Conclusions Text in EMRs is accessible, especially with open source information extraction algorithms, and significantly improves case detection when combined with codes. More harmonization of reporting within EMR studies is needed, particularly standardized reporting of algorithm accuracy metrics like positive predictive value (precision) and sensitivity (recall).",2016.0,"E. Ford, J. Carroll, Helen E. Smith, D. Scott, J. Cassell"
e0958ad6074485a6f2296c7b2ee552ce3d875559,https://www.semanticscholar.org/paper/e0958ad6074485a6f2296c7b2ee552ce3d875559,Green extraction technologies for high‐value metabolites from algae: a review,"Cultivation of algae (micro and macro) can be used to produce several high‐ value metabolites to supply industries as cosmetics, additives, and pigments, among others. Those metabolites can have physiological and nutritional benefits for human and animal health. However, the availability of high‐value metabolites from algae is still unaffordable due to traditional extraction techniques and their requirements of energy and use of pollutant solvents. Recently, green extraction technologies for the extraction of high‐value metabolites have become more desirable due to their sustainability and environmental benefits. However, the information about green extraction metabolites from algae is limited. Therefore, this review highlights the main green extraction technologies – supercritical fluid extraction (SFE), microwave assisted extraction (MAE), and pressurized liquid extraction (PLE) – and their optimal parameters for the extraction of high‐value metabolites from algae. First, general information is given regarding high‐ value metabolites from algae. Then, the review summarizes the principles, processes, advantages, and disadvantages of each technology. Finally, it presents recommendations and concluding remarks to select the best extraction technology. © 2016 Society of Chemical Industry and John Wiley & Sons, Ltd",2017.0,"D. Esquivel-Hernandez, Ingrid P. Ibarra-Garza, J. Rodríguez-Rodríguez, Sara P. Cuellar-Bermudez, M. Rostro-Alanís, G. S. Alemán-Nava, J. Garcia-Perez, R. Parra-Saldívar"
89254ab9cbbfc03ba6b2b1ce3cc10b8b46c86827,https://www.semanticscholar.org/paper/89254ab9cbbfc03ba6b2b1ce3cc10b8b46c86827,Textpresso: An Ontology-Based Information Retrieval and Extraction System for Biological Literature,"We have developed Textpresso, a new text-mining system for scientific literature whose capabilities go far beyond those of a simple keyword search engine. Textpresso's two major elements are a collection of the full text of scientific articles split into individual sentences, and the implementation of categories of terms for which a database of articles and individual sentences can be searched. The categories are classes of biological concepts (e.g., gene, allele, cell or cell group, phenotype, etc.) and classes that relate two objects (e.g., association, regulation, etc.) or describe one (e.g., biological process, etc.). Together they form a catalog of types of objects and concepts called an ontology. After this ontology is populated with terms, the whole corpus of articles and abstracts is marked up to identify terms of these categories. The current ontology comprises 33 categories of terms. A search engine enables the user to search for one or a combination of these tags and/or keywords within a sentence or document, and as the ontology allows word meaning to be queried, it is possible to formulate semantic queries. Full text access increases recall of biological data types from 45% to 95%. Extraction of particular biological facts, such as gene-gene interactions, can be accelerated significantly by ontologies, with Textpresso automatically performing nearly as well as expert curators to identify sentences; in searches for two uniquely named genes and an interaction term, the ontology confers a 3-fold increase of search efficiency. Textpresso currently focuses on Caenorhabditis elegans literature, with 3,800 full text articles and 16,000 abstracts. The lexicon of the ontology contains 14,500 entries, each of which includes all versions of a specific word or phrase, and it includes all categories of the Gene Ontology database. Textpresso is a useful curation tool, as well as search engine for researchers, and can readily be extended to other organism-specific corpora of text. Textpresso can be accessed at http://www.textpresso.org or via WormBase at http://www.wormbase.org.",2004.0,"Hans-Michael Müller, Eimear Kenny, P. Sternberg"
271afa1d279c340debdd6241a7cce61220fb5fd9,https://www.semanticscholar.org/paper/271afa1d279c340debdd6241a7cce61220fb5fd9,Information retrieval model: A social network extraction perspective,"Future Information Retrieval, especially in connection with the internet, will incorporate the content descriptions that are generated with social network extraction technologies and preferably incorporate the probability theory for assigning the semantic. Although there is an increasing interest about social network extraction, but a little of them has a significant impact to information retrieval. Therefore this paper proposes a model of information retrieval from the social network extraction.",2012.0,"M. K. Nasution, S. Noah"
56ed0d2509339b6a107bfc4c60a8ddf3089de3a6,https://www.semanticscholar.org/paper/56ed0d2509339b6a107bfc4c60a8ddf3089de3a6,Feature Extraction by Non-Parametric Mutual Information Maximization,"We present a method for learning discriminative feature transforms using as criterion the mutual information between class labels and transformed features. Instead of a commonly used mutual information measure based on Kullback-Leibler divergence, we use a quadratic divergence measure, which allows us to make an efficient non-parametric implementation and requires no prior assumptions about class densities. In addition to linear transforms, we also discuss nonlinear transforms that are implemented as radial basis function networks. Extensions to reduce the computational complexity are also presented, and a comparison to greedy feature selection is made.",2003.0,K. Torkkola
89af734dad7645ca3f3a7717443a1e49f37964bc,https://www.semanticscholar.org/paper/89af734dad7645ca3f3a7717443a1e49f37964bc,Yahoo! for Amazon: Sentiment Extraction from Small Talk on the Web,"We develop a methodology for extracting small investor sentiment from stock message boards. Five distinct classiﬁer algorithms coupled by a voting scheme are found to perform well against human and statistical benchmarks. Time series and cross-sectional aggregation of message information improves the quality of the resultant sentiment index. Empirical applications evidence a relationship with stock returns – visually, using phase-lag analysis, pattern recognition and statistical methods. Sentiment has an idiosyncratic component",2001.0,"Sanjiv Ranjan Das, Mike Y. Chen"
b84d525fd1ab0c2997e799fa030eb58cfb99b39a,https://www.semanticscholar.org/paper/b84d525fd1ab0c2997e799fa030eb58cfb99b39a,Methods of EEG Signal Features Extraction Using Linear Analysis in Frequency and Time-Frequency Domains,"Technically, a feature represents a distinguishing property, a recognizable measurement, and a functional component obtained from a section of a pattern. Extracted features are meant to minimize the loss of important information embedded in the signal. In addition, they also simplify the amount of resources needed to describe a huge set of data accurately. This is necessary to minimize the complexity of implementation, to reduce the cost of information processing, and to cancel the potential need to compress the information. More recently, a variety of methods have been widely used to extract the features from EEG signals, among these methods are time frequency distributions (TFD), fast fourier transform (FFT), eigenvector methods (EM), wavelet transform (WT), and auto regressive method (ARM), and so on. In general, the analysis of EEG signal has been the subject of several studies, because of its ability to yield an objective mode of recording brain stimulation which is widely used in brain-computer interface researches with application in medical diagnosis and rehabilitation engineering. The purposes of this paper, therefore, shall be discussing some conventional methods of EEG feature extraction methods, comparing their performances for specific task, and finally, recommending the most suitable method for feature extraction based on performance.",2014.0,"A. Al-Fahoum, Ausilah Al-Fraihat"
1e6a6ea63fe9e2ae28eb1e7243ca968c506b60de,https://www.semanticscholar.org/paper/1e6a6ea63fe9e2ae28eb1e7243ca968c506b60de,Emotion Recognition from EEG Signals Using Multidimensional Information in EMD Domain,"This paper introduces a method for feature extraction and emotion recognition based on empirical mode decomposition (EMD). By using EMD, EEG signals are decomposed into Intrinsic Mode Functions (IMFs) automatically. Multidimensional information of IMF is utilized as features, the first difference of time series, the first difference of phase, and the normalized energy. The performance of the proposed method is verified on a publicly available emotional database. The results show that the three features are effective for emotion recognition. The role of each IMF is inquired and we find that high frequency component IMF1 has significant effect on different emotional states detection. The informative electrodes based on EMD strategy are analyzed. In addition, the classification accuracy of the proposed method is compared with several classical techniques, including fractal dimension (FD), sample entropy, differential entropy, and discrete wavelet transform (DWT). Experiment results on DEAP datasets demonstrate that our method can improve emotion recognition performance.",2017.0,"Zhuang Ning, Ying Zeng, Li Tong, Chi Zhang, Hanming Zhang, Bin Yan"
8240101eace66a2a406b27428d8aa1a1858ebdc5,https://www.semanticscholar.org/paper/8240101eace66a2a406b27428d8aa1a1858ebdc5,The extraction of neural strategies from the surface EMG: an update.,"A surface EMG signal represents the linear transformation of motor neuron discharge times by the compound action potentials of the innervated muscle fibers and is often used as a source of information about neural activation of muscle. However, retrieving the embedded neural code from a surface EMG signal is extremely challenging. Most studies use indirect approaches in which selected features of the signal are interpreted as indicating certain characteristics of the neural code. These indirect associations are constrained by limitations that have been detailed previously (Farina D, Merletti R, Enoka RM. J Appl Physiol 96: 1486-1495, 2004) and are generally difficult to overcome. In an update on these issues, the current review extends the discussion to EMG-based coherence methods for assessing neural connectivity. We focus first on EMG amplitude cancellation, which intrinsically limits the association between EMG amplitude and the intensity of the neural activation and then discuss the limitations of coherence methods (EEG-EMG, EMG-EMG) as a way to assess the strength of the transmission of synaptic inputs into trains of motor unit action potentials. The debated influence of rectification on EMG spectral analysis and coherence measures is also discussed. Alternatively, there have been a number of attempts to identify the neural information directly by decomposing surface EMG signals into the discharge times of motor unit action potentials. The application of this approach is extremely powerful, but validation remains a central issue.",2014.0,"D. Farina, R. Merletti, R. Enoka"
70416780b69ea348f7d08784be27af1000117601,https://www.semanticscholar.org/paper/70416780b69ea348f7d08784be27af1000117601,Fusion of Dual Spatial Information for Hyperspectral Image Classification,"The inclusion of spatial information into spectral classifiers for fine-resolution hyperspectral imagery has led to significant improvements in terms of classification performance. The task of spectral-spatial hyperspectral image (HSI) classification has remained challenging because of high intraclass spectrum variability and low interclass spectral variability. This fact has made the extraction of spatial information highly active. In this work, a novel HSI classification framework using the fusion of dual spatial information is proposed, in which the dual spatial information is built by both exploiting pre-processing feature extraction and post-processing spatial optimization. In the feature extraction stage, an adaptive texture smoothing method is proposed to construct the structural profile (SP), which makes it possible to precisely extract discriminative features from HSIs. The SP extraction method is used here for the first time in the remote sensing community. Then, the extracted SP is fed into a spectral classifier. In the spatial optimization stage, a pixel-level classifier is used to obtain the class probability followed by an extended random walker-based spatial optimization technique. Finally, a decision fusion rule is utilized to fuse the class probabilities obtained by the two different stages. Experiments performed on three data sets from different scenes illustrate that the proposed method can outperform other state-of-the-art classification techniques. In addition, the proposed feature extraction method, i.e., SP, can effectively improve the discrimination between different land covers.",2020.0,"Puhong Duan, Pedram Ghamisi, Xudong Kang, Behnood Rasti, Shutao Li, R. Gloaguen"
636579046389c81135fff0269a66be2b317269f9,https://www.semanticscholar.org/paper/636579046389c81135fff0269a66be2b317269f9,Time-resolved photoemission by attosecond streaking: extraction of time information,"Attosecond streaking of atomic photoemission holds the promise to provide unprecedented information on the release time of the photoelectron. We show that attosecond streaking phase shifts indeed contain timing (or spectral phase) information associated with the Eisenbud–Wigner–Smith time delay matrix of quantum scattering. However, this is only accessible if the influence of the streaking infrared (IR) field on the emission process is properly accounted for. The IR probe field can strongly modify the observed streaking phase shift. We show that the part of the phase shift (‘time shift’) due to the interaction between the outgoing electron and the combined Coulomb and IR laser fields can be described classically. By contrast, the strong initial-state dependence of the streaking phase shift is only revealed through the solution of the time-dependent Schrödinger equation in its full dimensionality. We find a time delay between the hydrogenic 2s and 2p initial states in He+ exceeding 20 as for a wide range of IR intensities and XUV energies.",2011.0,"S. Nagele, R. Pazourek, J. Feist, Katharina Doblhoff-Dier, C. Lemell, K. THok'esi, J. Burgdorfer"
3b354076eb7196bce4a9926211f1ecb2aa6a1658,https://www.semanticscholar.org/paper/3b354076eb7196bce4a9926211f1ecb2aa6a1658,Knowledge Graphs: An Information Retrieval Perspective,"In this survey, we provide an overview of the literature on knowledge graphs (KGs) in the context of information retrieval (IR). Modern IR systems can beneﬁt from information available in KGs in multiple ways, independent of whether the KGs are publicly available or proprietary ones. We provide an overview of the components required when building IR systems that leverage KGs and use a task-oriented organization of the material that we discuss. As an understanding of the intersection of IR and KGs is beneﬁcial to many researchers and practitioners, we consider prior work from two complementary angles: leveraging KGs for information retrieval and enriching KGs using IR techniques. We start by discussing how KGs can be employed to support IR tasks, including document and entity retrieval. We then proceed by describing how IR—and language technology in general—can be utilized for the construction and completion of KGs. This includes tasks such as entity recognition, typing, and relation extraction. We discuss common issues that appear across the tasks that we consider and identify future directions for addressing them. We also provide pointers to datasets and other resources that should be useful for both newcomers and experienced researchers in the area.",2020.0,"R. Reinanda, E. Meij, M. de Rijke"
0f796ea78b1b08f8c99caf3d66c9ba5891e2a543,https://www.semanticscholar.org/paper/0f796ea78b1b08f8c99caf3d66c9ba5891e2a543,Seasonality extraction by function fitting to time-series of satellite sensor data,"A new method for extracting seasonality information from time-series of satellite sensor data is presented. The method is based on nonlinear least squares fits of asymmetric Gaussian model functions to the time-series. The smooth model functions are then used for defining key seasonality parameters, such as the number of growing seasons, the beginning and end of the seasons, and the rates of growth and decline. The method is implemented in a computer program TIMESAT and tested on Advanced Very High Resolution Radiometer (AVHRR) normalized difference vegetation index (NDVI) data over Africa. Ancillary cloud data [clouds from AVHRR (CLAVR)] are used as estimates of the uncertainty levels of the data values. Being general in nature, the proposed method can be applied also to new types of satellite-derived time-series data.",2002.0,"P. Jönsson, L. Eklundh"
03995790bf86a1c34fd02c072d05a1ecfaa603cb,https://www.semanticscholar.org/paper/03995790bf86a1c34fd02c072d05a1ecfaa603cb,Information Retrieval and Text Mining Technologies for Chemistry.,"Efficient access to chemical information contained in scientific literature, patents, technical reports, or the web is a pressing need shared by researchers and patent attorneys from different chemical disciplines. Retrieval of important chemical information in most cases starts with finding relevant documents for a particular chemical compound or family. Targeted retrieval of chemical documents is closely connected to the automatic recognition of chemical entities in the text, which commonly involves the extraction of the entire list of chemicals mentioned in a document, including any associated information. In this Review, we provide a comprehensive and in-depth description of fundamental concepts, technical implementations, and current technologies for meeting these information demands. A strong focus is placed on community challenges addressing systems performance, more particularly CHEMDNER and CHEMDNER patents tasks of BioCreative IV and V, respectively. Considering the growing interest in the construction of automatically annotated chemical knowledge bases that integrate chemical information and biological data, cheminformatics approaches for mapping the extracted chemical names into chemical structures and their subsequent annotation together with text mining applications for linking chemistry with biological information are also presented. Finally, future trends and current challenges are highlighted as a roadmap proposal for research in this emerging field.",2017.0,"Martin Krallinger, O. Rabal, A. Lourenço, J. Oyarzábal, A. Valencia"
a8d485f66f43bab50315f0c9a5cd042c8cc5d7ac,https://www.semanticscholar.org/paper/a8d485f66f43bab50315f0c9a5cd042c8cc5d7ac,"On methods and tools of table detection, extraction and annotation in PDF documents","Table detection, extraction and annotation have been an important research problem for years. To handle this issue, different approaches have been designed for different types of documents. Among these PDF is a widely used format for preserving and presenting different types of documents. We investigate the state of the art in table detection, extraction and annotation in PDF documents. Because of varying table structural anatomy, the state of the art in table-related research enumerates a number of approaches that are critically and analytically investigated for identifying their strengths and limitations as well as for making recommendations for further improvement. An evaluation framework is contributed that compares different information extraction tools that may be used in table detection, extraction and annotation. We found very limited attention towards these aspects in books, especially books in PDF format. There is no searching solution that can find books having tables that are semantically related to a table in a given book.",2015.0,"Shah Khusro, Asima Latif, Irfan Ullah"
7c7cb1e8c44ae1a56ff83e9ec9d8ddf4af8410a1,https://www.semanticscholar.org/paper/7c7cb1e8c44ae1a56ff83e9ec9d8ddf4af8410a1,Multiclass Common Spatial Patterns and Information Theoretic Feature Extraction,"We address two shortcomings of the common spatial patterns (CSP) algorithm for spatial filtering in the context of brain-computer interfaces (BCIs) based on electroencephalography/magnetoencephalography (EEG/MEG): First, the question of optimality of CSP in terms of the minimal achievable classification error remains unsolved. Second, CSP has been initially proposed for two-class paradigms. Extensions to multiclass paradigms have been suggested, but are based on heuristics. We address these shortcomings in the framework of information theoretic feature extraction (ITFE). We show that for two-class paradigms, CSP maximizes an approximation of mutual information of extracted EEG/MEG components and class labels. This establishes a link between CSP and the minimal classification error. For multiclass paradigms, we point out that CSP by joint approximate diagonalization (JAD) is equivalent to independent component analysis (ICA), and provide a method to choose those independent components (ICs) that approximately maximize mutual information of ICs and class labels. This eliminates the need for heuristics in multiclass CSP, and allows incorporating prior class probabilities. The proposed method is applied to the dataset IIIa of the third BCI competition, and is shown to increase the mean classification accuracy by 23.4% in comparison to multiclass CSP.",2008.0,"M. Grosse-Wentrup, M. Buss"
dff2fdf9357fd1d42bd1da91f442522022c38753,https://www.semanticscholar.org/paper/dff2fdf9357fd1d42bd1da91f442522022c38753,Automatic Extraction of Biological Information from Scientific Text: Protein-Protein Interactions,"We describe the basic design of a system for automatic detection of protein-protein interactions extracted from scientific abstracts. By restricting the problem domain and imposing a number of strong assumptions which include pre-specified protein names and a limited set of verbs that represent actions, we show that it is possible to perform accurate information extraction. The performance of the system is evaluated with different cases of real-world interaction networks, including the Drosophila cell cycle control. The results obtained computationally are in good agreement with current biological knowledge and demonstrate the feasibility of developing a fully automated system able to describe networks of protein interactions with sufficient accuracy.",1999.0,"C. Blaschke, Miguel Andrade, C. Ouzounis, A. Valencia"
6fbca0a45e0b976e7a8fad7745fbf1bbf74241a6,https://www.semanticscholar.org/paper/6fbca0a45e0b976e7a8fad7745fbf1bbf74241a6,TextRank Algorithm by Exploiting Wikipedia for Short Text Keywords Extraction,"The characteristic of poor information of short text often makes the effect of traditional keywords extraction not as good as expected. In this paper, we propose a graph-based ranking algorithm by exploiting Wikipedia as an external knowledge base for short text keywords extraction. To overcome the shortcoming of poor information of short text, we introduce the Wikipedia to enrich the short text. We regard each entry of Wikipedia as a concept, therefore the semantic information of each word can be represented by the distribution of Wikipedia's concept. And we measure the similarity between words by constructing the concept vector. Finally we construct keywords matrix and use TextRank for keywords extraction. The comparative experiments with traditional TextRank and baseline algorithm show that our method gets better precision, recall and F-measure value. It is shown that TextRank by exploiting Wikipedia is more suitable for short text keywords extraction.",2016.0,"Wengen Li, Jiabao Zhao"
88c21e06ed44da518a7e346fce416efedc771704,https://www.semanticscholar.org/paper/88c21e06ed44da518a7e346fce416efedc771704,Feature extraction via multi-view non-negative matrix factorization with local graph regularization,"Feature extraction is a crucial and difficult issue in pattern recognition tasks with the high-dimensional and multiple features. To extract the latent structure of multiple features without label information, multi-view learning algorithms have been developed. In this paper, motivated by manifold learning and multi-view Non-negative Matrix Factorization (NM-F), we introduce a novel feature extraction method via multi-view NMF with local graph regularization, where the inner-view relatedness between data is taken into consideration. We propose the matrix factorization objective function by constructing a nearest neighbor graph to integrate local geometrical information of each view and apply two iterative updating rules to effectively solve the optimization problem. In the experiment, we use the extracted feature to cluster several realistic datasets. The experimental results demonstrate the effectiveness of our proposed feature extraction approach.",2015.0,"Zhenfan Wang, Xiangwei Kong, Haiyan Fu, Ming Li, Yujia Zhang"
0c3411e652dc036239ec3103ed9ace2b4165783f,https://www.semanticscholar.org/paper/0c3411e652dc036239ec3103ed9ace2b4165783f,Automated extraction of information on protein-protein interactions from the biological literature,"MOTIVATION
To understand biological process, we must clarify how proteins interact with each other. However, since information about protein-protein interactions still exists primarily in the scientific literature, it is not accessible in a computer-readable format. Efficient processing of large amounts of interactions therefore needs an intelligent information extraction method. Our aim is to develop an efficient method for extracting information on protein-protein interaction from scientific literature.


RESULTS
We present a method for extracting information on protein-protein interactions from the scientific literature. This method, which employs only a protein name dictionary, surface clues on word patterns and simple part-of-speech rules, achieved high recall and precision rates for yeast (recall = 86.8% and precision = 94.3%) and Escherichia coli (recall = 82.5% and precision = 93.5%). The result of extraction suggests that our method should be applicable to any species for which a protein name dictionary is constructed.


AVAILABILITY
The program is available on request from the authors.",2001.0,"T. Ono, H. Hishigaki, A. Tanigami, T. Takagi"
8a8832216fa59867aab8bb98270763fc2de3d8d8,https://www.semanticscholar.org/paper/8a8832216fa59867aab8bb98270763fc2de3d8d8,A Shortest Path Dependency Kernel for Relation Extraction,"We present a novel approach to relation extraction, based on the observation that the information required to assert a relationship between two named entities in the same sentence is typically captured by the shortest path between the two entities in the dependency graph. Experiments on extracting top-level relations from the ACE (Automated Content Extraction) newspaper corpus show that the new shortest path dependency kernel outperforms a recent approach based on dependency tree kernels.",2005.0,"Razvan C. Bunescu, R. Mooney"
9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d,TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems,"TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.",2016.0,"Martín Abadi, Ashish Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. Corrado, Andy Davis, J. Dean, M. Devin, Sanjay Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Yangqing Jia, R. Józefowicz, Lukasz Kaiser, M. Kudlur, J. Levenberg, Dandelion Mané, R. Monga, Sherry Moore, D. Murray, C. Olah, M. Schuster, Jonathon Shlens, Benoit Steiner, I. Sutskever, Kunal Talwar, P. Tucker, Vincent Vanhoucke, Vijay Vasudevan, F. Viégas, O. Vinyals, Pete Warden, M. Wattenberg, M. Wicke, Yuan Yu, Xiaoqiang Zheng"
fa548e74c66c116ec436303203cfce50e33ffd9e,https://www.semanticscholar.org/paper/fa548e74c66c116ec436303203cfce50e33ffd9e,Supercritical Fluid Extraction: Principles and Practice,"The authors cover virtually every facet of supercritical fluid (SCF) technology: the history of SCF extraction, its underlying thermodynamic principles, process principles, industrial applications, and analysis of SCF research and development efforts. A review of 75 major SCF extraction patents provides an important source of technical and business information. Computer programs that can be used to calculate critical mixture and phase border curves for binary and ternary liquid-SCF and solid-SCF mixtures are included in the appendix. The book discusses the following contents: Introduction . Historical perspective . Phase diagrams for SCF-solute mixtures at high pressures . Experimental techniques for high pressure studies - Thermodynamic modeling of SCF-solute phase behaviour . Process operations . Early industrial applications - SCF process studies: 1976-81 . Polymer and monomer processing with SCF . SCF processing of pharmaceuticals, natural products and chemicals . Reactions in SCF . Special applications of SCF solvents . References and Index.",1986.0,"M. McHugh, V. Krukonis"
55a9c0361fc8f4c96e5811d067d6e6b1c4f0052b,https://www.semanticscholar.org/paper/55a9c0361fc8f4c96e5811d067d6e6b1c4f0052b,Review on Recent Advances in Information Mining From Big Consumer Opinion Data for Product Design,"In this paper, based on more than ten years' studies on this dedicated research thrust, a comprehensive review concerning information mining from big consumer opinion data in order to assist product design is presented. First, the research background and the essential terminologies regarding online consumer opinion data are introduced. Next, studies concerning information extraction and information utilization of big consumer opinion data for product design are reviewed. Studies on information extraction of big consumer opinion data are explained from various perspectives, including data acquisition, opinion target recognition, feature identification and sentiment analysis, opinion summarization and sampling, etc. Reviews on information utilization of big consumer opinion data for product design are explored in terms of how to extract critical customer needs from big consumer opinion data, how to connect the voice of the customers with product design, how to make effective comparisons and reasonable ranking on similar products, how to identify ever-evolving customer concerns efficiently, and so on. Furthermore, significant and practical aspects of research trends are highlighted for future studies. This survey will facilitate researchers and practitioners to understand the latest development of relevant studies and applications centered on how big consumer opinion data can be processed, analyzed, and exploited in aiding product design.",2018.0,"Jian Jin, Ying Liu, P. Ji, C. Kwong"
397556b032146ba3fa124d4014fe5cacae232c9b,https://www.semanticscholar.org/paper/397556b032146ba3fa124d4014fe5cacae232c9b,Efficient and robust feature extraction by maximum margin criterion,"In pattern recognition, feature extraction techniques are widely employed to reduce the dimensionality of data and to enhance the discriminatory information. Principal component analysis (PCA) and linear discriminant analysis (LDA) are the two most popular linear dimensionality reduction methods. However, PCA is not very effective for the extraction of the most discriminant features, and LDA is not stable due to the small sample size problem . In this paper, we propose some new (linear and nonlinear) feature extractors based on maximum margin criterion (MMC). Geometrically, feature extractors based on MMC maximize the (average) margin between classes after dimensionality reduction. It is shown that MMC can represent class separability better than PCA. As a connection to LDA, we may also derive LDA from MMC by incorporating some constraints. By using some other constraints, we establish a new linear feature extractor that does not suffer from the small sample size problem, which is known to cause serious stability problems for LDA. The kernelized (nonlinear) counterpart of this linear feature extractor is also established in the paper. Our extensive experiments demonstrate that the new feature extractors are effective, stable, and efficient.",2003.0,"Jun Liu, Songcan Chen, Xiaoyang Tan, Daoqiang Zhang"
21f80ae77d4a3dbecdbc56285c77bf0d3d6261a5,https://www.semanticscholar.org/paper/21f80ae77d4a3dbecdbc56285c77bf0d3d6261a5,An efficient location extraction algorithm by leveraging web contextual information,"A typical location extraction approach consists of two steps, location name detection and location entity disambiguation. Promising results have been obtained in the last decade based on natural language processing technologies. However, there are still two challenges which requires further investigation: 1)How to leverage the prior and contextual evidence to improve the location extraction performance, and 2) How to utilize the interdependence information between the named entity recognition step and disambiguation step. In this paper, we propose an iterative detection-ranking framework to address these problems as well as a set of novel features to mine contextual information from web resources. Experimental results show that our solution outperforms the state-of-the-art approaches, including Metacarta GeoTagger and Yahoo Placemaker.",2010.0,"Teng Qin, Rong Xiao, Lei Fang, Xing Xie, Lei Zhang"
3eb6fb89dfdd008adc6f2ca2ceffa255aa00b034,https://www.semanticscholar.org/paper/3eb6fb89dfdd008adc6f2ca2ceffa255aa00b034,Extracting Accurate Precursor Information for Tandem Mass Spectra by RawConverter.,"Extraction of data from the proprietary RAW files generated by Thermo Fisher mass spectrometers is the primary step for subsequent data analysis. High resolution and high mass accuracy data obtained by state-of-the-art mass spectrometers (e.g., Orbitraps) can significantly improve both peptide/protein identification and quantification. We developed RawConverter, a stand-alone software tool, to improve data extraction on RAW files from high-resolution Thermo Fisher mass spectrometers. RawConverter extracts full scan and MS(n) data from RAW files like its predecessor RawXtract; most importantly, it associates the accurate precursor mass-to-charge (m/z) value with the tandem mass spectrum. RawConverter accepts RAW data generated by either data-dependent acquisition (DDA) or data-independent acquisition (DIA). It generates output into MS1/MS2/MS3, MGF, or mzXML file formats, which fulfills the format requirements for most data identification and quantification tools. Using the tandem mass spectra extracted by RawConverter with corrected m/z values, 32.8%, 27.1%, and 84.1%, peptide spectra matches (PSMs) produce 17.4% (13.0%), 14.4% (11.5%), and 45.7% (36.2%) more peptide (protein) identifications than ProteoWizard, pXtract, and RawXtract, respectively. RawConverter is implemented in C# and is freely accessible at http://fields.scripps.edu/rawconv.",2015.0,"Lin He, J. Diedrich, Yen-Yin Chu, J. Yates"
7399b03ee38465722db5a2ddc90e2f108b43300b,https://www.semanticscholar.org/paper/7399b03ee38465722db5a2ddc90e2f108b43300b,Robust feature extraction via information theoretic learning,"In this paper, we present a robust feature extraction framework based on information-theoretic learning. Its formulated objective aims at simultaneously maximizing the Renyi's quadratic information potential of features and the Renyi's cross information potential between features and class labels. This objective function reaps the advantages in robustness from both redescending M-estimator and manifold regularization, and can be efficiently optimized via half-quadratic optimization in an iterative manner. In addition, the popular algorithms LPP, SRDA and LapRLS for feature extraction are all justified to be the special cases within this framework. Extensive comparison experiments on several real-world data sets, with contaminated features or labels, well validate the encouraging gain in algorithmic robustness from this proposed framework.",2009.0,"Xiao-Tong Yuan, Bao-Gang Hu"
d9aa2d1544abad1c9d7f3eab72df02275a4180c2,https://www.semanticscholar.org/paper/d9aa2d1544abad1c9d7f3eab72df02275a4180c2,Optimal extraction of information from finite quantum ensembles.,"Given only a finite ensemble of identically prepared particles, how precisely can one determine their states We describe optimal measurement procedures in the case of spin 1/2 particles. Furthermore, we prove that optimal measurement procedures must necessarily view the ensemble as a single composite system rather than as the sum of its components, i.e., optimal measurements cannot be realized by separate measurements on each particle.",1995.0,"S. Massar, S. Popescu"
ba557f3c549bc42c2a8c7a554eb052118d179a7f,https://www.semanticscholar.org/paper/ba557f3c549bc42c2a8c7a554eb052118d179a7f,"Efficient Feature Extraction, Encoding, and Classification for Action Recognition","Local video features provide state-of-the-art performance for action recognition. While the accuracy of action recognition has been continuously improved over the recent years, the low speed of feature extraction and subsequent recognition prevents current methods from scaling up to real-size problems. We address this issue and first develop highly efficient video features using motion information in video compression. We next explore feature encoding by Fisher vectors and demonstrate accurate action recognition using fast linear classifiers. Our method improves the speed of video feature extraction, feature encoding and action classification by two orders of magnitude at the cost of minor reduction in recognition accuracy. We validate our approach and compare it to the state of the art on four recent action recognition datasets.",2014.0,"Vadim Kantorov, I. Laptev"
7e79ac32bf9c15fe0348ebae4ed94feba60cdf27,https://www.semanticscholar.org/paper/7e79ac32bf9c15fe0348ebae4ed94feba60cdf27,Unsupervised Relation Extraction by Mining Wikipedia Texts Using Information from the Web,"This paper presents an unsupervised relation extraction method for discovering and enhancing relations in which a specified concept in Wikipedia participates. Using respective characteristics of Wikipedia articles and Web corpus, we develop a clustering approach based on combinations of patterns: dependency patterns from dependency analysis of texts in Wikipedia, and surface patterns generated from highly redundant information related to the Web. Evaluations of the proposed approach on two different domains demonstrate the superiority of the pattern combination over existing approaches. Fundamentally, our method demonstrates how deep linguistic patterns contribute complementarily with Web surface patterns to the generation of various relations.",2009.0,"Yulan Yan, Naoaki Okazaki, Y. Matsuo, Zhenglu Yang, M. Ishizuka"
fd71613b23a8209d278bb6672e76848da9ea3873,https://www.semanticscholar.org/paper/fd71613b23a8209d278bb6672e76848da9ea3873,Exploiting Shallow Linguistic Information for Relation Extraction from Biomedical Literature,"We propose an approach for extracting relations between entities from biomedical literature based solely on shallow linguistic information. We use a combination of kernel functions to integrate two different information sources: (i) the whole sentence where the relation appears, and (ii) the local contexts around the interacting entities. We performed experiments on extracting gene and protein interactions from two different data sets. The results show that our approach outperforms most of the previous methods based on syntactic and semantic information.",2006.0,"C. Giuliano, A. Lavelli, Lorenza Romano"
ef789e187f472b6f0732b4ec6fbd2154ad0f5700,https://www.semanticscholar.org/paper/ef789e187f472b6f0732b4ec6fbd2154ad0f5700,Extraction and exploration of spatio-temporal information in documents,"In the past couple of years, there have been significant advances in the areas of temporal information retrieval (TIR) and geographic information retrieval (GIR), each focusing on extracting and utilizing temporal and geographic information, respectively, from documents for search and exploration tasks. Interestingly, there is only little work that combines models, techniques and applications from these two areas to support scenarios and applications where temporal and geographic information in combination provide interesting meaningful nuggets in document exploration tasks, such as visualizing a chronological sequence of events with their locations.
 In this paper, we present an approach that combines the two areas of TIR and GIR. Using temporal and geographic information extracted from documents and recorded in temporal and geographic document profiles, we show how co-occurrences of such information are determined and spatio-temporal document profiles are computed. Such profiles then provide the basis for a variety of document search and exploration tasks, such as visualizing the sequences of events on a map. We present a prototypical implementation of our system and demonstrate the effectiveness of combining GIR and TIR in the context of document exploration tasks.",2010.0,"Jannik Strotgen, Michael Gertz, Pavel Popov"
efbeedfbf13db70878618553f0c4a0fec6f493fe,https://www.semanticscholar.org/paper/efbeedfbf13db70878618553f0c4a0fec6f493fe,Learning Collaborative Information Filters,"Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms.",1998.0,"Daniel Billsus, M. Pazzani"
d3353cdd2f3ae1c4a5e3ede5daa04e214137d621,https://www.semanticscholar.org/paper/d3353cdd2f3ae1c4a5e3ede5daa04e214137d621,Tree Kernel-Based Relation Extraction with Context-Sensitive Structured Parse Tree Information,"This paper proposes a tree kernel with contextsensitive structured parse tree information for relation extraction. It resolves two critical problems in previous tree kernels for relation extraction in two ways. First, it automatically determines a d ynamic context-sensitive tree span for relation extraction by extending the widely -used Shortest Path-enclosed Tree (SPT) to include necessary context information outside SPT. Second, it pr oposes a context -sensitive convolution tree kernel, which enumerates both context-free and contextsensitive sub-trees by consid ering their ancestor node paths as their contexts. Moreover, this paper evaluates the complementary nature between our tree kernel and a state -of-the-art linear kernel. Evaluation on the ACE RDC corpora shows that our dynamic context-sensitive tree span is much more suitable for relation extraction than SPT and our tree kernel outperforms the state-of-the-art Collins and Duffy’s convolution tree kernel. It also shows that our tree kernel achieves much better performance than the state-of-the-art linear kernels . Finally, it shows that feature-based and tree kernel-based methods much complement each other and the composite kernel can well integrate both flat and structured features.",2007.0,"Guodong Zhou, Min Zhang, D. Ji, Qiaoming Zhu"
0d040308f3c812fefe00f2be32859b0cee23f2d7,https://www.semanticscholar.org/paper/0d040308f3c812fefe00f2be32859b0cee23f2d7,"Automated Building Extraction from High-Resolution Satellite Imagery in Urban Areas Using Structural, Contextual, and Spectral Information","High-resolution satellite imagery provides an important new data source for building extraction. We demonstrate an integrated strategy for identifying buildings in 1-meter resolution satellite imagery of urban areas. Buildings are extracted using structural, contextual, and spectral information. First, a series of geodesic opening and closing operations are used to build a differential morphological profile (DMP) that provides image structural information. Building hypotheses are generated and verified through shape analysis applied to the DMP. Second, shadows are extracted using the DMP to provide reliable contextual information to hypothesize position and size of adjacent buildings. Seed building rectangles are verified and grown on a finely segmented image. Next, bright buildings are extracted using spectral information. The extraction results from the different information sources are combined after independent extraction. Performance evaluation of the building extraction on an urban test site using IKONOS satellite imagery of the City of Columbia, Missouri, is reported. With the combination of structural, contextual, and spectral information, of the building areas are extracted with a quality percentage.",2005.0,"Xiaoying Jin, C. Davis"
a6cf9c107456d47fb5c0e3510fd0976174da82f1,https://www.semanticscholar.org/paper/a6cf9c107456d47fb5c0e3510fd0976174da82f1,PAT-tree-based keyword extraction for Chinese information retrieval,"urgent need to promote Chinese in this paper we will raise the significance of keyword extraction using a new PAT-treebased approach, which is efficient in automatic keyword extraction from a set of relevant Chinese documents. This approach has been successfully applied in several IR researches, such as document classification, book indexing and relevance feedback. Many Chinese language processing applications therefore step ahead from character level to word/phrase level,",1997.0,Lee-Feng Chien
e25fa2027b70b9b528efdda62d8ac4f4c1116f83,https://www.semanticscholar.org/paper/e25fa2027b70b9b528efdda62d8ac4f4c1116f83,On the extraction of the channel allocation information in spectrum pooling systems,"The spectrum pooling strategy allows a license owner to share a part of his licensed spectrum with a secondary wireless system (the rental system, RS) during its idle times. The coexistence of two mobile systems on the same frequency band poses many new challenges, one of which is the reliable extraction of the channel allocation information (CAI), i.e. the channel occupation of the licensed system (LS). This paper presents a strategy for the extraction of the CAI based on exploiting the distinct cyclostationary characteristics of the LS and RS signals and demonstrates, via simulations, its application on a specific spectrum pooling scenario, where the LS is a GSM network and the RS is an OFDM based WLAN system",2007.0,"Mengüç Öner, F. Jondral"
5740f80fb61c4489674c9a0beb40c4f5e0ed19ff,https://www.semanticscholar.org/paper/5740f80fb61c4489674c9a0beb40c4f5e0ed19ff,YAGO: A Core of Semantic Knowledge Unifying WordNet and Wikipedia,"We present YAGO, a lightweight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains more than 1 million entities and 5 million facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as hasWonPrize). The facts have been automatically extracted from Wikipedia and unified with WordNet, using a carefully designed combination of rule-based and heuris-tic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations , products, etc. with their semantic relationships – and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact cor-rectness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information extraction techniques.",2007.0,"Fabian M. Suchanek, Gjergji Kasneci, G. Weikum"
ef696f12d7c4308ad1addd9b4e6909b9d2a44201,https://www.semanticscholar.org/paper/ef696f12d7c4308ad1addd9b4e6909b9d2a44201,Maximization of Mutual Information for Supervised Linear Feature Extraction,"In this paper, we present a novel scheme for linear feature extraction in classification. The method is based on the maximization of the mutual information (MI) between the features extracted and the classes. The sum of the MI corresponding to each of the features is taken as an heuristic that approximates the MI of the whole output vector. Then, a component-by-component gradient-ascent method is proposed for the maximization of the MI, similar to the gradient-based entropy optimization used in independent component analysis (ICA). The simulation results show that not only is the method competitive when compared to existing supervised feature extraction methods in all cases studied, but it also remarkably outperform them when the data are characterized by strongly nonlinear boundaries between classes.",2007.0,"J. Leiva-Murillo, Antonio Artés-Rodríguez"
cefa51093d04b15299a7d00f3e6c3cf43176868c,https://www.semanticscholar.org/paper/cefa51093d04b15299a7d00f3e6c3cf43176868c,Extraction of high-resolution frames from video sequences,"The human visual system appears to be capable of temporally integrating information in a video sequence in such a way that the perceived spatial resolution of a sequence appears much higher than the spatial resolution of an individual frame. While the mechanisms in the human visual system that do this are unknown, the effect is not too surprising given that temporally adjacent frames in a video sequence contain slightly different, but unique, information. This paper addresses the use of both the spatial and temporal information present in a short image sequence to create a single high-resolution video frame. A novel observation model based on motion compensated subsampling is proposed for a video sequence. Since the reconstruction problem is ill-posed, Bayesian restoration with a discontinuity-preserving prior image model is used to extract a high-resolution video still given a short low-resolution sequence. Estimates computed from a low-resolution image sequence containing a subpixel camera pan show dramatic visual and quantitative improvements over bilinear, cubic B-spline, and Bayesian single frame interpolations. Visual and quantitative improvements are also shown for an image sequence containing objects moving with independent trajectories. Finally, the video frame extraction algorithm is used for the motion-compensated scan conversion of interlaced video data, with a visual comparison to the resolution enhancement obtained from progressively scanned frames.",1996.0,"R. Schultz, R. Stevenson"
f31ecc778a6d384e7e22d836d84951804eeb0101,https://www.semanticscholar.org/paper/f31ecc778a6d384e7e22d836d84951804eeb0101,Information Discriminant Analysis: Feature Extraction with an Information-Theoretic Objective,"Using elementary information-theoretic tools, we develop a novel technique for linear transformation from the space of observations into a low-dimensional (feature) subspace for the purpose of classification. The technique is based on a numerical optimization of an information-theoretic objective function, which can be computed analytically. The advantages of the proposed method over several other techniques are discussed and the conditions under which the method reduces to linear discriminant analysis are given. We show that the novel objective function enjoys many of the properties of the mutual information and the Bayes error and we give sufficient conditions for the method to be Bayes-optimal. Since the objective function is maximized numerically, we show how the calculations can be accelerated to yield feasible solutions. The performance of the method compares favorably to other linear discriminant-based feature extraction methods on a number of simulated and real-world data sets.",2007.0,Z. Nenadic
a54f1bb74352fa15435e4492073b7c9b8480c0be,https://www.semanticscholar.org/paper/a54f1bb74352fa15435e4492073b7c9b8480c0be,Extraction of facial regions and features using color and shape information,"There are many applications for systems coping with the problem of face localization and recognition, e.g. model-based video coding, security systems and mug shot matching. Due to variations in illumination, back-ground, visual angle and facial expressions, the problem of machine face recognition is complex. In this paper we present a robust approach for the extraction of facial regions and features out of color images. First, face candidates are located based on the color and shape information. Then the topographic grey-level relief of facial regions is evaluated to determine the position of facial features as eyes and month. Results are shown for two example scenes.",1996.0,"K. Sobottka, I. Pitas"
c8d0bbb9cfaf417baf33daaa94f2980df1cd7e4b,https://www.semanticscholar.org/paper/c8d0bbb9cfaf417baf33daaa94f2980df1cd7e4b,"STALKER: Learning Extraction Rules for Semistructured, Web-based Information Sources *","Information mediators are systems capable of providing a unified view of several information sources. Central to any mediator that accesses Web-based sources is a set of wrappers that can extract relevant information from Web pages. In this paper, we present a wrapper-induction algorithm that generates extraction rules for Web-based information sources. We introduce landmark automata, a formalism that describes classes of extraction rules. Our wrapper induction algorithm, STALKER, generates extraction rules that are expressed as simple landmark grammars, which are a class of landmark automata that is more expressive than the existing extraction languages. Based on just a few training examples STALKER learns extraction rules for documents with multiple levels of embedding. The experimental results show that our approach successfully wraps classes of documents that can not be wrapped by existing techniques.",1998.0,"Ion Muslea, S. Minton, Craig A. Knoblock"
f0a3cf484443f6cb4a32ef790318fd525881ec1f,https://www.semanticscholar.org/paper/f0a3cf484443f6cb4a32ef790318fd525881ec1f,Gait Recognition Using Compact Feature Extraction Transforms and Depth Information,"This paper proposes an innovative gait identification and authentication method based on the use of novel 2-D and 3-D features. Depth-related data are assigned to the binary image silhouette sequences using two new transforms: the 3-D radial silhouette distribution transform and the 3-D geodesic silhouette distribution transform. Furthermore, the use of a genetic algorithm is presented for fusing information from different feature extractors. Specifically, three new feature extraction techniques are proposed: the two of them are based on the generalized radon transform, namely the radial integration transform and the circular integration transform, and the third is based on the weighted Krawtchouk moments. Extensive experiments carried out on USF ldquoGait Challengerdquo and proprietary HUMABIO gait database demonstrate the validity of the proposed scheme.",2007.0,"D. Ioannidis, D. Tzovaras, I. Damousis, Savvas Argyropoulos, K. Moustakas"
208f7041a2b90c522d4a352fc941864caa660967,https://www.semanticscholar.org/paper/208f7041a2b90c522d4a352fc941864caa660967,Application of Information Technology: An Integrated Software Suite for Surface-based Analyses of Cerebral Cortex,"The authors describe and illustrate an integrated trio of software programs for carrying out surface-based analyses of cerebral cortex. The first component of this trio, SureFit (Surface Reconstruction by Filtering and Intensity Transformations), is used primarily for cortical segmentation, volume visualization, surface generation, and the mapping of functional neuroimaging data onto surfaces. The second component, Caret (Computerized Anatomical Reconstruction and Editing Tool Kit), provides a wide range of surface visualization and analysis options as well as capabilities for surface flattening, surface-based deformation, and other surface manipulations. The third component, SuMS (Surface Management System), is a database and associated user interface for surface-related data. It provides for efficient insertion, searching, and extraction of surface and volume data from the database.",2001.0,"D. C. V. Essen, H. A. Drury, James Dickson, John W. Harwell, Donna Hanlon, Charles H. Anderson"
79920a9fee63a809ba7cecad587f816d2ab7de1b,https://www.semanticscholar.org/paper/79920a9fee63a809ba7cecad587f816d2ab7de1b,Benchmarking information technology investment and benefits extraction,"Despite the fact that many companies are increasing their expenditure on information technology (IT) to obtain or even sustain a competitive advantage in their respective marketplaces, many studies show that the benefits from IT systems have been considerably less than expected. Managers are often left with the quandary of how to evaluate investments and realise maximum benefits in IT. Reasons for this difficulty have been suggested in the normative literature centring around the socio‐technical (human and organisational) dimensions associated with IT deployment. The inability of managers to determine the true costs of deploying IT are considered attributable to a lack of knowledge and understanding of IT‐related costs and benefits measurements. This paper discusses from a critical point of view the evaluation of IT/IS investment and best practices in benefits extraction from such investment. The discussion is based on relevant literature and information from ongoing research by the authors involving companies in the construction, pharmaceutical and computer hardware sectors.",2003.0,"S. Alshawi, Z. Irani, L. Baldwin"
607bbfc2f48a95203621391e5b52bbc9e9f1f14c,https://www.semanticscholar.org/paper/607bbfc2f48a95203621391e5b52bbc9e9f1f14c,Ontology-based extraction and structuring of information from data-rich unstructured documents,"We present a new approach to extracting information from unstructured documents based on an application ontology that describes a domain of interest. Starting with such an ontology, we formulate rules to extract constants and context keywords from unstructured documents. For each unstructured document of interest, we extract its constants and keywords and apply a recognizer to organize extracted constants as attribute values of tuples in a generated database schema. To make our approach general, we fix all the processes and change only the ontological description for a different application domain. In experiments we conducted on two different types of unstructured documents taken from the Web, our approach attained recall ratios in the 80% and 90% range and precision ratios near 98%.",1998.0,"D. Embley, D. M. Campbell, Randy D. Smith, Stephen W. Liddle"
5d4f1ad04d4392b2dc569944e35d57bfce90fe2a,https://www.semanticscholar.org/paper/5d4f1ad04d4392b2dc569944e35d57bfce90fe2a,Feature extraction using information-theoretic learning,"A classification system typically consists of both a feature extractor (preprocessor) and a classifier. These two components can be trained either independently or simultaneously. The former option has an implementation advantage since the extractor need only be trained once for use with any classifier, whereas the latter has an advantage since it can be used to minimize classification error directly. Certain criteria, such as minimum classification error, are better suited for simultaneous training, whereas other criteria, such as mutual information, are amenable for training the feature extractor either independently or simultaneously. Herein, an information-theoretic criterion is introduced and is evaluated for training the extractor independently of the classifier. The proposed method uses nonparametric estimation of Renyi's entropy to train the extractor by maximizing an approximation of the mutual information between the class labels and the output of the feature extractor. The evaluations show that the proposed method, even though it uses independent training, performs at least as well as three feature extraction methods that train the extractor and classifier simultaneously",2006.0,"K. Hild, Deniz Erdoğmuş, K. Torkkola, J. Príncipe"
311eb232e4bd3ed53b1ef3381d75b65615d4e29c,https://www.semanticscholar.org/paper/311eb232e4bd3ed53b1ef3381d75b65615d4e29c,Typed Tensor Decomposition of Knowledge Bases for Relation Extraction,"While relation extraction has traditionally been viewed as a task relying solely on textual data, recent work has shown that by taking as input existing facts in the form of entity-relation triples from both knowledge bases and textual data, the performance of relation extraction can be improved significantly. Following this new paradigm, we propose a tensor decomposition approach for knowledge base embedding that is highly scalable, and is especially suitable for relation extraction. By leveraging relational domain knowledge about entity type information, our learning algorithm is significantly faster than previous approaches and is better able to discover new relations missing from the database. In addition, when applied to a relation extraction task, our approach alone is comparable to several existing systems, and improves the weighted mean average precision of a state-of-theart method by 10 points when used as a subcomponent.",2014.0,"Kai-Wei Chang, Wen-tau Yih, Bishan Yang, Christopher Meek"
37e5ddd73bfe681e5ada49fee62138511887eca2,https://www.semanticscholar.org/paper/37e5ddd73bfe681e5ada49fee62138511887eca2,PAT-tree-based adaptive keyphrase extraction for intelligent Chinese information retrieval,"Considering the urgent need for keyphrase extraction techniques in intelligent information retrieval, in this paper we present a PAT-tree-based adaptive approach, which is critical and fundamental for Chinese and other oriental languages. Compared with conventional dictionary-based approaches, the proposed approach can reduce the reliance on rigid lexicon and sophisticated word segmentation, and compared with conventional statistics-based approaches, it can handle phrases composed of high-frequency words regardless of phrase length. Furthermore, the approach has been designed carefully with Internet utilization in mind. For instance, it can be easily integrated into text retrieval systems to provide automatic term suggestion and is adaptable to changes of the database content. The proposed approach has been successfully used in several information retrieval applications, such as automatic term suggestion, domain-specific lexicon construction, book indexing and document classification. Many Chinese and oriental language processing applications are, therefore, able to move ahead from the character level to the word or phrase level.",1999.0,Lee-Feng Chien
a2afe51ee6456d69b46fd5c81cfbe7df0486a947,https://www.semanticscholar.org/paper/a2afe51ee6456d69b46fd5c81cfbe7df0486a947,A methodology for information theoretic feature extraction,"We discuss an unsupervised feature extraction method which is driven by an information theoretic based criterion: mutual information. While information theoretic signal processing has been examined by many authors the method presented here is more closely related to the approaches of Linsker (1988, 1990), Bell and Sejnowski (1995), and Viola et al. (1996). The method we discuss differs from previous work in several aspects. It is extensible to a feed-forward multilayer perceptron with an arbitrary number of layers. No assumptions are made about the underlying PDF of the input space. It exploits a property of entropy coupled with a saturating nonlinearity resulting in a method for entropy manipulation with computational complexity proportional to the number of data samples squared This represents a significant computational savings over previous methods. As mutual information is a function of two entropy terms, the method for entropy manipulation can be directly applied to the mutual information as well.",1998.0,"John W. Fisher III, J. Príncipe"
6d02b9401c8cd7298515bfec3c2294ef13face78,https://www.semanticscholar.org/paper/6d02b9401c8cd7298515bfec3c2294ef13face78,Corpus-based terminology extraction applied to information access,"This paper presents an application of corpus-based terminology extraction in interactive information retrieval. In this approach, the terminology obtained in an automatic extraction procedure is used, without any manual revision, to provide retrieval indexes and a “browsing by phrases” facility for document accessing in an interactive retrieval search interface. We argue that the combination of automatic terminology extraction and interactive search provides an optimal balance between controlled-vocabulary document retrieval (where thesauri are costly to acquire and maintain) and free text retrieval (where complex terms associated to domain specific concepts are largely overseen).",2001.0,"Anselmo Peñas, F. Verdejo, Julio Gonzalo"
c2dedd7ac6bf7828534ebd7853a402c238d9307f,https://www.semanticscholar.org/paper/c2dedd7ac6bf7828534ebd7853a402c238d9307f,Extracting Information from Textual Documents in the Electronic Health Record: A Review of Recent Research,"Summary Objectives We examine recent published research on the extraction of information from textual documents in the Electronic Health Record (EHR). Methods Literature review of the research published after 1995, based on PubMed, conference proceedings, and the ACM Digital Library, as well as on relevant publications referenced in papers already included. Results 174 publications were selected and are discussed in this review in terms of methods used, pre-processing of textual documents, contextual features detection and analysis, extraction of information in general, extraction of codes and of information for decision-support and enrichment of the EHR, information extraction for surveillance, research, automated terminology management, and data mining, and de-identification of clinical text. Conclusions Performance of information extraction systems with clinical text has improved since the last systematic review in 1995, but they are still rarely applied outside of the laboratory they have been developed in. Competitive challenges for information extraction from clinical text, along with the availability of annotated clinical text corpora, and further improvements in system performance are important factors to stimulate advances in this field and to increase the acceptance and usage of these systems in concrete clinical and biomedical research contexts.",2008.0,"S. Meystre, G. Savova, K. Kipper-Schuler, John F. Hurdle"
033f25ad905ef2ed32a8331cf38b83953ff15922,https://www.semanticscholar.org/paper/033f25ad905ef2ed32a8331cf38b83953ff15922,A Review of Relational Machine Learning for Knowledge Graphs,"Relational machine learning studies methods for the statistical analysis of relational, or graph-structured, data. In this paper, we provide a review of how such statistical models can be “trained” on large knowledge graphs, and then used to predict new facts about the world (which is equivalent to predicting new edges in the graph). In particular, we discuss two fundamentally different kinds of statistical relational models, both of which can scale to massive data sets. The first is based on latent feature models such as tensor factorization and multiway neural networks. The second is based on mining observable patterns in the graph. We also show how to combine these latent and observable models to get improved modeling power at decreased computational cost. Finally, we discuss how such statistical models of graphs can be combined with text-based information extraction methods for automatically constructing knowledge graphs from the Web. To this end, we also discuss Google's knowledge vault project as an example of such combination.",2015.0,"Maximilian Nickel, K. Murphy, Volker Tresp, E. Gabrilovich"
5a26636fd98774b15d510ab3edee048fc8947702,https://www.semanticscholar.org/paper/5a26636fd98774b15d510ab3edee048fc8947702,Face localization and facial feature extraction based on shape and color information,"Recognition of human faces out of still images or image sequences is a research field of fast increasing interest. At first, facial regions and facial features like eyes and mouth have to be extracted. In the present paper we propose an approach that copes with problems of these first two steps. We perform face localization based on the observation that human faces are characterized by their oval shape and skin-color, also in the case of varying light conditions. For that we segment faces by evaluating shape and color (HSV) information. Then face hypotheses are verified by searching for facial features inside of the face-like regions. This is done by applying morphological operations and minima localization to intensity images.",1996.0,"K. Sobottka, I. Pitas"
b8060eb3ad269d088b3c5bc183d3e91cb31045f5,https://www.semanticscholar.org/paper/b8060eb3ad269d088b3c5bc183d3e91cb31045f5,Using the web for automated translation extraction in cross-language information retrieval,"There have been significant advances in Cross-Language Information Retrieval (CLIR) in recent years. One of the major remaining reasons that CLIR does not perform as well as monolingual retrieval is the presence of out of vocabulary (OOV) terms. Previous work has either relied on manual intervention or has only been partially successful in solving this problem. We use a method that extends earlier work in this area by augmenting this with statistical analysis, and corpus-based translation disambiguation to dynamically discover translations of OOV terms. The method can be applied to both Chinese-English and English-Chinese CLIR, correctly extracting translations of OOV terms from the Web automatically, and thus is a significant improvement on earlier work.",2004.0,"Y. Zhang, Phil Vines"
4a48e0d97d9f61a92841151b7cd3d4923edd6a8e,https://www.semanticscholar.org/paper/4a48e0d97d9f61a92841151b7cd3d4923edd6a8e,Learning user information interests through extraction of semantically significant phrases,"InformationFinder is an intelligent agent hat learns user information interests from sets of messages or other on-line documents that users have classified. While this problem has been addressed by a number of recent research initiatives, hiformationFinder’s approach is innovative in a number of ways. First, the agent uses heuristics to extract significant phrases from documents for learning rather than use standard mathematical techniques. This enables it to learn highly general search criteria based on a small number of sample documents. Second, the agent learns standard ecision trees for each user category. These decision trees are easily transformed into search query strings for standard search systems rather than requiring specialized search engines. 1. Large-scale on-line information systems A growing number of businesses and institutions are using distributed information repositories to store large numbers of documents of various types. The growth of Intemet services such as the World Wide Web and Gopher, the continued increase in use of Usenet bulletin boards, and the emergence on the market of distributed database platforms such as Lotus Notes TM all enable organizations of any size to collect and organize large heterogeneous collections of documents ranging from working notes, memos and electronic mail to complete reports, proposals, design documentation, and databases. However, traditional techniques for identifying and gathering relevant documents become unmanageable when the organizations and document collections get very large. This problem exists outside of corporate information repositories as well. On the Internet’s World Wide Web, for instance, it is impossible to even attempt to see all pages that may be of interest. It is equally impossible to simply scan all of the news media (such as newspaper and magazine articles) that are becoming available on the Web. The same is true of other information systems based on the Internet and other world-wide networks, such as Usenet bulletin boards This paper describes an intelligent agent developed to address this problem similar to research systems under development for similar tasks [Holte and Drummond, 1994; Knoblock and Arens, 1994; Levy et. aL, 1994; Pazzani et. aL, 1995] or for other tasks such as e-mail filtering or Usenet message filtering. The agent learns a search query string for each of the user’s interest categories, and searches nightly for new documents hat match these interests to send to the user. Our most significant finding is that effective results depend largely on extracting high-quality indicator phrases from the documents for input to the learning algorithm and less on the particular induction algorithms employed. We present our solution in the context of a Lotus Notes system, consisting of electronic mail, bulletin boards, news services, and databases, but our approach is equally applicable to both the World Wide Web and Usenet. We are planning to make our InformationFinder publicly available for these systems in the near future. 2. Learning user interests Figure 1 shows a user reading a document about Java, a language for Intemet development. Upon reading this document, he user decides that it is representative of his interest in Java. To indicate this to InfoFinder the user selects the ""smiley face"" icon in the upper right comer. The agent asks the user to categorize his interest in the document, which he gives as ""Java."" These categories are fully user-specified and need not be given names representative of the content: they are used simply for grouping of documents (e.g., [Gil, 1994; Lieberman, 1994]) and communication with the user. The document is copied into a collection of sample documents for subsequent processing. 110 From: AAAI Technical Report SS-96-05. Compilation copyright © 1996, AAAI (www.aaai.org). All rights reserved.",1996.0,"B. Krulwich, Chad Burkey"
3a81ea76d2f1f496dfe3be903f50a0d81883bfa1,https://www.semanticscholar.org/paper/3a81ea76d2f1f496dfe3be903f50a0d81883bfa1,Symmetry in 3D Geometry: Extraction and Applications,"The concept of symmetry has received significant attention in computer graphics and computer vision research in recent years. Numerous methods have been proposed to find, extract, encode and exploit geometric symmetries and high‐level structural information for a wide variety of geometry processing tasks. This report surveys and classifies recent developments in symmetry detection. We focus on elucidating the key similarities and differences between existing methods to gain a better understanding of a fundamental problem in digital geometry processing and shape understanding in general. We discuss a variety of applications in computer graphics and geometry processing that benefit from symmetry information for more effective processing. An analysis of the strengths and limitations of existing algorithms highlights the plenitude of opportunities for future research both in terms of theory and applications.",2013.0,"N. Mitra, M. Pauly, Michael Wand, Duygu Ceylan"
5a68e5ecbb878bfef82a9aebf1ce6d5c252da2b0,https://www.semanticscholar.org/paper/5a68e5ecbb878bfef82a9aebf1ce6d5c252da2b0,BUSINESS KNOWLEDGE EXTRACTION FROM LEGACY INFORMATION SYSTEMS,This article discusses the process of enterprise knowledge extraction from relational database and source code of legacy information systems. Problems of legacy systems and main solutions for them are briefly described here. The uses of data reverse engineering and program understanding techniques to automatically infer as much as possible the schema and semantics of a legacy information system is analyzed. Eight step data reverse engineering algorithm for knowledge extraction from legacy systems is provided. A hypothetical example of knowledge extraction from legacy information system is presented.,2006.0,"B. Paradauskas, A. Laurikaitis"
ba8977eb5d737d643f38d84d0c1476211bb88986,https://www.semanticscholar.org/paper/ba8977eb5d737d643f38d84d0c1476211bb88986,Extracting information nuggets from disaster- Related messages in social media,"Microblogging sites such as Twitter can play a vital role in spreading information during “natural” or man-made disasters. But the volume and velocity of tweets posted during crises today tend to be extremely high, making it hard for disaster-affected communities and professional emergency responders to process the information in a timely manner. Furthermore, posts tend to vary highly in terms of their subjects and usefulness; from messages that are entirely off-topic or personal in nature, to messages containing critical information that augments situational awareness. Finding actionable information can accelerate disaster response and alleviate both property and human losses. In this paper, we describe automatic methods for extracting information from microblog posts. Specifically, we focus on extracting valuable “information nuggets”, brief, self-contained information items relevant to disaster response. Our methods leverage machine learning methods for classifying posts and information extraction. Our results, validated over one large disaster-related dataset, reveal that a careful design can yield an effective system, paving the way for more sophisticated data analysis and visualization systems.",2013.0,"Muhammad Imran, Shady Elbassuoni, Carlos Castillo, Fernando Diaz, P. Meier"
aaac5d1c1932a4b9d7fa0298587c612480af3c18,https://www.semanticscholar.org/paper/aaac5d1c1932a4b9d7fa0298587c612480af3c18,PCA-based feature extraction using class information,"Feature extraction is necessary to classify a data with large dimension such as image data. It is important that the obtained features include the maximum information of input data. The representative methods for feature extraction are PCA, ICA, LDA and MLP etc. PCA, LDA are unsupervised type algorithms, and LDA, MLP are supervised type algorithms. Supervised type algorithms are more suitable for feature extraction because of using input data with class information. In this paper, we suggest the feature extraction scheme which uses class information to extract features by PCA. We test our algorithm using Yale face database and analyze the performance to compare with other algorithms.",2005.0,"M. Park, J. Na, J. Choi"
45e0f73e263408c3003552aeb34a66e988a4e5ac,https://www.semanticscholar.org/paper/45e0f73e263408c3003552aeb34a66e988a4e5ac,Extraction of Motion Information from Peripheral Processes,"This paper is mainly concerned with low-level processes in machine perception of motion. A motion analysis system should exploit information contained in ``early warning signals'' during the intensity based peripheral phase of motion perception. We show that intensity based difference pictures contain motion information about objects in a dynamic scene, and present methods for the extraction of motion information in the peripheral phase. Some experiments with laboratory generated and real world scenes demonstrate the potential of the technique.",1981.0,R. Jain
956e627612bc8044c2a16e0b91e210f33b1de24b,https://www.semanticscholar.org/paper/956e627612bc8044c2a16e0b91e210f33b1de24b,Extraction of information to the left of the fixated word in reading.,"The present experiment used 2 different eye-contingent display change techniques to determine whether information is extracted from English text even when it is to the left of the currently fixated word. Preview display changes were during the 1st saccade entering the target word region, whereas postview display changes were during the 1st saccade leaving that region. Previews and postviews were either identical, related, or unrelated to the target word. ""Wrong"" information in the target-word region affected reading even when that information was seen only after readers were fixating to the right of that region: When readers skipped the target word, such information caused readers to regress to the target word more; when readers initially fixated the target word, such information increased ""2nd-pass"" processing time on the target region. The data suggest that readers often still attend to a word after it is skipped and that when readers fixate a word, they occasionally attend to the word after they have begun to fixate the next word.",1999.0,"K. Binder, A. Pollatsek, Keith Rayner"
2d29dab354525bdbfdb759129396759e521a7cc9,https://www.semanticscholar.org/paper/2d29dab354525bdbfdb759129396759e521a7cc9,Multilingual single document keyword extraction for information retrieval,"Keywords play an important role in many aspects of information retrieval (IR). From Web searches to text summarization good keywords are a necessity. In a typical IR system algorithms are used which require the entire document collection to be built beforehand. While some research has been done on extracting keywords from a single document, the quality of the keywords was not based on how well they perform in IR tasks. Moreover, they are designed for only one language and the applicability to other languages is unknown. As such, this paper proposes a new algorithm that is applicable to multiple languages and extracts effective keywords that, to a high degree, uniquely identify a document. It needs only a single document to extract keywords and does not rely on machine learning methods. It was tested on a Japanese-English bilingual corpus and a portion of the Reuter's corpus using a keyword search algorithm. The results show that the extracted keywords do a good job at uniquely identifying the documents.",2005.0,"D. Bracewell, F. Ren, S. Kuriowa"
4d58fbb388884caf78a0652a02ee279883d86f75,https://www.semanticscholar.org/paper/4d58fbb388884caf78a0652a02ee279883d86f75,Extraction of Semantic Information from an Ordinary English Dictionary and its Evaluation,"The automatic extraction of semantic information especially semantic relationships between words, from an odinary English dictionary is described. For the extraction, the magnetic tape version of LDOCE (Longman Dictionary of Contemporary English, 1978 edition) is loaded into a relational database system. Developed extraction programs analyze a definition sentence in LDOCE with a pattern matching based algorithm. Since this algorithm is not perfect, the result of the extraction has been compared with semantic information (semantic markers) which the magnetic tape version of LDOCE contains. The result of comparison is also discussed for evaluating the reliability of such an automatic extraction.",1988.0,"J. Nakamura, M. Nagao"
2cfcae505ea3a95388ca0c535a30ce7b0b6c03fd,https://www.semanticscholar.org/paper/2cfcae505ea3a95388ca0c535a30ce7b0b6c03fd,KPSpotter: a flexible information gain-based keyphrase extraction system,"To tackle the issue of information overload, we present an Information Gain-based KeyPhrase Extraction System, called KPSpotter. KPSpotter is a flexible web-enabled keyphrase extraction system, capable of processing various formats of input data, including web data, and generating the extraction model as well as the list of keyphrases in XML. In KPSpotter, the following two features were selected for training and extracting keyphrases: 1) TF*IDF and 2) Distance from First Occurrence. Input training and testing collections were processed in three stages: 1) Data Cleaning, 2) Data Tokenizing, and 3) Data Discretizing. To measure the system performance, the keyphrases extracted by KPSpotter are compared with the ones that the authors assigned. Our experiments show that the performance of KPSpotter was evaluated to be equivalent to KEA, a well-known keyphrase extraction system. KPSpotter, however, is differentiated from other extraction systems in the followings: First, KPSpotter employs a new keyphrase extraction technique that combines the Information Gain data mining measure and several Natural Language Processing techniques such as stemming and case-folding. Second, KPSpotter is able to process various types of input data such as XML, HTML, and unstructured text data and generate XML output. Third, the user can provide input data and execute KPSpotter through the Internet. Fourth, for efficiency and performance reason, KPSpotter stores candidate keyphrases and its related information such as frequency and stemmed form into an embedded database management system.",2003.0,"Min Song, I. Song, Xiaohua Hu"
967972821567b8a34dc058c9fbf60c4054dc3b69,https://www.semanticscholar.org/paper/967972821567b8a34dc058c9fbf60c4054dc3b69,GATE: an Architecture for Development of Robust HLT applications,"In this paper we present GATE, a framework and graphical development environment which enables users to develop and deploy language engineering components and resources in a robust fashion. The GATE architecture has enabled us not only to develop a number of successful applications for various language processing tasks (such as Information Extraction), but also to build and annotate corpora and carry out evaluations on the applications generated. The framework can be used to develop applications and resources in multiple languages, based on its thorough Unicode support.",2002.0,"H. Cunningham, D. Maynard, Kalina Bontcheva, V. Tablan"
4ade421185657495a673cced08d8f79faaae3f69,https://www.semanticscholar.org/paper/4ade421185657495a673cced08d8f79faaae3f69,Feature Extraction Based on Morlet Wavelet and its Application for Mechanical Fault Diagnosis,"Abstract The vibration signals of a machine always carry the dynamic information of the machine. These signals are very useful for the feature extraction and fault diagnosis. However, in many cases, because these signals have very low signal-to-noise ratio (SNR), to extract feature components becomes difficult and the applicability of information drops down. Wavelet analysis in an effective tool for signal processing and feature extraction. In this paper, a denoising method based on wavelet analysis is applied to feature extraction for mechanical vibration signals. This method is an advanced version of the famous “soft-thresholding denoising method” proposed by Donoho and Johnstone. Based on the Morlet wavelet, the time-frequency resolution can be adapted to different signals of interest. In this paper, this denoising method is introduced in detail. The results of the application in rolling bearing diagnosis and gear-box diagnosis are satisfactory.",2000.0,"Jing Lin, L. Qu"
40d44fad89f4c49cf898766b7808185ba4a5a1c9,https://www.semanticscholar.org/paper/40d44fad89f4c49cf898766b7808185ba4a5a1c9,Topic detection and tracking: event-based information organization,"Topic Detection and Tracking: Event-based Information Organization brings together in one place state-of-the-art research in Topic Detection and Tracking (TDT). This collection of technical papers from leading researchers in the field not only provides several chapters devoted to the research program and its evaluation paradigm, but also presents the most current research results and describes some of the remaining open challenges. Topic Detection and Tracking: Event-based Information Organization is an excellent reference for researchers and practitioners in a variety of fields related to TDT, including information retrieval, automatic speech recognition, machine learning, and information extraction",2002.0,James Allan
8169628b9419df40e3f51024fae02a5dbcdb9f07,https://www.semanticscholar.org/paper/8169628b9419df40e3f51024fae02a5dbcdb9f07,A Survey of Shape Feature Extraction Techniques,"""A picture is worth one thousand words"". This proverb comes from Confucius a Chinese philosopher before about 2500 years ago. Now, the essence of these words is universally understood. A picture can be magical in its ability to quickly communicate a complex story or a set of ideas that can be recalled by the viewer later in time. Visual information plays an important role in our society, it will play an increasingly pervasive role in our lives, and there will be a growing need to have these sources processed further. The pictures or images are used in many application areas like architectural and engineering design, fashion, journalism, advertising, entertainment, etc. Thus it provides the necessary opportunity for us to use the abundance of images. However, the knowledge will be useless if one can't _nd it. In the face of the substantive and increasing apace images, how to search and to retrieve the images that we interested with facility is a fatal problem: it brings a necessity for image retrieval systems. As we know, visual features of the images provide a description of their content. Content-based image retrieval (CBIR), emerged as a promising mean for retrieving images and browsing large images databases. CBIR has been a topic of intensive research in recent years. It is the process of retrieving images from a collection based on automatically extracted features.",2008.0,"Ming Yang, K. Kidiyo, R. Joseph"
0c897fe4f9b00157ace71f70ea344f7d7e3b9d9e,https://www.semanticscholar.org/paper/0c897fe4f9b00157ace71f70ea344f7d7e3b9d9e,Secret Key Extraction from Wireless Signal Strength in Real Environments,"We evaluate the effectiveness of secret key extraction, for private communication between two wireless devices, from the received signal strength (RSS) variations on the wireless channel between the two devices. We use real world measurements of RSS in a variety of environments and settings. The results from our experiments with 802.11-based laptops show that in certain environments, due to lack of variations in the wireless channel, the extracted bits have very low entropy making these bits unsuitable for a secret key, an adversary can cause predictable key generation in these static environments, and in dynamic scenarios where the two devices are mobile, and/or where there is a significant movement in the environment, high entropy bits are obtained fairly quickly. Building on the strengths of existing secret key extraction approaches, we develop an environment adaptive secret key generation scheme that uses an adaptive lossy quantizer in conjunction with Cascade-based information reconciliation and privacy amplification. Our measurements show that our scheme, in comparison to the existing ones that we evaluate, performs the best in terms of generating high entropy bits at a high bit rate. The secret key bit streams generated by our scheme also pass the randomness tests of the NIST test suite that we conduct. We also build and evaluate the performance of secret key extraction using small, low-power, hand-held devices-Google Nexus One phones-that are equipped 802.11 wireless network cards. Last, we evaluate secret key extraction in a multiple input multiple output (MIMO)-like sensor network testbed that we create using multiple TelosB sensor nodes. We find that our MIMO-like sensor environment produces prohibitively high bit mismatch, which we address using an iterative distillation stage that we add to the key extraction process. Ultimately, we show that the secret key generation rate is increased when multiple sensors are involved in the key extraction process.",2009.0,"S. Jana, S. N. Premnath, M. Clark, S. Kasera, Neal Patwari, S. Krishnamurthy"
1c909ac1c331c0c246a88da047cbdcca9ec9b7e7,https://www.semanticscholar.org/paper/1c909ac1c331c0c246a88da047cbdcca9ec9b7e7,Large-Scale Named Entity Disambiguation Based on Wikipedia Data,"This paper presents a large-scale system for the recognition and semantic disambiguation of named entities based on information extracted from a large encyclopedic collection and Web search results. It describes in detail the disambiguation paradigm employed and the information extraction process from Wikipedia. Through a process of maximizing the agreement between the contextual information extracted from Wikipedia and the context of a document, as well as the agreement among the category tags associated with the candidate entities, the implemented system shows high disambiguation accuracy on both news stories and Wikipedia articles.",2007.0,Silviu Cucerzan
621a9399f24952469b99865ba9e45af10927aed9,https://www.semanticscholar.org/paper/621a9399f24952469b99865ba9e45af10927aed9,Isogeometric finite element data structures based on Bézier extraction of T‐splines,"We develop finite element data structures for T‐splines based on Bézier extraction generalizing our previous work for NURBS. As in traditional finite element analysis, the extracted Bézier elements are defined in terms of a fixed set of polynomial basis functions, the so‐called Bernstein basis. The Bézier elements may be processed in the same way as in a standard finite element computer program, utilizing exactly the same data processing arrays. In fact, only the shape function subroutine needs to be modified while all other aspects of a finite element program remain the same. A byproduct of the extraction process is the element extraction operator. This operator localizes the topological and global smoothness information to the element level, and represents a canonical treatment of T‐junctions, referred to as ‘hanging nodes’ in finite element analysis and a fundamental feature of T‐splines. A detailed example is presented to illustrate the ideas. Copyright © 2011 John Wiley & Sons, Ltd.",2010.0,"M. Scott, Michael J. Borden, C. Verhoosel, T. Sederberg, Thomas J. R. Hughes"
ff75055d4e47737702d3b550879d6128cec13233,https://www.semanticscholar.org/paper/ff75055d4e47737702d3b550879d6128cec13233,Extracting Product Features and Opinions from Reviews,"Consumers are often forced to wade through many on-line reviews in order to make an informed product choice. This paper introduces Opine, an unsupervised information-extraction system which mines reviews in order to build a model of important product features, their evaluation by reviewers, and their relative quality across products.Compared to previous work, Opine achieves 22% higher precision (with only 3% lower recall) on the feature extraction task. Opine's novel use of relaxation labeling for finding the semantic orientation of words in context leads to strong performance on the tasks of finding opinion phrases and their polarity.",2005.0,"Ana-Maria Popescu, Oren Etzioni"
51ddf8e1bb6cedcea7440419b591fa5f81aaee3d,https://www.semanticscholar.org/paper/51ddf8e1bb6cedcea7440419b591fa5f81aaee3d,Microblogging during two natural hazards events: what twitter may contribute to situational awareness,"We analyze microblog posts generated during two recent, concurrent emergency events in North America via Twitter, a popular microblogging service. We focus on communications broadcast by people who were ""on the ground"" during the Oklahoma Grassfires of April 2009 and the Red River Floods that occurred in March and April 2009, and identify information that may contribute to enhancing situational awareness (SA). This work aims to inform next steps for extracting useful, relevant information during emergencies using information extraction (IE) techniques.",2010.0,"Sarah Vieweg, Amanda Lee Hughes, Kate Starbird, L. Palen"
0f87d24e8376f2c6482ecfbe257616d95f832bc9,https://www.semanticscholar.org/paper/0f87d24e8376f2c6482ecfbe257616d95f832bc9,Extraction for metabolomics: access to the metabolome.,"INTRODUCTION
The value of information obtained from a metabolomic study depends on how much of the metabolome is present in analysed samples. Thus, only a comprehensive and reproducible extraction method will provide reliable data because the metabolites that will be measured are those that were extracted and all conclusions will be built around this information.


OBJECTIVE
To discuss the efficiency and reliability of available sample pre-treatment methods and their application in different fields of metabolomics.


METHODS
The review has three sections: the first deals with pre-extraction techniques, the second discusses the choice of extraction solvents and their main features and the third includes a brief description of the most used extraction techniques: microwave-assisted extraction, solid-phase extraction, supercritical fluid extraction, Soxhlet and a new method developed in our laboratory--the comprehensive extraction method.


RESULTS
Examination of over 200 studies showed that sample collection, homogenisation, grinding and storage could affect the yield and reproducibility of results. They also revealed that apart from the solvent used for extraction, the extraction techniques have a decisive role on the metabolites available for analysis.


CONCLUSION
It is essential to evaluate efficacy and reproducibility of sample pre-treatment as a first step to ensure the reliability of a metabolomic study. Among the reviewed methods, the comprehensive extraction method appears to provide a promising approach for extracting diverse types of metabolites.",2014.0,"M. Mushtaq, Y. Choi, R. Verpoorte, E. Wilson"
834cb8e1e738b8d2c6d24e652ac966d6e7089a46,https://www.semanticscholar.org/paper/834cb8e1e738b8d2c6d24e652ac966d6e7089a46,Connecting Language and Knowledge Bases with Embedding Models for Relation Extraction,"This paper proposes a novel approach for relation extraction from free text which is trained to jointly use information from the text and from existing knowledge. Our model is based on scoring functions that operate by learning low-dimensional embeddings of words, entities and relationships from a knowledge base. We empirically show on New York Times articles aligned with Freebase relations that our approach is able to efficiently use the extra information provided by a large subset of Freebase data (4M entities, 23k relationships) to improve over methods that rely on text features alone.",2013.0,"J. Weston, Antoine Bordes, Oksana Yakhnenko, Nicolas Usunier"
68cd1c7c0651b116a83abab8a7a46a29975d3b5f,https://www.semanticscholar.org/paper/68cd1c7c0651b116a83abab8a7a46a29975d3b5f,Exploring Various Knowledge in Relation Extraction,"Extracting semantic relationships between entities is challenging. This paper investigates the incorporation of diverse lexical, syntactic and semantic knowledge in feature-based relation extraction using SVM. Our study illustrates that the base phrase chunking information is very effective for relation extraction and contributes to most of the performance improvement from syntactic aspect while additional information from full parsing gives limited further enhancement. This suggests that most of useful information in full parse trees for relation extraction is shallow and can be captured by chunking. We also demonstrate how semantic information such as WordNet and Name List, can be used in feature-based relation extraction to further improve the performance. Evaluation on the ACE corpus shows that effective incorporation of diverse features enables our system outperform previously best-reported systems on the 24 ACE relation subtypes and significantly outperforms tree kernel-based systems by over 20 in F-measure on the 5 ACE relation types.",2005.0,"Guodong Zhou, Jian Su, Jie Zhang, Min Zhang"
a8a7f187de931c74d50dde47d004bcf427612561,https://www.semanticscholar.org/paper/a8a7f187de931c74d50dde47d004bcf427612561,A Review of Remote Sensing Image Classification Techniques: the Role of Spatio-contextual Information,"Abstract This paper reviewed major remote sensing image classification techniques, including pixel-wise, sub-pixel-wise, and object-based image classification methods, and highlighted the importance of incorporating spatio-contextual information in remote sensing image classification. Further, this paper grouped spatio-contextual analysis techniques into three major categories, including 1) texture extraction, 2) Markov random fields (MRFs) modeling, and 3) image segmentation and object-based image analysis. Finally, this paper argued the necessity of developing geographic information analysis models for spatial-contextual classifications using two case studies.",2014.0,"Miao Li, S. Zang, Bing Zhang, Shanshan Li, Changshan Wu"
abe8a57dc27598937c2cffde3fc21c1e6d1f11ce,https://www.semanticscholar.org/paper/abe8a57dc27598937c2cffde3fc21c1e6d1f11ce,Deep EHR: A Survey of Recent Advances in Deep Learning Techniques for Electronic Health Record (EHR) Analysis,"The past decade has seen an explosion in the amount of digital information stored in electronic health records (EHRs). While primarily designed for archiving patient information and performing administrative healthcare tasks like billing, many researchers have found secondary use of these records for various clinical informatics applications. Over the same period, the machine learning community has seen widespread advances in the field of deep learning. In this review, we survey the current research on applying deep learning to clinical tasks based on EHR data, where we find a variety of deep learning techniques and frameworks being applied to several types of clinical applications including information extraction, representation learning, outcome prediction, phenotyping, and deidentification. We identify several limitations of current research involving topics such as model interpretability, data heterogeneity, and lack of universal benchmarks. We conclude by summarizing the state of the field and identifying avenues of future deep EHR research.",2017.0,"B. Shickel, P. Tighe, A. Bihorac, Parisa Rashidi"
b21b927c251c415b601b6d7f785a42cc5c292635,https://www.semanticscholar.org/paper/b21b927c251c415b601b6d7f785a42cc5c292635,"Multi-Task Identification of Entities, Relations, and Coreference for Scientific Knowledge Graph Construction","We introduce a multi-task setup of identifying entities, relations, and coreference clusters in scientific articles. We create SciERC, a dataset that includes annotations for all three tasks and develop a unified framework called SciIE with shared span representations. The multi-task setup reduces cascading errors between tasks and leverages cross-sentence relations through coreference links. Experiments show that our multi-task model outperforms previous models in scientific information extraction without using any domain-specific features. We further show that the framework supports construction of a scientific knowledge graph, which we use to analyze information in scientific literature.",2018.0,"Yi Luan, Luheng He, Mari Ostendorf, Hannaneh Hajishirzi"
6723dda58e5e09089ec78ba42827b65859f030e2,https://www.semanticscholar.org/paper/6723dda58e5e09089ec78ba42827b65859f030e2,Message Understanding Conference- 6: A Brief History,"We have recently completed the sixth in a series of ""Message Understanding Conferences"" which are designed to promote and evaluate research in information extraction. MUC-6 introduced several innovations over prior MUCs, most notably in the range of different tasks for which evaluations were conducted. We describe some of the motivations for the new format and briefly discuss some of the results of the evaluations.",1996.0,"R. Grishman, B. Sundheim"
cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d,https://www.semanticscholar.org/paper/cf5ea582bccc7cb21a2ebeb7a0987f79652bde8d,Knowledge vault: a web-scale approach to probabilistic knowledge fusion,"Recent years have witnessed a proliferation of large-scale knowledge bases, including Wikipedia, Freebase, YAGO, Microsoft's Satori, and Google's Knowledge Graph. To increase the scale even further, we need to explore automatic methods for constructing knowledge bases. Previous approaches have primarily focused on text-based extraction, which can be very noisy. Here we introduce Knowledge Vault, a Web-scale probabilistic knowledge base that combines extractions from Web content (obtained via analysis of text, tabular data, page structure, and human annotations) with prior knowledge derived from existing knowledge repositories. We employ supervised machine learning methods for fusing these distinct information sources. The Knowledge Vault is substantially bigger than any previously published structured knowledge repository, and features a probabilistic inference system that computes calibrated probabilities of fact correctness. We report the results of multiple studies that explore the relative utility of the different information sources and extraction methods.",2014.0,"X. Dong, E. Gabrilovich, Geremy Heitz, Wilko Horn, N. Lao, K. Murphy, Thomas Strohmann, Shaohua Sun, Wei Zhang"
d81c1f5a196d33fe734affe285e4f428fd3aea46,https://www.semanticscholar.org/paper/d81c1f5a196d33fe734affe285e4f428fd3aea46,Best practices in exploratory factor analysis: four recommendations for getting the most from your analysis.,"Exploratory factor analysis (EFA) is a complex, multi-step process. The goal of this paper is to collect, in one article, information that will allow researchers and practitioners to understand the various choices available through popular software packages, and to make decisions about ”best practices” in exploratory factor analysis. In particular, this paper provides practical information on making decisions regarding (a) extraction, (b) rotation, (c) the number of factors to interpret, and (d) sample size.",2005.0,"Anna Costello, J. Osborne"
d1bcd047a8688917b543bb93f9e719aa31c7008c,https://www.semanticscholar.org/paper/d1bcd047a8688917b543bb93f9e719aa31c7008c,Driver Drowsiness Classification Using Fuzzy Wavelet-Packet-Based Feature-Extraction Algorithm,"Driver drowsiness and loss of vigilance are a major cause of road accidents. Monitoring physiological signals while driving provides the possibility of detecting and warning of drowsiness and fatigue. The aim of this paper is to maximize the amount of drowsiness-related information extracted from a set of electroencephalogram (EEG), electrooculogram (EOG), and electrocardiogram (ECG) signals during a simulation driving test. Specifically, we develop an efficient fuzzy mutual-information (MI)- based wavelet packet transform (FMIWPT) feature-extraction method for classifying the driver drowsiness state into one of predefined drowsiness levels. The proposed method estimates the required MI using a novel approach based on fuzzy memberships providing an accurate-information content-estimation measure. The quality of the extracted features was assessed on datasets collected from 31 drivers on a simulation test. The experimental results proved the significance of FMIWPT in extracting features that highly correlate with the different drowsiness levels achieving a classification accuracy of 95%-97% on an average across all subjects.",2011.0,"R. Khushaba, S. Kodagoda, S. Lal, G. Dissanayake"
8bf3dc56e3c02f68a3e0090186bb14d11ef59090,https://www.semanticscholar.org/paper/8bf3dc56e3c02f68a3e0090186bb14d11ef59090,Epoch Extraction From Speech Signals,"Epoch is the instant of significant excitation of the vocal-tract system during production of speech. For most voiced speech, the most significant excitation takes place around the instant of glottal closure. Extraction of epochs from speech is a challenging task due to time-varying characteristics of the source and the system. Most epoch extraction methods attempt to remove the characteristics of the vocal-tract system, in order to emphasize the excitation characteristics in the residual. The performance of such methods depends critically on our ability to model the system. In this paper, we propose a method for epoch extraction which does not depend critically on characteristics of the time-varying vocal-tract system. The method exploits the nature of impulse-like excitation. The proposed zero resonance frequency filter output brings out the epoch locations with high accuracy and reliability. The performance of the method is demonstrated using CMU-Arctic database using the epoch information from the electroglottograph as reference. The proposed method performs significantly better than the other methods currently available for epoch extraction. The interesting part of the results is that the epoch extraction by the proposed method seems to be robust against degradations like white noise, babble, high-frequency channel, and vehicle noise.",2008.0,"K. Murty, B. Yegnanarayana"
f56c6d1485cfec26866702f7f718e90fbdb415a9,https://www.semanticscholar.org/paper/f56c6d1485cfec26866702f7f718e90fbdb415a9,Face recognition using Eigenfaces,Face is a complex multidimensional visual model and developing a computational model for face recognition is difficult. The paper presents a methodology for face recognition based on information theory approach of coding and decoding the face image. Proposed methodology is connection of two stages - Feature extraction using Principle Component Analysis and recognition using the feed forward back propagation Neural Network. The goal is to implement the system (model) for a particular face and distinguish it from a large number of stored faces with some real-time variations as well. The Eigenface approach uses Principal Component Analysis (PCA) algorithm for the recognition of the images. It gives us efficient way to find the lower dimensional space.,2011.0,"V. Kshirsagar, M. Baviskar, M. Gaikwad"
7e1874986cf6433fabf96fff93ef42b60bdc49f8,https://www.semanticscholar.org/paper/7e1874986cf6433fabf96fff93ef42b60bdc49f8,Weisfeiler-Lehman Graph Kernels,"In this article, we propose a family of efficient kernels for large graphs with discrete node labels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node attributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accuracy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.",2011.0,"N. Shervashidze, Pascal Schweitzer, E. J. V. Leeuwen, K. Mehlhorn, Karsten M. Borgwardt"
0ab4f5f03665fadd1d838b22fc3991062fb91928,https://www.semanticscholar.org/paper/0ab4f5f03665fadd1d838b22fc3991062fb91928,"Entity Linking with a Knowledge Base: Issues, Techniques, and Solutions","The large number of potential applications from bridging web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions.",2015.0,"Wei Shen, Jianyong Wang, Jiawei Han"
df7d26339adf4eb0c07160947b9d2973c24911ba,https://www.semanticscholar.org/paper/df7d26339adf4eb0c07160947b9d2973c24911ba,Extracting Training Data from Large Language Models,"It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. 
We demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. 
We comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models.",2020.0,"Nicholas Carlini, Florian Tramèr, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom B. Brown, D. Song, Ú. Erlingsson, Alina Oprea, Colin Raffel"
18b632db2d3df6c3f8954f17677ba8af0145b369,https://www.semanticscholar.org/paper/18b632db2d3df6c3f8954f17677ba8af0145b369,The Properties of Gases and Liquids,"Completely rewritten and reorganized to reflect the latest developments in estimating the properties of gases and liquids, this new edition of the highly regarded reference presents a comprehensive survey of the most reliable estimation methods in use today. It provides instantly usable information on estimating both physical and thermodynamic properties when experimental data are not available (for example, constants such as critical temperature, critical pressure, acentric factor, and others); thermodynamic properties of gases and liquids, both pure and mixtures, including enthalpies, entropies, fugacity coefficients, heat capacities, and critical points; vapor-liquid and liquid-liquid equilibria as needed in separation operations such as distillation, absorption, and extraction. An invaluable reference that provides property values for more than 600 pure chemicals, this is the only book in its field to include a critical analysis of existing methods as well as practical recommendations.",1977.0,"R. Reid, T. Sherwood, Robert E. Street"
5c27487c3e0894b65e976a287e6f8c9aa40f089c,https://www.semanticscholar.org/paper/5c27487c3e0894b65e976a287e6f8c9aa40f089c,Face recognition by elastic bunch graph matching,"We present a system for recognizing human faces from single images out of a large database containing one image per person. Faces are represented by labeled graphs, based on a Gabor wavelet transform. Image graphs of new faces are extracted by an elastic graph matching process and can be compared by a simple similarity function. The system differs from Lades et al. (1993) in three respects. Phase information is used for accurate node positioning. Object-adapted graphs are used to handle large rotations in depth. Image graph extraction is based on a novel data structure, the bunch graph, which is constructed from a small set of sample image graphs.",1997.0,"Laurenz Wiskott, J. Fellous, N. Krüger, C. Malsburg"
d82c0a17f328e30e861cb7e77507ed60c5aeabad,https://www.semanticscholar.org/paper/d82c0a17f328e30e861cb7e77507ed60c5aeabad,Time-Frequency Signal Analysis and Processing: A Comprehensive Reference,"Time Frequency Signal Analysis and Processing covers fundamental concepts, principles and techniques, treatment of specialised and advanced topics, methods and applications, including results of recent research. This book deals with the modern methodologies, key techniques and concepts that form the core of new technologies used in IT, multimedia, telecommunications as well as most fields of engineering, science and technology. It focuses on advanced techniques and methods that allow a refined extraction and processing of information, allowing efficient and effective decision making that would not be possible with classical techniques.",2015.0,B. Boashash
55b332c19d7a2b44e43e7c2674302addb973350c,https://www.semanticscholar.org/paper/55b332c19d7a2b44e43e7c2674302addb973350c,FULL EXTRACTION OF THE SURPLUS IN BAYESIAN AND DOMINANT STRATEGY AUCTIONS,"The authors consider auctions for a single indivisible object when bidders have information about each other that is unavailable to the seller. They show that the seller can use this information to his own benefit, and they characterize th e environments in which a well-chosen auction gives him the same expected payoff as that obtainable were he able to see the object und er full information. This hinges on the possibility of constructing lotteries with the correct properties. The authors study the problem for auctions where the bidders have dominant strategies and those where the relevant equilibrium concept is Bayesian-Nash. Copyright 1988 by The Econometric Society.",1988.0,"J. Crémer, R. McLean"
155f50770f43b7e52c85583a0a2d552f5b21cb81,https://www.semanticscholar.org/paper/155f50770f43b7e52c85583a0a2d552f5b21cb81,Computer Vision and Image Understanding,"We propose a novel feature extraction approach for 3D facial expression recognition by incorporating non-rigid registration in face-model-free analysis, which in turn makes feasible data-driven, i.e., feature-model-free recognition of expressions. The resulting simplicity of feature representation is due to the fact that facial information is adapted to the input faces via shape model-free dense registration, and this provides a dynamic feature extraction mechanism. This approach eliminates the necessity of complex feature representations as required in the case of static feature extraction methods, where the complexity arises from the necessity to model the local context; higher degree of complexity persists in deep feature hier- archies enabled by end-to-end learning on large-scale datasets. Face-model-free recognition implies inde-pendence from limitations and biases due to committed face models, bypassing complications of model ﬁtting, and avoiding the burden of manual model construction. We show via information gain maps that non-rigid registration enables extraction of highly informative features, as it provides invariance to local- shifts due to physiognomy (subject invariance) and residual pose misalignments; in addition, it allows estimation of local correspondences of expressions. To maximize the recognition rate, we use the strat- egy of employing a rich but computationally manageable set of local correspondence structures, and to this effect we propose a framework to optimally select multiple registration references. Our features are re-sampled surface curvature values at individual coordinates which are chosen per expression-class and per reference pair. We show the superior performance of our novel dynamic feature extraction approach on three distinct recognition problems, namely, action unit detection, basic expression recognition, and emotion dimension recognition.",2022.0,"Wenming Tang, Yuanhao Gong, Guoping Qiu"
02f89cd1fd6f013a1a301a292936ff8fb06aff25,https://www.semanticscholar.org/paper/02f89cd1fd6f013a1a301a292936ff8fb06aff25,"Uncertainty relation for resolution in space, spatial frequency, and orientation optimized by two-dimensional visual cortical filters.","Two-dimensional spatial linear filters are constrained by general uncertainty relations that limit their attainable information resolution for orientation, spatial frequency, and two-dimensional (2D) spatial position. The theoretical lower limit for the joint entropy, or uncertainty, of these variables is achieved by an optimal 2D filter family whose spatial weighting functions are generated by exponentiated bivariate second-order polynomials with complex coefficients, the elliptic generalization of the one-dimensional elementary functions proposed in Gabor's famous theory of communication [J. Inst. Electr. Eng. 93, 429 (1946)]. The set includes filters with various orientation bandwidths, spatial-frequency bandwidths, and spatial dimensions, favoring the extraction of various kinds of information from an image. Each such filter occupies an irreducible quantal volume (corresponding to an independent datum) in a four-dimensional information hyperspace whose axes are interpretable as 2D visual space, orientation, and spatial frequency, and thus such a filter set could subserve an optimally efficient sampling of these variables. Evidence is presented that the 2D receptive-field profiles of simple cells in mammalian visual cortex are well described by members of this optimal 2D filter family, and thus such visual neurons could be said to optimize the general uncertainty relations for joint 2D-spatial-2D-spectral information resolution. The variety of their receptive-field dimensions and orientation and spatial-frequency bandwidths, and the correlations among these, reveal several underlying constraints, particularly in width/length aspect ratio and principal axis organization, suggesting a polar division of labor in occupying the quantal volumes of information hyperspace.(ABSTRACT TRUNCATED AT 250 WORDS)",1985.0,J. Daugman
0bd9e7956d567646697df3129cce3b4f97a65489,https://www.semanticscholar.org/paper/0bd9e7956d567646697df3129cce3b4f97a65489,"TBtools, a Toolkit for Biologists integrating various HTS-data handling tools with a user-friendly interface","Various softwares or pipelines have been developed for biological information mining from high-throughput sequencing (HTS) data, and most of them relies on programming and command-line environment with which most biologists are unfamiliar. Bioinformatic tools with an user-friendly interface are preferred by wet-lab biologists. Here, we describe TBtools, a Toolkit for Biologists integrating various HTS-data handling tools with a user-friendly interface. It includes a large collection of functions, which facilitate many simple, routine but elaborate tasks working on HTS data, such as bulk sequence extraction, gene set functional enrichment, venn diagram and etc. TBtools can run under all operating systems with JRE1.6 and is freely available at github.com/CJ-Chen/TBtools. Since its development, it has been used by many researchers. It will be a useful toolkit for wet-lab biologists to work on all kinds of high-throughput data.",2018.0,"Chengjie Chen, Rui Xia, Hao Chen, Yehua He"
30321b036607a7936221235ea8ec7cf7c1627100,https://www.semanticscholar.org/paper/30321b036607a7936221235ea8ec7cf7c1627100,Knowledge Graph Embedding: A Survey of Approaches and Applications,"Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-the-arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth.",2017.0,"Quan Wang, Zhendong Mao, Bin Wang, Li Guo"
438219194cedac00974ad28604b63a66e0b6f436,https://www.semanticscholar.org/paper/438219194cedac00974ad28604b63a66e0b6f436,Opensmile: the munich versatile and fast open-source audio feature extractor,"We introduce the openSMILE feature extraction toolkit, which unites feature extraction algorithms from the speech processing and the Music Information Retrieval communities. Audio low-level descriptors such as CHROMA and CENS features, loudness, Mel-frequency cepstral coefficients, perceptual linear predictive cepstral coefficients, linear predictive coefficients, line spectral frequencies, fundamental frequency, and formant frequencies are supported. Delta regression and various statistical functionals can be applied to the low-level descriptors. openSMILE is implemented in C++ with no third-party dependencies for the core functionality. It is fast, runs on Unix and Windows platforms, and has a modular, component based architecture which makes extensions via plug-ins easy. It supports on-line incremental processing for all implemented features as well as off-line and batch processing. Numeric compatibility with future versions is ensured by means of unit tests. openSMILE can be downloaded from http://opensmile.sourceforge.net/.",2010.0,"F. Eyben, M. Wöllmer, Björn Schuller"
ff0a68bcbf72e35c70510ffba07f13ea585e98b4,https://www.semanticscholar.org/paper/ff0a68bcbf72e35c70510ffba07f13ea585e98b4,Person re-identification by symmetry-driven accumulation of local features,"In this paper, we present an appearance-based method for person re-identification. It consists in the extraction of features that model three complementary aspects of the human appearance: the overall chromatic content, the spatial arrangement of colors into stable regions, and the presence of recurrent local motifs with high entropy. All this information is derived from different body parts, and weighted opportunely by exploiting symmetry and asymmetry perceptual principles. In this way, robustness against very low resolution, occlusions and pose, viewpoint and illumination changes is achieved. The approach applies to situations where the number of candidates varies continuously, considering single images or bunch of frames for each individual. It has been tested on several public benchmark datasets (ViPER, iLIDS, ETHZ), gaining new state-of-the-art performances.",2010.0,"M. Farenzena, Loris Bazzani, A. Perina, Vittorio Murino, M. Cristani"
09622b0c84bf812814af5b64b0c83dce796899c4,https://www.semanticscholar.org/paper/09622b0c84bf812814af5b64b0c83dce796899c4,Content-based book recommending using learning for text categorization,"Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast,content-based methods use information about an item itself to make suggestions.This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.",1999.0,"R. Mooney, Loriene Roy"
b6e438917998e864e709632108f81ce9e9a160f0,https://www.semanticscholar.org/paper/b6e438917998e864e709632108f81ce9e9a160f0,Reversible data hiding,"We present a novel reversible (lossless) data hiding (embedding) technique, which enables the exact recovery of the original host signal upon extraction of the embedded information. A generalization of the well-known LSB (least significant bit) modification is proposed as the data embedding method, which introduces additional operating points on the capacity-distortion curve. Lossless recovery of the original is achieved by compressing portions of the signal that are susceptible to embedding distortion, and transmitting these compressed descriptions as a part of the embedded payload. A prediction-based conditional entropy coder which utilizes static portions of the host as side-information improves the compression efficiency, and thus the lossless data embedding capacity.",2002.0,"M. Celik, Gaurav Sharma, A. Tekalp, Eli Saber"
4cb835ace34f53c0d69c1f155d2527a2ec9f35ef,https://www.semanticscholar.org/paper/4cb835ace34f53c0d69c1f155d2527a2ec9f35ef,Rotation Forest: A New Classifier Ensemble Method,"We propose a method for generating classifier ensembles based on feature extraction. To create the training data for a base classifier, the feature set is randomly split into K subsets (K is a parameter of the algorithm) and principal component analysis (PCA) is applied to each subset. All principal components are retained in order to preserve the variability information in the data. Thus, K axis rotations take place to form the new features for a base classifier. The idea of the rotation approach is to encourage simultaneously individual accuracy and diversity within the ensemble. Diversity is promoted through the feature extraction for each base classifier. Decision trees were chosen here because they are sensitive to rotation of the feature axes, hence the name ""forest"". Accuracy is sought by keeping all principal components and also using the whole data set to train each base classifier. Using WEKA, we examined the rotation forest ensemble on a random selection of 33 benchmark data sets from the UCI repository and compared it with bagging, AdaBoost, and random forest. The results were favorable to rotation forest and prompted an investigation into diversity-accuracy landscape of the ensemble models. Diversity-error diagrams revealed that rotation forest ensembles construct individual classifiers which are more accurate than these in AdaBoost and random forest, and more diverse than these in bagging, sometimes more accurate as well",2006.0,"Juan José Rodríguez Diez, L. Kuncheva, C. Alonso"
127e48956100a45f821b1ed04e974d13da35a6f5,https://www.semanticscholar.org/paper/127e48956100a45f821b1ed04e974d13da35a6f5,Image and depth from a conventional camera with a coded aperture,"A conventional camera captures blurred versions of scene information away from the plane of focus. Camera systems have been proposed that allow for recording all-focus images, or for extracting depth, but to record both simultaneously has required more extensive hardware and reduced spatial resolution. We propose a simple modification to a conventional camera that allows for the simultaneous recovery of both (a) high resolution image information and (b) depth information adequate for semi-automatic extraction of a layered depth representation of the image. Our modification is to insert a patterned occluder within the aperture of the camera lens, creating a coded aperture. We introduce a criterion for depth discriminability which we use to design the preferred aperture pattern. Using a statistical model of images, we can recover both depth information and an all-focus image from single photographs taken with the modified camera. A layered depth map is then extracted, requiring user-drawn strokes to clarify layer assignments in some cases. The resulting sharp image and layered depth map can be combined for various photographic applications, including automatic scene segmentation, post-exposure refocusing, or re-rendering of the scene from an alternate viewpoint.",2007.0,"Anat Levin, R. Fergus, F. Durand, W. Freeman"
12e6e118d0f02cd266f21db52001d83752ab2543,https://www.semanticscholar.org/paper/12e6e118d0f02cd266f21db52001d83752ab2543,Power analysis attacks - revealing the secrets of smart cards,"Power analysis attacks allow the extraction of secret information from smart cards. Smart cards are used in many applications including banking, mobile communications, pay TV, and electronic signatures. In all these applications, the security of the smart cards is of crucial importance. Power Analysis Attacks: Revealing the Secrets of Smart Cards is the first comprehensive treatment of power analysis attacks and countermeasures. Based on the principle that the only way to defend against power analysis attacks is to understand them, this book explains how power analysis attacks work. Using many examples, it discusses simple and differential power analysis as well as advanced techniques like template attacks. Furthermore, the authors provide an extensive discussion of countermeasures like shuffling, masking, and DPA-resistant logic styles. By analyzing the pros and cons of the different countermeasures, this volume allows practitioners to decide how to protect smart cards.",2007.0,"S. Mangard, Elisabeth Oswald, Thomas Popp"
e0c01df98a6b633b25c96c1a99b713ac96f1c5be,https://www.semanticscholar.org/paper/e0c01df98a6b633b25c96c1a99b713ac96f1c5be,Placing search in context: the concept revisited,"Keyword-based search engines are in widespread use today as a popular means for Web-based information retrieval. Although such systems seem deceptively simple, a considerable amount of skill is required in order to satisfy non-trivial information needs. This paper presents a new conceptual paradigm for performing search in context, that largely automates the search process, providing even non-professional users with highly relevant results. This paradigm is implemented in practice in the IntelliZap system, where search is initiated from a text query marked by the user in a document she views, and is guided by the text surrounding the marked query in that document (""the context""). The context-driven information retrieval process involves semantic keyword extraction and clustering to automatically generate new, augmented queries. The latter are submitted to a host of general and domain-specific search engines. Search results are then semantically reranked, using context. Experimental results testify that using context to guide search, effectively offers even inexperienced users an advanced search tool on the Web.",2002.0,"L. Finkelstein, E. Gabrilovich, Yossi Matias, E. Rivlin, Zach Solan, G. Wolfman, E. Ruppin"
ed4f21d6fd4ea0cd2decba6f126cf05e9d7c1326,https://www.semanticscholar.org/paper/ed4f21d6fd4ea0cd2decba6f126cf05e9d7c1326,Classification and feature extraction for remote sensing images from urban areas based on morphological transformations,"Classification of panchromatic high-resolution data from urban areas using morphological and neural approaches is investigated. The proposed approach is based on three steps. First, the composition of geodesic opening and closing operations of different sizes is used in order to build a differential morphological profile that records image structural information. Although, the original panchromatic image only has one data channel, the use of the composition operations will give many additional channels, which may contain redundancies. Therefore, feature extraction or feature selection is applied in the second step. Both discriminant analysis feature extraction and decision boundary feature extraction are investigated in the second step along with a simple feature selection based on picking the largest indexes of the differential morphological profiles. Third, a neural network is used to classify the features from the second step. The proposed approach is applied in experiments on high-resolution Indian Remote Sensing 1C (IRS-1C) and IKONOS remote sensing data from urban areas. In experiments, the proposed method performs well in terms of classification accuracies. It is seen that relatively few features are needed to achieve the same classification accuracies as in the original feature space.",2003.0,"J. Benediktsson, M. Pesaresi, Kolbeinn Amason"
487ed99e00bf6803a53a6059ceccd1510a63e72d,https://www.semanticscholar.org/paper/487ed99e00bf6803a53a6059ceccd1510a63e72d,An Analysis of Active Learning Strategies for Sequence Labeling Tasks,"Active learning is well-suited to many problems in natural language processing, where unlabeled data may be abundant but annotation is slow and expensive. This paper aims to shed light on the best active learning approaches for sequence labeling tasks such as information extraction and document segmentation. We survey previously used query selection strategies for sequence models, and propose several novel algorithms to address their shortcomings. We also conduct a large-scale empirical comparison using multiple corpora, which demonstrates that our proposed methods advance the state of the art.",2008.0,"Burr Settles, M. Craven"
b84720c6a517c7db44c2ff54a082e1a98b765eed,https://www.semanticscholar.org/paper/b84720c6a517c7db44c2ff54a082e1a98b765eed,A brief survey of web data extraction tools,"In the last few years, several works in the literature have addressed the problem of data extraction from Web pages. The importance of this problem derives from the fact that, once extracted, the data can be handled in a way similar to instances of a traditional database. The approaches proposed in the literature to address the problem of Web data extraction use techniques borrowed from areas such as natural language processing, languages and grammars, machine learning, information retrieval, databases, and ontologies. As a consequence, they present very distinct features and capabilities which make a direct comparison difficult to be done. In this paper, we propose a taxonomy for characterizing Web data extraction fools, briefly survey major Web data extraction tools described in the literature, and provide a qualitative analysis of them. Hopefully, this work will stimulate other studies aimed at a more comprehensive analysis of data extraction approaches and tools for Web data.",2002.0,"Alberto H. F. Laender, B. Ribeiro-Neto, A. D. Silva, Juliana S. Teixeira"
6d962e9f04c653f732da82073a3446f75a371055,https://www.semanticscholar.org/paper/6d962e9f04c653f732da82073a3446f75a371055,The KDD process for extracting useful knowledge from volumes of data,"AS WE MARCH INTO THE AGE of digital information, the problem of data overload looms ominously ahead. Our ability to analyze and understand massive datasets lags far behind our ability to gather and store the data. A new generation of computational techniques and tools is required to support the extraction of useful knowledge from the rapidly growing volumes of data. These techniques and tools are the subject of the emerging field of knowledge discovery in databases (KDD) and data mining. Large databases of digital information are ubiquitous. Data from the neighborhood store’s checkout register, your bank’s credit card authorization device, records in your doctor’s office, patterns in your telephone calls, and many more applications generate streams of digital records archived in huge databases, sometimes in so-called data warehouses. Current hardware and database technology allow efficient and inexpensive reliable data storage and access. However, whether the context is business, medicine, science, or government, the datasets themselves (in raw form) are of little direct value. What is of value is the knowledge that can be inferred from the data and put to use. For example, the marketing database of a consumer U s a m a F a y y a d ,",1996.0,"U. Fayyad, G. Piatetsky-Shapiro, Padhraic Smyth"
c84660958513194f6788cd7ee480d967c8f1d50b,https://www.semanticscholar.org/paper/c84660958513194f6788cd7ee480d967c8f1d50b,Pole extraction from real-frequency information,"This paper describes a procedure, analogous to Prony's method, for extracting the complex-frequency poles of electromagnetic transfer functions. The method is refined mathematically and is applied to both electrical and mechanical test cases. The paper explains a multiple processing technique, involving the overlaying of several pole sets, by which redundant data are used to separate actual from curve-fitting poles. Identification of an unknown target by comparing its scattered field with the poles sets of known targets is also illustrated for simple targets.",1980.0,"J. Brittingham, E. Miller, J. L. Willows"
5e095981ebf4d389e9356bd56e59e0ade1b42e88,https://www.semanticscholar.org/paper/5e095981ebf4d389e9356bd56e59e0ade1b42e88,"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text","The 2010 i2b2/VA Workshop on Natural Language Processing Challenges for Clinical Records presented three tasks: a concept extraction task focused on the extraction of medical concepts from patient reports; an assertion classification task focused on assigning assertion types for medical problem concepts; and a relation classification task focused on assigning relation types that hold between medical problems, tests, and treatments. i2b2 and the VA provided an annotated reference standard corpus for the three tasks. Using this reference standard, 22 systems were developed for concept extraction, 21 for assertion classification, and 16 for relation classification. These systems showed that machine learning approaches could be augmented with rule-based systems to determine concepts, assertions, and relations. Depending on the task, the rule-based systems can either provide input for machine learning or post-process the output of machine learning. Ensembles of classifiers, information from unlabeled data, and external knowledge sources can help when the training data are inadequate.",2011.0,"Özlem Uzuner, B. South, Shuying Shen, S. Duvall"
087a6f471b5177014b08a342968901e1ef083ed1,https://www.semanticscholar.org/paper/087a6f471b5177014b08a342968901e1ef083ed1,Automatic Analysis of Facial Expressions: The State of the Art,"Humans detect and interpret faces and facial expressions in a scene with little or no effort. Still, development of an automated system that accomplishes this task is rather difficult. There are several related problems: detection of an image segment as a face, extraction of the facial expression information, and classification of the expression (e.g., in emotion categories). A system that performs these operations accurately and in real time would form a big step in achieving a human-like interaction between man and machine. The paper surveys the past work in solving these problems. The capability of the human visual system with respect to these problems is discussed, too. It is meant to serve as an ultimate goal and a guide for determining recommendations for development of an automatic facial expression analyzer.",2000.0,"M. Pantic, L. Rothkrantz"
9169e91141c8acb727c5fed7af24f02931114cc3,https://www.semanticscholar.org/paper/9169e91141c8acb727c5fed7af24f02931114cc3,Semisupervised Local Discriminant Analysis for Feature Extraction in Hyperspectral Images,"We propose a novel semisupervised local discriminant analysis method for feature extraction in hyperspectral remote sensing imagery, with improved performance in both ill-posed and poor-posed conditions. The proposed method combines unsupervised methods (local linear feature extraction methods and supervised method (linear discriminant analysis) in a novel framework without any free parameters. The underlying idea is to design an optimal projection matrix, which preserves the local neighborhood information inferred from unlabeled samples, while simultaneously maximizing the class discrimination of the data inferred from the labeled samples. Experimental results on four real hyperspectral images demonstrate that the proposed method compares favorably with conventional feature extraction methods.",2013.0,"Wenzi Liao, A. Pižurica, P. Scheunders, W. Philips, Y. Pi"
92d8a0238aaba7df1bcc1b55d8ca567272b88609,https://www.semanticscholar.org/paper/92d8a0238aaba7df1bcc1b55d8ca567272b88609,MIR in Matlab (II): A Toolbox for Musical Feature Extraction from Audio,"We present the MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio files. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize. This paper offers an overview of the set of features, related, among others, to timbre, tonality, rhythm or form, that can be extracted with the MIRtoolbox. One particular analysis is provided as an example. The toolbox also includes functions for statistical analysis, segmentation and clustering. Particular attention has been paid to the design of a syntax that offers both simplicity of use and transparent adaptiveness to a multiplicity of possible input types. Each feature extraction method can accept as argument an audio file, or any preliminary result from intermediary stages of the chain of operations. Also the same syntax can be used for analyses of single audio files, batches of files, series of audio segments, multi-channel signals, etc. For that purpose, the data and methods of the toolbox are organised in an object-oriented architecture. 1 MOTIVATION AND APPROACH MIRtoolbox is a Matlab toolbox dedicated to the extraction of musically-related features from audio recordings. It has been designed in particular with the objective of enabling the computation of a large range of features from databases of audio files, that can be subjected to statistical analyses. Few softwares have been proposed in this area. One particularity of our own approach relies in the use of the Matlab computing environment, which offers good visualisation capabilities and gives access to a large variety of other toolboxes. In particular, the MIRtoolbox makes use of functions available in public-domain toolboxes such as the Auditory Toolbox [6], NetLab [5] and SOMtoolbox [10]. Other toolboxes, such as the Statistics toolbox or the Neural Network toolbox from MathWorks, can be directly used for further analyses of the features extracted c © 2007 Austrian Computer Society (OCG). by MIRtoolbox without having to export the data from one software to another. Such computational framework, because of its general objectives, could be useful to the research community in Music Information Retrieval (MIR), but also for educational purposes. For that reason, particular attention has been paid concerning the ease of use of the toolbox. In particular, complex analytic processes can be designed using a very simple syntax, whose expressive power comes from the use of an object-oriented paradigm. The different musical features extracted from the audio files are highly interdependent: in particular, as can be seen in figure 1, some features are based on the same initial computations. In order to improve the computational efficiency, it is important to avoid redundant computations of these common components. Each of these intermediary components, and the final musical features, are therefore considered as building blocks that can been freely articulated one with each other. Besides, in keeping with the objective of optimal ease of use of the toolbox, each building block has been conceived in a way that it can adapt to the type of input data. For instance, the computation of the MFCCs can be based on the waveform of the initial audio signal, or on the intermediary representations such as spectrum, or mel-scale spectrum (see Fig. 1). Similarly, autocorrelation is computed for different range of delays depending on the type of input data (audio waveform, envelope, spectrum). This decomposition of all feature extraction algorithms into a common set of building blocks has the advantage of offering a synthetic overview of the different approaches studied in this domain of research. 2 FEATURE EXTRACTION 2.1 Feature overview Figure 1 shows an overview of the main features implemented in the toolbox. All the different processes start from the audio signal (on the left) and form a chain of operations proceeding to right. Each musical feature is related to one of the musical dimensions traditionally defined in music theory. Boldface characters highlight features related to pitch and tonality. Bold italics indicate features related to rhythm. Simple italics highlight a large set of features that can be associated to timbre and dynamics. Among them, all the operators in grey italics can be Audio signal waveform Zero-crossing rate RMS energy Envelope Low Energy Rate Attack Slope Attack Time Envelope Autocorrelation Tempo Onsets",2007.0,"O. Lartillot, P. Toiviainen"
a42954d4b9d0ccdf1036e0af46d87a01b94c3516,https://www.semanticscholar.org/paper/a42954d4b9d0ccdf1036e0af46d87a01b94c3516,Second Order Derivatives for Network Pruning: Optimal Brain Surgeon,"We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization.",1992.0,"B. Hassibi, D. Stork"
b17ab11d613094d6a0e52b28f1956d2c9388dede,https://www.semanticscholar.org/paper/b17ab11d613094d6a0e52b28f1956d2c9388dede,Is there a future for sequential chemical extraction?,"Since their introduction in the late 1970s, sequential extraction procedures have experienced a rapid increase in use. They are now applied for a large number of potentially toxic elements in a wide range of sample types. This review uses evidence from the literature to consider the usefulness and limitations of sequential extraction and thereby to assess its future role in environmental chemical analysis. It is not the intention to provide a comprehensive survey of all applications of sequential extractions or to consider the merits and disadvantages of individual schemes. These aspects have been covered adequately in other, recent reviews. This review focuses in particular on various key issues surrounding sequential extractions such as nomenclature, methodologies, presentation of data and interpretation of data, and discusses typical applications from the recent literature for which sequential extraction can provide useful and meaningful information. Also covered are emerging developments such as accelerated procedures using ultrasound- or microwave energy-assisted extractions, dynamic extractions, the use of chemometrics, the combination of sequential extraction with isotope analysis, and the extension of the approach to non-traditional analytes such as arsenic, mercury, selenium and radionuclides.",2008.0,"J. Bacon, C. Davidson"
b6eaec7917439d79ce840fa97bc371552e9b6685,https://www.semanticscholar.org/paper/b6eaec7917439d79ce840fa97bc371552e9b6685,MixFormer: End-to-End Tracking with Iterative Mixed Attention,"Tracking often uses a multistage pipeline of feature extraction, target information integration, and bounding box estimation. To simplify this pipeline and unify the process of feature extraction and target information integration, we present a compact tracking framework, termed as MixFormer, built upon transformers. Our core design is to utilize the flexibility of attention operations, and propose a Mixed Attention Module (MAM) for simultaneous feature extraction and target information integration. This synchronous modeling scheme allows to extract target-specific discriminative features and perform extensive communication between target and search area. Based on MAM, we build our MixFormer tracking framework simply by stacking multiple MAMs with progressive patch embedding and placing a localization head on top. In addition, to handle multiple target templates during online tracking, we devise an asymmetric attention scheme in MAM to reduce computational cost, and propose an effective score prediction module to select high-quality templates. Our MixFormer sets a new state-of-the-art performance on five tracking benchmarks, including LaSOT, TrackingNet, VOT2020, GOT-10k, and UAV123. In particular, our MixFormer-L achieves NP score of 79.9% on LaSOT, 88.9% on TrackingNet and EAO of 0.555 on VOT2020. We also perform in-depth ablation studies to demonstrate the effectiveness of simultaneous feature extraction and information integration. Code and trained models are publicly available at https://github.com/MCG-NJU/MixFormer.",2022.0,"Yutao Cui, Jiang Cheng, Limin Wang, Gangshan Wu"
d51e5171bbb03ae4b984e6c7704b93c385618484,https://www.semanticscholar.org/paper/d51e5171bbb03ae4b984e6c7704b93c385618484,Music Information Retrieval: Recent Developments and Applications,"We provide a survey of the field of Music Information Retrieval (MIR), in particular paying attention to latest developments, such as semantic auto-tagging and user-centric retrieval and recommendation approaches. We first elaborate on well-established and proven methods for feature extraction and music indexing, from both the audio signal and contextual data sources about music items, such as web pages or collaborative tags. These in turn enable a wide variety of music retrieval tasks, such as semantic music search or music identification (""query by example""). Subsequently, we review current work on user analysis and modeling in the context of music recommendation and retrieval, addressing the recent trend towards user-centric and adaptive approaches and systems. A discussion follows about the important aspect of how various MIR approaches to different problems are evaluated and compared. Eventually, a discussion about the major open challenges concludes the survey.",2014.0,"M. Schedl, E. Gómez, Julián Urbano"
abce5f6d32ce30e707d53b62dd83fdf6c0111caf,https://www.semanticscholar.org/paper/abce5f6d32ce30e707d53b62dd83fdf6c0111caf,Voice Recognition Algorithms using Mel Frequency Cepstral Coefficient (MFCC) and Dynamic Time Warping (DTW) Techniques,"Abstract — Digital processing of speech signal and voice recognition algorithm is very important for fast and accurate automatic voice recognition technology. The voice is a signal of infinite information. A direct analysis and synthesizing the complex voice signal is due to too much information contained in the signal. Therefore the digital signal processes such as Feature Extraction and Feature Matching are introduced to represent the voice signal. Several methods such as Liner Predictive Predictive Coding (LPC), Hidden Markov Model (HMM), Artificial Neural Network (ANN) and etc are evaluated with a view to identify a straight forward and effective method for voice signal. The extraction and matching process is implemented right after the Pre Processing or filtering signal is performed. The non-parametric method for modelling the human auditory perception system, Mel Frequency Cepstral Coefficients (MFCCs) are utilize as extraction techniques. The non linear sequence alignment known as Dynamic Time Warping (DTW) introduced by Sakoe Chiba has been used as features matching techniques. Since it’s obvious that the voice signal tends to have different temporal rate, the alignment is important to produce the better performance.This paper present the viability of MFCC to extract features and DTW to compare the test patterns.",2010.0,"Lindasalwa Muda, M. Begam, I. Elamvazuthi"
133b97e40017a9bbbadd10bcd7f13088a97ca3cc,https://www.semanticscholar.org/paper/133b97e40017a9bbbadd10bcd7f13088a97ca3cc,Dissecting Recall of Factual Associations in Auto-Regressive Language Models,"Transformer-based language models (LMs) are known to capture factual knowledge in their parameters. While previous work looked into where factual associations are stored, only little is known about how they are retrieved internally during inference. We investigate this question through the lens of information flow. Given a subject-relation query, we study how the model aggregates information about the subject and relation to predict the correct attribute. With interventions on attention edges, we first identify two critical points where information propagates to the prediction: one from the relation positions followed by another from the subject positions. Next, by analyzing the information at these points, we unveil a three-step internal mechanism for attribute extraction. First, the representation at the last-subject position goes through an enrichment process, driven by the early MLP sublayers, to encode many subject-related attributes. Second, information from the relation propagates to the prediction. Third, the prediction representation""queries""the enriched subject to extract the attribute. Perhaps surprisingly, this extraction is typically done via attention heads, which often encode subject-attribute mappings in their parameters. Overall, our findings introduce a comprehensive view of how factual associations are stored and extracted internally in LMs, facilitating future research on knowledge localization and editing.",2023.0,"Mor Geva, Jasmijn Bastings, Katja Filippova, A. Globerson"
84b7d0a808aa166209953707a02a2ea6d34bae78,https://www.semanticscholar.org/paper/84b7d0a808aa166209953707a02a2ea6d34bae78,Lossless generalized-LSB data embedding,"We present a novel lossless (reversible) data-embedding technique, which enables the exact recovery of the original host signal upon extraction of the embedded information. A generalization of the well-known least significant bit (LSB) modification is proposed as the data-embedding method, which introduces additional operating points on the capacity-distortion curve. Lossless recovery of the original is achieved by compressing portions of the signal that are susceptible to embedding distortion and transmitting these compressed descriptions as a part of the embedded payload. A prediction-based conditional entropy coder which utilizes unaltered portions of the host signal as side-information improves the compression efficiency and, thus, the lossless data-embedding capacity.",2005.0,"M. Celik, Gaurav Sharma, A. Tekalp, E. Saber"
f13c82cc881f2438881994e5f359f8f00420be0f,https://www.semanticscholar.org/paper/f13c82cc881f2438881994e5f359f8f00420be0f,Rethinking the Image Fusion: A Fast Unified Image Fusion Network based on Proportional Maintenance of Gradient and Intensity,"In this paper, we propose a fast unified image fusion network based on proportional maintenance of gradient and intensity (PMGI), which can end-to-end realize a variety of image fusion tasks, including infrared and visible image fusion, multi-exposure image fusion, medical image fusion, multi-focus image fusion and pan-sharpening. We unify the image fusion problem into the texture and intensity proportional maintenance problem of the source images. On the one hand, the network is divided into gradient path and intensity path for information extraction. We perform feature reuse in the same path to avoid loss of information due to convolution. At the same time, we introduce the pathwise transfer block to exchange information between different paths, which can not only pre-fuse the gradient information and intensity information, but also enhance the information to be processed later. On the other hand, we define a uniform form of loss function based on these two kinds of information, which can adapt to different fusion tasks. Experiments on publicly available datasets demonstrate the superiority of our PMGI over the state-of-the-art in terms of both visual effect and quantitative metric in a variety of fusion tasks. In addition, our method is faster compared with the state-of-the-art.",2020.0,"Hao Zhang, Han Xu, Yang Xiao, Xiaojie Guo, Jiayi Ma"
d00c31df3c50cd57eba2ad39709e1cb14a208246,https://www.semanticscholar.org/paper/d00c31df3c50cd57eba2ad39709e1cb14a208246,Relation Classification via Multi-Level Attention CNNs,"Relation classification is a crucial ingredient 
in numerous information extraction systems 
seeking to mine structured facts from 
text. We propose a novel convolutional 
neural network architecture for this task, 
relying on two levels of attention in order 
to better discern patterns in heterogeneous 
contexts. This architecture enables endto-end 
learning from task-specific labeled 
data, forgoing the need for external knowledge 
such as explicit dependency structures. 
Experiments show that our model outperforms 
previous state-of-the-art methods, including 
those relying on much richer forms 
of prior knowledge.",2016.0,"Linlin Wang, Zhu Cao, Gerard de Melo, Zhiyuan Liu"
591e53602b390746851040047dfd1e499144ef64,https://www.semanticscholar.org/paper/591e53602b390746851040047dfd1e499144ef64,Generalized Multiview Analysis: A discriminative latent space,"This paper presents a general multi-view feature extraction approach that we call Generalized Multiview Analysis or GMA. GMA has all the desirable properties required for cross-view classification and retrieval: it is supervised, it allows generalization to unseen classes, it is multi-view and kernelizable, it affords an efficient eigenvalue based solution and is applicable to any domain. GMA exploits the fact that most popular supervised and unsupervised feature extraction techniques are the solution of a special form of a quadratic constrained quadratic program (QCQP), which can be solved efficiently as a generalized eigenvalue problem. GMA solves a joint, relaxed QCQP over different feature spaces to obtain a single (non)linear subspace. Intuitively, GMA is a supervised extension of Canonical Correlational Analysis (CCA), which is useful for cross-view classification and retrieval. The proposed approach is general and has the potential to replace CCA whenever classification or retrieval is the purpose and label information is available. We outperform previous approaches for textimage retrieval on Pascal and Wiki text-image data. We report state-of-the-art results for pose and lighting invariant face recognition on the MultiPIE face dataset, significantly outperforming other approaches.",2012.0,"Abhishek Sharma, Abhishek Kumar, Hal Daumé, D. Jacobs"
a8af16d0e5679fb53d07731cb90b9c91b559e8df,https://www.semanticscholar.org/paper/a8af16d0e5679fb53d07731cb90b9c91b559e8df,Skeleton extraction by mesh contraction,"Extraction of curve-skeletons is a fundamental problem with many applications in computer graphics and visualization. In this paper, we present a simple and robust skeleton extraction method based on mesh contraction. The method works directly on the mesh domain, without pre-sampling the mesh model into a volumetric representation. The method first contracts the mesh geometry into zero-volume skeletal shape by applying implicit Laplacian smoothing with global positional constraints. The contraction does not alter the mesh connectivity and retains the key features of the original mesh. The contracted mesh is then converted into a 1D curve-skeleton through a connectivity surgery process to remove all the collapsed faces while preserving the shape of the contracted mesh and the original topology. The centeredness of the skeleton is refined by exploiting the induced skeleton-mesh mapping. In addition to producing a curve skeleton, the method generates other valuable information about the object's geometry, in particular, the skeleton-vertex correspondence and the local thickness, which are useful for various applications. We demonstrate its effectiveness in mesh segmentation and skinning animation.",2008.0,"Oscar Kin-Chung Au, Chiew-Lan Tai, Hung-Kuo Chu, D. Cohen-Or, Tong-Yee Lee"
acec622ca4fb7e01a56116522d35ded149969d0a,https://www.semanticscholar.org/paper/acec622ca4fb7e01a56116522d35ded149969d0a,Automatically Generating Extraction Patterns from Untagged Text,"Many corpus-based natural language processing systems rely on text corpora that have been manually annotated with syntactic or semantic tags. In particular, all previous dictionary construction systems for information extraction have used an annotated training corpus or some form of annotated input. We have developed a system called AutoSlog-TS that creates dictionaries of extraction patterns using only untagged text. AutoSlog-TS is based on the AutoSlog system, which generated extraction patterns using annotated text and a set of heuristic rules. By adapting AutoSlog and combining it with statistical techniques, we eliminated its dependency on tagged text. In experiments with the MUG-4 terrorism domain, AutoSlog-TS created a dictionary of extraction patterns that performed comparably to a dictionary created by AutoSlog, using only preclassified texts as input.",1996.0,E. Riloff
96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7,https://www.semanticscholar.org/paper/96430cc91ed91fd2d4042fa6fcb7ecf4005d77a7,A Brief Survey of Text Mining,"The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.",2005.0,"A. Hotho, A. Nürnberger, G. Paass"
ee8085bd00f437318cce5b919b47233de8c5f6ff,https://www.semanticscholar.org/paper/ee8085bd00f437318cce5b919b47233de8c5f6ff,Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features,"Abstract Objective Social media is becoming increasingly popular as a platform for sharing personal health-related information. This information can be utilized for public health monitoring tasks, particularly for pharmacovigilance, via the use of natural language processing (NLP) techniques. However, the language in social media is highly informal, and user-expressed medical concepts are often nontechnical, descriptive, and challenging to extract. There has been limited progress in addressing these challenges, and thus far, advanced machine learning-based NLP techniques have been underutilized. Our objective is to design a machine learning-based approach to extract mentions of adverse drug reactions (ADRs) from highly informal text in social media. Methods We introduce ADRMine, a machine learning-based concept extraction system that uses conditional random fields (CRFs). ADRMine utilizes a variety of features, including a novel feature for modeling words’ semantic similarities. The similarities are modeled by clustering words based on unsupervised, pretrained word representation vectors (embeddings) generated from unlabeled user posts in social media using a deep learning technique. Results ADRMine outperforms several strong baseline systems in the ADR extraction task by achieving an F-measure of 0.82. Feature analysis demonstrates that the proposed word cluster features significantly improve extraction performance. Conclusion It is possible to extract complex medical concepts, with relatively high performance, from informal, user-generated content. Our approach is particularly scalable, suitable for social media mining, as it relies on large volumes of unlabeled data, thus diminishing the need for large, annotated training data sets.",2015.0,"Azadeh Nikfarjam, A. Sarker, K. O’Connor, Rachel E. Ginn, G. Gonzalez-Hernandez"
7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e,https://www.semanticscholar.org/paper/7aa63f414a4d7c6e4369a15a04dc5d3eb5da2b0e,Automatic Feature Engineering for Answer Selection and Extraction,"This paper proposes a framework for automatically engineering features for two important tasks of question answering: answer sentence selection and answer extraction. We represent question and answer sentence pairs with linguistic structures enriched by semantic information, where the latter is produced by automatic classifiers, e.g., question classifier and Named Entity Recognizer. Tree kernels applied to such structures enable a simple way to generate highly discriminative structural features that combine syntactic and semantic information encoded in the input trees. We conduct experiments on a public benchmark from TREC to compare with previous systems for answer sentence selection and answer extraction. The results show that our models greatly improve on the state of the art, e.g., up to 22% on F1 (relative improvement) for answer extraction, while using no additional resources and no manual feature engineering.",2013.0,"Aliaksei Severyn, Alessandro Moschitti"
686bac23cd3879c37e73eafeffb67f138de6c83b,https://www.semanticscholar.org/paper/686bac23cd3879c37e73eafeffb67f138de6c83b,Implicit Active Contours Driven by Local Binary Fitting Energy,"Local image information is crucial for accurate segmentation of images with intensity inhomogeneity. However, image information in local region is not embedded in popular region-based active contour models, such as the piecewise constant models. In this paper, we propose a region-based active contour model that is able to utilize image information in local regions. The major contribution of this paper is the introduction of a local binary fitting energy with a kernel function, which enables the extraction of accurate local image information. Therefore, our model can be used to segment images with intensity inhomogeneity, which overcomes the limitation of piecewise constant models. Comparisons with other major region-based models, such as the piece-wise smooth model, show the advantages of our method in terms of computational efficiency and accuracy. In addition, the proposed method has promising application to image denoising.",2007.0,"Chunming Li, C. Kao, J. Gore, Z. Ding"
6c518aabdbba2c073eab6a3bb4120023851e524c,https://www.semanticscholar.org/paper/6c518aabdbba2c073eab6a3bb4120023851e524c,Person Recognition System Based on a Combination of Body Images from Visible Light and Thermal Cameras,"The human body contains identity information that can be used for the person recognition (verification/recognition) problem. In this paper, we propose a person recognition method using the information extracted from body images. Our research is novel in the following three ways compared to previous studies. First, we use the images of human body for recognizing individuals. To overcome the limitations of previous studies on body-based person recognition that use only visible light images for recognition, we use human body images captured by two different kinds of camera, including a visible light camera and a thermal camera. The use of two different kinds of body image helps us to reduce the effects of noise, background, and variation in the appearance of a human body. Second, we apply a state-of-the art method, called convolutional neural network (CNN) among various available methods, for image features extraction in order to overcome the limitations of traditional hand-designed image feature extraction methods. Finally, with the extracted image features from body images, the recognition task is performed by measuring the distance between the input and enrolled samples. The experimental results show that the proposed method is efficient for enhancing recognition accuracy compared to systems that use only visible light or thermal images of the human body.",2017.0,"T. Nguyen, Hyung Gil Hong, Ki-Wan Kim, K. Park"
dba20d0c217667dad6ddf4341b559b0f9d1af9e5,https://www.semanticscholar.org/paper/dba20d0c217667dad6ddf4341b559b0f9d1af9e5,Toward Merging Untargeted and Targeted Methods in Mass Spectrometry-Based Metabolomics and Lipidomics.,"in Mass Spectrometry-Based Metabolomics and Lipidomics Tomas Cajka† and Oliver Fiehn*,†,‡ †UC Davis Genome Center−Metabolomics, University of California Davis, 451 Health Sciences Drive, Davis, California 95616, United States ‡King Abdulaziz University, Faculty of Science, Biochemistry Department, P.O. Box 80203, Jeddah 21589, Saudi Arabia ■ CONTENTS Sample Extraction 525 Extraction of Polar Metabolites (Metabolomics) 525 Extraction of Lipids (Lipidomics) 527 Combined Extraction of Amphiphilic and Lipophilic Metabolites 527 Mass Spectrometry-Based Metabolomics and Lipidomics 528 Direct Infusion MS 528 Ion Mobility-Mass Spectrometry (IM-MS) 529 Liquid Chromatography−Mass Spectrometry (LC−MS) 533 Reversed-Phase Liquid Chromatography (RPLC) 533 Hydrophilic Interaction Chromatography (HILIC) 534 Normal-Phase Liquid Chromatography (NPLC) 535 Supercritical Fluid Chromatography (SFC) 535 Two-Dimensional Liquid Chromatography (2D-LC) 535 Mass Spectrometric Detection 536 Data Processing 540 Quality Control 541 Conclusions 541 Author Information 542 Corresponding Author 542 Notes 542 Biographies 542 Acknowledgments 542 References 542",2016.0,"T. Cajka, O. Fiehn"
bc89fc6d6333e358e3b65c8e956567621cb60a3d,https://www.semanticscholar.org/paper/bc89fc6d6333e358e3b65c8e956567621cb60a3d,Point feature extraction on 3D range scans taking into account object boundaries,"In this paper we address the topic of feature extraction in 3D point cloud data for object recognition and pose identification. We present a novel interest keypoint extraction method that operates on range images generated from arbitrary 3D point clouds, which explicitly considers the borders of the objects identified by transitions from foreground to background. We furthermore present a feature descriptor that takes the same information into account. We have implemented our approach and present rigorous experiments in which we analyze the individual components with respect to their repeatability and matching capabilities and evaluate the usefulness for point feature based object detection methods.",2011.0,"B. Steder, R. Rusu, K. Konolige, Wolfram Burgard"
8a99634e0b418ee61c9bd81f61d334b80486dc53,https://www.semanticscholar.org/paper/8a99634e0b418ee61c9bd81f61d334b80486dc53,Single Document Keyphrase Extraction Using Neighborhood Knowledge,"Existing methods for single document keyphrase extraction usually make use of only the information contained in the specified document. This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results demonstrate the good effectiveness and robustness of our proposed approach.",2008.0,"Xiaojun Wan, Jianguo Xiao"
1c72d8090f281f58d5b141a3917e341250ab4c8c,https://www.semanticscholar.org/paper/1c72d8090f281f58d5b141a3917e341250ab4c8c,Recursive Neural Conditional Random Fields for Aspect-based Sentiment Analysis,"In aspect-based sentiment analysis, extracting aspect terms along with the opinions being expressed from user-generated content is one of the most important subtasks. Previous studies have shown that exploiting connections between aspect and opinion terms is promising for this task. In this paper, we propose a novel joint model that integrates recursive neural networks and conditional random fields into a unified framework for explicit aspect and opinion terms co-extraction. The proposed model learns high-level discriminative features and double propagate information between aspect and opinion terms, simultaneously. Moreover, it is flexible to incorporate hand-crafted features into the proposed model to further boost its information extraction performance. Experimental results on the SemEval Challenge 2014 dataset show the superiority of our proposed model over several baseline methods as well as the winning systems of the challenge.",2016.0,"Wenya Wang, Sinno Jialin Pan, Daniel Dahlmeier, Xiaokui Xiao"
0ad0a038a0bd241561462a005742193a7c623478,https://www.semanticscholar.org/paper/0ad0a038a0bd241561462a005742193a7c623478,Stanford typed dependencies manual,"The Stanford typed dependencies representation was designed to provide a simple description of the grammatical relationships in a sentence that can easily be understood and effectively used by people without linguistic expertise who want to extract textual relations. In particular, rather than the phrase structure representations that have long dominated in the computational linguistic community, it represents all sentence relationships uniformly as typed dependency relations. That is, as triples of a relation between pairs of words, such as “the subject of distributes is Bell.” Our experience is that this simple, uniform representation is quite accessible to non-linguists thinking about tasks involving information extraction from text and is quite effective in relation extraction applications. Here is an example sentence:",2010.0,"Marie-Catherine de Marnee, Christopher D. Manning"
80d8f048ec3d5ab8595ae58cc9105eaafbc57f14,https://www.semanticscholar.org/paper/80d8f048ec3d5ab8595ae58cc9105eaafbc57f14,Natural Language Processing in Radiology: A Systematic Review.,"Radiological reporting has generated large quantities of digital content within the electronic health record, which is potentially a valuable source of information for improving clinical care and supporting research. Although radiology reports are stored for communication and documentation of diagnostic imaging, harnessing their potential requires efficient and automated information extraction: they exist mainly as free-text clinical narrative, from which it is a major challenge to obtain structured data. Natural language processing (NLP) provides techniques that aid the conversion of text into a structured representation, and thus enables computers to derive meaning from human (ie, natural language) input. Used on radiology reports, NLP techniques enable automatic identification and extraction of information. By exploring the various purposes for their use, this review examines how radiology benefits from NLP. A systematic literature search identified 67 relevant publications describing NLP methods that support practical applications in radiology. This review takes a close look at the individual studies in terms of tasks (ie, the extracted information), the NLP methodology and tools used, and their application purpose and performance results. Additionally, limitations, future challenges, and requirements for advancing NLP in radiology will be discussed.",2016.0,"E. Pons, Loes M. M. Braun, M. Hunink, J. Kors"
f49e35b6a85b8d81d2c9d9e26e8bf19dd94fad3a,https://www.semanticscholar.org/paper/f49e35b6a85b8d81d2c9d9e26e8bf19dd94fad3a,Web data extraction based on partial tree alignment,"This paper studies the problem of extracting data from a Web page that contains several structured data records. The objective is to segment these data records, extract data items/fields from them and put the data in a database table. This problem has been studied by several researchers. However, existing methods still have some serious limitations. The first class of methods is based on machine learning, which requires human labeling of many examples from each Web site that one is interested in extracting data from. The process is time consuming due to the large number of sites and pages on the Web. The second class of algorithms is based on automatic pattern discovery. These methods are either inaccurate or make many assumptions. This paper proposes a new method to perform the task automatically. It consists of two steps, (1) identifying individual data records in a page, and (2) aligning and extracting data items from the identified data records. For step 1, we propose a method based on visual information to segment data records, which is more accurate than existing methods. For step 2, we propose a novel partial alignment technique based on tree matching. Partial alignment means that we align only those data fields in a pair of data records that can be aligned (or matched) with certainty, and make no commitment on the rest of the data fields. This approach enables very accurate alignment of multiple data records. Experimental results using a large number of Web pages from diverse domains show that the proposed two-step technique is able to segment data records, align and extract data from them very accurately.",2005.0,"Yanhong Zhai, B. Liu"
f6c480ecfd35fd355eda7ca3fa0d9904ae7ed356,https://www.semanticscholar.org/paper/f6c480ecfd35fd355eda7ca3fa0d9904ae7ed356,Crawling the Hidden Web,"Current-day crawlers retrieve content only from the publicly indexable Web, i.e., the set of Web pages reachable purely by following hypertext links, ignoring search forms and pages that require authorization or prior registration. In particular, they ignore the tremendous amount of high quality content “hidden” behind search forms, in large searchable electronic databases. In this paper, we address the problem of designing a crawler capable of extracting content from this hidden Web. We introduce a generic operational model of a hidden Web crawler and describe how this model is realized in HiWE (Hidden Web Exposer), a prototype crawler built at Stanford. We introduce a new Layout-based Information Extraction Technique (LITE) and demonstrate its use in automatically extracting semantic information from search forms and response pages. We also present results from experiments conducted to test and validate our techniques.",2001.0,"S. Raghavan, H. Garcia-Molina"
23d92dc4598dd7dfa78560f187fa75524584c038,https://www.semanticscholar.org/paper/23d92dc4598dd7dfa78560f187fa75524584c038,A quantitative and comparative analysis of endmember extraction algorithms from hyperspectral data,"Linear spectral unmixing is a commonly accepted approach to mixed-pixel classification in hyperspectral imagery. This approach involves two steps. First, to find spectrally unique signatures of pure ground components, usually known as endmembers, and, second, to express mixed pixels as linear combinations of endmember materials. Over the past years, several algorithms have been developed for autonomous and supervised endmember extraction from hyperspectral data. Due to a lack of commonly accepted data and quantitative approaches to substantiate new algorithms, available methods have not been rigorously compared by using a unified scheme. In this paper, we present a comparative study of standard endmember extraction algorithms using a custom-designed quantitative and comparative framework that involves both the spectral and spatial information. The algorithms considered in this study represent substantially different design choices. A database formed by simulated and real hyperspectral data collected by the Airborne Visible and Infrared Imaging Spectrometer (AVIRIS) is used to investigate the impact of noise, mixture complexity, and use of radiance/reflectance data on algorithm performance. The results obtained indicate that endmember selection and subsequent mixed-pixel interpretation by a linear mixture model are more successful when methods combining spatial and spectral information are applied.",2004.0,"A. Plaza, P. Martínez, R. Pérez, J. Plaza"
639598a428b667cb4573cc3688c1e8b3f3d667b7,https://www.semanticscholar.org/paper/639598a428b667cb4573cc3688c1e8b3f3d667b7,Using Document Level Cross-Event Inference to Improve Event Extraction,"Event extraction is a particularly challenging type of information extraction (IE). Most current event extraction systems rely on local information at the phrase or sentence level. However, this local context may be insufficient to resolve ambiguities in identifying particular types of events; information from a wider scope can serve to resolve some of these ambiguities. In this paper, we use document level information to improve the performance of ACE event extraction. In contrast to previous work, we do not limit ourselves to information about events of the same type, but rather use information about other types of events to make predictions or resolve ambiguities regarding a given event. We learn such relationships from the training corpus and use them to help predict the occurrence of events and event arguments in a text. Experiments show that we can get 9.0% (absolute) gain in trigger (event) classification, and more than 8% gain for argument (role) classification in ACE event extraction.",2010.0,"Shasha Liao, R. Grishman"
a26accf878216be4388ad9a0474e658aa03d33e2,https://www.semanticscholar.org/paper/a26accf878216be4388ad9a0474e658aa03d33e2,SemEval 2017 Task 10: ScienceIE - Extracting Keyphrases and Relations from Scientific Publications,"We describe the SemEval task of extracting keyphrases and relations between them from scientific documents, which is crucial for understanding which publications describe which processes, tasks and materials. Although this was a new task, we had a total of 26 submissions across 3 evaluation scenarios. We expect the task and the findings reported in this paper to be relevant for researchers working on understanding scientific content, as well as the broader knowledge base population and information extraction communities.",2017.0,"Isabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, A. McCallum"
e83eda2c3806e285a231d762424bb1efb890f170,https://www.semanticscholar.org/paper/e83eda2c3806e285a231d762424bb1efb890f170,Constructing Information Networks Using One Single Model,"In this paper, we propose a new framework that unifies the output of three information extraction (IE) tasks - entity mentions, relations and events as an information network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many relations can now be considered as the resultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and significantly advances state-of-the-art end-toend event argument extraction.",2014.0,"Qi Li, Heng Ji, Yu Hong, Sujian Li"
f98ebc10ce8c48020d21cca041de2a3346ce31d9,https://www.semanticscholar.org/paper/f98ebc10ce8c48020d21cca041de2a3346ce31d9,Refining Event Extraction through Cross-Document Inference,"We apply the hypothesis of “One Sense Per Discourse” (Yarowsky, 1995) to information extraction (IE), and extend the scope of “discourse” from one single document to a cluster of topically-related documents. We employ a similar approach to propagate consistent event arguments across sentences and documents. Combining global evidence from related documents with local decisions, we design a simple scheme to conduct cross-document inference for improving the ACE event extraction task 1 . Without using any additional labeled data this new approach obtained 7.6% higher F-Measure in trigger labeling and 6% higher F-Measure in argument labeling over a state-of-the-art IE system which extracts events independently for each sentence.",2008.0,"Heng Ji, R. Grishman"
534c361fcadc7f8c283fa839bbc90ebff4fc3d38,https://www.semanticscholar.org/paper/534c361fcadc7f8c283fa839bbc90ebff4fc3d38,Vessel Pattern Knowledge Discovery from AIS Data: A Framework for Anomaly Detection and Route Prediction,"Understanding maritime traffic patterns is key to Maritime Situational Awareness applications, in particular, to classify and predict activities. Facilitated by the recent build-up of terrestrial networks and satellite constellations of Automatic Identification System (AIS) receivers, ship movement information is becoming increasingly available, both in coastal areas and open waters. The resulting amount of information is increasingly overwhelming to human operators, requiring the aid of automatic processing to synthesize the behaviors of interest in a clear and effective way. Although AIS data are only legally required for larger vessels, their use is growing, and they can be effectively used to infer different levels of contextual information, from the characterization of ports and off-shore platforms to spatial and temporal distributions of routes. An unsupervised and incremental learning approach to the extraction of maritime movement patterns is presented here to convert from raw data to information supporting decisions. This is a basis for automatically detecting anomalies and projecting current trajectories and patterns into the future. The proposed methodology, called TREAD (Traffic Route Extraction and Anomaly Detection) was developed for different levels of intermittency (i.e., sensor coverage and performance), persistence (i.e., time lag between subsequent observations) and data sources (i.e., ground-based and space-based receivers).",2013.0,"G. Pallotta, M. Vespe, K. Bryan"
777ef98b19cac4dd83bd2318f6d29a38ea6c8294,https://www.semanticscholar.org/paper/777ef98b19cac4dd83bd2318f6d29a38ea6c8294,Fine-Grained Entity Recognition,"
 
 Entity Recognition (ER) is a key component of relation extraction systems and many other natural-language processing applications. Unfortunately, most ER systems are restricted to produce labels from to a small set of entity classes, e.g., person, organization, location or miscellaneous. In order to intelligently understand text and extract a wide range of information, it is useful to more precisely determine the semantic classes of entities mentioned in unstructured text. This paper defines a fine-grained set of 112 tags, formulates the tagging problem as multi-class, multi-label classification, describes an unsupervised method for collecting training data, and presents the FIGER implementation. Experiments show that the system accurately predicts the tags for entities. Moreover, it provides useful information for a relation extraction system, increasing the F1 score by 93%. We make FIGER and its data available as a resource for future work.
 
",2012.0,"Xiao Ling, Daniel S. Weld"
4582e2350e4822834dcf266522690722dd4430d4,https://www.semanticscholar.org/paper/4582e2350e4822834dcf266522690722dd4430d4,PRADA: Protecting Against DNN Model Stealing Attacks,"Machine learning (ML) applications are increasingly prevalent. Protecting the confidentiality of ML models becomes paramount for two reasons: (a) a model can be a business advantage to its owner, and (b) an adversary may use a stolen model to find transferable adversarial examples that can evade classification by the original model. Access to the model can be restricted to be only via well-defined prediction APIs. Nevertheless, prediction APIs still provide enough information to allow an adversary to mount model extraction attacks by sending repeated queries via the prediction API. In this paper, we describe new model extraction attacks using novel approaches for generating synthetic queries, and optimizing training hyperparameters. Our attacks outperform state-of-the-art model extraction in terms of transferability of both targeted and non-targeted adversarial examples (up to +29-44 percentage points, pp), and prediction accuracy (up to +46 pp) on two datasets. We provide take-aways on how to perform effective model extraction attacks. We then propose PRADA, the first step towards generic and effective detection of DNN model extraction attacks. It analyzes the distribution of consecutive API queries and raises an alarm when this distribution deviates from benign behavior. We show that PRADA can detect all prior model extraction attacks with no false positives.",2018.0,"Mika Juuti, S. Szyller, A. Dmitrenko, Samuel Marchal, N. Asokan"
a8dff94485a16320e1ef980420bc74b2882babee,https://www.semanticscholar.org/paper/a8dff94485a16320e1ef980420bc74b2882babee,Event extraction across multiple levels of biological organization,"Motivation: Event extraction using expressive structured representations has been a significant focus of recent efforts in biomedical information extraction. However, event extraction resources and methods have so far focused almost exclusively on molecular-level entities and processes, limiting their applicability. Results: We extend the event extraction approach to biomedical information extraction to encompass all levels of biological organization from the molecular to the whole organism. We present the ontological foundations, target types and guidelines for entity and event annotation and introduce the new multi-level event extraction (MLEE) corpus, manually annotated using a structured representation for event extraction. We further adapt and evaluate named entity and event extraction methods for the new task, demonstrating that both can be achieved with performance broadly comparable with that for established molecular entity and event extraction tasks. Availability: The resources and methods introduced in this study are available from http://nactem.ac.uk/MLEE/. Contact: pyysalos@cs.man.ac.uk Supplementary information: Supplementary data are available at Bioinformatics online.",2012.0,"S. Pyysalo, Tomoko Ohta, Makoto Miwa, Han-Cheol Cho, Junichi Tsujii, S. Ananiadou"
e51889e4f08c58bd0aad781c6f6284443c4f7a3c,https://www.semanticscholar.org/paper/e51889e4f08c58bd0aad781c6f6284443c4f7a3c,Speaker Identification and Verification by Combining MFCC and Phase Information,"In conventional speaker recognition methods based on Mel-frequency cepstral coefficients (MFCCs), phase information has hitherto been ignored. In this paper, we propose a phase information extraction method that normalizes the change variation in the phase according to the frame position of the input speech and combines the phase information with MFCCs in text-independent speaker identification and verification methods. There is a problem with the original phase information extraction method when comparing two phase values. For example, the difference in the two values of π-\mathtildeθ1 and \mathtildeθ2=-π+\mathtildeθ1 is 2π-2\mathtildeθ1 . If \mathtildeθ1 ≈ 0, then the difference ≈ 2π, despite the two phases being very similar to one another. To address this problem, we map the phase into coordinates on a unit circle. Speaker identification and verification experiments are performed using the NTT database which consists of sentences uttered by 35 (22 male and 13 female) Japanese speakers with normal, fast and slow speaking modes during five sessions. Although the phase information-based method performs worse than the MFCC-based method, it augments the MFCC and the combination is useful for speaker recognition. The proposed modified phase information is more robust than the original phase information for all speaking modes. By integrating the modified phase information with the MFCCs, the speaker identification rate was improved to 98.8% from 97.4% (MFCC), and equal error rate for speaker verification was reduced to 0.45% from 0.72% (MFCC), respectively. We also conducted the speaker identification and verification experiments on a large-scale Japanese Newspaper Article Sentences (JNAS) database, a similar trend as NTT database was obtained.",2012.0,"S. Nakagawa, Longbiao Wang, Shinji Ohtsuka"
6c8898cda9a1f13607e24306f6f64f20e0ff2ae7,https://www.semanticscholar.org/paper/6c8898cda9a1f13607e24306f6f64f20e0ff2ae7,The Tradeoffs Between Open and Traditional Relation Extraction,"Traditional Information Extraction (IE) takes a relation name and hand-tagged examples of that relation as input. Open IE is a relationindependent extraction paradigm that is tailored to massive and heterogeneous corpora such as the Web. An Open IE system extracts a diverse set of relational tuples from text without any relation-specific input. How is Open IE possible? We analyze a sample of English sentences to demonstrate that numerous relationships are expressed using a compact set of relation-independent lexico-syntactic patterns, which can be learned by an Open IE system. What are the tradeoffs between Open IE and traditional IE? We consider this question in the context of two tasks. First, when the number of relations is massive, and the relations themselves are not pre-specified, we argue that Open IE is necessary. We then present a new model for Open IE called O-CRF and show that it achieves increased precision and nearly double the recall than the model employed by TEXTRUNNER, the previous stateof-the-art Open IE system. Second, when the number of target relations is small, and their names are known in advance, we show that O-CRF is able to match the precision of a traditional extraction system, though at substantially lower recall. Finally, we show how to combine the two types of systems into a hybrid that achieves higher precision than a traditional extractor, with comparable recall.",2008.0,"Michele Banko, Oren Etzioni"
deeec803eaceec8cd5254c64e73b67959c5e7670,https://www.semanticscholar.org/paper/deeec803eaceec8cd5254c64e73b67959c5e7670,Lightweight DDoS flooding attack detection using NOX/OpenFlow,"Distributed denial-of-service (DDoS) attacks became one of the main Internet security problems over the last decade, threatening public web servers in particular. Although the DDoS mechanism is widely understood, its detection is a very hard task because of the similarities between normal traffic and useless packets, sent by compromised hosts to their victims. This work presents a lightweight method for DDoS attack detection based on traffic flow features, in which the extraction of such information is made with a very low overhead compared to traditional approaches. This is possible due to the use of the NOX platform which provides a programmatic interface to facilitate the handling of switch information. Other major contributions include the high rate of detection and very low rate of false alarms obtained by flow analysis using Self Organizing Maps.",2010.0,"Rodrigo Braga, E. Mota, A. Passito"
55d8406b42ab7a831a184ab635945558e82ace95,https://www.semanticscholar.org/paper/55d8406b42ab7a831a184ab635945558e82ace95,Fault Diagnosis for Rotating Machinery Using Multiple Sensors and Convolutional Neural Networks,"This paper presents a convolutional neural network (CNN) based approach for fault diagnosis of rotating machinery. The proposed approach incorporates sensor fusion by taking advantage of the CNN structure to achieve higher and more robust diagnosis accuracy. Both temporal and spatial information of the raw data from multiple sensors is considered during the training process of the CNN. Representative features can be extracted automatically from the raw signals. It avoids manual feature extraction or selection, which relies heavily on prior knowledge of specific machinery and fault types. The effectiveness of the developed method is evaluated by using datasets from two types of typical rotating machinery, roller bearings, and gearboxes. Compared with traditional approaches using manual feature extraction, the results show the superior diagnosis performance of the proposed method. The present approach can be extended to fault diagnosis of other machinery with various types of sensors due to its end to end feature learning capability.",2018.0,"Min Xia, Teng Li, Lin Xu, Lizhi Liu, C. D. de Silva"
12f98bdeb982e4562dbe7abf6fe7ca933f2b4edf,https://www.semanticscholar.org/paper/12f98bdeb982e4562dbe7abf6fe7ca933f2b4edf,A Hierarchy of Information Quantities for Finite Block Length Analysis of Quantum Tasks,"We consider two fundamental tasks in quantum information theory, data compression with quantum side information, as well as randomness extraction against quantum side information. We characterize these tasks for general sources using so-called one-shot entropies. These characterizations-in contrast to earlier results-enable us to derive tight second-order asymptotics for these tasks in the i.i.d. limit. More generally, our derivation establishes a hierarchy of information quantities that can be used to investigate information theoretic tasks in the quantum domain: The one-shot entropies most accurately describe an operational quantity, yet they tend to be difficult to calculate for large systems. We show that they asymptotically agree (up to logarithmic terms) with entropies related to the quantum and classical information spectrum, which are easier to calculate in the i.i.d. limit. Our technique also naturally yields bounds on operational quantities for finite block lengths.",2012.0,"M. Tomamichel, Masahito Hayashi"
d78b8c05e8d3eacc7c58865cff09dd29d3815ae8,https://www.semanticscholar.org/paper/d78b8c05e8d3eacc7c58865cff09dd29d3815ae8,"Opinion Extraction, Summarization and Tracking in News and Blog Corpora","Humans like to express their opinions and are eager to know others’ opinions. Automatically mining and organizing opinions from heterogeneous information sources are very useful for individuals, organizations and even governments. Opinion extraction, opinion summarization and opinion tracking are three important techniques for understanding opinions. Opinion extraction mines opinions at word, sentence and document levels from articles. Opinion summarization summarizes opinions of articles by telling sentiment polarities, degree and the correlated events. In this paper, both news and web blog articles are investigated. TREC, NTCIR and articles collected from web blogs serve as the information sources for opinion extraction. Documents related to the issue of animal cloning are selected as the experimental materials. Algorithms for opinion extraction at word, sentence and document level are proposed. The issue of relevant sentence selection is discussed, and then topical and opinionated information are summarized. Opinion summarizations are visualized by representative sentences. Text-based summaries in different languages, and from different sources, are compared. Finally, an opinionated curve showing supportive and nonsupportive degree along the timeline is illustrated by an opinion tracking system.",2006.0,"Lun-Wei Ku, Yu-Ting Liang, Hsin-Hsi Chen"
d31183e6a451f673104aee750d60a341024d4fbc,https://www.semanticscholar.org/paper/d31183e6a451f673104aee750d60a341024d4fbc,Feature sensitive surface extraction from volume data,"The representation of geometric objects based on volumetric data structures has advantages in many geometry processing applications that require, e.g., fast surface interrogation or boolean operations such as intersection and union. However, surface based algorithms like shape optimization (fairing) or freeform modeling often need a topological manifold representation where neighborhood information within the surface is explicitly available. Consequently, it is necessary to find effective conversion algorithms to generate explicit surface descriptions for the geometry which is implicitly defined by a volumetric data set. Since volume data is usually sampled on a regular grid with a given step width, we often observe severe alias artifacts at sharp features on the extracted surfaces. In this paper we present a new technique for surface extraction that performs feature sensitive sampling and thus reduces these alias effects while keeping the simple algorithmic structure of the standard Marching Cubes algorithm. We demonstrate the effectiveness of the new technique with a number of application examples ranging from CSG modeling and simulation to surface reconstruction and remeshing of polygonal models.",2001.0,"L. Kobbelt, M. Botsch, Ulrich Schwanecke, H. Seidel"
273ebdbefd2b0d9653491fed7bb7fb9c645a7171,https://www.semanticscholar.org/paper/273ebdbefd2b0d9653491fed7bb7fb9c645a7171,Spatial/spectral endmember extraction by multidimensional morphological operations,"Spectral mixture analysis provides an efficient mechanism for the interpretation and classification of remotely sensed multidimensional imagery. It aims to identify a set of reference signatures (also known as endmembers) that can be used to model the reflectance spectrum at each pixel of the original image. Thus, the modeling is carried out as a linear combination of a finite number of ground components. Although spectral mixture models have proved to be appropriate for the purpose of large hyperspectral dataset subpixel analysis, few methods are available in the literature for the extraction of appropriate endmembers in spectral unmixing. Most approaches have been designed from a spectroscopic viewpoint and, thus, tend to neglect the existing spatial correlation between pixels. This paper presents a new automated method that performs unsupervised pixel purity determination and endmember extraction from multidimensional datasets; this is achieved by using both spatial and spectral information in a combined manner. The method is based on mathematical morphology, a classic image processing technique that can be applied to the spectral domain while being able to keep its spatial characteristics. The proposed methodology is evaluated through a specifically designed framework that uses both simulated and real hyperspectral data.",2002.0,"A. Plaza, P. Martínez, R. Pérez, J. Plaza"
f07bd250b96042425d9fa240147c4c45a6d846f2,https://www.semanticscholar.org/paper/f07bd250b96042425d9fa240147c4c45a6d846f2,Review of methods of small‐footprint airborne laser scanning for extracting forest inventory data in boreal forests,"Experiences from Nordic countries and Canada have shown that the retrieval of the stem volume and mean tree height of a tree or at stand level from laser scanner data performs as well as, or better than, photogrammetric methods, and better than other remote sensing methods. This paper reviews the methods of small‐footprint airborne laser scanning for extracting forest inventory data, mainly in the boreal forest zone. The methods are divided into the following categories: extraction of terrain and canopy height model; feature extraction approaches (canopy height distribution and individual‐tree‐based techniques, techniques based on the synergetic use of aerial images and lidar, and other new approaches); tree species classification and forest growth using laser scanner; and the use of intensity and waveform data in forest information extraction. Despite this, the focus is on methods, some review of quality obtained, especially in the boreal forest area, is included. Several recommendations for future research are given to foster the methodology development.",2008.0,"J. Hyyppä, H. Hyyppä, D. Leckie, F. Gougeon, Xiaowei Yu, M. Maltamo"
f78867834f7f6797ca6396f98edb10aad2a864fb,https://www.semanticscholar.org/paper/f78867834f7f6797ca6396f98edb10aad2a864fb,Extraction of Visual Features for Lipreading,"The multimodal nature of speech is often ignored in human-computer interaction, but lip deformations and other body motion, such as those of the head, convey additional information. We integrate speech cues from many sources and this improves intelligibility, especially when the acoustic signal is degraded. The paper shows how this additional, often complementary, visual speech information can be used for speech recognition. Three methods for parameterizing lip image sequences for recognition using hidden Markov models are compared. Two of these are top-down approaches that fit a model of the inner and outer lip contours and derive lipreading features from a principal component analysis of shape or shape and appearance, respectively. The third, bottom-up, method uses a nonlinear scale-space analysis to form features directly from the pixel intensity. All methods are compared on a multitalker visual speech recognition task of isolated letters.",2002.0,"I. Matthews, Tim Cootes, J. Bangham, S. Cox, Richard Harvey"
851e7b462e225414aa5dda7f626055e3d98bfe2b,https://www.semanticscholar.org/paper/851e7b462e225414aa5dda7f626055e3d98bfe2b,Human-competitive tagging using automatic keyphrase extraction,"This paper connects two research areas: automatic tagging on the web and statistical keyphrase extraction. First, we analyze the quality of tags in a collaboratively created folksonomy using traditional evaluation techniques. Next, we demonstrate how documents can be tagged automatically with a state-of-the-art keyphrase extraction algorithm, and further improve performance in this new domain using a new algorithm, ""Maui"", that utilizes semantic information extracted from Wikipedia. Maui outperforms existing approaches and extracts tags that are competitive with those assigned by the best performing human taggers.",2009.0,"Olena Medelyan, E. Frank, I. Witten"
8446830f3c05b97c4d12a0751c022d1ae6a5115b,https://www.semanticscholar.org/paper/8446830f3c05b97c4d12a0751c022d1ae6a5115b,Learning to Extract Symbolic Knowledge from the World Wide Web,"The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.",1998.0,"M. Craven, Dan DiPasquo, Dayne Freitag, A. McCallum, Tom Michael Mitchell, K. Nigam, Seán Slattery"
128a71d926c7eb5369f7d2aed87207dcc8079c39,https://www.semanticscholar.org/paper/128a71d926c7eb5369f7d2aed87207dcc8079c39,A comparison of features for synthetic speech detection,"The performance of biometric systems based on automatic speaker recognition technology is severely degraded due to spoofing attacks with synthetic speech generated using diff erent voice conversion (VC) and speech synthesis (SS) techniques. Various countermeasures are proposed to detect this type of attack, and in this context, choosing an appropriate feature extraction technique for capturing relevant information from speech is an important issue. This paper presents a concise experimental review of different features for synthetic speech detection task. A wide variety of features considered in this stud y include previously investigated features as well as some other potentially useful features for characterizing real and sy nthetic speech. The experiments are conducted on recently released ASVspoof 2015 corpus containing speech data from a large number of VC and SS technique. Comparative results using two different classifiers indicate that features representing spectral information in high-frequency region, dynamic information of speech, and detailed information related to subband characteristics are considerably more useful in detecting synthetic sp eech. Index Terms: anti-spoofing, ASVspoof 2015, feature extraction, countermeasures",2015.0,"Md. Sahidullah, T. Kinnunen, C. Hanilçi"
c8d665766b6e7661dbdef3fe80ccd5be7579aafb,https://www.semanticscholar.org/paper/c8d665766b6e7661dbdef3fe80ccd5be7579aafb,Emotion Recognition From EEG Using Higher Order Crossings,"Electroencephalogram (EEG)-based emotion recognition is a relatively new field in the affective computing area with challenging issues regarding the induction of the emotional states and the extraction of the features in order to achieve optimum classification performance. In this paper, a novel emotion evocation and EEG-based feature extraction technique is presented. In particular, the mirror neuron system concept was adapted to efficiently foster emotion induction by the process of imitation. In addition, higher order crossings (HOC) analysis was employed for the feature extraction scheme and a robust classification method, namely HOC-emotion classifier (HOC-EC), was implemented testing four different classifiers [quadratic discriminant analysis (QDA), k-nearest neighbor, Mahalanobis distance, and support vector machines (SVMs)], in order to accomplish efficient emotion recognition. Through a series of facial expression image projection, EEG data have been collected by 16 healthy subjects using only 3 EEG channels, namely Fp1, Fp2, and a bipolar channel of F3 and F4 positions according to 10-20 system. Two scenarios were examined using EEG data from a single-channel and from combined-channels, respectively. Compared with other feature extraction methods, HOC-EC appears to outperform them, achieving a 62.3% (using QDA) and 83.33% (using SVM) classification accuracy for the single-channel and combined-channel cases, respectively, differentiating among the six basic emotions, i.e., happiness , surprise, anger, fear, disgust, and sadness. As the emotion class-set reduces its dimension, the HOC-EC converges toward maximum classification rate (100% for five or less emotions), justifying the efficiency of the proposed approach. This could facilitate the integration of HOC-EC in human machine interfaces, such as pervasive healthcare systems, enhancing their affective character and providing information about the user's emotional status (e.g., identifying user's emotion experiences, recurring affective states, time-dependent emotional trends).",2010.0,"P. Petrantonakis, L. Hadjileontiadis"
4a3e9fcfc88d53cdf0b7633fbc914f98d1e92225,https://www.semanticscholar.org/paper/4a3e9fcfc88d53cdf0b7633fbc914f98d1e92225,"Fuzzy Connectedness and Object Definition: Theory, Algorithms, and Applications in Image Segmentation","Images are by nature fuzzy. Approaches to object information extraction from images should attempt to use this fact and retain fuzziness as realistically as possible. In past image segmentation research, the notion of “hanging togetherness” of image elements specified by their fuzzy connectedness has been lacking. We present a theory of fuzzy objects forn-dimensional digital spaces based on a notion of fuzzy connectedness of image elements. Although our definitions lead to problems of enormous combinatorial complexity, the theoretical results allow us to reduce this dramatically, leading us to practical algorithms for fuzzy object extraction. We present algorithms for extracting a specified fuzzy object and for identifying all fuzzy objects present in the image data. We demonstrate the utility of the theory and algorithms in image segmentation based on several practical examples all drawn from medical imaging.",1996.0,"J. Udupa, S. Samarasekera"
c15e3ad08e22705bacdd8aa102765cbd5a7ff62e,https://www.semanticscholar.org/paper/c15e3ad08e22705bacdd8aa102765cbd5a7ff62e,A SURVEY OF ONTOLOGY EVALUATION TECHNIQUES,"An ontology is an explicit formal conceptualization of some domain of interest. Ontologies are increasingly used in various fields such as knowledge management, information extraction, and the semantic web. Ontology evaluation is the problem of assessing a given ontology from the point of view of a particular criterion of application, typically in order to determine which of several ontologies would best suit a particular purpose. This paper presents a survey of the state of the art in ontology evaluation.",2005.0,"J. Brank, M. Grobelnik"
9c7f4412b8f0310a91334aed79b8553b2ad70908,https://www.semanticscholar.org/paper/9c7f4412b8f0310a91334aed79b8553b2ad70908,Constructing Biological Knowledge Bases by Extracting Information from Text Sources,"Recently, there has been much effort in making databases for molecular biology more accessible and interoperable. However, information in text form, such as MEDLINE records, remains a greatly underutilized source of biological information. We have begun a research effort aimed at automatically mapping information from text sources into structured representations, such as knowledge bases. Our approach to this task is to use machine-learning methods to induce routines for extracting facts from text. We describe two learning methods that we have applied to this task--a statistical text classification method, and a relational learning method--and our initial experiments in learning such information-extraction routines. We also present an approach to decreasing the cost of learning information-extraction routines by learning from ""weakly"" labeled training data.",1999.0,"M. Craven, J. Kumlien"
b3d49c58b86ec141639cee7d8012ffa1bdc5edc0,https://www.semanticscholar.org/paper/b3d49c58b86ec141639cee7d8012ffa1bdc5edc0,Information Theory and Radar Waveform Design,"The use of information theory to design waveforms for the measurement of extended radar targets exhibiting resonance phenomena is investigated. The target impulse response is introduced to model target scattering behavior. Two radar waveform design problems with constraints on waveform energy and duration are then solved. In the first, a deterministic target impulse response is used to design waveform/receiver-filter pairs for the optimal detection of extended targets in additive noise. In the second, a random target impulse response is used to design waveforms that maximize the mutual information between a target ensemble and the received signal in additive Gaussian noise. The two solutions are contrasted to show the difference between the characteristics of waveforms for extended target detection and information extraction. The optimal target detection solution places as much energy as possible in the largest target scattering mode under the imposed constraints on waveform duration and energy. The optimal information extraction solution distributes the energy among the target scattering modes in order to maximize the mutual information between the target ensemble and the received radar waveform.",1993.0,M. Bell
a21369bc1adeb28aa98298f93aa28dfee63fe637,https://www.semanticscholar.org/paper/a21369bc1adeb28aa98298f93aa28dfee63fe637,"Surface Electromyography: Physiology, engineering, and applications","Reflects on developments in noninvasive electromyography, and includes advances and applications in signal detection, processing and interpretation. Addresses EMG imaging technology together with the issue of decomposition of surface EMG Includes advanced single and multi-channel techniques for information extraction from surface EMG signals Presents the analysis and information extraction of surface EMG at various scales, from motor units to the concept of muscle synergies.",2016.0,R. Merletti
ee3e2a5608033acda22bcc7abc9f767e3e8fc912,https://www.semanticscholar.org/paper/ee3e2a5608033acda22bcc7abc9f767e3e8fc912,GENIES: a natural-language processing system for the extraction of molecular pathways from journal articles,"Systems that extract structured information from natural language passages have been highly successful in specialized domains. The time is opportune for developing analogous applications for molecular biology and genomics. We present a system, GENIES, that extracts and structures information about cellular pathways from the biological literature in accordance with a knowledge model that we developed earlier. We implemented GENIES by modifying an existing medical natural language processing system, MedLEE, and performed a preliminary evaluation study. Our results demonstrate the value of the underlying techniques for the purpose of acquiring valuable knowledge from biological journals.",2001.0,"C. Friedman, Pauline Kra, Hong Yu, M. Krauthammer, A. Rzhetsky"
61569a320b634b7631cbd75411e6e4ac0b38e560,https://www.semanticscholar.org/paper/61569a320b634b7631cbd75411e6e4ac0b38e560,Texture information in run-length matrices,We use a multilevel dominant eigenvector estimation algorithm to develop a new run-length texture feature extraction algorithm that preserves much of the texture information in run-length matrices and significantly improves image classification accuracy over traditional run-length techniques. The advantage of this approach is demonstrated experimentally by the classification of two texture data sets. Comparisons with other methods demonstrate that the run-length matrices contain great discriminatory information and that a good method of extracting such information is of paramount importance to successful classification.,1998.0,Xiaoou Tang
036ab2e828bfb0bd080a3d2f936d3e973199b717,https://www.semanticscholar.org/paper/036ab2e828bfb0bd080a3d2f936d3e973199b717,Wavelet packet feature extraction for vibration monitoring,"Condition monitoring of dynamic systems based on vibration signatures has generally relied upon Fourier based analysis as a means of translating vibration signals in time domain into the frequency domain. However, Fourier analysis provided a poor representation of signals well localized in time. The wavelet packet transform is introduced as an alternative means of extracting time-frequency information from vibration signature. Moreover, with the aid of statistical based feature selection criteria, a lot of feature components containing little discriminant information could be discarded resulting in a feature subset with reduced number of parameters. This significantly reduces the long training time that is often associated with neural network classifier and increases the generalization ability of the neural network classifier.",1999.0,"G. Yen, Kuo-Chung Lin"
4bb8b75308b5e9021c02285584e1851479089d91,https://www.semanticscholar.org/paper/4bb8b75308b5e9021c02285584e1851479089d91,Identifying Sources of Opinions with Conditional Random Fields and Extraction Patterns,"Recent systems have been developed for sentiment classification, opinion recognition, and opinion analysis (e.g., detecting polarity and strength). We pursue another aspect of opinion analysis: identifying the sources of opinions, emotions, and sentiments. We view this problem as an information extraction task and adopt a hybrid approach that combines Conditional Random Fields (Lafferty et al., 2001) and a variation of AutoSlog (Riloff, 1996a). While CRFs model source identification as a sequence tagging task, AutoSlog learns extraction patterns. Our results show that the combination of these two methods performs better than either one alone. The resulting system identifies opinion sources with 79.3% precision and 59.5% recall using a head noun matching measure, and 81.2% precision and 60.6% recall using an overlap measure.",2005.0,"Yejin Choi, Claire Cardie, E. Riloff, Siddharth Patwardhan"
2f057ecc6706993f3456f1dc7cea1a40efb3eab1,https://www.semanticscholar.org/paper/2f057ecc6706993f3456f1dc7cea1a40efb3eab1,Automatic Ontology-Based Knowledge Extraction from Web Documents,"To bring the Semantic Web to life and provide advanced knowledge services, we need efficient ways to access and extract knowledge from Web documents. Although Web page annotations could facilitate such knowledge gathering, annotations are rare and will probably never be rich or detailed enough to cover all the knowledge these documents contain. Manual annotation is impractical and unscalable, and automatic annotation tools remain largely undeveloped. Specialized knowledge services therefore require tools that can search and extract specific knowledge directly from unstructured text on the Web, guided by an ontology that details what type of knowledge to harvest. An ontology uses concepts and relations to classify domain knowledge. Other researchers have used ontologies to support knowledge extraction, but few have explored their full potential in this domain. The paper considers the Artequakt project which links a knowledge extraction tool with an ontology to achieve continuous knowledge support and guide information extraction. The extraction tool searches online documents and extracts knowledge that matches the given classification structure. It provides this knowledge in a machine-readable format that will be automatically maintained in a knowledge base (KB). Knowledge extraction is further enhanced using a lexicon-based term expansion mechanism that provides extended ontology terminology.",2003.0,"Harith Alani, Sanghee Kim, D. Millard, M. Weal, W. Hall, P. Lewis, N. Shadbolt"
3ee5f9f05f25f229d09db43eb4e47d31755fa0ec,https://www.semanticscholar.org/paper/3ee5f9f05f25f229d09db43eb4e47d31755fa0ec,A survey of current work in biomedical text mining,"The volume of published biomedical research, and therefore the underlying biomedical knowledge base, is expanding at an increasing rate. Among the tools that can aid researchers in coping with this information overload are text mining and knowledge extraction. Significant progress has been made in applying text mining to named entity recognition, text classification, terminology extraction, relationship extraction and hypothesis generation. Several research groups are constructing integrated flexible text-mining systems intended for multiple uses. The major challenge of biomedical text mining over the next 5-10 years is to make these systems useful to biomedical researchers. This will require enhanced access to full text, better understanding of the feature space of biomedical literature, better methods for measuring the usefulness of systems to users, and continued cooperation with the biomedical research community to ensure that their needs are addressed.",2005.0,"A. Cohen, W. Hersh"
0d9d8be5ee0c1cda47beafea0ef0b14722cbd908,https://www.semanticscholar.org/paper/0d9d8be5ee0c1cda47beafea0ef0b14722cbd908,"SemEval-2013 Task 1: TempEval-3: Evaluating Time Expressions, Events, and Temporal Relations","Within the SemEval-2013 evaluation exercise, the TempEval-3 shared task aims to advance research on temporal information processing. It follows on from TempEval-1 and -2, with: a three-part structure covering temporal expression, event, and temporal relation extraction; a larger dataset; and new single measures to rank systems ‐ in each task and in general. In this paper, we describe the participants’ approaches, results, and the observations from the results, which may guide future research in this area.",2013.0,"N. UzZaman, H. Llorens, Leon Derczynski, James F. Allen, M. Verhagen, J. Pustejovsky"
6991606a1a9d5c285af385ee9159fd46cc14048e,https://www.semanticscholar.org/paper/6991606a1a9d5c285af385ee9159fd46cc14048e,Table extraction using conditional random fields,"The ability to find tables and extract information from them is a necessary component of data mining, question answering, and other information retrieval tasks. Documents often contain tables in order to communicate densely packed, multi-dimensional information. Tables do this by employing layout patterns to efficiently indicate fields and records in two-dimensional form.Their rich combination of formatting and content present difficulties for traditional language modeling techniques, however. This paper presents the use of conditional random fields (CRFs) for table extraction, and compares them with hidden Markov models (HMMs). Unlike HMMs, CRFs support the use of many rich and overlapping layout and language features, and as a result, they perform significantly better. We show experimental results on plain-text government statistical reports in which tables are located with 92% F1, and their constituent lines are classified into 12 table-related categories with 94% accuracy. We also discuss future work on undirected graphical models for segmenting columns, finding cells, and classifying them as data cells or label cells.",2003.0,"David Pinto, A. McCallum, Xing Wei, W. Bruce Croft"
4d135641931a6efce82bd9c1d69d86e08d3cd28d,https://www.semanticscholar.org/paper/4d135641931a6efce82bd9c1d69d86e08d3cd28d,Extracting Opinion Targets in a Single and Cross-Domain Setting with Conditional Random Fields,"In this paper, we focus on the opinion target extraction as part of the opinion mining task. We model the problem as an information extraction task, which we address based on Conditional Random Fields (CRF). As a baseline we employ the supervised algorithm by Zhuang et al. (2006), which represents the state-of-the-art on the employed data. We evaluate the algorithms comprehensively on datasets from four different domains annotated with individual opinion target instances on a sentence level. Furthermore, we investigate the performance of our CRF-based approach and the baseline in a single- and cross-domain opinion target extraction setting. Our CRF-based approach improves the performance by 0.077, 0.126, 0.071 and 0.178 regarding F-Measure in the single-domain extraction in the four domains. In the cross-domain setting our approach improves the performance by 0.409, 0.242, 0.294 and 0.343 regarding F-Measure over the baseline.",2010.0,"Niklas Jakob, Iryna Gurevych"
6bb6eba248b6ef8d228dc33e6113c3e12dd48fee,https://www.semanticscholar.org/paper/6bb6eba248b6ef8d228dc33e6113c3e12dd48fee,Cooperative computation of stereo disparity.,"The extraction of stereo-disparity information from two images depends upon establishing a correspondence between them. In this article we analyze the nature of the correspondence computation and derive a cooperative algorithm that implements it. We show that this algorithm successfully extracts information from random-dot stereograms, and its implications for the psychophysics and neurophysiology of the visual system are briefly discussed.",1976.0,"D. Marr, T. Poggio"
3a8d4fd2c30e5031a574bc25363c8639912b3bbd,https://www.semanticscholar.org/paper/3a8d4fd2c30e5031a574bc25363c8639912b3bbd,Effects of Adjective Orientation and Gradability on Sentence Subjectivity,"Subjectivity is a pragmatic, sentence-level feature that has important implications for text processing applications such as information extraction and information retrieval. We study the effects of dynamic adjectives, semantically oriented adjectives, and gradable adjectives on a simple subjectivity classifier, and establish that they are strong predictors of subjectivity. A novel trainable method that statistically combines two indicators of gradability is presented and evaluated, complementing existing automatic techniques for assigning orientation labels.",2000.0,"V. Hatzivassiloglou, Janyce Wiebe"
c7ca021cf3db77724600826cac587cae2183bbee,https://www.semanticscholar.org/paper/c7ca021cf3db77724600826cac587cae2183bbee,A New Time-of-Flight Aerosol Mass Spectrometer (TOF-AMS)—Instrument Description and First Field Deployment,"We report the development and first field deployment of a new version of the Aerosol Mass Spectrometer (AMS), which is capable of measuring non-refractory aerosol mass concentrations, chemically speciated mass distributions and single particle information. The instrument was constructed by interfacing the well-characterized Aerodyne AMS vacuum system, particle focusing, sizing, and evaporation/ionization components, with a compact TOFWERK orthogonal acceleration reflectron time-of-flight mass spectrometer. In this time-of-flight aerosol mass spectrometer (TOF-AMS) aerosol particles are focused by an aerodynamic lens assembly as a narrow beam into the vacuum chamber. Non-refractory particle components flash-vaporize after impaction onto the vaporizer and are ionized by electron impact. The ions are continuously guided into the source region of the time-of-flight mass spectrometer, where ions are extracted into the TOF section at a repetition rate of 83.3 kHz. Each extraction generates a complete mass spectrum, which is processed by a fast (sampling rate 1 Gs/s) data acquisition board and a PC. Particle size information is obtained by chopping the particle beam followed by time-resolved detection of the particle evaporation events. Due to the capability of the time-of-flight mass spectrometer of measuring complete mass spectra for every extraction, complete single particle mass spectra can be collected. This mode provides quantitative information on single particle composition. The TOF-AMS allows a direct measurement of internal and external mixture of non-refractory particle components as well as sensitive ensemble average particle composition and chemically resolved size distribution measurements. Here we describe for the first time the TOF-AMS and its operation as well as results from its first field deployment during the PM 2.5 Technology Assessment and Characterization Study—New York (PMTACS-NY) Winter Intensive in January 2004 in Queens, New York. These results show the capability of the TOF-AMS to measure quantitative aerosol composition and chemically resolved size distributions of the ambient aerosol. In addition it is shown that the single particle information collected with the instrument gives direct information about internal and external mixture of particle components.",2005.0,"F. Drewnick, S. S. Hings, P. DeCarlo, J. Jayne, M. Gonin, K. Fuhrer, S. Weimer, J. Jimenez, K. Demerjian, S. Borrmann, D. Worsnop"
bab23bfe022b1c11a583f87065eb8a439a5212c8,https://www.semanticscholar.org/paper/bab23bfe022b1c11a583f87065eb8a439a5212c8,Evaluating discourse-based answer extraction for why-question answering,30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2007),2007.0,"H. Balakrishnan, K. Lakshminarayanan, Sylvia Ratnasamy, S. Shenker, Ion Stoica, Michael Walfish"
93c2712a8d21b69cab7db3b9337d32e5f118a130,https://www.semanticscholar.org/paper/93c2712a8d21b69cab7db3b9337d32e5f118a130,VIPS: a Vision-based Page Segmentation Algorithm,"A new web content structure analysis based on visual representation is proposed in this paper. Many web applications such as information retrieval, information extraction and automatic page adaptation can benefit from this structure. This paper presents an automatic top-down, tag-tree independent approach to detect web content structure. It simulates how a user understands web layout structure based on his visual perception. Comparing to other existing techniques such as DOM tree, our approach is independent to the HTML documentation representation. Our method can work well even when the HTML structure is quite different from the visual layout structure. Several experiments show the effectiveness of our method.",2003.0,"Deng Cai, Shipeng Yu, Ji-Rong Wen, Wei-Ying Ma"
f095d26c79d76c119cf0d65759c9dfb6693ddc35,https://www.semanticscholar.org/paper/f095d26c79d76c119cf0d65759c9dfb6693ddc35,Polyglot: automatic extraction of protocol message format using dynamic binary analysis,"Protocol reverse engineering, the process of extracting the application-level protocol used by an implementation, without access to the protocol specification, is important for many network security applications. Recent work [17] has proposed protocol reverse engineering by using clustering on network traces. That kind of approach is limited by the lack of semantic information on network traces. In this paper we propose a new approach using program binaries. Our approach, shadowing, uses dynamic analysis and is based on a unique intuition - the way that an implementation of the protocol processes the received application data reveals a wealth of information about the protocol message format. We have implemented our approach in a system called Polyglot and evaluated it extensively using real-world implementations of five different protocols: DNS, HTTP, IRC, Samba and ICQ. We compare our results with the manually crafted message format, included in Wireshark, one of the state-of-the-art protocol analyzers. The differences we find are small and usually due to different implementations handling fields in different ways. Finding such differences between implementations is an added benefit, as they are important for problems such as fingerprint generation, fuzzing, and error detection.",2007.0,"Juan Caballero, Heng Yin, Zhenkai Liang, D. Song"
65ec5824f6a997df0322827285ee691510b4527a,https://www.semanticscholar.org/paper/65ec5824f6a997df0322827285ee691510b4527a,Automatic Wrappers for Large Scale Web Extraction,"We present a generic framework to make wrapper induction algorithms tolerant to noise in the training data. This enables us to learn wrappers in a completely unsupervised manner from automatically and cheaply obtained noisy training data, e.g., using dictionaries and regular expressions. By removing the site-level supervision that wrapper-based techniques require, we are able to perform information extraction at web-scale, with accuracy unattained with existing unsupervised extraction techniques. Our system is used in production at Yahoo! and powers live applications.",2011.0,"Nilesh N. Dalvi, Ravi Kumar, Mohamed A. Soliman"
66ca8f42176507fa24eff8578a1c8f82355936f7,https://www.semanticscholar.org/paper/66ca8f42176507fa24eff8578a1c8f82355936f7,Extended x-ray absorption fine structure—its strengths and limitations as a structural tool,"The authors review the development of extended x-ray absorption fine structure (EXAFS) within the last decade. Advances in experimental techniques have been largely stimulated by the availability of synchrotron radiation. The theory of EXAFS has also matured to the point where quantitative comparison with experiments can be made. The authors review in some detail the analysis of EXAFS data, starting from the treatment of raw data to the extraction of distances and amplitude information, and they also discuss selected examples of applications of EXAFS chosen to illustrate both the strength and limitations of EXAFS as a structural tool.",1981.0,"P. Lee, P. Citrin, P. Eisenberger, B. Kincaid"
d096c65116af1e0452172f281e1e347a554542b3,https://www.semanticscholar.org/paper/d096c65116af1e0452172f281e1e347a554542b3,Tensor decompositions for feature extraction and classification of high dimensional datasets,"Feature extraction and selection are key factors in model reduction, classification and pattern recognition problems. This is especially important for input data with large dimensions such as brain recording or multiview images, where appropriate feature extraction is a prerequisite to classification. To ensure that the reduced dataset contains maximum information about input data we propose algorithms for feature extraction and classification. This is achieved based on orthogonal or nonnegative tensor (multi-array) decompositions, and higher order (multilinear) discriminant analysis (HODA), whereby input data are considered as tensors instead of more conventional vector or matrix representations. The developed algorithms are verified on benchmark datasets, using constraints imposed on tensors and/or factor matrices such as orthogonality and nonnegativity.",2010.0,"A. Phan, A. Cichocki"
98844c17c22a93e7a6b40a06433427b7be8f68e2,https://www.semanticscholar.org/paper/98844c17c22a93e7a6b40a06433427b7be8f68e2,Culture Shapes How We Look at Faces,"Background Face processing, amongst many basic visual skills, is thought to be invariant across all humans. From as early as 1965, studies of eye movements have consistently revealed a systematic triangular sequence of fixations over the eyes and the mouth, suggesting that faces elicit a universal, biologically-determined information extraction pattern. Methodology/Principal Findings Here we monitored the eye movements of Western Caucasian and East Asian observers while they learned, recognized, and categorized by race Western Caucasian and East Asian faces. Western Caucasian observers reproduced a scattered triangular pattern of fixations for faces of both races and across tasks. Contrary to intuition, East Asian observers focused more on the central region of the face. Conclusions/Significance These results demonstrate that face processing can no longer be considered as arising from a universal series of perceptual events. The strategy employed to extract visual information from faces differs across cultures.",2008.0,"C. Blais, Rachael E. Jack, Christoph Scheepers, D. Fiset, R. Caldara"
1de8083a9b33e45dc4723937548ed103539f908e,https://www.semanticscholar.org/paper/1de8083a9b33e45dc4723937548ed103539f908e,Genomic Definition of Hypervirulent and Multidrug-Resistant Klebsiella pneumoniae Clonal Groups,"We created a Web-accessible genome database to enable rapid extraction of genotype, virulence, and resistance information from sequences.",2014.0,"Suzanne Bialek-Davenet, A. Criscuolo, Florent Ailloud, Virginie Passet, Louis M. Jones, Anne-Sophie Delannoy-Vieillard, B. Garin, S. le Hello, G. Arlet, M. Nicolas-Chanoine, D. Decré, S. Brisse"
3816f5616598da7e172ffed3971d907517701147,https://www.semanticscholar.org/paper/3816f5616598da7e172ffed3971d907517701147,Unsupervised Spatial–Spectral Feature Learning by 3D Convolutional Autoencoder for Hyperspectral Classification,"Feature learning technologies using convolutional neural networks (CNNs) have shown superior performance over traditional hand-crafted feature extraction algorithms. However, a large number of labeled samples are generally required for CNN to learn effective features under classification task, which are hard to be obtained for hyperspectral remote sensing images. Therefore, in this paper, an unsupervised spatial–spectral feature learning strategy is proposed for hyperspectral images using 3-Dimensional (3D) convolutional autoencoder (3D-CAE). The proposed 3D-CAE consists of 3D or elementwise operations only, such as 3D convolution, 3D pooling, and 3D batch normalization, to maximally explore spatial–spectral structure information for feature extraction. A companion 3D convolutional decoder network is also designed to reconstruct the input patterns to the proposed 3D-CAE, by which all the parameters involved in the network can be trained without labeled training samples. As a result, effective features are learned in an unsupervised mode that label information of pixels is not required. Experimental results on several benchmark hyperspectral data sets have demonstrated that our proposed 3D-CAE is very effective in extracting spatial–spectral features and outperforms not only traditional unsupervised feature extraction algorithms but also many supervised feature extraction algorithms in classification application.",2019.0,"Shaohui Mei, Jingyu Ji, Yunhao Geng, Zhi Zhang, Xu Li, Q. Du"
5ab34e1e0d79aa09ebfd85c9f70236093b3a978d,https://www.semanticscholar.org/paper/5ab34e1e0d79aa09ebfd85c9f70236093b3a978d,"EDGAR: extraction of drugs, genes and relations from the biomedical literature.","EDGAR (Extraction of Drugs, Genes and Relations) is a natural language processing system that extracts information about drugs and genes relevant to cancer from the biomedical literature. This automatically extracted information has remarkable potential to facilitate computational analysis in the molecular biology of cancer, and the technology is straightforwardly generalizable to many areas of biomedicine. This paper reports on the mechanisms for automatically generating such assertions and on a simple application, conceptual clustering of documents. The system uses a stochastic part of speech tagger, generates an underspecified syntactic parse and then uses semantic and pragmatic information to construct its assertions. The system builds on two important existing resources: the MEDLINE database of biomedical citations and abstracts and the Unified Medical Language System, which provides syntactic and semantic information about the terms found in biomedical abstracts.",1999.0,"Thomas C. Rindflesch, L. Tanabe, J. Weinstein, L. Hunter"
230f34ad99419af83f22d403bbcc910be3d34e18,https://www.semanticscholar.org/paper/230f34ad99419af83f22d403bbcc910be3d34e18,Functional properties of proteins in foods: A survey,"Proteins for foods, in addition to providing nutrition, should also possess specific functional properties that facilitate processing and serve as the basis of product performance. Functional properties of proteins for foods connote the physicochemical properties which govern the behavior of protein in foods. This general article collates the published information concerning the major functional properties of food proteins, e.g., solubility, binding properties, surfactant properties, viscogenic texturizing characteristics, etc. The effects of extraction and processing on functional properties and possible correlations between structure and function are discussed, in relation to practical performance in food systems. Modification of proteins to improve functional characteristics is briefly mentioned.",1976.0,"J. Kinsella, N. Melachouris"
65eb4104bce891076a68c96c505fcda2f2e9c13a,https://www.semanticscholar.org/paper/65eb4104bce891076a68c96c505fcda2f2e9c13a,A model for enriching trajectories with semantic geographical information,"The collection of moving object data is becoming more and more common, and therefore there is an increasing need for the efficient analysis and knowledge extraction of these data in different application domains. Trajectory data are normally available as sample points, and do not carry semantic information, which is of fundamental importance for the comprehension of these data. Therefore, the analysis of trajectory data becomes expensive from a computational point of view and complex from a user's perspective. Enriching trajectories with semantic geographical information may simplify queries, analysis, and mining of moving object data. In this paper we propose a data preprocessing model to add semantic information to trajectories in order to facilitate trajectory data analysis in different application domains. The model is generic enough to represent the important parts of trajectories that are relevant to the application, not being restricted to one specific application. We present an algorithm to compute the important parts and show that the query complexity for the semantic analysis of trajectories will be significantly reduced with the proposed model.",2007.0,"L. Alvares, V. Bogorny, B. Kuijpers, J. Macêdo, Bart Moelans, A. Vaisman"
915ceac70544cf69202b74cc4582e4d42ab7ed46,https://www.semanticscholar.org/paper/915ceac70544cf69202b74cc4582e4d42ab7ed46,A Bootstrapping Method for Learning Semantic Lexicons using Extraction Pattern Contexts,"This paper describes a bootstrapping algorithm called Basilisk that learns high-quality semantic lexicons for multiple categories. Basilisk begins with an unannotated corpus and seed words for each semantic category, which are then bootstrapped to learn new words for each category. Basilisk hypothesizes the semantic class of a word based on collective information over a large body of extraction pattern contexts. We evaluate Basilisk on six semantic categories. The semantic lexicons produced by Basilisk have higher precision than those produced by previous techniques, with several categories showing substantial improvement.",2002.0,"Michael Thelen, E. Riloff"
83cc20d078920efa6cc3876a83065beddb592474,https://www.semanticscholar.org/paper/83cc20d078920efa6cc3876a83065beddb592474,Automatic web news extraction using tree edit distance,"The Web poses itself as the largest data repository ever available in the history of humankind. Major efforts have been made in order to provide efficient access to relevant information within this huge repository of data. Although several techniques have been developed to the problem of Web data extraction, their use is still not spread, mostly because of the need for high human intervention and the low quality of the extraction results.In this paper, we present a domain-oriented approach to Web data extraction and discuss its application to automatically extracting news from Web sites. Our approach is based on a highly efficient tree structure analysis that produces very effective results. We have tested our approach with several important Brazilian on-line news sites and achieved very precise results, correctly extracting 87.71% of the news in a set of 4088 pages distributed among 35 different sites.",2004.0,"D. C. Reis, P. B. Golgher, A. D. Silva, Alberto H. F. Laender"
a79c4801090e7b6c1f31b83559f2d2b4af4b9c68,https://www.semanticscholar.org/paper/a79c4801090e7b6c1f31b83559f2d2b4af4b9c68,Multi-Document Summarization By Sentence Extraction,"This paper discusses a text extraction approach to multi-document summarization that builds on single-document summarization methods by using additional, available information about the document set as a whole and the relationships between the documents. Multi-document summarization differs from single in that the issues of compression, speed, redundancy and passage selection are critical in the formation of useful summaries. Our approach addresses these issues by using domain-independent techniques based mainly on fast, statistical processing, a metric for reducing redundancy and maximizing diversity in the selected passages, and a modular framework to allow easy parameterization for different genres, corpora characteristics and user requirements.",2000.0,"J. Goldstein, Vibhu Mittal, J. Carbonell, M. Kantrowitz"
e05b3038d73f6a1cd4aa197f4b52509e5287e734,https://www.semanticscholar.org/paper/e05b3038d73f6a1cd4aa197f4b52509e5287e734,Detection of linear features in SAR images: application to road network extraction,"The authors propose a two-step algorithm for almost unsupervised detection of linear structures, in particular, main axes in road networks, as seen in synthetic aperture radar (SAR) images. The first step is local and is used to extract linear features from the speckle radar image, which are treated as road-segment candidates. The authors present two local line detectors as well as a method for fusing information from these detectors. In the second global step, they identify the real roads among the segment candidates by defining a Markov random field (MRF) on a set of segments, which introduces contextual knowledge about the shape of road objects. The influence of the parameters on the road detection is studied and results are presented for various real radar images.",1998.0,"F. Tupin, H. Maître, J. F. Mangin, J. Nicolas, E. Pechersky"
be3cb96c21f70c2713530180bb8c7df5e6f86130,https://www.semanticscholar.org/paper/be3cb96c21f70c2713530180bb8c7df5e6f86130,Deaths and cardiovascular injuries due to device-assisted implantable cardioverter–defibrillator and pacemaker lead extraction,"Aims An estimated 10 000–15 000 pacemaker and implantable cardioverter–defibrillator (ICD) leads are extracted annually worldwide using specialized tools that disrupt encapsulating fibrous tissue. Additional information is needed regarding the safety of the devices that have been approved for lead extraction. The aim of this study was to determine whether complications due to device-assisted lead extraction might be more hazardous than published data suggest, and whether procedural safety precautions are effective. Methods and results We searched the US Food and Drug Administration's (FDA) Manufacturers and User Defined Experience (MAUDE) database from 1995 to 2008 using the search terms ‘lead extraction and death’ and ‘lead extraction and injury’. Additional product specific searches were performed for the terms ‘death’ and ‘injury’. Between 1995 and 2008, 57 deaths and 48 serious cardiovascular injuries associated with device-assisted lead extraction were reported to the FDA. Owing to underreporting, the FDA database does not contain all adverse events that occurred during this period. Of the 105 events, 27 deaths and 13 injuries occurred in 2007–2008. During these 2 years, 23 deaths were linked with excimer laser or mechanical dilator sheath extractions. The majority of deaths and injuries involved ICD leads, and most were caused by lacerations of the right atrium, superior vena cava, or innominate vein. Overall, 62 patients underwent emergency surgical repair of myocardial perforations and venous lacerations and 35 (56%) survived. Conclusion These findings suggest that device-assisted lead extraction is a high-risk procedure and that serious complications including death may not be mitigated by emergency surgery. However, skilled standby cardiothoracic surgery is essential when performing pacemaker and ICD lead extractions. Although the incidence of these complications is unknown, the results of our study imply that device-assisted lead extractions should be performed by highly qualified physicians and their teams in specialized centres.",2009.0,"R. Hauser, W. Katsiyiannis, C. Gornick, A. Almquist, Linda M. Kallinen"
4b512f10838e05f5b2eee94bfbd20f3d9c4ecb9b,https://www.semanticscholar.org/paper/4b512f10838e05f5b2eee94bfbd20f3d9c4ecb9b,Algorithms for Scoring Coreference Chains,"Scoring the performance of a system is an extremely important aspect of coreference algorithm performance. The score for a particular run is the single strongest measure of how well the system is performing and it can strongly determine directions for further improvements. In this paper, we present several di(cid:11)erent scoring algorithms and detail their respective strengths and weaknesses for varying classes of processing. We also demonstrate that tasks like information extraction have very di(cid:11)erent needs from information retrieval in terms of how to score the performance of coreference annotation.",1998.0,"Amit Bagga, B. Baldwin"
75596ab5e8001cb92b1173d93602ecffea6b25ee,https://www.semanticscholar.org/paper/75596ab5e8001cb92b1173d93602ecffea6b25ee,"YAAFE, an Easy to Use and Efficient Audio Feature Extraction Software","Music Information Retrieval systems are commonly built on a feature extraction stage. For applications involving automatic classification (e.g. speech/music discrimination, music genre or mood recognition, ...), traditional approaches will consider a large set of audio features to be extracted on a large dataset. In some cases, this will lead to computationally intensive systems and there is, therefore, a strong need for efficient feature extraction. In this paper, a new audio feature extraction software, YAAFE 1 , is presented and compared to widely used libraries. The main advantage of YAAFE is a significantly lower complexity due to the appropriate exploitation of redundancy in the feature calculation. YAAFE remains easy to configure and each feature can be parameterized independently. Finally, the YAAFE framework and most of its core feature library are released in source code under the GNU Lesser General Public License.",2010.0,"B. Mathieu, S. Essid, T. Fillon, J. Prado, G. Richard"
e47635e3dc21cf1b4633afd337acb38acadac53b,https://www.semanticscholar.org/paper/e47635e3dc21cf1b4633afd337acb38acadac53b,POLYPHONET: an advanced social network extraction system from the web,"Social networks play important roles in the Semantic Web: knowledge management, information retrieval, ubiquitous computing, and so on. We propose a social network extraction system called POLYPHONET, which employs several advanced techniques to extract relations of persons, detect groups of persons, and obtain keywords for a person. Search engines, especially Google, are used to measure co-occurrence of information and obtain Web documents.Several studies have used search engines to extract social networks from the Web, but our research advances the following points: First, we reduce the related methods into simple pseudocodes using Google so that we can build up integrated systems. Second, we develop several new algorithms for social networking mining such as those to classify relations into categories, to make extraction scalable, and to obtain and utilize person-to-word relations. Third, every module is implemented in POLYPHONET, which has been used at four academic conferences, each with more than 500 participants. We overview that system. Finally, a novel architecture called Super Social Network Mining is proposed; it utilizes simple modules using Google and is characterized by scalability and Relate-Identify processes: Identification of each entity and extraction of relations are repeated to obtain a more precise social network.",2006.0,"Y. Matsuo, Junichiro Mori, Masahiro Hamasaki, Takuichi Nishimura, Hideaki Takeda, K. Hasida, M. Ishizuka"
20381972ee66e03ea218e5b3d39d6423b6e35f0f,https://www.semanticscholar.org/paper/20381972ee66e03ea218e5b3d39d6423b6e35f0f,Data Processing and Text Mining Technologies on Electronic Medical Records: A Review,"Currently, medical institutes generally use EMR to record patient's condition, including diagnostic information, procedures performed, and treatment results. EMR has been recognized as a valuable resource for large-scale analysis. However, EMR has the characteristics of diversity, incompleteness, redundancy, and privacy, which make it difficult to carry out data mining and analysis directly. Therefore, it is necessary to preprocess the source data in order to improve data quality and improve the data mining results. Different types of data require different processing technologies. Most structured data commonly needs classic preprocessing technologies, including data cleansing, data integration, data transformation, and data reduction. For semistructured or unstructured data, such as medical text, containing more health information, it requires more complex and challenging processing methods. The task of information extraction for medical texts mainly includes NER (named-entity recognition) and RE (relation extraction). This paper focuses on the process of EMR processing and emphatically analyzes the key techniques. In addition, we make an in-depth study on the applications developed based on text mining together with the open challenges and research issues for future work.",2018.0,"Wencheng Sun, Zhiping Cai, Yangyang Li, Fang Liu, S. Fang, Guoyan Wang"
5660bd8df219d3b520f91661e5efc34fb3c1810d,https://www.semanticscholar.org/paper/5660bd8df219d3b520f91661e5efc34fb3c1810d,"Formal Ontology in Information Systems : Proceedings of the First International Conference(FOIS'98), June 6-8, Trento, Italy","Research on ontology is becoming increasingly widespread in the computer science community. While this term has been rather confined to the philosophical sphere in the past, it is now gaining a specific role in areas such as Artificial Intelligence, Computational Linguistics, and Databases. Its importance has been recognized in fields as diverse as knowledge engineering, knowledge representation, qualitative modeling, language engineering, database design, information integration, object-oriented analysis, information retrieval and extraction, knowledge management and organization, agent-based systems design. Current applications areas are disparate, including enterprise integration, natural language translation, medicine, mechanical engineering, electronic commerce, geographic information systems, legal information systems, and biological information systems. Various workshops addressing the engineering aspects of ontology have been held in the recent years. However, ontology by 'its very nature' ought to be a unifying discipline. Insights in this field have potential impact on the whole area of information systems (taking this term in its broadest sense), as testified by the interest recently shown by international standards organizations. In order to provide a solid general foundation for this work, it is therefore important to focus on the common scientific principles and open problems arising from current tools, methodologies, and applications of ontology.",1998.0,Nicola Guarino
56f78d1363678ce9549f9e5d866c1f6e5c2e0692,https://www.semanticscholar.org/paper/56f78d1363678ce9549f9e5d866c1f6e5c2e0692,Extraction of Adverse Drug Effects from Clinical Records,"With the rapidly growing use of electronic health records, the possibility of large-scale clinical information extraction has drawn much attention. We aim to extract adverse drug events and effects from records. As the first step of this challenge, this study assessed (1) how much adverse-effect information is contained in records, and (2) automatic extracting accuracy of the current standard Natural Language Processing (NLP) system. Results revealed that 7.7% of records include adverse event information, and that 59% of them (4.5% in total) can be extracted automatically. This result is particularly encouraging, considering the massive amounts of records, which are increasing daily.",2010.0,"E. Aramaki, Yasuhide Miura, M. Tonoike, Tomoko Ohkuma, Hiroshi Masuichi, K. Waki, K. Ohe"
ba86f939cc588f82478a4719942ced254ec33ebe,https://www.semanticscholar.org/paper/ba86f939cc588f82478a4719942ced254ec33ebe,Spatial Preprocessing for Endmember Extraction,"Endmember extraction is the process of selecting a collection of pure signature spectra of the materials present in a remotely sensed hyperspectral scene. These pure signatures are then used to decompose the scene into abundance fractions by means of a spectral unmixing algorithm. Most techniques available in the endmember extraction literature rely on exploiting the spectral properties of the data alone. As a result, the search for endmembers in a scene is conducted by treating the data as a collection of spectral measurements with no spatial arrangement. In this paper, we propose a novel strategy to incorporate spatial information into the traditional spectral-based endmember search process. Specifically, we propose to estimate, for each pixel vector, a scalar spatially derived factor that relates to the spectral similarity of pixels lying within a certain spatial neighborhood. This scalar value is then used to weigh the importance of the spectral information associated to each pixel in terms of its spatial context. Two key aspects of the proposed methodology are given as follows: 1) No modification of existing image spectral-based endmember extraction methods is necessary in order to apply the proposed approach. 2) The proposed preprocessing method enhances the search for image spectral endmembers in spatially homogeneous areas. Our experimental results, which were obtained using both synthetic and real hyperspectral data sets, indicate that the spectral endmembers obtained after spatial preprocessing can be used to accurately model the original hyperspectral scene using a linear mixture model. The proposed approach is suitable for jointly combining spectral and spatial information when searching for image-derived endmembers in highly representative hyperspectral image data sets.",2009.0,"M. Zortea, A. Plaza"
f1ae781dfe425c8026a20dbb474770b696cfb172,https://www.semanticscholar.org/paper/f1ae781dfe425c8026a20dbb474770b696cfb172,Complex event extraction at PubMed scale,"Motivation: There has recently been a notable shift in biomedical information extraction (IE) from relation models toward the more expressive event model, facilitated by the maturation of basic tools for biomedical text analysis and the availability of manually annotated resources. The event model allows detailed representation of complex natural language statements and can support a number of advanced text mining applications ranging from semantic search to pathway extraction. A recent collaborative evaluation demonstrated the potential of event extraction systems, yet there have so far been no studies of the generalization ability of the systems nor the feasibility of large-scale extraction. Results: This study considers event-based IE at PubMed scale. We introduce a system combining publicly available, state-of-the-art methods for domain parsing, named entity recognition and event extraction, and test the system on a representative 1% sample of all PubMed citations. We present the first evaluation of the generalization performance of event extraction systems to this scale and show that despite its computational complexity, event extraction from the entire PubMed is feasible. We further illustrate the value of the extraction approach through a number of analyses of the extracted information. Availability: The event detection system and extracted data are open source licensed and available at http://bionlp.utu.fi/. Contact: jari.bjorne@utu.fi",2010.0,"Jari Björne, Filip Ginter, S. Pyysalo, Junichi Tsujii, T. Salakoski"
0c7b7c1d701b6a702f435cc3451cbbc15732972c,https://www.semanticscholar.org/paper/0c7b7c1d701b6a702f435cc3451cbbc15732972c,Jointly Extracting Event Triggers and Arguments by Dependency-Bridge RNN and Tensor-Based Argument Interaction,"
 
 Event extraction plays an important role in natural language processing (NLP) applications including question answering and information retrieval. Traditional event extraction relies heavily on lexical and syntactic features, which require intensive human engineering and may not generalize to different datasets. Deep neural networks, on the other hand, are able to automatically learn underlying features, but existing networks do not make full use of syntactic relations. In this paper, we propose a novel dependency bridge recurrent neural network (dbRNN) for event extraction. We build our model upon a recurrent neural network, but enhance it with dependency bridges, which carry syntactically related information when modeling each word.We illustrates that simultaneously applying tree structure and sequence structure in RNN brings much better performance than only uses sequential RNN. In addition, we use a tensor layer to simultaneously capture the various types of latent interaction between candidate arguments as well as identify/classify all arguments of an event. Experiments show that our approach achieves competitive results compared with previous work.
 
",2018.0,"Lei Sha, Feng Qian, Baobao Chang, Zhifang Sui"
1bd8bbd66524ccf605c879982cd35ef3a3d52160,https://www.semanticscholar.org/paper/1bd8bbd66524ccf605c879982cd35ef3a3d52160,Exploratory Factor Analysis ; Concepts and Theory,"Exploratory factor analysis is a complex and multivariate statistical technique commonly employed in information system, social science, education and psychology. This paper intends to provide a simplified collection of information for researchers and practitioners undertaking exploratory factor analysis (EFA) and to make decisions about best practice in EFA. Particularly, the objective of the paper is to provide practical and theoretical information on decision making of sample size, extraction, number of factors to retain and rotational methods.",2014.0,"Hamed Taherdoost, S. Sahibuddin, N. Jalaliyoon"
2882cbe709b231bf56a18aa16150f6a4d2b6be35,https://www.semanticscholar.org/paper/2882cbe709b231bf56a18aa16150f6a4d2b6be35,Inferring Personal Information from Demand-Response Systems,"Current and upcoming demand-response systems provide increasingly detailed power-consumption data to utilities and a growing array of players angling to assist consumers in understanding and managing their energy use. The granularity of this data, as well as new players' entry into the energy market, creates new privacy concerns. The detailed per-household consumption data that advanced metering systems generate reveals information about in-home activities that such players can mine and combine with other readily available information to discover more about occupants' activities. The authors explore the technological aspects of this claim, focusing on the ways in which personally identifying information can be collected and repurposed. Their results show that, even with relatively unsophisticated hardware and data-extraction algorithms, some information about occupant behavior can be estimated with a high degree of accuracy. The authors propose a disclosure metric to aid in quantifying the impact of data collection on in-home privacy and construct an example metric for their experiment.",2010.0,"M. A. Lisovich, D. K. Mulligan, S. B. Wicker"
b7b6d83dab32f5495e0781111082a5e693519573,https://www.semanticscholar.org/paper/b7b6d83dab32f5495e0781111082a5e693519573,Unsupervised Approaches for Automatic Keyword Extraction Using Meeting Transcripts,"This paper explores several unsupervised approaches to automatic keyword extraction using meeting transcripts. In the TFIDF (term frequency, inverse document frequency) weighting framework, we incorporated part-of-speech (POS) information, word clustering, and sentence salience score. We also evaluated a graph-based approach that measures the importance of a word based on its connection with other sentences or words. The system performance is evaluated in different ways, including comparison to human annotated keywords using F-measure and a weighted score relative to the oracle system performance, as well as a novel alternative human evaluation. Our results have shown that the simple unsupervised TFIDF approach performs reasonably well, and the additional information from POS and sentence score helps keyword extraction. However, the graph method is less effective for this domain. Experiments were also performed using speech recognition output and we observed degradation and different patterns compared to human transcripts.",2009.0,"F. Liu, Deana Pennell, Fei Liu, Yang Liu"
97eb205c9782bdc44b283b6bdaf9b9c7317b9a97,https://www.semanticscholar.org/paper/97eb205c9782bdc44b283b6bdaf9b9c7317b9a97,Domain-Specific Keyphrase Extraction,"Keyphrases are an important means of document summarization, clustering, and topic search. Only a small minority of documents have author-assigned keyphrases, and manually assigning keyphrases to existing documents is very laborious. Therefore it is highly desirable to automate the keyphrase extraction process. This paper shows that a simple procedure for keyphrase extraction based on the naive Bayes learning scheme performs comparably to the state of the art. It goes on to explain how this procedure's performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand. Results on a large collection of technical reports in computer science show that the quality of the extracted keyphrases improves significantly when domain-specific information is exploited.",1999.0,"E. Frank, G. Paynter, I. Witten, C. Gutwin, C. Nevill-Manning"
5a1d9cbbbd7fb63a15c21461dcdcca8a006229d6,https://www.semanticscholar.org/paper/5a1d9cbbbd7fb63a15c21461dcdcca8a006229d6,"Natural language processing for online applications : text retrieval, extraction and categorization","This text covers the emerging technologies of document retrieval, information extraction, and text categorization in a way which highlights commonalities in terms of both general principles and practical issues. It seeks to satisfy a need on the part of technology practitioners in the Internet space, faced with having to make difficult decisions as to what research has been done and what the best practices are. It is not intended as a vendor guide or as a recipe for building applications. But it does identify the key technologies, the issues involved, and the strengths and weaknesses of the various approaches. There is also a strong emphasis on evaluation in every chapter, both in terms of methodology (how to evaluate) and what controlled experimentation and industrial experience have to tell us.",2002.0,"Peter Jackson, Isabelle Moulinier"
4efa5ee94e64ff63b8ecd8d6809742f813844cd8,https://www.semanticscholar.org/paper/4efa5ee94e64ff63b8ecd8d6809742f813844cd8,Overview of BioNLP Shared Task 2013,"The BioNLP Shared Task 2013 is the third edition of the BioNLP Shared Task series that is a community-wide effort to address fine-grained, structural information extraction from biomedical literature. The BioNLP Shared Task 2013 was held from January to April 2013. Six main tasks were proposed. 38 final submissions were received, from 22 teams. The results show advances in the state of the art and demonstrate that extraction methods can be successfully generalized in various aspects.",2013.0,"Jin-Dong Kim, S. Pyysalo, Tomoko Ohta, Robert Bossy, N. Nguyen, Junichi Tsujii"
d6bfe51abb006ab56ee039b4b54a643fd53ed7fa,https://www.semanticscholar.org/paper/d6bfe51abb006ab56ee039b4b54a643fd53ed7fa,Discriminant analysis for recognition of human face images,"In this paper the discriminatory power of various human facial features is studied and a new scheme for Automatic Face Recognition (AFR) is proposed. Using Linear Discriminant Analysis (LDA) of different aspects of human faces in spatial domain, we first evaluate the significance of visual information in different parts/features of the face for identifying the human subject. The LDA of faces also provides us with a small set of features that carry the most relevant information for classification purposes. The features are obtained through eigenvector analysis of scatter matrices with the objective of maximizing between-class and minimizing within-class variations. The result is an efficient projection-based feature extraction and classification scheme for AFR. Soft decisions made based on each of the projections are combined, using probabilistic or evidential approaches to multisource data analysis. For medium-sized databases of human faces, good classification accuracy is achieved using very low-dimensional feature vectors.",1997.0,"K. Etemad, Ramalingam Chellappa"
5581992944c66522dd1b11f8a6150aeef2d95b7a,https://www.semanticscholar.org/paper/5581992944c66522dd1b11f8a6150aeef2d95b7a,Learning Subjective Adjectives from Corpora,"Subjectivity tagging is distinguishing sentences used to present opinions and evaluations from sentences used to objectively present factual information. There are numerous applications for which subjectivity tagging is relevant, including information extraction and information retrieval. This paper identifies strong clues of subjectivity using the results of a method for clustering words according to distributional similarity (Lin 1998), seeded by a small amount of detailed manual annotation. These features are then further refined with the addition of lexical semantic features of adjectives, specifically polarity and gradability (Hatzivassiloglou & McKeown 1997), which can be automatically learned from corpora. In 10-fold cross validation experiments, features based on both similarity clusters and the lexical semantic features are shown to have higher precision than features based on each alone.",2000.0,Janyce Wiebe
0ad95f065f57fe26c0b8a9e6ee41c1a4c932cff1,https://www.semanticscholar.org/paper/0ad95f065f57fe26c0b8a9e6ee41c1a4c932cff1,The Operational Meaning of Min- and Max-Entropy,"In this paper, we show that the conditional min-entropy <i>H</i> <sub>min</sub>(<i>A</i> |<i>B</i>) of a bipartite state <i>rhoAB</i> is directly related to the maximum achievable overlap with a maximally entangled state if only local actions on the <i>B</i>-part of <i>rhoAB</i> are allowed. In the special case where <i>A</i> is classical, this overlap corresponds to the probability of guessing <i>A</i> given <i>B</i>. In a similar vein, we connect the conditional max-entropy <i>H</i> <sub>max</sub>(<i>A</i> |<i>B</i>) to the maximum fidelity of <i>rhoAB</i> with a product state that is completely mixed on <i>A</i>. In the case where <i>A</i> is classical, this corresponds to the security of <i>A</i> when used as a secret key in the presence of an adversary holding <i>B</i>. Because min- and max-entropies are known to characterize information-processing tasks such as randomness extraction and state merging, our results establish a direct connection between these tasks and basic operational problems. For example, they imply that the (logarithm of the) probability of guessing <i>A</i> given <i>B</i> is a lower bound on the number of uniform secret bits that can be extracted from <i>A</i> relative to an adversary holding <i>B</i>.",2008.0,"R. König, R. Renner, Christian Schaffner"
bc0d68391c13ee42faa5f07a2816c2d97fbf0435,https://www.semanticscholar.org/paper/bc0d68391c13ee42faa5f07a2816c2d97fbf0435,A Review of Relation Extraction,"Many applications in information extraction, natural language understanding, information retrieval require an understanding of the semantic relations between entities. We present a comprehensive review of various aspects of the entity relation extraction task. Some of the most important supervised and semi-supervised classiﬁcation approaches to the relation extraction task are covered in sufﬁcient detail along with critical analyses. We also discuss extensions to higher-order relations. Evaluation methodologies for both supervised and semi-supervised meth-ods are described along with pointers to the commonly used performance evaluation datasets. Finally, we also give short descriptions of two important applications of relation extraction, namely question answering and biotext mining.",2007.0,"Nguyen Bach, Sameer Badaskar"
117da44f01ef45ef8223bec8f9c2346b131321f4,https://www.semanticscholar.org/paper/117da44f01ef45ef8223bec8f9c2346b131321f4,Knowledge-Based Trust: Estimating the Trustworthiness of Web Sources,"
 The quality of web sources has been traditionally evaluated using
 exogenous
 signals such as the hyperlink structure of the graph. We propose a new approach that relies on
 endogenous
 signals, namely, the correctness of factual information provided by the source. A source that has few false facts is considered to be trustworthy.
 
 The facts are automatically extracted from each source by information extraction methods commonly used to construct knowledge bases. We propose a way to distinguish errors made in the extraction process from factual errors in the web source per se, by using joint inference in a novel multi-layer probabilistic model.
 
 We call the trustworthiness score we computed
 Knowledge-Based Trust (KBT)
 . On synthetic data, we show that our method can reliably compute the true trustworthiness levels of the sources. We then apply it to a database of 2.8B facts extracted from the web, and thereby estimate the trustworthiness of 119M webpages. Manual evaluation of a subset of the results confirms the effectiveness of the method.
",2015.0,"X. Dong, E. Gabrilovich, K. Murphy, Van Dang, Wilko Horn, Camillo Lugaresi, Shaohua Sun, Wei Zhang"
cc5891d024fb51ef9922c935248b0965c819bc46,https://www.semanticscholar.org/paper/cc5891d024fb51ef9922c935248b0965c819bc46,Collaborative information seeking and retrieval,"L'A. examine la recherche concernant les systemes et les pratiques permettant a des individus de collaborer dans leurs activites de recherche d'information : collecte, partage, extraction d'informations et navigation au sein d'espace informationnels, mais aussi requete et filtrage collaboratifs et fouille de donnees. Il distingue la recherche d'information collaborative dans quatre environnements : la communaute scientifique, l'industrie, le monde medical et le cadre militaire. La recherche sur les approches sociales et collaboratives des tâches d'information est multidisciplinaire, issue d'etudes sur les sciences de l'information, la recherche d'information, l'interaction homme-machine et le travail collaboratif assiste par ordinateur.",2006.0,Jonathan Foster
d8cc06f9035fd11cf30ecd1b36266cc7b31c4df1,https://www.semanticscholar.org/paper/d8cc06f9035fd11cf30ecd1b36266cc7b31c4df1,The Mathematics of Learning: Dealing with Data,Abstract Learning is key to developing systems tailored to a broad range of data analysis and information extraction tasks. We outline the mathematical foundations of learning theory and describe a key algorithm of it.,2005.0,"T. Poggio, S. Smale"
7bf34d65eec494a0b758f4ab3f58db8a89815e1f,https://www.semanticscholar.org/paper/7bf34d65eec494a0b758f4ab3f58db8a89815e1f,Leveraging Knowledge Bases in LSTMs for Improving Machine Reading,"This paper focuses on how to take advantage of external knowledge bases (KBs) to improve recurrent neural networks for machine reading. Traditional methods that exploit knowledge from KBs encode knowledge as discrete indicator features. Not only do these features generalize poorly, but they require task-specific feature engineering to achieve good performance. We propose KBLSTM, a novel neural model that leverages continuous representations of KBs to enhance the learning of recurrent neural networks for machine reading. To effectively integrate background knowledge with information from the currently processed text, our model employs an attention mechanism with a sentinel to adaptively decide whether to attend to background knowledge and which information from KBs is useful. Experimental results show that our model achieves accuracies that surpass the previous state-of-the-art results for both entity extraction and event extraction on the widely used ACE2005 dataset.",2017.0,"Bishan Yang, Tom Michael Mitchell"
2721da85295e55ba3d191c9f395b43d7498ba602,https://www.semanticscholar.org/paper/2721da85295e55ba3d191c9f395b43d7498ba602,Automatic extraction of protein interactions from scientific abstracts.,"This paper motivates the use of Information Extraction (IE) for gathering data on protein interactions, describes the customization of an existing IE system, SRI's Highlight, for this task and presents the results of an experiment on unseen Medline abstracts which show that customization to a new domain can be fast, reliable and cost-effective.",1999.0,"James Thomas, D. Milward, C. Ouzounis, S. Pulman, Mark Carroll"
d7688956dd79708b18b497117da2f0398c106493,https://www.semanticscholar.org/paper/d7688956dd79708b18b497117da2f0398c106493,Force Field Feature Extraction for Ear Biometrics,"The overall objective in defining feature space is to reduce the dimensionality of the original pattern space, whilst maintaining discriminatory power for classification. To meet this objective in the context of ear biometrics a new force field transformation treats the image as an array of mutually attracting particles that act as the source of a Gaussian force field. Underlying the force field there is a scalar potential energy field, which in the case of an ear takes the form of a smooth surface that resembles a small mountain with a number of peaks joined by ridges. The peaks correspond to potential energy wells and to extend the analogy the ridges correspond to potential energy channels. Since the transform also turns out to be invertible, and since the surface is otherwise smooth, information theory suggests that much of the information is transferred to these features, thus confirming their efficacy. We previously described how field line feature extraction, using an algorithm similar to gradient descent, exploits the directional properties of the force field to automatically locate these channels and wells, which then form the basis of characteristic ear features. We now show how an analysis of the mechanism of this algorithmic approach leads to a closed analytical description based on the divergence of force direction, which reveals that channels and wells are really manifestations of the same phenomenon. We further show that this new operator, with its own distinct advantages, has a striking similarity to the Marr-Hildreth operator, but with the important difference that it is non-linear. As well as addressing faster implementation, invertibility, and brightness sensitivity, the technique is also validated by performing recognition on a database of ears selected from the XM2VTS face database, and by comparing the results with the more established technique of Principal Components Analysis. This confirms not only that ears do indeed appear to have potential as a biometric, but also that the new approach is well suited to their description, being robust especially in the presence of noise, and having the advantages that the ear does not need to be explicitly extracted from the background.",2005.0,David J. Hurley
b0485fba23aabda526358f31cb5a382b66a08270,https://www.semanticscholar.org/paper/b0485fba23aabda526358f31cb5a382b66a08270,Text Classification Using Machine Learning Techniques,"Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question- answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.",2005.0,"M. Ikonomakis, S. Kotsiantis, Vassilis Tampakas"
534f3b725dc609c0f9b3794a96c1f21b4ee88265,https://www.semanticscholar.org/paper/534f3b725dc609c0f9b3794a96c1f21b4ee88265,Web-scale extraction of structured data,"A long-standing goal of Web research has been to construct a unified Web knowledge base. Information extraction techniques have shown good results on Web inputs, but even most domain-independent ones are not appropriate for Web-scale operation. In this paper we describe three recent extraction systems that can be operated on the entire Web (two of which come from Google Research). The TextRunner system focuses on raw natural language text, the WebTables system focuses on HTML-embedded tables, and the deep-web surfacing system focuses on ""hidden"" databases. The domain, expressiveness, and accuracy of extracted data can depend strongly on its source extractor; we describe differences in the characteristics of data produced by the three extractors. Finally, we discuss a series of unique data applications (some of which have already been prototyped) that are enabled by aggregating extractedWeb information.",2009.0,"Michael J. Cafarella, Jayant Madhavan, A. Halevy"
6965c8e2dcd6ecf2a5f3f9320de5fced37391e42,https://www.semanticscholar.org/paper/6965c8e2dcd6ecf2a5f3f9320de5fced37391e42,Classification and modeling with linguistic information granules - advanced approaches to linguistic data mining,Linguistic Information Granules.- Pattern Classification with Linguistic Rules.- Learning of Linguistic Rules.- Input Selection and Rule Selection.- Genetics-Based Machine Learning.- Multi-Objective Design of Linguistic Models.- Comparison of Linguistic Discretization with Interval Discretization.- Modeling with Linguistic Rules.- Design of Compact Linguistic Models.- Linguistic Rules with Consequent Real Numbers.- Handling of Linguistic Rules in Neural Networks.- Learning of Neural Networks from Linguistic Rules.- Linguistic Rule Extraction from Neural Networks.- Modeling of Fuzzy Input-Output Relations.,2004.0,"H. Ishibuchi, T. Nakashima, M. Nii"
6f8f3c79d38ffb64f54adfdcfe2b43dd5b59692a,https://www.semanticscholar.org/paper/6f8f3c79d38ffb64f54adfdcfe2b43dd5b59692a,Extracting Relations with Integrated Information Using Kernel Methods,"Entity relation detection is a form of information extraction that finds predefined relations between pairs of entities in text. This paper describes a relation detection approach that combines clues from different levels of syntactic processing using kernel methods. Information from three different levels of processing is considered: tokenization, sentence parsing and deep dependency analysis. Each source of information is represented by kernel functions. Then composite kernels are developed to integrate and extend individual kernels so that processing errors occurring at one level can be overcome by information from other levels. We present an evaluation of these methods on the 2004 ACE relation detection task, using Support Vector Machines, and show that each level of syntactic processing contributes useful information for this task. When evaluated on the official test data, our approach produced very competitive ACE value scores. We also compare the SVM with KNN on different kernels.",2005.0,"Shubin Zhao, R. Grishman"
5f9b342307b1149615a49486ff7d2abf783314ff,https://www.semanticscholar.org/paper/5f9b342307b1149615a49486ff7d2abf783314ff,The World-Wide Web: quagmire or gold mine?,"Skeptics believe the Web is too unstructured for Web mining to succeed. Indeed, data mining has been applied traditionally to databases, yet much of the information on the Web lies buried in documents designed for human consumption such as home pages or product catalogs. Furthermore, much of the information on the Web is presented in natural-language text with no machine-readable semantics; HTML annotations structure the display of Web pages, but provide little insight into their content. Some have advocated transforming the Web into a massive layered database to facilitate data mining [12], but the Web is too dynamic and chaotic to be tamed in this manner. Others have attempted to hand code site-specific “wrappers” that facilitate the extraction of information from individual Web resources (e.g., [8]). Hand coding is convenient but cannot keep up with the explosive growth of the Web. As an alternative, this article argues for the structured Web hypothesis: Information on the Web is sufficiently structured to facilitate effective Web mining. Examples of Web structure include linguistic and typographic conventions, HTML annotations (e.g., <title>), classes of semi-structured documents (e.g., product catalogs), Web indices and directories, and much more. To support the structured Web hypothesis, this article will survey preliminary Web mining successes and suggest directions for future work. Web mining may be organized into the following subtasks:",1996.0,Oren Etzioni
e429e4874f4b3a397a754f28a919362d282b7966,https://www.semanticscholar.org/paper/e429e4874f4b3a397a754f28a919362d282b7966,Road Extraction Using SVM and Image Segmentation,"Accurate road information is vital for transportation applications, including as part of geographical information systems (GIS). This article reports on the development of a two-step approach for road extraction that utilizes pixel spectral information for classification and image segmentation-derived object features. In the first step, support vector machine (SVM) was employed merely to classify the image into two groups of categories: a road group and a non-road group. For this classification, support vector machine (SVM) achieved higher accuracy than Gaussian maximum likelihood (GML). In the second step, the road group image was segmented into geometrically homogeneous objects using a region growing technique based on a similarity criterion, with higher weighting on shape factors over spectral criteria. A simple thresholding on the shape index and density features derived from these objects was performed to extract road features, which were further processed by thinning and vectorization to obtain road centerlines. The authors conclude that the proposed approach worked well with images comprised by both rural and urban area features.",2004.0,"Mingjun Song, D. Civco"
2559417f8a3d6ab922cfa824b43f9f0c642a1dae,https://www.semanticscholar.org/paper/2559417f8a3d6ab922cfa824b43f9f0c642a1dae,Exploiting dictionaries in named entity extraction: combining semi-Markov extraction processes and data integration methods,"We consider the problem of improving named entity recognition (NER) systems by using external dictionaries---more specifically, the problem of extending state-of-the-art NER systems by incorporating information about the similarity of extracted entities to entities in an external dictionary. This is difficult because most high-performance named entity recognition systems operate by sequentially classifying words as to whether or not they participate in an entity name; however, the most useful similarity measures score entire candidate names. To correct this mismatch we formalize a semi-Markov extraction process, which is based on sequentially classifying segments of several adjacent words, rather than single words. In addition to allowing a natural way of coupling high-performance NER methods and high-performance similarity functions, this formalism also allows the direct use of other useful entity-level features, and provides a more natural formulation of the NER problem than sequential word classification. Experiments in multiple domains show that the new model can substantially improve extraction performance over previous methods for using external dictionaries in NER.",2004.0,"William W. Cohen, Sunita Sarawagi"
4764b94f2326ba2ea44a9677365aa381657a6388,https://www.semanticscholar.org/paper/4764b94f2326ba2ea44a9677365aa381657a6388,A fully automated object extraction system for the World Wide Web,"This paper presents a fully automated object extraction system Omini. A distinct feature of Omini is the suite of algorithms and the automatically learned information extraction rules for discovering and extracting objects from dynamic Web pages or static Web pages that contain multiple object instances. We evaluated the system using more than 2,000 Web pages over 40 sites. It achieves 100% precision (returns only correct objects) and excellent recall (between 99% and 98%, with very few significant objects left out). The object boundary identification algorithms are fast, about 0.1 second per page with a simple optimization.",2001.0,"David J. Buttler, Ling Liu, C. Pu"
5f4eb3e0ee8e0e6e842d1b855bb6ef22dbc098e0,https://www.semanticscholar.org/paper/5f4eb3e0ee8e0e6e842d1b855bb6ef22dbc098e0,NLP Techniques for Term Extraction and Ontology Population,"This chapter investigates NLP techniques for ontology population, using a combination of rule-based approaches and machine learning. We describe a method for term recognition using linguistic and statistical techniques, making use of contextual information to bootstrap learning. We then investigate how term recognition techniques can be useful for the wider task of information extraction, making use of similarity metrics and contextual information. We describe two tools we have developed which make use of contextual information to help the development of rules for named entity recognition. Finally, we evaluate our ontology-based information extraction results using a novel technique we have developed which makes use of similarity-based metrics first developed for term recognition.",2008.0,"D. Maynard, Yaoyong Li, Wim Peters"
863c472f3639337bd7a11b0b76e7937198d480a8,https://www.semanticscholar.org/paper/863c472f3639337bd7a11b0b76e7937198d480a8,Automatic text summarization based on sentences clustering and extraction,"Technology of automatic text summarization plays an important role in information retrieval and text classification, and may provide a solution to the information overload problem. Text summarization is a process of reducing the size of a text while preserving its information content. This paper proposes a sentences clustering based summarization approach. The proposed approach consists of three steps: first clusters the sentences based on the semantic distance among sentences in the document, and then on each cluster calculates the accumulative sentence similarity based on the multi-features combination method, at last chooses the topic sentences by some extraction rules. The purpose of present paper is to show that summarization result is not only depends the sentence features, but also depends on the sentence similarity measure. The experimental result on the DUC 2003 dataset show that our proposed approach can improve the performance compared to other summarization methods.",2009.0,"P. Zhang, Cun-he Li"
77d2698e8efadda698b0edb457cd8de75224bfa0,https://www.semanticscholar.org/paper/77d2698e8efadda698b0edb457cd8de75224bfa0,Knowledge Base Population: Successful Approaches and Challenges,"In this paper we give an overview of the Knowledge Base Population (KBP) track at the 2010 Text Analysis Conference. The main goal of KBP is to promote research in discovering facts about entities and augmenting a knowledge base (KB) with these facts. This is done through two tasks, Entity Linking -- linking names in context to entities in the KB -- and Slot Filling -- adding information about an entity to the KB. A large source collection of newswire and web documents is provided from which systems are to discover information. Attributes (""slots"") derived from Wikipedia infoboxes are used to create the reference KB. In this paper we provide an overview of the techniques which can serve as a basis for a good KBP system, lay out the remaining challenges by comparison with traditional Information Extraction (IE) and Question Answering (QA) tasks, and provide some suggestions to address these challenges.",2011.0,"Heng Ji, R. Grishman"
de527db5a41b3e0c5dc3e646a5ad5e81754e0ec3,https://www.semanticscholar.org/paper/de527db5a41b3e0c5dc3e646a5ad5e81754e0ec3,Automatic extraction of object-oriented component interfaces,"Component-based software design is a popular and effective approach to designing large systems. While components typically have well-defined interfaces, sequencing information---which calls must come in which order---is often not formally specified.This paper proposes using multiple finite statemachine (FSM) submodels to model the interface of a class. A submodel includes a subset of methods that, for example, implement a Java interface, or access some particular field. Each state-modifying method is represented as a state in the FSM, and transitions of the FSMs represent allow able pairs of consecutive methods. In addition, state-preserving methods are constrained to execute only under certain states.We have designed and implemented a system that includes static analyses to deduce illegal call sequences in a program, dynamic instrumentation techniques to extract models from execution runs, and a dynamic model checker that ensures that the code conforms to the model. Extracted models can serve as documentation; they can serve as constraints to be enforced by a static checker; they can be studied directly by developers to determine if the program is exhibiting unexpected behavior; or they can be used to determine the completeness of a test suite.Our system has been run on several large code bases, including the joeq virtual machine, the basic Java libraries, and the Java 2 Enterprise Edition library code. Our experience suggests that this approach yields useful information.",2002.0,"John Whaley, Michael C. Martin, Monica S. Lam"
d72852fb6d915a8c4538a83e89e19e709272dd3b,https://www.semanticscholar.org/paper/d72852fb6d915a8c4538a83e89e19e709272dd3b,Exploiting discriminant information in nonnegative matrix factorization with application to frontal face verification,"In this paper, two supervised methods for enhancing the classification accuracy of the Nonnegative Matrix Factorization (NMF) algorithm are presented. The idea is to extend the NMF algorithm in order to extract features that enforce not only the spatial locality, but also the separability between classes in a discriminant manner. The first method employs discriminant analysis in the features derived from NMF. In this way, a two-phase discriminant feature extraction procedure is implemented, namely NMF plus Linear Discriminant Analysis (LDA). The second method incorporates the discriminant constraints inside the NMF decomposition. Thus, a decomposition of a face to its discriminant parts is obtained and new update rules for both the weights and the basis images are derived. The introduced methods have been applied to the problem of frontal face verification using the well-known XM2VTS database. Both methods greatly enhance the performance of NMF for frontal face verification",2006.0,"S. Zafeiriou, A. Tefas, I. Buciu, I. Pitas"
d0567609da19ae90f1742800f1ff873b9f1bd411,https://www.semanticscholar.org/paper/d0567609da19ae90f1742800f1ff873b9f1bd411,Extraction of Text Objects in Video Documents: Recent Progress,"Text extraction in video documents, as an important research field of content-based information indexing and retrieval, has been developing rapidly since 1990s. This has led to much progress in text extraction, performance evaluation, and related applications. By reviewing the approaches proposed during the past five years, this paper introduces the progress made in this area and discusses promising directions for future research.",2008.0,"Jing Zhang, R. Kasturi"
44a418cbd4116d050c6164ccc36933935057f0a7,https://www.semanticscholar.org/paper/44a418cbd4116d050c6164ccc36933935057f0a7,ChemDataExtractor 2.0: Autopopulated Ontologies for Materials Science,"The ever-growing abundance of data found in heterogeneous sources, such as scientific publications, has forced the development of automated techniques for data extraction. While in the past, in the physical sciences domain, the focus has been on the precise extraction of individual properties, attention has recently been devoted to the extraction of higher-level relationships. Here, we present a framework for an automated population of ontologies. That is, the direct extraction of a larger group of properties linked by a semantic network. We exploit data-rich sources, such as tables within documents, and present a new model concept that enables data extraction for chemical and physical properties with the ability to organize hierarchical data as nested information. Combining these capabilities with automatically generated parsers for data extraction and forward-looking interdependency resolution, we illustrate the power of our approach via the automatic extraction of a crystallographic hierarchy of information. This includes 18 interrelated submodels of nested data, extracted from an evaluation set of scientific articles, yielding an overall precision of 92.2%, across 26 different journals. Our method and associated toolkit, ChemDataExtractor 2.0, offers a key step toward the seamless integration of primary literature sources into a data-driven scientific framework.",2021.0,"Juraj Mavračić, Callum J Court, Taketomo Isazawa, S. Elliott, J. Cole"
8111a8b7831332e45cceda8572ed19603c2fb02a,https://www.semanticscholar.org/paper/8111a8b7831332e45cceda8572ed19603c2fb02a,The CoNLL-2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text,"The CoNLL-2010 Shared Task was dedicated to the detection of uncertainty cues and their linguistic scope in natural language texts. The motivation behind this task was that distinguishing factual and uncertain information in texts is of essential importance in information extraction. This paper provides a general overview of the shared task, including the annotation protocols of the training and evaluation datasets, the exact task definitions, the evaluation metrics employed and the overall results. The paper concludes with an analysis of the prominent approaches and an overview of the systems submitted to the shared task.",2010.0,"Richárd Farkas, V. Vincze, György Móra, J. Csirik, György Szarvas"
b078a4454d2fef90e76248117d6085bd9cae882f,https://www.semanticscholar.org/paper/b078a4454d2fef90e76248117d6085bd9cae882f,Japanese Named Entity Extraction with Redundant Morphological Analysis,"Named Entity (NE) extraction is an important subtask of document processing such as information extraction and question answering. A typical method used for NE extraction of Japanese texts is a cascade of morphological analysis, POS tagging and chunking. However, there are some cases where segmentation granularity contradicts the results of morphological analysis and the building units of NEs, so that extraction of some NEs are inherently impossible in this setting. To cope with the unit problem, we propose a character-based chunking method. Firstly, the input sentence is analyzed redundantly by a statistical morphological analyzer to produce multiple (n-best) answers. Then, each character is annotated with its character types and its possible POS tags of the top n-best answers. Finally, a support vector machine-based chunker picks up some portions of the input sentence as NEs. This method introduces richer information to the chunker than previous methods that base on a single morphological analysis result. We apply our method to IREX NE extraction task. The cross validation result of the F-measure being 87.2 shows the superiority and effectiveness of the method.",2003.0,"Masayuki Asahara, Yuji Matsumoto"
a92eb50c300c77431d69be6a6644280bb4e2e63d,https://www.semanticscholar.org/paper/a92eb50c300c77431d69be6a6644280bb4e2e63d,Tweedr: Mining twitter to inform disaster response,"In this paper, we introduce Tweedr, a Twitter-mining tool that extracts actionable information for disaster relief workers during natural disasters. The Tweedr pipeline consists of three main parts: classification, clustering and extraction. In the classification phase, we use a variety of classification methods (sLDA, SVM, and logistic regression) to identify tweets reporting damage or casualties. In the clustering phase, we use filters to merge tweets that are similar to one another; and finally, in the extraction phase, we extract tokens and phrases that report specific information about different classes of infrastructure damage, damage types, and casualties. We empirically validate our approach with tweets collected from 12 different crises in the United States since 2006.",2014.0,"Zahra Ashktorab, Christopher Brown, M. Nandi, A. Culotta"
c51814c42bdea32aec74db7960489c8b31f00e94,https://www.semanticscholar.org/paper/c51814c42bdea32aec74db7960489c8b31f00e94,What You Seek Is What You Get: Extraction of Class Attributes from Query Logs,"Within the larger area of automatic acquisition of knowledge from the Web, we introduce a method for extracting relevant attributes, or quantifiable properties, for various classes of objects. The method extracts attributes such as capital city and President for the class Country, or cost, manufacturer and side effects for the class Drug, without relying on any expensive language resources or complex processing tools. In a departure from previous approaches to large-scale information extraction, we explore the role of Web query logs, rather than Web documents, as an alternative source of class attributes. The quality of the extracted attributes recommends query logs as a valuable, albeit little explored, resource for information extraction.",2007.0,"Marius Pasca, Benjamin Van Durme"
e569c4b8233d3109c31ebb89d67c1f95e808860c,https://www.semanticscholar.org/paper/e569c4b8233d3109c31ebb89d67c1f95e808860c,Remote Sensing for Sustainable Forest Management,"INTRODUCTION Forest Management Questions Remote Sensing Data and Methods Categories of Applications of Remote Sensing Organization of the Book SUSTAINABLE FOREST MANAGEMENT Definition of Sustainable Forest Management Ecosystem Management Criteria and Indicators of Sustainable Forest Management Information Needs of Forest Managers Role of Remote Sensing ACQUISITION OF IMAGERY Field, Aerial, and Satellite Imagery Data Characteristics Resolution and Scale Aerial Platforms and Sensors Satellite Platforms and Sensors General Limits of Airborne and Satellite Remote Sensing Data IMAGE CALIBRATION AND PROCESSING Georadiometric Effects and Spectral Response Image Processing Systems and Functionality Image Analysis Support Functions Image Information Extraction Image Understanding FOREST MODELING AND GIS Geographical Information Science Ecosystem Process Models Spatial Pattern Modeling FOREST CLASSIFICATION Information on Forest Classes Classification Systems for Use with Remote Sensing Data Level I Classes Level II Classes Level III Classes FOREST STRUCTURE ESTIMATION Information on Forest Structure Forest Inventory Variables Biomass Volume and Growth Assessment FOREST CHANGE DETECTION Information on Forest Change Harvesting and Silviculture Activity Natural Disturbances Change in Spatial Structure CONCLUSION The Technological Approach - Revisited References",2001.0,S. Franklin
a06692bf3f95c664fa7b3384a86c97cec711b8ca,https://www.semanticscholar.org/paper/a06692bf3f95c664fa7b3384a86c97cec711b8ca,Learning Adaptive Value of Information for Structured Prediction,"Discriminative methods for learning structured models have enabled wide-spread use of very rich feature representations. However, the computational cost of feature extraction is prohibitive for large-scale or time-sensitive applications, often dominating the cost of inference in the models. Significant efforts have been devoted to sparsity-based model selection to decrease this cost. Such feature selection methods control computation statically and miss the opportunity to fine-tune feature extraction to each input at run-time. We address the key challenge of learning to control fine-grained feature extraction adaptively, exploiting non-homogeneity of the data. We propose an architecture that uses a rich feedback loop between extraction and prediction. The run-time control policy is learned using efficient value-function approximation, which adaptively determines the value of information of features at the level of individual variables for each input. We demonstrate significant speedups over state-of-the-art methods on two challenging datasets. For articulated pose estimation in video, we achieve a more accurate state-of-the-art model that is also faster, with similar results on an OCR task.",2013.0,"David J. Weiss, B. Taskar"
f5a1b2c112071715d372c807151cdab9e0b3f992,https://www.semanticscholar.org/paper/f5a1b2c112071715d372c807151cdab9e0b3f992,A Novel Method of Combined Feature Extraction for Recognition,"Multimodal recognition is an emerging technique to overcome the non-robustness of the unimodal recognition in real applications. Canonical correlation analysis (CCA) has been employed as a powerful tool for feature fusion in the realization of such multimodal system. However, CCA is the unsupervised feature extraction and it does not utilize the class information of the samples, resulting in the constraint of the recognition performance. In this paper, the class information is incorporated into the framework of CCA for combined feature extraction, and a novel method of combined feature extraction for multimodal recognition, called discriminative canonical correlation analysis (DCCA), is proposed. The experiments show that DCCA outperforms some related methods of both unimodal recognition and multimodal recognition.",2008.0,"Tingkai Sun, Songcan Chen, Jing-yu Yang, P. Shi"
569804d3ca477524045fc0c4945f32a5f3156b3d,https://www.semanticscholar.org/paper/569804d3ca477524045fc0c4945f32a5f3156b3d,Automatic Extraction of Acronym-meaning Pairs from MEDLINE Databases,"Acronyms are widely used in biomedical and other technical texts. Understanding their meaning constitutes an important problem in the automatic extraction and mining of information from text. Here we present a system called ACROMED that is part of a set of Information Extraction tools designed for processing and extracting information from abstracts in the Medline database. In this paper, we present the results of two strategies for finding the long forms for acronyms in biomedical texts. These strategies differ from previous automated acronym extraction methods by being tuned to the complex phrase structures of the biomedical lexicon and by incorporating shallow parsing of the text into the acronym recognition algorithm. The performance of our system was tested with several data sets obtaining a performance of 72 % recall with 97 % precision. These results are found to be better for biomedical texts than the performance of other acronym extraction systems designed for unrestricted text.",2001.0,"J. Pustejovsky, J. Castaño, Brent Cochran, M. Kotecki, Michael Morrell"
7f9844f16b6c011e561ee2afef9157405c1d7e0d,https://www.semanticscholar.org/paper/7f9844f16b6c011e561ee2afef9157405c1d7e0d,ViBE: A powerful random technique to estimate the background in video sequences,"Background subtraction is a crucial step in many automatic video content analysis applications. While numerous acceptable techniques have been proposed so far for background extraction, there is still a need to produce more efficient algorithms in terms of adaptability to multiple environments, noise resilience, and computation efficiency. In this paper, we present a powerful method for background extraction that improves in accuracy and reduces the computational load. The main innovation concerns the use of a random policy to select values to build a samples-based estimation of the background. To our knowledge, it is the first time that a random aggregation is used in the field of background extraction. In addition we propose a novel policy that propagates information between neighboring pixels of an image. Experiment detailed in this paper show how our method improves on other widely used techniques, and how it outperforms these techniques for noisy images.",2009.0,"Olivier Barnich, Marc Van Droogenbroeck"
08e8ee6fd84dd21028e7f42aa68b333ce4bf13c0,https://www.semanticscholar.org/paper/08e8ee6fd84dd21028e7f42aa68b333ce4bf13c0,Information theory-based shot cut/fade detection and video summarization,"New methods for detecting shot boundaries in video sequences and for extracting key frames using metrics based on information theory are proposed. The method for shot boundary detection relies on the mutual information (MI) and the joint entropy (JE) between the frames. It can detect cuts, fade-ins and fade-outs. The detection technique was tested on the TRECVID2003 video test set having different types of shots and containing significant object and camera motion inside the shots. It is demonstrated that the method detects both fades and abrupt cuts with high accuracy. The information theory measure provides us with better results because it exploits the inter-frame information in a more compact way than frame subtraction. It was also successfully compared to other methods published in literature. The method for key frame extraction uses MI as well. We show that it captures satisfactorily the visual content of the shot.",2006.0,"Z. Černeková, I. Pitas, Christophoros Nikou"
ce0f8d942e927852a7d7be14f0375d6aa51e0e84,https://www.semanticscholar.org/paper/ce0f8d942e927852a7d7be14f0375d6aa51e0e84,Nearly defect-free F0 trajectory extraction for expressive speech modifications based on STRAIGHT,"A new method for source information extraction is proposed. The aim of the method is to provide optimal source information for the very high quality speech manipulation system STRAIGHT. The method is based on both time interval and frequency cues, and it provides fundamental frequency and periodicity information within each frequency band, to allow mixed mode excitation. The method is designed to minimize perceptual disturbance due to errors in source information extraction. A preliminary evaluation using a database of simultaneously recorded EGG and speech signals yielded very low gross error rates (0.029% for females and 0.14% for males). In addition, the method is designed so as to minimize the perceptual disturbance caused by any such gross error.",2005.0,"Hideki Kawahara, A. Cheveigné, Hideki Banno, Toru Takahashi, T. Irino"
9dc7620d3d5aa90835fc45238bd75206eab41527,https://www.semanticscholar.org/paper/9dc7620d3d5aa90835fc45238bd75206eab41527,Topic Detection and Extraction in Chat,"Internet-based Chat environments such as Internet relay Chat and instant messaging pose a challenge for data mining and information retrieval systems due to the multi-threaded, overlapping nature of the dialog and the nonstandard usage of language. In this paper we present preliminary methods of topic detection and topic thread extraction that augment a typical TF-IDF-based vector space model approach with temporal relationship information between posts of the Chat dialog combined with WordNet hypernym augmentation. We show results that promise better performance than using only a TF-IDF bag-of-words vector space model.",2008.0,"Paige Adams, C. Martell"
e10d14fdebd1e6ea7102fbd238592c743adba08b,https://www.semanticscholar.org/paper/e10d14fdebd1e6ea7102fbd238592c743adba08b,In situ soil water extraction: a review.,"The knowledge of the composition and fluxes of vadose zone water is essential for a wide range of scientific and practical fields, including water-use management, pesticide registration, fate of xenobiotics, monitoring of disposal from mining and industries, nutrient management of agricultural and forest ecosystems, ecology, and environmental protection. Nowadays, water and solute flow can be monitored using either in situ methods or minimally invasive geophysical measurements. In situ information, however, is necessary to interpret most geophysical data sets and to determine the chemical composition of seepage water. Therefore, we present a comprehensive review of in situ soil water extraction methods to monitor solute concentration, solute transport, and to calculate mass balances in natural soils. We distinguished six different sampling devices: porous cups, porous plates, capillary wicks, pan lysimeters, resin boxes, and lysimeters. For each of the six sampling devices we discuss the basic principles, the advantages and disadvantages, and limits of data acquisition. We also give decision guidance for the selection of the appropriate sampling system. The choice of material is addressed in terms of potential contamination, filtering, and sorption of the target substances. The information provided in this review will support scientists and professionals in optimizing their experimental set-up for meeting their specific goals.",2007.0,"L. Weihermüller, Jan Siemens, M. Deurer, S. Knoblauch, H. Rupp, A. Göttlein, Thomas Pütz"
ce0f8d942e927852a7d7be14f0375d6aa51e0e84,https://www.semanticscholar.org/paper/ce0f8d942e927852a7d7be14f0375d6aa51e0e84,Nearly defect-free F0 trajectory extraction for expressive speech modifications based on STRAIGHT,"A new method for source information extraction is proposed. The aim of the method is to provide optimal source information for the very high quality speech manipulation system STRAIGHT. The method is based on both time interval and frequency cues, and it provides fundamental frequency and periodicity information within each frequency band, to allow mixed mode excitation. The method is designed to minimize perceptual disturbance due to errors in source information extraction. A preliminary evaluation using a database of simultaneously recorded EGG and speech signals yielded very low gross error rates (0.029% for females and 0.14% for males). In addition, the method is designed so as to minimize the perceptual disturbance caused by any such gross error.",2005.0,"Hideki Kawahara, A. Cheveigné, Hideki Banno, Toru Takahashi, T. Irino"
9dc7620d3d5aa90835fc45238bd75206eab41527,https://www.semanticscholar.org/paper/9dc7620d3d5aa90835fc45238bd75206eab41527,Topic Detection and Extraction in Chat,"Internet-based Chat environments such as Internet relay Chat and instant messaging pose a challenge for data mining and information retrieval systems due to the multi-threaded, overlapping nature of the dialog and the nonstandard usage of language. In this paper we present preliminary methods of topic detection and topic thread extraction that augment a typical TF-IDF-based vector space model approach with temporal relationship information between posts of the Chat dialog combined with WordNet hypernym augmentation. We show results that promise better performance than using only a TF-IDF bag-of-words vector space model.",2008.0,"Paige Adams, C. Martell"
e10d14fdebd1e6ea7102fbd238592c743adba08b,https://www.semanticscholar.org/paper/e10d14fdebd1e6ea7102fbd238592c743adba08b,In situ soil water extraction: a review.,"The knowledge of the composition and fluxes of vadose zone water is essential for a wide range of scientific and practical fields, including water-use management, pesticide registration, fate of xenobiotics, monitoring of disposal from mining and industries, nutrient management of agricultural and forest ecosystems, ecology, and environmental protection. Nowadays, water and solute flow can be monitored using either in situ methods or minimally invasive geophysical measurements. In situ information, however, is necessary to interpret most geophysical data sets and to determine the chemical composition of seepage water. Therefore, we present a comprehensive review of in situ soil water extraction methods to monitor solute concentration, solute transport, and to calculate mass balances in natural soils. We distinguished six different sampling devices: porous cups, porous plates, capillary wicks, pan lysimeters, resin boxes, and lysimeters. For each of the six sampling devices we discuss the basic principles, the advantages and disadvantages, and limits of data acquisition. We also give decision guidance for the selection of the appropriate sampling system. The choice of material is addressed in terms of potential contamination, filtering, and sorption of the target substances. The information provided in this review will support scientists and professionals in optimizing their experimental set-up for meeting their specific goals.",2007.0,"L. Weihermüller, Jan Siemens, M. Deurer, S. Knoblauch, H. Rupp, A. Göttlein, Thomas Pütz"
b4e9c14c67b8aa431a40041cce0a3564144e1a2a,https://www.semanticscholar.org/paper/b4e9c14c67b8aa431a40041cce0a3564144e1a2a,MFCC and its applications in speaker recognition,": Speech processing is emerged as one of the important application area of digital signal processing. Various fields for research in speech processing are speech recognition, speaker recognition, speech synthesis, speech coding etc. The objective of automatic speaker recognition is to extract, characterize and recognize the information about speaker identity. Feature extraction is the first step for speaker recognition. Many algorithms are suggested/developed by the researchers for feature extraction. In this work, the Mel Frequency Cepstrum Coefficient (MFCC) feature has been used for designing a text dependent speaker identification system. Some modifications to the existing technique of MFCC for feature extraction are also suggested to improve the speaker recognition efficiency.",2010.0,V. Tiwari
da7ab96e96208d0b934b7885aad1c0867cb6206a,https://www.semanticscholar.org/paper/da7ab96e96208d0b934b7885aad1c0867cb6206a,Reasons for tooth extraction in the western states of Germany.,"The purpose of this study was to collect information on the main causes of tooth loss in the western states of Germany as perceived by dentists and their patients. Sixty-eight dentists, out of 80 that were selected with a systematic random method for an epidemiological study in the western states of Germany, recorded their reason for tooth extraction. Included in the study were only extractions of permanent teeth during a period of 2 weeks (March 1990), up to a maximum of 20 patients per dentist. Of 926 returned questionnaires, 882 could be evaluated. In all 1215 teeth in 882 patients were extracted. The extraction of third molars was included as a reason, when caries, periodontal reasons and others were not indicated. Caries was the reason given for 20.7% of all extractions; periodontal diseases for 27.3%; caries and periodontal reasons for 18.7%; third molars for 14.7%; prosthetic reasons for 11.2%; orthodontic reasons for 4.1%; trauma for 0.4% and others for 2.9%. While caries is a major reason in all age groups, periodontal diseases and the combination of caries and periodontal reasons are more frequent than all other reasons for the age groups beyond 40 or 45 yr, respectively. The third molar was the most often extracted tooth. The patients were asked for their main reason for tooth extraction. For the patients, pain was the major reason for extraction (47.2%). According to the participating dentists periodontal disease is the most frequent cause of tooth extraction for people over 40 yr of age, while for those below 40 yr of age, caries and third molar extractions are the most frequent reasons.",1993.0,"E. Reich, K. Hiller"
2d1e4b126cf61492c240dd3cf1c40d27b14d86df,https://www.semanticscholar.org/paper/2d1e4b126cf61492c240dd3cf1c40d27b14d86df,Single color extraction and image query,"We propose a method for automatic color extraction and indexing to support color queries of image and video databases. This approach identifies the regions within images that contain colors from predetermined color sets. By searching over a large number of color sets, a color index for the database is created in a fashion similar to that for file inversion. This allows very fast indexing of the image collection by the color contents of the images. Furthermore, information about the identified regions, such as the color set, size, and location, enables a rich variety of queries that specify both color content and spatial relationships of regions. We present the single color extraction and indexing method and contrast it to other color approaches. We examine single and multiple color extraction and image query on a database of 3000 color images.",1995.0,"John R. Smith, Shih-Fu Chang"
08e036080bac77c2a5f57f9cec78ce4eb4674d9b,https://www.semanticscholar.org/paper/08e036080bac77c2a5f57f9cec78ce4eb4674d9b,Knowledge Extraction by Using an Ontology Based Annotation Tool,"This paper describes a Semantic Annotation Tool for extraction of knowledge structures from web pages through the use of simple user-defined knowledge extraction patterns. The semantic annotation tool contains: an ontology-based mark-up component which allows the user to browse and to mark-up relevant pieces of information; a learning component (Crystal from the University of Massachusetts at Amherst) which learns rules from examples and an information extraction component which extracts the objects and relation between these objects. Our final aim is to provide support for ontology population by using the information extraction component. Our system uses as domain of study “KMi Planet”, a Webbased news server that helps to communicate relevant information between members in our institute.",2001.0,"M. Vargas-Vera, E. Motta, J. Domingue, S. B. Shum, M. Lanzoni"
d22cdc6f9b5db84cc785101218533a32f69819ea,https://www.semanticscholar.org/paper/d22cdc6f9b5db84cc785101218533a32f69819ea,"Fuzzy-trace theory and framing effects in choice: Gist extraction, truncation, and conversion","In the first section of this paper, we analyze classic framing effects according to principles of fuzzy-trace theory. The key principle of the theory is that reasoning prefers to operate on simple gist, as opposed to exact details. Then, we introduce new data in three experiments designed to test this fuzzy-processing assumption. In the first experiment, framing effects were conserved when numerical information was omitted from standard problems, arguing against a critical role for numerical processing. In the second experiment, evidence is presented that some subjects simplified framing problems by mentally truncating linguistically redundant complements in gambles. Experimentally deleting parts of gambles mimiced such effects, and choices varied depending on the information that remained explicit. In the third experiment, truncation effects were also demonstrated for mixed-frame problems, in which one option is positive and the other is negative. The data disconfirmed a ‘halo’ hypothesis that subjects merely selected the positive option over the negative one. Instead, choices were accounted for by conversion, that is, transforming problems into uniformly positive representations to avoid the complexity of negation. In all three experiments, choices could be explained as a consequence of radically simplifying decision information.",1991.0,"V. Reyna, C. Brainerd"
70e9347aca34471e1b5552d52556bb204fe2d109,https://www.semanticscholar.org/paper/70e9347aca34471e1b5552d52556bb204fe2d109,Extraction of Specific Signals with Temporal Structure,"In this work we develop a very simple batch learning algorithm for semi-blind extraction of a desired source signal with temporal structure from linear mixtures. Although we use the concept of sequential blind extraction of sources and independent component analysis, we do not carry out the extraction in a completely blind manner; neither do we assume that sources are statistically independent. In fact, we show that the a priori information about the autocorrelation function of primary sources can be used to extract the desired signals (sources of interest) from their linear mixtures. Extensive computer simulations and real data application experiments confirm the validity and high performance of the proposed algorithm.",2001.0,"A. Barros, A. Cichocki"
af4c292d79caeaa662c32620b7e9a7cf3c592a0f,https://www.semanticscholar.org/paper/af4c292d79caeaa662c32620b7e9a7cf3c592a0f,Geodynamic Information in Peridotite Petrology,"Systematic differences are observed in the petrology and major element geochemistry of natural peridotite samples from the sea floor near oceanic ridges and subduction zones, the mantle section of ophiolites, massif peridotites, and xenoliths of cratonic mantle in kimberlite. Some of these differences reflect variable temperature and pressure conditions of melt extraction, and these have been calibrated by a parameterization of experimental data on fertile mantle peridotite. Abyssal peridotites are examples of cold residues produced at oceanic ridges. High-MgO peridotites from the Ronda massif are examples of hot residues produced in a plume. Most peridotites from subduction zones and ophiolites are too enriched in SiO2 and too depleted in Al2O3 to be residues, and were produced by melt–rock reaction of a precursor protolith. Peridotite xenoliths from the Japan, Cascades and Chile–Patagonian back-arcs are possible examples of arc precursors, and they have the characteristics of hot residues. Opx-rich cratonic mantle is similar to subduction zone peridotites, but there are important differences in FeOT. Opx-poor xenoliths of cratonic mantle were hot residues of primary magmas with 16–20% MgO, and they may have formed in either ancient plumes or hot ridges. Cratonic mantle was not produced as a residue of Archean komatiites.",2004.0,C. Herzberg
1e057f46cdc34e1c774ca5682f6ff13d4b0e9980,https://www.semanticscholar.org/paper/1e057f46cdc34e1c774ca5682f6ff13d4b0e9980,"Text Detection, Tracking and Recognition in Video: A Comprehensive Survey","The intelligent analysis of video data is currently in wide demand because a video is a major source of sensory data in our lives. Text is a prominent and direct source of information in video, while the recent surveys of text detection and recognition in imagery focus mainly on text extraction from scene images. Here, this paper presents a comprehensive survey of text detection, tracking, and recognition in video with three major contributions. First, a generic framework is proposed for video text extraction that uniformly describes detection, tracking, recognition, and their relations and interactions. Second, within this framework, a variety of methods, systems, and evaluation protocols of video text extraction are summarized, compared, and analyzed. Existing text tracking techniques, tracking-based detection and recognition techniques are specifically highlighted. Third, related applications, prominent challenges, and future directions for video text extraction (especially from scene videos and web videos) are also thoroughly discussed.",2016.0,"Xu-Cheng Yin, Ze-Yu Zuo, Shu Tian, Cheng-Lin Liu"
b49b3bfc48a6f9fa03889b219233f5fcc248e747,https://www.semanticscholar.org/paper/b49b3bfc48a6f9fa03889b219233f5fcc248e747,Multiscale Edge-Based Text Extraction from Complex Images,"Text that appears in images contains important and useful information. Detection and extraction of text in images have been used in many applications. In this paper, we propose a multiscale edge-based text extraction algorithm, which can automatically detect and extract text in complex images. The proposed method is a general-purpose text detection and extraction algorithm, which can deal not only with printed document images but also with scene text. It is robust with respect to the font size, style, color, orientation, and alignment of text and can be used in a large variety of application fields, such as mobile robot navigation, vehicle license detection and recognition, object identification, document retrieving, page segmentation, etc",2006.0,"Xiaoqing Liu, J. Samarabandu"
ca949388d57455b8b4391913dec25e1e66b4d3a2,https://www.semanticscholar.org/paper/ca949388d57455b8b4391913dec25e1e66b4d3a2,Data-rich section extraction from HTML pages,"We propose a novel algorithm, DSE (data-rich subtree extraction) to recognize and extract the data-rich section of an HTML page. We apply the DSE algorithm as a pre-processing ""clean-up"" step for two typical Web information retrieval problems: topic distillation and Web information extraction. Our experiments show that, for the test data sets used, the DSE algorithm can correctly identify data-rich sections of HTML pages with 100% accuracy. Therefore, it can effectively reduce the root set size for the topic distillation problem thereby improving the precision and accuracy of the IETS algorithm. Furthermore, when applied to the Web information extraction problem using the IEPAD algorithm, it can decrease the number of patterns discovered by this algorithm, thus shortening its time cost to generalize a wrapper for HTML pages.",2002.0,"Jiying Wang, F. Lochovsky"
076e5a741d0c7485b1cf43b5c3ec4cde08e73ec0,https://www.semanticscholar.org/paper/076e5a741d0c7485b1cf43b5c3ec4cde08e73ec0,Adaptive network for optimal linear feature extraction,"A network of highly interconnected linear neuron-like processing units and a simple, local, unsupervised rule for the modification of connection strengths between these units are proposed. After training the network on a high (m) dimensional distribution of input vectors, the lower (n) dimensional output will be a projection into the subspace of the n largest principal components (the subspace spanned by the n eigenvectors of the largest eigenvalues of the input covariance matrix) and maximize the mutual information between the input and the output in the same way as principal component analysis does. The purely local nature of the synaptic modification rule (simple Hebbian and anti-Hebbian) makes the implementation of the network easier, faster, and biologically more plausible than rules depending on error propagation.<<ETX>>",1989.0,Peter Ftrldiiik
a2e6eee581657bead67e8d23107e8599ea7a864e,https://www.semanticscholar.org/paper/a2e6eee581657bead67e8d23107e8599ea7a864e,Extraction of regulatory gene/protein networks from Medline,"MOTIVATION
We have previously developed a rule-based approach for extracting information on the regulation of gene expression in yeast. The biomedical literature, however, contains information on several other equally important regulatory mechanisms, in particular phosphorylation, which we now expanded for our rule-based system also to extract.


RESULTS
This paper presents new results for extraction of relational information from biomedical text. We have improved our system, STRING-IE, to capture both new types of linguistic constructs as well as new types of biological information [i.e. (de-)phosphorylation]. The precision remains stable with a slight increase in recall. From almost one million PubMed abstracts related to four model organisms, we manage to extract regulatory networks and binary phosphorylations comprising 3,319 relation chunks. The accuracy is 83-90% and 86-95% for gene expression and (de-)phosphorylation relations, respectively. To achieve this, we made use of an organism-specific resource of gene/protein names considerably larger than those used in most other biology related information extraction approaches. These names were included in the lexicon when retraining the part-of-speech (POS) tagger on the GENIA corpus. For the domain in question, an accuracy of 96.4% was attained on POS tags. It should be noted that the rules were developed for yeast and successfully applied to both abstracts and full-text articles related to other organisms with comparable accuracy.


AVAILABILITY
The revised GENIA corpus, the POS tagger, the extraction rules and the full sets of extracted relations are available from http://www.bork.embl.de/Docu/STRING-IE",2006.0,"Jasmin Saric, L. Jensen, Rossitza Ouzounova, Isabel Rojas, P. Bork"
4e4d3a22c74acee6c9196fcbd3c6d8c060aa6466,https://www.semanticscholar.org/paper/4e4d3a22c74acee6c9196fcbd3c6d8c060aa6466,Ontology module extraction for ontology reuse: an ontology engineering perspective,"Problems resulting from the management of shared, distributed knowledge has led to ontologies being employed as a solution, in order to effectively integrate information across applications. This is dependent on having ways to share and reuse existing ontologies; with the increased availability of ontologies on the web, some of which include thousands of concepts, novel and more efficient methods for reuse are being devised. One possible way to achieve efficient ontology reuse is through the process of ontology module extraction. A novel approach to ontology module extraction is presented that aims to achieve more efficient reuse of very large ontologies; the motivation is drawn from an Ontology Engineering perspective. This paper provides a definition of ontology modules from the reuse perspective and an approach to module extraction based on such a definition. An abstract graph model for module extraction has been defined, along with a module extraction algorithm. The novel contribution of this paper is a module extraction algorithm that is independent of the language in which the ontology is expressed. This has been implemented in ModTool; a tool that produces ontology modules via extraction. Experiments were conducted to compare ModTool to other modularisation methods.",2007.0,"Paul Doran, V. Tamma, L. Iannone"
50fdfc0cb490f2eb63bdf92c65183b7ddd6cce2d,https://www.semanticscholar.org/paper/50fdfc0cb490f2eb63bdf92c65183b7ddd6cce2d,Development of mild extraction methods for the analysis of natural dyes in textiles of historical interest using LC-diode array detector-MS.,"Analysis of dyes extracted from textiles of historical interest can give valuable information as to where, when, and how the textiles were made. The most widely used method for extraction of colorants involves heating with HCl, which frequently decomposes glycosidic dye components to their parent aglycons, with consequent loss of information about the source of the dye. This is particularly true for flavonoid dyes, many of which are glycosides. We have developed or improved upon two mild textile extraction methods that use ethylenediaminetetraacetic acid (EDTA) and formic acid and are efficient in extracting dyes, but preserve glycosidic linkages. The relative efficiencies of the HCl, EDTA, and formic acid extraction methods are compared by analyzing extracts of dyed samples of silk using HPLC coupled with diode array and mass spectrometric detection. HPLC profiles of EDTA or formic acid extracts of silk dyed, for example, with pagoda tree buds and onionskins are clearly distinguishable as to the plant material used, whereas profiles of HCl extracts are not. Thus, extraction of textiles with EDTA or formic acid reagents can yield significantly more information about the original dyestuff than can extraction with a strong acid.",2005.0,"Xian Zhang, R. Laursen"
61f07b5cc1714badd8aded796f2aa2f3905751c7,https://www.semanticscholar.org/paper/61f07b5cc1714badd8aded796f2aa2f3905751c7,The C-value/NC-value domain-independent method for multi-word term extraction,"In this paper we present a domain-independent method for the automatic extraction of multi-word(technical)terms,from machine-readable special language corpora. The method,(C-value/NC-value),combines linguistic and statistical information. The first part,C-value enhances the common statistical measure of frequency of occurrence for term extraction,making it sensitive to a particular type of multi-word terms,the nested terms.Nested terms are those which also exist as substrings of other terms.The second part,NC-value,gives two things:1)a method for the extraction of term context words(words that tend to appear with terms),2)the incorporation of information from term context words to the extraction of terms.We apply the method to a medical corpus and compare the results with those produced by frequency of occurrence also applied on the same corpus.Frequency of occurrence was chosen for the comparison since it is the most commonly used statistical method for automatic term extraction to date.We show that using C-value we improve the extraction of nested multi-word terms,while using context information(NC-value) we improve the extraction of multi-word terms in general.In the evaluation sections, we give directions for the further improvement of the method.",1999.0,"K. Frantzi, S. Ananiadou"
2d4912597fd5f7c9091214fd06322946b5fb9f24,https://www.semanticscholar.org/paper/2d4912597fd5f7c9091214fd06322946b5fb9f24,The Automatic Extraction of Roads from LIDAR data,"A method for the automatic detection of roads from airborne laser scanner data is presented. Traditionally, intensity information has not been used in feature extraction from LIDAR data because the data is too noisy. This article deals with using as much of the recorded laser information as possible thus both height and intensity are used. To extract roads from a LIDAR point cloud, a hierarchical classification technique is used to classify the LIDAR points progressively into road or non-road. Initially, an accurate digital terrain model (DTM) model is created by using successive morphological openings with different structural element sizes. Individual laser points are checked for both a valid intensity range and height difference from the subsequent DTM. A series of filters are then passed over the road candidate image to improve the accuracy of the classification. The success rate of road detection and the level of detail of the resulting road image both depend on the resolution of the laser scanner data and the types of roads expected to be found. The presence of road-like features within the survey area such as private roads and car parks is discussed and methods to remove this information are entertained. All algorithms used are described and applied to an example urban test site.",2004.0,"S. Clode, Peter James Kootsookos, F. Rottensteiner"
c541f5bc33363156e11df161eb7fa8a17493bc66,https://www.semanticscholar.org/paper/c541f5bc33363156e11df161eb7fa8a17493bc66,Domain-Speci c Keyphrase Extraction,"Keyphrases are an important means of document summarization, clustering, and topic search. Only a small minority of documents have author-assigned keyphrases, and manually assigning keyphrases to existing documents is very laborious. Therefore it is highly desirable to automate the keyphrase extraction process. This paper shows that a simple procedure for keyphrase extraction based on the naive Bayes learning scheme performs comparably to the state of the art. It goes on to explain how this procedure's performance can be boosted by automatically tailoring the extraction process to the particular document collection at hand. Results on a large collection of technical reports in computer science show that the quality of the extracted keyphrases improves signi cantly when domain-speci c information is exploited.",1999.0,"E. Frank, G. Paynter, I. Witten, C. Gutwin, C. Nevill-Manning"
7bfd8a9800e21747e37a69b780dc1409e5f829d5,https://www.semanticscholar.org/paper/7bfd8a9800e21747e37a69b780dc1409e5f829d5,Correlated Information and Mechanism Design,"In models of asymmetric information, possession of private information leads to rents for the possessors. This induces mechanism designers to distort away from efficiency. The authors show that this is an artifact of the presumption that information is independently distributed. Rent extraction in a large class of mechanism design games is analyzed, and a necessary and sufficient condition for arbitrarily small rents to private information is provided. Additionally, the two-person bargaining game is shown to have an efficient solution under first-order stochastic dominance and a hazard rate condition. Similar conditions allow full rent extraction in Milgrom-Weber auctions. Copyright 1992 by The Econometric Society.",1992.0,"R. McAfee, P. Reny"
ea8158bbb075c46936ba6505e47f6ba74f102409,https://www.semanticscholar.org/paper/ea8158bbb075c46936ba6505e47f6ba74f102409,Synonymy in collocation extraction,"This paper describes the use of WordNet in a new technique for collocation extraction. The approach is based on restrictions on the possible substitutions for synonyms within candidate phrases. Following a general discussion of collocations and their applications, current extraction methods are briefly described. This is followed by a detailed description of the new approach and results and evaluation of experiments that utilise WordNet as a source of synonymic information.",2001.0,Darren Pearce
0f8b52e54b7aca44360abbf3d87f9f60d39a000c,https://www.semanticscholar.org/paper/0f8b52e54b7aca44360abbf3d87f9f60d39a000c,Unknown Word Extraction for Chinese Documents,"There is no blank to mark word boundaries in Chinese text. As a result, identifying words is difficult, because of segmentation ambiguities and occurrences of unknown words. Conventionally unknown words were extracted by statistical methods because statistical methods are simple and efficient. However the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall, since character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistical methods. In addition to statistical information, we try to use as much information as possible, such as morphology, syntax, semantics, and world knowledge. The identification system fully utilizes the context and content information of unknown words in the steps of detection process, extraction process, and verification process. A practical unknown word extraction system was implemented which online identifies new words, including low frequency new words, with high precision and high recall rates.",2002.0,"Keh-Jiann Chen, Wei-Yun Ma"
dcd8b259d530f16b66b9077903478306548df96a,https://www.semanticscholar.org/paper/dcd8b259d530f16b66b9077903478306548df96a,Automatic Text Summarization by Paragraph Extraction,"Over the years, the amount of information available electronically has grown manifold. There is an increasing demand for automatic methods for text summarization. Domain-independent techniques for automatic summarization by paragraph extraction have been proposed in [12, 15]. In this study, we attempt to evaluate these methods by comparingthe automatically generated extracts to ones generated by humans. In view of the fact that extracts generated by two humans for the same article are surprisingly dissimilar, the performance of the automatic methods is satisfactory. Even though this observation calls into question the feasibility of producing perfect summaries by extraction, given the unavailability of other e(cid:11)ective domain-independent summarization tools, we believe that this is a reasonable, though imperfect, alternative.",1997.0,"Mandar Mitra, A. Singhal, C. Buckley"
0a7c454ba1137298f9d953e9fc76b62a7b5dcc9e,https://www.semanticscholar.org/paper/0a7c454ba1137298f9d953e9fc76b62a7b5dcc9e,Automatic Extraction of Opinion Propositions and their Holders,"We identify a new task in the ongoing analysis of opinions: finding propositional opinions, sentential complements which for many verbs contain the actual opinion, rather than full opinion sentences. We propose an extension of semantic parsing techniques, coupled with additional lexical and syntactic features, that can produce labels for propositional opinions as opposed to other syntactic constituents. We describe the annotation of a small corpus of 5,139 sentences with propositional opinion information, and use this corpus to evaluate our methods. We also present results that indicate that the proposed methods can be extended to the related task of identifying opinion holders and associating them with propositional",2004.0,"Steven Bethard, Hong Yu, Ashley Thornton, V. Hatzivassiloglou, Dan Jurafsky"
99cb7a821ac260fcc0b1466d5f59f0cfe66489fa,https://www.semanticscholar.org/paper/99cb7a821ac260fcc0b1466d5f59f0cfe66489fa,Automatic extraction of music descriptors from acoustic signals,"High-Level music descriptors are key ingredients for music information retrieval systems. Although there is a long tradition in extracting information from acoustic signals, the field of music information extraction is largely heuristic in nature. We present here a heuristicbased generic approach for extracting automatically high-level music descriptors from acoustic signals. This approach is based on Genetic Programming, used to build relevant features as functions of mathematical and signal processing operators. The search of relevant features is guided by specialized heuristics that embody knowledge about the signal processing functions built by the system. Signal processing patterns are used in order to control the general processing methods. In addition, rewriting rules are introduced to simplify overly complex expressions, and a caching system further reduces the computing cost of each cycle. Finally, the features build by the system are combined into an optimized machine learning descriptor model, and an executable program is generated to compute the model on any audio signal. In this paper, we describe the overall system and compare its results against traditional approaches in musical feature extraction a la Mpeg7.",2004.0,"F. Pachet, Aymeric Zils"
fa6c2434c52060e898f417656b49fa664a82da66,https://www.semanticscholar.org/paper/fa6c2434c52060e898f417656b49fa664a82da66,Lightweight lexical source model extraction,"Software engineers maintaining an existing software system often depend on the mechanized extraction of information from system artifacts. Some useful kinds of information—source models—are well known: call graphs, file dependences, etc. Predicting every kind of source model that a software engineer may need is impossible. We have developed a lightweight approach for generating flexible and tolerant source model extractors from lexical specifications. The approach is lightweight in that the specifications are relatively small and easy to write. It is flexible in that there are few constraints on the kinds of artifacts from which source models are extracted (e.g., we can extract from source code, structured data files, documentation, etc.). It is tolerant in that there are few constraints on the condition of the artifacts. For example, we can extract from source that cannot necessarily be compiled. Our approach extended the kinds of source models that can be easily produced from lexical information while avoiding the constraints and brittleness of most parser-based approaches. We have developed tools to support this approach and applied the tools to the extraction of a number of different source models (file dependences, event interactions, call graphs) from a variety of system artifacts (C, C++, CLOS, Eiffel. TCL, structured data). We discuss our approach and describe its application to extract source models not available using existing systems; for example, we compute the implicitly-invokes relation over Field tools. We compare and contrast our approach to the conventional lexical and syntactic approaches of generating source models.",1996.0,"G. Murphy, D. Notkin"
bd602960baa4a54bd03eb5b19d2b0b1612ab3f52,https://www.semanticscholar.org/paper/bd602960baa4a54bd03eb5b19d2b0b1612ab3f52,An Approach Based on Multilingual Thesauri and Model Combination for Bilingual Lexicon Extraction,"This paper focuses on exploiting different models and methods in bilingual lexicon extraction, either from parallel or comparable corpora, in specialized domains. First, a special attention is given to the use of multilingual thesauri, and different search strategies based on such thesauri are investigated. Then, a method to combine the different models for bilingual lexicon extraction is presented. Our results show that the combination of the models significantly improves results, and that the use of the hierarchical information contained in our thesaurus, UMLS/MeSH, is of primary importance. Lastly, methods for bilingual terminology extraction and thesaurus enrichment are discussed.",2002.0,"Hervé Déjean, Éric Gaussier, F. Sadat"
fa220cf96b31fa000d53fab641b4f2984a03d813,https://www.semanticscholar.org/paper/fa220cf96b31fa000d53fab641b4f2984a03d813,Token-based extraction of straight lines,"The authors present a computational approach to the extraction of straight lines based on the principles of perceptual organization. In particular, they consider how local information that is spatially distributed can be organized into a large-scale geometric structure in a computationally efficient manner. Symbolic tokens representing line segments and relations which are primarily geometric in nature and used to control a hierarchical grouping process. The relational measures on pairs of lines are based on collinearity, proximity, and similarity in contrast. The algorithm is implemented within a local, parallel, hierarchical framework for symbolic grouping that involves a cycle of linking, optimization, and replacement steps. Experimental results on a variety of natural scene images demonstrate effectiveness of the filtering and optimization stages in the extraction of straight lines. Issues in the development of a more general framework for symbolic grouping are also discussed. >",1989.0,"Michael Boldt, R. Weiss, E. Riseman"
4b098c34ccd2fc212367916f89124ae6752ff0d3,https://www.semanticscholar.org/paper/4b098c34ccd2fc212367916f89124ae6752ff0d3,Corporate Governance and Dividend Pay-Out Policy in Germany,"An alternative explanation of why dividends may be informative is put forward in this paper. We find evidence that dividends signal the severity of the conflict between the large, controlling owner and small, outside shareholders. Accordingly, dividend change announcements provide new information about this conflict. To test the rent extraction hypothesis and to discriminate it from the cash flow signaling explanation, we utilize information on the ownership and control structure of the firm. We analyze 736 dividend change announcements in Germany over the period 1992 to 1998 and find significantly larger negative wealth effects in the order of two percentage points for companies where the ownership and control structure makes the expropriation of minority shareholders more likely than for other firms. The rent extraction hypothesis has also implications for the levels of dividends paid. We find larger holdings of the largest owner to reduce, while larger holdings of the second largest shareholder to increase the dividend pay-out ratio. Deviations from the one-share-one-vote rule of ultimate owners due to pyramidal and cross-ownership structures are also associated with larger negative wealth effects and lower pay-out ratios. Our results call for better minority shareholder rights protection and increased transparency in the course of European Capital Market Reform.",2003.0,"K. Gugler, B. Yurtoglu"
db2a8e37a456438fbf1e8f4897e29ce4d2e30424,https://www.semanticscholar.org/paper/db2a8e37a456438fbf1e8f4897e29ce4d2e30424,Screen-Shooting Resilient Watermarking,"This paper proposes a novel screen-shooting resilient watermarking scheme, which means that if the watermarked image is displayed on the screen and the screen information is captured by the camera, we can still extract the watermark message from the captured photo. To realize such demands, we analyzed the special distortions caused by the screen-shooting process, including lens distortion, light source distortion, and moiré distortion. To resist the geometric deformation caused by lens distortion, we proposed an intensity-based scale-invariant feature transform (I-SIFT) algorithm which can accurately locate the embedding regions. As for the loss of image details caused by light source distortion and moiré distortion, we put forward a small-size template algorithm to repeatedly embed the watermark into different regions, so that at least one complete information region can survive from distortions. At the extraction side, we designed a cross-validation-based extraction algorithm to cope with repeated embedding. The validity and correctness of the extraction method are verified by hypothesis testing. Furthermore, to boost the extraction speed, we proposed a SIFT feature editing algorithm to enhance the intensity of the keypoints, based on which, the extraction accuracy and extraction speed can be greatly improved. The experimental results show that the proposed watermarking scheme achieves high robustness for screen-shooting process. Compared with the previous schemes, our algorithm provides significant improvement in robustness for screen-shooting process and extraction efficiency.",2019.0,"H. Fang, Weiming Zhang, Hang Zhou, Hao Cui, Nenghai Yu"
505cdc1db319d6e769b173f4e471ed57bffff70a,https://www.semanticscholar.org/paper/505cdc1db319d6e769b173f4e471ed57bffff70a,Opinion Extraction and Summarization on the Web,"The Web has become an excellent source for gathering consumer opinions. There are now numerous Web sources containing such opinions, e.g., product reviews, forums, discussion groups. and blogs. Techniques are now being developed to exploit these sources to help organizations and individuals to gain such important information easily and quickly. In this paper, we first discuss several aspects of the problem in the AI context, and then present some results of our existing work published in KDD-04 and WWW-05.",2006.0,"Minqing Hu, B. Liu"
27ca855612dd64d53adce55fabdeec3467cdd086,https://www.semanticscholar.org/paper/27ca855612dd64d53adce55fabdeec3467cdd086,An introduction to bipolar representations of information and preference,"Bipolarity seems to pervade human understanding of information and preference, and bipolar representations look very useful in the development of intelligent technologies. Bipolarity refers to an explicit handling of positive and negative sides of information. Basic notions and background on bipolar representations are provided. Three forms of bipolarity are laid bare: symmetric univariate, dual bivariate, and asymmetric (or heterogeneous) bipolarity. They can be instrumental in the logical handling of incompleteness and inconsistency, rule representation and extraction, argumentation, learning, and decision analysis. © 2008 Wiley Periodicals, Inc.",2008.0,"D. Dubois, H. Prade"
29db7187f5423bdbb71b9841d5d52bef5405f25d,https://www.semanticscholar.org/paper/29db7187f5423bdbb71b9841d5d52bef5405f25d,Soil sampling and isolation of extracellular DNA from large amount of starting material suitable for metabarcoding studies,"DNA metabarcoding refers to the DNA‐based identification of multiple species from a single complex and degraded environmental sample. We developed new sampling and extraction protocols suitable for DNA metabarcoding analyses targeting soil extracellular DNA. The proposed sampling protocol has been designed to reduce, as much as possible, the influence of local heterogeneity by processing a large amount of soil resulting from the mixing of many different cores. The DNA extraction is based on the use of saturated phosphate buffer. The sampling and extraction protocols were validated first by analysing plant DNA from a set of 12 plots corresponding to four plant communities in alpine meadows, and, second, by conducting pilot experiments on fungi and earthworms. The results of the validation experiments clearly demonstrated that sound biological information can be retrieved when following these sampling and extraction procedures. Such a protocol can be implemented at any time of the year without any preliminary knowledge of specific types of organisms during the sampling. It offers the opportunity to analyse all groups of organisms using a single sampling/extraction procedure and opens the possibility to fully standardize biodiversity surveys.",2012.0,"P. Taberlet, S. Prud'homme, E. Campione, J. Roy, C. Miquel, W. Shehzad, L. Gielly, D. Rioux, P. Choler, J. Clement, C. Melodelima, F. Pompanon, É. Coissac"
500e402d263652d027d4f025e980ff04c1650258,https://www.semanticscholar.org/paper/500e402d263652d027d4f025e980ff04c1650258,Feature Extraction for Image Mining,"Due to the digitization of data and advances in technology, it has become extremely easy to obtain and store large quantities of data, particularly Multimedia data. Fields ranging from Commercial to Military need to analyze these data in an efficient and fast manner. Presently, tools for mining images are few and require human intervention. Feature selection and extraction is the pre-processing step of Image Mining. Obviously this is a critical step in the entire scenario of Image Mining. Our approach to mine from Images – to extract patterns and derive knowledge from large collections of images, deals mainly with identification and extraction of unique features for a particular domain. Though there are various features available, the aim is to identify the best features and thereby extract relevant information from the images. We have tried various methods for extraction; the features extracted and the techniques used are evaluated for their contribution to solving the problem. Experimental results show that the features used are sufficient to identify the patterns from the Images. The extracted features were evaluated for goodness and tested on test images. An interactive system was developed which allows the user to define new features and to resolve uncertain regions.",2002.0,"P. G. Foschi, Deepak Kolippakkam, Huan Liu, A. Mandvikar"
a5092afc0083adf3a1082e508e167e6ceaa358d1,https://www.semanticscholar.org/paper/a5092afc0083adf3a1082e508e167e6ceaa358d1,Extracting domain models from natural-language requirements: approach and industrial evaluation,"Domain modeling is an important step in the transition from natural-language requirements to precise specifications. For large systems, building a domain model manually is a laborious task. Several approaches exist to assist engineers with this task, whereby candidate domain model elements are automatically extracted using Natural Language Processing (NLP). Despite the existing work on domain model extraction, important facets remain under-explored: (1) there is limited empirical evidence about the usefulness of existing extraction rules (heuristics) when applied in industrial settings; (2) existing extraction rules do not adequately exploit the natural-language dependencies detected by modern NLP technologies; and (3) an important class of rules developed by the information retrieval community for information extraction remains unutilized for building domain models. Motivated by addressing the above limitations, we develop a domain model extractor by bringing together existing extraction rules in the software engineering literature, extending these rules with complementary rules from the information retrieval literature, and proposing new rules to better exploit results obtained from modern NLP dependency parsers. We apply our model extractor to four industrial requirements documents, reporting on the frequency of different extraction rules being applied. We conduct an expert study over one of these documents, investigating the accuracy and overall effectiveness of our domain model extractor.",2016.0,"Chetan Arora, M. Sabetzadeh, L. Briand, Frank Zimmer"
5273b3fb30c5da2d4f53bdb7bfffad625abdbde1,https://www.semanticscholar.org/paper/5273b3fb30c5da2d4f53bdb7bfffad625abdbde1,A distributed approach to sub-ontology extraction,"The new era of semantic Web has enabled users to extract semantically relevant data from the Web. The backbone of the semantic Web is a shared uniform structure which defines how Web information is split up regardless of the implementation language or the syntax used to represent the data. This structure is known as an ontology. As information on the Web increases significantly in size, Web ontologies also tend to grow bigger, to such an extent that they become too large to be used in their entirety by any single application. This has stimulated our work in the area of sub-ontology extraction where each user may extract optimized sub-ontologies from an existing base ontology. Sub-ontologies are valid independent ontologies, known as materialized ontologies, that are specifically extracted to meet certain needs. Because of the size of the original ontology, the process of repeatedly iterating the millions of nodes and relationships to form an optimized sub-ontology can be very extensive. Therefore we have identified the need for a distributed approach to the extraction process. As ontologies are currently widely used, our proposed approach for distributed ontology extraction will play an important role in improving the efficiency of information retrieval.",2004.0,"M. Bhatt, Andrew Flahive, C. Wouters, W. Rahayu, D. Taniar, T. Dillon"
38afe960fb869a6e3a458d327f0300cfa23e597b,https://www.semanticscholar.org/paper/38afe960fb869a6e3a458d327f0300cfa23e597b,Answer Extraction,"Information retrieval systems have typically concentrated on retrieving a set of documents which are relevant to a user's query. This paper describes a system that attempts to retrieve a much smaller section of text, namely, a direct answer to a user's question. The SMART IR system is used to extract a ranked set of passages that are relevant to the query. Entities are extracted from these passages as potential answers to the question, and ranked for plausibility according to how well their type matches the query, and according to their frequency and position in the passages. The system was evaluated at the TREC-8 question answering track: we give results and error analysis on these queries.",2000.0,"Steven P. Abney, Michael Collins, A. Singhal"
05c5960461a28f6b149c21127ebd849b32035bc3,https://www.semanticscholar.org/paper/05c5960461a28f6b149c21127ebd849b32035bc3,An Information-Processing Analysis of Graph Perception,"Abstract Recent work on graph perception has focused on the nature of the processes that operate when people decode the information represented in graphs. We began our investigations by gathering evidence that people have generic expectations about what types of information will be the major messages in various types of graphs. These graph schemata suggested how graph type and judgment type would interact to determine the speed and accuracy of quantitative information extraction. These predictions were confirmed by the finding that a comparison judgment was most accurate when the judgment required assessing position along a common scale (simple bar chart), had intermediate accuracy on length judgments (divided bar chart), and was least accurate when assessing angles (pie chart). In contrast, when the judgment was an estimate of the proportion of the whole, angle assessments (pie chart) were as accurate as position (simple bar chart) and more accurate than length (divided bar chart). Proposals for elementa...",1987.0,"David K. Simkin, R. Hastie"
033795f6cf5f16c062ff0197f9f52aa2d8adb91b,https://www.semanticscholar.org/paper/033795f6cf5f16c062ff0197f9f52aa2d8adb91b,Extracting Spatiotemporal Interest Points using Global Information,"Local spatiotemporal features or interest points provide compact but descriptive representations for efficient video analysis and motion recognition. Current local feature extraction approaches involve either local filtering or entropy computation which ignore global information (e.g. large blobs of moving pixels) in video inputs. This paper presents a novel extraction method which utilises global information from each video input so that moving parts such as a moving hand can be identified and are used to select relevant interest points for a condensed representation. The proposed method involves obtaining a small set of subspace images, which can synthesise frames in the video input from their corresponding coefficient vectors, and then detecting interest points from the subspaces and the coefficient vectors. Experimental results indicate that the proposed method can yield a sparser set of interest points for motion recognition than existing methods.",2007.0,"Shu-Fai Wong, R. Cipolla"
84767758b70b00cb674e807c765d73c940ea29c1,https://www.semanticscholar.org/paper/84767758b70b00cb674e807c765d73c940ea29c1,A Neural Expert System with Automated Extraction of Fuzzy If-Then Rules,This paper proposes ajuzzy neural expert system (FNES) with the following two functions: (1) Generalization of the information derived from the training data and embodiment of knowledge in the form of the fuzzy neural network; (2) Extraction of fuzzy If-Then rules with linguistic relative importance of each proposition in an antecedent (I f -part) from a trained neural network. This paper also gives a method to extract automatically fuzzy If-Then rules from the trained neural network. To prove the effectiveness and validity of the proposed fuzzy neural expert system. a fuzzy neural expert system for medical diagnosis has been developed.,1990.0,Y. Hayashi
2289bbc55457e602dd6dfece8d96633bd327bad9,https://www.semanticscholar.org/paper/2289bbc55457e602dd6dfece8d96633bd327bad9,Relevant term suggestion in interactive web search based on contextual information in query session logs,"This paper proposes an effective term suggestion approach to interactive Web search. Conventional approaches to making term suggestions involve extracting co-occurring keyterms from highly ranked retrieved documents. Such approaches must deal with term extraction difficulties and interference from irrelevant documents, and, more importantly, have difficulty extracting terms that are conceptually related but do not frequently co-occur in documents. In this paper, we present a new, effective log-based approach to relevant term extraction and term suggestion. Using this approach, the relevant terms suggested for a user query are those that co-occur in similar query sessions from search engine logs, rather than in the retrieved documents. In addition, the suggested terms in each interactive search step can be organized according to its relevance to the entire query session, rather than to the most recent single query as in conventional approaches. The proposed approach was tested using a proxy server log containing about two million query transactions submitted to search engines in Taiwan. The obtained experimental results show that the proposed approach can provide organized and highly relevant terms, and can exploit the contextual information in a user's query session to make more effective suggestions.",2003.0,"Chien-Kang Huang, Lee-Feng Chien, Yen-Jen Oyang"
985ec818b640fa697ecb77d20f9c3d529b303c06,https://www.semanticscholar.org/paper/985ec818b640fa697ecb77d20f9c3d529b303c06,A comparative study on content-based music genre classification,"Content-based music genre classification is a fundamental component of music information retrieval systems and has been gaining importance and enjoying a growing amount of attention with the emergence of digital music on the Internet. Currently little work has been done on automatic music genre classification, and in addition, the reported classification accuracies are relatively low. This paper proposes a new feature extraction method for music genre classification, DWCHs. DWCHs stands for Daubechies Wavelet Coefficient Histograms. DWCHs capture the local and global information of music signals simultaneously by computing histograms on their Daubechies wavelet coefficients. Effectiveness of this new feature and of previously studied features are compared using various machine learning classification algorithms, including Support Vector Machines and Linear Discriminant Analysis. It is demonstrated that the use of DWCHs significantly improves the accuracy of music genre classification.",2003.0,"Tao Li, Mitsunori Ogihara, Qi Li"
61a67a47e508eb20d196b768f2c80ad032924f77,https://www.semanticscholar.org/paper/61a67a47e508eb20d196b768f2c80ad032924f77,Topic extraction from news archive using TF*PDF algorithm,"Since the Web became widespread, the amount of electronically available information online, especially news archives, has proliferated and threatens to become overwhelming. We propose an information system that will extract main topics in a news archive on a weekly basis. By obtaining a weekly report, a user can know what the main news events were in the past week.",2002.0,"Khoo Khyou Bun, M. Ishizuka"
1cc263c84b85027164bd39db169f5d5959ef6822,https://www.semanticscholar.org/paper/1cc263c84b85027164bd39db169f5d5959ef6822,A hierarchical approach to wrapper induction,"With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques.",1999.0,"Ion Muslea, Steven Minton, Craig A. Knoblock"
d0846a50e729ed5f3db6725394da45965f9b63da,https://www.semanticscholar.org/paper/d0846a50e729ed5f3db6725394da45965f9b63da,Extracting social networks and contact information from email and the Web,"Abstract : We present an end-to-end system that extracts a user's social network and its members' contact information given the user's email inbox. The system identifies unique people in email, finds their Web presence, and automatically fills the fields of a contact address book using conditional random fields a type of probabilistic model well-suited for such information extraction tasks. By recursively calling itself on new people discovered on the Web, the system builds a social network with multiple degrees of separation from the user. Additionally, a set of expertise-describing keywords are extracted and associated with each person. We outline the collection statistical and learning components that enable this system, and present experimental results on the real email of two user; we also present results with a simple method of learning transfer, and discuss the capabilities of the system for address book population, expert-finding, and social network analysis.",2004.0,"A. Culotta, Ron Bekkerman, A. McCallum"
008a2291a257072f22764196a3acf0a394bf203a,https://www.semanticscholar.org/paper/008a2291a257072f22764196a3acf0a394bf203a,Representing Text Chunks,"Dividing sentences in chunks of words is a useful preprocessing step for parsing, information extraction and information retrieval. (Ramshaw and Marcus, 1995) have introduced a ""convenient"" data representation for chunking by converting it to a tagging task. In this paper we will examine seven different data representations for the problem of recognizing noun phrase chunks. We will show that the the data representation choice has a minor influence on chunking performance. However, equipped with the most suitable data representation, our memory-based learning chunker was able to improve the best published chunking results for a standard data set.",1999.0,"E. Tjong Kim Sang, Jorn Veenstra"
546eb7963454f52f2b96d53f4b5f6dc15dd89f80,https://www.semanticscholar.org/paper/546eb7963454f52f2b96d53f4b5f6dc15dd89f80,Optimized Rate-Distortion Extraction with Quality Layers,"In this paper, we present the concept of quality layers that has been introduced in scalable video coding (SVC) amendment of MPEG4-AVC. The quality layers are used to evaluate and signal the impact on rate and distortion of the various enhancement information pieces. Using this quality layers information allows to gain up to 0.5 dB with respect to the basic standard software verification model (SVM) extractor that was proposed initially in SVC. Thanks to the signaling of this information in the header of the network abstraction layer (NAL) units, the rate adaptation can be performed with a simple parser, e.g. for rate adaptation in an intelligent network node.",2006.0,"Isabelle Amonou, Nathalie Cammas, S. Kervadec, S. Pateux"
5cdf771754fa0b58567d33693eaee82273f654b6,https://www.semanticscholar.org/paper/5cdf771754fa0b58567d33693eaee82273f654b6,BioRAT: extracting biological information from full-length papers,"MOTIVATION
Converting the vast quantity of free-format text found in journals into a concise, structured format makes the researcher's quest for information easier. Recently, several information extraction systems have been developed that attempt to simplify the retrieval and analysis of biological and medical data. Most of this work has used the abstract alone, owing to the convenience of access and the quality of data. Abstracts are generally available through central collections with easy direct access (e.g. PubMed). The full-text papers contain more information, but are distributed across many locations (e.g. publishers' web sites, journal web sites and local repositories), making access more difficult. In this paper, we present BioRAT, a new information extraction (IE) tool, specifically designed to perform biomedical IE, and which is able to locate and analyse both abstracts and full-length papers. BioRAT is a Biological Research Assistant for Text mining, and incorporates a document search ability with domain-specific IE.


RESULTS
We show first, that BioRAT performs as well as existing systems, when applied to abstracts; and second, that significantly more information is available to BioRAT through the full-length papers than via the abstracts alone. Typically, less than half of the available information is extracted from the abstract, with the majority coming from the body of each paper. Overall, BioRAT recalled 20.31% of the target facts from the abstracts with 55.07% precision, and achieved 43.6% recall with 51.25% precision on full-length papers.",2004.0,"D. Corney, B. Buxton, W. Langdon, David T. Jones"
800f154526751e6a80d01bf0c132bb78cf61ef11,https://www.semanticscholar.org/paper/800f154526751e6a80d01bf0c132bb78cf61ef11,Personal Authentication Using Hand Vein Triangulation and Knuckle Shape,"This paper presents a new approach to authenticate individuals using triangulation of hand vein images and simultaneous extraction of knuckle shape information. The proposed method is fully automated and employs palm dorsal hand vein images acquired from the low-cost, near infrared, contactless imaging. The knuckle tips are used as key points for the image normalization and extraction of region of interest. The matching scores are generated in two parallel stages: (i) hierarchical matching score from the four topologies of triangulation in the binarized vein structures and (ii) from the geometrical features consisting of knuckle point perimeter distances in the acquired images. The weighted score level combination from these two matching scores are used to authenticate the individuals. The achieved experimental results from the proposed system using contactless palm dorsal-hand vein images are promising (equal error rate of 1.14%) and suggest more user friendly alternative for user identification.",2009.0,"Ajay Kumar, K. Prathyusha"
b8c76e11f45be209c83c481a603e9e510af12426,https://www.semanticscholar.org/paper/b8c76e11f45be209c83c481a603e9e510af12426,Photonic Maxwell's Demon.,We report an experimental realization of Maxwell's demon in a photonic setup. We show that a measurement at the few-photons level followed by a feed-forward operation allows the extraction of work from intense thermal light into an electric circuit. The interpretation of the experiment stimulates the derivation of an equality relating work extraction to information acquired by measurement. We derive a bound using this relation and show that it is in agreement with the experimental results. Our work puts forward photonic systems as a platform for experiments related to information in thermodynamics.,2015.0,"M. Vidrighin, M. Vidrighin, O. Dahlsten, M. Barbieri, Myungshik S. Kim, V. Vedral, V. Vedral, I. Walmsley"
697fa7b5aa11ba146158ab44105ac4ac80cece20,https://www.semanticscholar.org/paper/697fa7b5aa11ba146158ab44105ac4ac80cece20,Inferring the directionality of coupling with conditional mutual information.,"Uncovering the directionality of coupling is a significant step in understanding drive-response relationships in complex systems. In this paper, we discuss a nonparametric method for detecting the directionality of coupling based on the estimation of information theoretic functionals. We consider several different methods for estimating conditional mutual information. The behavior of each estimator with respect to its free parameter is shown using a linear model where an analytical estimate of conditional mutual information is available. Numerical experiments in detecting coupling directionality are performed using chaotic oscillators, where the influence of the phase extraction method and relative frequency ratio is investigated.",2008.0,"M. Vejmelka, M. Paluš"
f2124f8995a9cdf4e168baba426472d2d811c0f1,https://www.semanticscholar.org/paper/f2124f8995a9cdf4e168baba426472d2d811c0f1,A Novel Use of Statistical Parsing to Extract Information from Text,"Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.",2000.0,"Scott Miller, Heidi Fox, L. Ramshaw, R. Weischedel"
275e22d93e3fd6c4dea578fd365064fd99a852e3,https://www.semanticscholar.org/paper/275e22d93e3fd6c4dea578fd365064fd99a852e3,The C Information Abstraction System,"A system for analyzing program structures is described. The system extracts relational information from C programs according to a conceptual model and stores the information in a database. It is shown how several interesting software tasks can be performed by using the relational views. These tasks include generation of graphical views, subsystem extraction, program layering, dead code elimination and binding analysis. >",1990.0,"Y. Chen, Michael Y. Nishimoto, C. Ramamoorthy"
0dad0da221dea30c3a0e90c45a0699aeb850af49,https://www.semanticscholar.org/paper/0dad0da221dea30c3a0e90c45a0699aeb850af49,Using Semantic Roles to Improve Question Answering,"Shallow semantic parsing, the automatic identification and labeling of sentential constituents, has recently received much attention. Our work examines whether semantic role information is beneficial to question answering. We introduce a general framework for answer extraction which exploits semantic role annotations in the FrameNet paradigm. We view semantic role assignment as an optimization problem in a bipartite graph and answer extraction as an instance of graph matching. Experimental results on the TREC datasets demonstrate improvements over state-of-the-art models.",2007.0,"Dan Shen, Mirella Lapata"
64d8482fc94cc344429134e6c541579b59932b7c,https://www.semanticscholar.org/paper/64d8482fc94cc344429134e6c541579b59932b7c,To Link or Not to Link? A Study on End-to-End Tweet Entity Linking,"Information extraction from microblog posts is an important task, as today microblogs capture an unprecedented amount of information and provide a view into the pulse of the world. As the core component of information extraction, we consider the task of Twitter entity linking in this paper. In the current entity linking literature, mention detection and entity disambiguation are frequently cast as equally important but distinct problems. However, in our task, we find that mention detection is often the performance bottleneck. The reason is that messages on micro-blogs are short, noisy and informal texts with little context, and often contain phrases with ambiguous meanings. To rigorously address the Twitter entity linking problem, we propose a structural SVM algorithm for entity linking that jointly optimizes mention detection and entity disambiguation as a single end-to-end task. By combining structural learning and a variety of firstorder, second-order, and context-sensitive features, our system is able to outperform existing state-of-the art entity linking systems by 15% F1.",2013.0,"Stephen D. Guo, Ming-Wei Chang, Emre Kıcıman"
130cbc5e907cccbd0fcd4f9138bc9886dc3217d7,https://www.semanticscholar.org/paper/130cbc5e907cccbd0fcd4f9138bc9886dc3217d7,Learning to Extract Text-Based Information from the World Wide Web,"There is a wealth of information to be mined from narrative text on the World Wide Web. Unfortunately, standard natural language processing (NLP) extraction techniques expect full, grammatical sentences, and perform poorly on the choppy sentence fragments that are often found on web pages. 
 
This paper1 introduces Webfoot, a preprocessor that parses web pages into logically coherent segments based on page layout cues. Output from Webfoot is then passed on to CRYSTAL, an NLP system that learns text extraction rules from example. Webfoot and CRYSTAL transform the text into a formal representation that is equivalent to relational database entries. This is a necessary first step for knowledge discovery and other automated analysis of free text.",1997.0,S. Soderland
7a7cf891db710dda08cdff0b0f60d32fb563889c,https://www.semanticscholar.org/paper/7a7cf891db710dda08cdff0b0f60d32fb563889c,A Multilevel Context-Based System for Classification of Very High Spatial Resolution Images,"This paper proposes a novel pixel-based system for the supervised classification of very high geometrical (spatial) resolution images. This system is aimed at obtaining accurate and reliable maps both by preserving the geometrical details in the images and by properly considering the spatial-context information. It is made up of two main blocks: 1) a novel feature-extraction block that, extending and developing some concepts previously presented in the literature, adaptively models the spatial context of each pixel according to a complete hierarchical multilevel representation of the scene and 2) a classifier, based on support vector machines (SVMs), capable of analyzing hyperdimensional feature spaces. The choice of adopting an SVM-based classification architecture is motivated by the potentially large number of parameters derived from the contextual feature-extraction stage. Experimental results and comparisons with a standard technique developed for the analysis of very high spatial resolution images confirm the effectiveness of the proposed system",2006.0,"L. Bruzzone, L. Carlin"
c10d759cf4c3fe4439ada038150aadf21a23ef54,https://www.semanticscholar.org/paper/c10d759cf4c3fe4439ada038150aadf21a23ef54,Automatic Detection of Causal Relations for Question Answering,"Causation relations are a pervasive feature of human language. Despite this, the automatic acquisition of causal information in text has proved to be a difficult task in NLP. This paper provides a method for the automatic detection and extraction of causal relations. We also present an inductive learning approach to the automatic discovery of lexical and semantic constraints necessary in the disambiguation of causal relations that are then used in question answering. We devised a classification of causal questions and tested the procedure on a QA system.",2003.0,Roxana Girju
18a3b4bc6644b0ac62bcb5d6c812917d742e8f20,https://www.semanticscholar.org/paper/18a3b4bc6644b0ac62bcb5d6c812917d742e8f20,Ontology-Driven Information Integration,"The integration of information of different kinds, such as spatial and alphanumeric, at different levels of detail is a challenge. While a solution is not reached, it is widely recognized that the need to integrate information is so pressing that it does not matter if detail is lost, as long as integration is achieved. This paper shows the potential for extraction of different levels of information, within the framework of ontology-driven geographic information systems.",2000.0,"F. Fonseca, M. Egenhofer, C. Davis"
0de77c11601a5d3e898423698f0b233f239ddc06,https://www.semanticscholar.org/paper/0de77c11601a5d3e898423698f0b233f239ddc06,The island status of clausal complements: Evidence in favor of an information structure explanation,"Abstract The present paper provides evidence that suggests that speakers determine which constructions can be combined, at least in part, on the basis of the compatibility of the information structure properties of the constructions involved. The relative “island” status of the following sentence complement constructions are investigated: “bridge” verb complements, manner-of-speaking verb complements and factive verb complements. Questionnaire data is reported that demonstrates a strong correlation between acceptability judgments and a negation test used to operationalize the notion of “backgroundedness”. Semantic similarity of the main verbs involved to think or say (the two verbs that are found most frequently in long-distance extraction from complement clauses) did not account for any variance; this finding undermines an account which might predict acceptability by analogy to a fixed formula involving think or say. While the standard subjacency account also does not predict the results, the findings strongly support the idea that constructions act as islands to wh-extraction to the degree that they are backgrounded in discourse.",2008.0,"Ben Ambridge, Adele E. Goldberg"
efc70023386afa8fc502a87f9b2c5e74ff2b5648,https://www.semanticscholar.org/paper/efc70023386afa8fc502a87f9b2c5e74ff2b5648,Semantic relations in information science,"This chapter examines the nature of semantic relations and their main applications in information science. The nature and types of semantic relations are discussed from the perspectives of linguistics and psychology. An overview of the semantic relations used in knowledge structures such as thesauri and ontologies are provided, as well as the main techniques used in the automatic extraction of semantic relations from text. The chapter then reviews the use of semantic relations in information extraction, information retrieval, question-answering and automatic text summarization applications.",2006.0,"Christopher S. G. Khoo, Jin-Cheon Na"
8fb75ebe870abebfd526cd6b9a8d1badceb9b550,https://www.semanticscholar.org/paper/8fb75ebe870abebfd526cd6b9a8d1badceb9b550,Canonicalizing Open Knowledge Bases,"Open information extraction approaches have led to the creation of large knowledge bases from the Web. The problem with such methods is that their entities and relations are not canonicalized, leading to redundant and ambiguous facts. For example, they may store {Barack Obama, was born, Honolulu and {Obama, place of birth, Honolulu}. In this paper, we present an approach based on machine learning methods that can canonicalize such Open IE triples, by clustering synonymous names and phrases. We also provide a detailed discussion about the different signals, features and design choices that influence the quality of synonym resolution for noun phrases in Open IE KBs, thus shedding light on the middle ground between ""open"" and ""closed"" information extraction systems.",2014.0,"Luis Galárraga, Geremy Heitz, K. Murphy, Fabian M. Suchanek"
e421fa069c6c76e767c94a32f7aa8de09ccd679a,https://www.semanticscholar.org/paper/e421fa069c6c76e767c94a32f7aa8de09ccd679a,Hyperspectral Remote Sensing: Principles and Applications,Preface History and Description of Hyperspectral Imaging Spectral Radiometry Imaging Spectrometers: Operational Considerations Hyperspectral Remote Sensing and the Atmosphere Information Extraction from Optical Image Data Hyperspectral and Ultraspectral Information Extraction Approaches Agricultural Applications Environmental Applications Forestry Applications Geology Applications Index,2007.0,"M. Borengasser, W. Hungate, R. Watkins"
63b3b84056a34277214265fefc3353de5ebc13d6,https://www.semanticscholar.org/paper/63b3b84056a34277214265fefc3353de5ebc13d6,Impacts of hunting on mammals in African tropical moist forests: a review and synthesis,"1Available information on the consumption of wild meat in West and Central Africa is reviewed. We show that mammals are the prime source of bushmeat, and that ungulates and rodents make up the highest proportion of biomass extracted. 
2We present data on current knowledge of extraction patterns of wild mammals in West and Central Africa, and evidence that at current off-take levels, within the range states, mammals as bushmeat are being depleted on an unprecedented scale. Extraction rates are orders of magnitude higher there than in comparable ecosystems like the Amazon, and much less likely to be sustainable. 
3However, basic knowledge of the biology of harvestable tropical moist forest mammals, and the consequences of hunting on mammalian communities, which permits accurate estimation of maximal production rate (the excess of growth over replacement rate), is largely unavailable, and this hinders estimation of hunting quotas and sustainability. Comparisons are made with the existing information available on Amazon basin mammals and hunting patterns reported there.",2009.0,"J. Fa, David Brown"
65c523047f680b4068a657255a3e515c0452a3f3,https://www.semanticscholar.org/paper/65c523047f680b4068a657255a3e515c0452a3f3,Spatial information retrieval from remote-sensing images. I. Information theoretical perspective,"Automatic interpretation of remote-sensing (RS) images and the growing interest for query by image content from large remote-sensing image archives rely on the ability and robustness of information extraction from observed data. In Parts I and II of this article, the authors turn the attention to the modern Bayesian way of thinking and introduce a pragmatic approach to extract structural information from RS images by selecting from a library of a priori models those which best explain the structures within an image. Part I introduces the Bayesian approach and defines the information extraction as a two-level procedure: 1) model fitting, which is the incertitude alleviation over the model parameters, and 2) model selection, which is the incertitude alleviation over the class of models. The superiority of the Bayesian results is commented from an information theoretical perspective. The theoretical assay concludes with the proposal of a new systematic method for scene understanding from RS images: search for the scene that best explains the observed data. The method is demonstrated for high accuracy restoration of synthetic aperture radar (SAR) images with emphasis on new optimization algorithms for simultaneous model selection and parameter estimation. Examples are given for three families of Gibbs random fields (GRF) used as prior model libraries. Based on the Bayesian approach, a new method for optimal joint scale and model selection is demonstrated. Examples are given using a nested family of GRFs utilized as prior models for information extraction with applications both to SAR and optical images.",1998.0,"M. Datcu, K. Seidel, M. Walessa"
05788738c5a5d71ea4634f3e8589821dfb94195e,https://www.semanticscholar.org/paper/05788738c5a5d71ea4634f3e8589821dfb94195e,Towards a realtime Twitter analysis during crises for operational crisis management,"Today's crises attract great attention on social media, from local and distant citizens as well as from news media. This study investigates the possibilities of real-time and automated analysis of Twitter messages during crises. The analysis was performed through application of an information extraction tool to nearly 97,000 tweets that were published shortly before, during and after a storm hit the Pukkelpop 2011 festival in Belgium. As soon as the storm hit the festival tweet activity increased exponentially, peaking at 576 tweets per minute. The extraction tool enabled analyzing tweets through predefined (geo)graphical displays, message content filters (damage, casualties) and tweet type filters (e.g., retweets). Important topics that emerged were 'early warning tweets', 'rumors' and the 'self-organization of disaster relief' on Twitter. Results indicate that automated filtering of information provides valuable information for operational response and crisis communication. Steps for further research are discussed. © 2012 ISCRAM. Environmental Systems Research Institute, Inc. (ESRI)",2012.0,"T. Terpstra, R.J.P. Stronkman, A. D. Vries, G. Paradies"
01f4536d79cfa435066fd4d7980a2107a306975f,https://www.semanticscholar.org/paper/01f4536d79cfa435066fd4d7980a2107a306975f,Towards a Core Ontology for Information Integration,"In this paper, we argue that a core ontology is one of the key building blocks necessary to enable the scalable assimilation of information from diverse sources. A complete and extensible ontology that expresses the basic concepts that are common across a variety of domains and can provide the basis for specialization into domain-specific concepts and vocabularies, is essential for well-defined mappings between domain-specific knowledge representations (i.e., metadata vocabularies) and the subsequent building of a variety of services such as cross-domain searching, browsing, data mining and knowledge extraction. This paper describes the results of a series of three workshops held in 2001 and 2002 which brought together representatives from the cultural heritage and digital library communities with the goal of harmonizing their knowledge perspectives and producing a core ontology. The knowledge perspectives of these two communities were represented by the CIDOC/CRM [31], an ontology for information exchange in the cultural heritage and museum community, and the ABC ontology [33], a model for the exchange and integration of digital library information. This paper describes the mediation process between these two different knowledge biases and the results of this mediation - the harmonization of the ABC and CIDOC/CRM ontologies, which we believe may provide a useful basis for information integration in the wider scope of the involved communities.",2003.0,"M. Doerr, J. Hunter, C. Lagoze"
2b013ebac519c005f91c18284eb2351830507f6e,https://www.semanticscholar.org/paper/2b013ebac519c005f91c18284eb2351830507f6e,Generating Natural Language Summaries from Multiple On-Line Sources,"We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.",1998.0,"Dragomir R. Radev, K. McKeown"
cd8e51ec7c09f3f0b356c64547dccc9326c64a83,https://www.semanticscholar.org/paper/cd8e51ec7c09f3f0b356c64547dccc9326c64a83,Extracting metadata for spatially-aware information retrieval on the internet,"This paper presents methods used to extract geospatial information from web pages for use in SPIRIT, a new Geographic Information Retrieval (GIR) system for the web. The resulting geospatial markup tools have been used to annotate around 900,000 web pages taken from a 1TB web crawl, focused on regions in the UK, France, Germany and Switzerland. This paper discusses a versatile geo-parsing tool for extracting spatial metadata based upon the GATE Information Extraction (IE) system, and a simple geo-coding program based on default sense to assign spatial coordinates to extracted locations. A preliminary analysis of markup accuracy for geo-parsing and geo-coding is provided, and an initial statistical and geographical analysis of the SPIRIT collection presented.",2005.0,Paul D. Clough
a8978cf911ee385dc97dbe982b01c57ccd72494a,https://www.semanticscholar.org/paper/a8978cf911ee385dc97dbe982b01c57ccd72494a,Formal Ontology in Information Systems,"Research on ontology is becoming increasingly widespread in the com- puter science community, and its importance is being recognized in a multiplicity of research fields and application areas, including knowledge engineering, database design and integration, information retrieval and extraction. We shall use the generic term ""in- formation systems"", in its broadest sense, to collectively refer to these application per- spectives. We argue in this paper that so-called ontologies present their own methodo- logical and architectural peculiarities: on the methodological side, their main peculiar- ity is the adoption of a highly interdisciplinary approach, while on the architectural side the most interesting aspect is the centrality of the role they can play in an infor- mation system, leading to the perspective of ontology-driven information systems.",1998.0,"Nicola Guarino, Corso Stati Uniti"
7eb6b3c556755146897fb06524e66af3da8af572,https://www.semanticscholar.org/paper/7eb6b3c556755146897fb06524e66af3da8af572,Fully automatic wrapper generation for search engines,"When a query is submitted to a search engine, the search engine returns a dynamically generated result page containing the result records, each of which usually consists of a link to and/or snippet of a retrieved Web page. In addition, such a result page often also contains information irrelevant to the query, such as information related to the hosting site of the search engine and advertisements. In this paper, we present a technique for automatically producing wrappers that can be used to extract search result records from dynamically generated result pages returned by search engines. Automatic search result record extraction is very important for many applications that need to interact with search engines such as automatic construction and maintenance of metasearch engines and deep Web crawling. The novel aspect of the proposed technique is that it utilizes both the visual content features on the result page as displayed on a browser and the HTML tag structures of the HTML source file of the result page. Experimental results indicate that this technique can achieve very high extraction accuracy.",2005.0,"Hongkun Zhao, W. Meng, Zonghuan Wu, Vijay V. Raghavan, Clement T. Yu"
2242c94d96a8f8f440d96e78b6b8f893d9d28e03,https://www.semanticscholar.org/paper/2242c94d96a8f8f440d96e78b6b8f893d9d28e03,Book Recommending Using Text Categorization with Extracted Information,"Content-based recommender systems suggest documents, items, and services to users based on learning a prole of the user from rated examples containing information about the given items. Text categorization methods are very useful for this task but generally rely on unstructured text. We have developed a bookrecommending system that utilizes semi-structured information about items gathered from the web using simple information extraction techniques. Initial experimental results demonstrate that this approach can produce fairly accurate recommendations.",1998.0,Raymond J. Mooney and Paul N. Bennett and Loriene Roy
09ee6f19142729fa2b3dd314483d110a72381f3f,https://www.semanticscholar.org/paper/09ee6f19142729fa2b3dd314483d110a72381f3f,Direct and Indirect Sale of Information,"The authors compare two methods for a monopolist to sell information to traders in a financial market. In a direct sale, information buyers observe versions of the seller's signal while in an indirect sale the seller sells shares in a portfolio based on his private information. It is shown that, when traders are identical and pricing is linear, there is a trade-off between optimal surplus extraction that is possible under direct sale and more effective control of the usage of information that is possible under indirect sale. The optimal selling method depends on how much information is revealed by equilibrium prices. Copyright 1990 by The Econometric Society.",1990.0,"Anat R. Admati, P. Pfleiderer"
019ef56f6b2147d0fa05a19e2972e6cecc73d956,https://www.semanticscholar.org/paper/019ef56f6b2147d0fa05a19e2972e6cecc73d956,The paraphrase search assistant: terminological feedback for iterative information seeking,"We present a new linguistic approach to the construction of terminological feedback for use in interactive query refinement. The method exploits the tendency for key domain concepts within result sets to participate in families of semantically related lexical compounds. We outline an algorithm for computing a ranked list of result set “themes” and describe a web application, the Paraphrase Search Assistant, designed to make use of the theme extraction algorithm to support a recognition-based, iterative information seeking dialog.",1999.0,"Peter G. Anick, S. Tipirneni"
5a1758531c9104b2fbe8ba9d9e55d91f80c9fccc,https://www.semanticscholar.org/paper/5a1758531c9104b2fbe8ba9d9e55d91f80c9fccc,Natural language processing,"The abundant volume of natural language text in the connected world, though having a large content of knowledge, but it is becoming increasingly difficult to disseminate it by a human to discover the knowledge/wisdom in it, specifically within any given time limits. The automated NLP is aimed to do this job effectively and with accuracy, like a human does it (for a limited of amount text). This chapter presents the challenges of NLP, progress so far made in this field, NLP applications, components of NLP, and grammar of English language—the way machine requires it. In addition, covers the specific areas like probabilistic parsing, ambiguities and their resolution, information extraction, discourse analysis, NL question-answering, commonsense interfaces, commonsense thinking and reasoning, causal-diversity, and various tools for NLP. Finally, the chapter summary, and a set of relevant exercises are presented.",2005.0,G. Chowdhury
ddd1376ccfba7bc8116e3908b5eb0b42cb2d4429,https://www.semanticscholar.org/paper/ddd1376ccfba7bc8116e3908b5eb0b42cb2d4429,Classification of hyperspectral image based on deep belief networks,"Generally, dimensionality reduction methods, such as Principle Component Analysis (PCA) and Negative Matrix Factorization (NMF), are always applied as the preprocessing part in hyperspectral image classification so as to classify the constituent elements of every pixel in the scene efficiently. The results, however, would suffer the loss of detailed information inevitably. In this paper, deep learning frameworks, restricted Boltzmann machine (RBM) model and its deep structure deep belief networks (DBN), are introduced in hyperspectral image processing as the feature extraction and classification approach. The experiments are conducted on an airborne hyperspectral image. Further in the experiments, spatial-spectral classification is also practiced. Meanwhile, SVM with and without some classical feature extraction methods adopting before classification are employed as comparison. The results show the superior performance of the proposed approach.",2014.0,"Tong Li, Junping Zhang, Ye Zhang"
79f21f3dc4e01c1830b3d7f5cf10e170dbc948c1,https://www.semanticscholar.org/paper/79f21f3dc4e01c1830b3d7f5cf10e170dbc948c1,Using Prerequisites to Extract Concept Maps fromTextbooks,"We present a framework for constructing a specific type of knowledge graph, a concept map from textbooks. Using Wikipedia, we derive prerequisite relations among these concepts. A traditional approach for concept map extraction consists of two sub-problems: key concept extraction and concept relationship identification. Previous work for the most part had considered these two sub-problems independently. We propose a framework that jointly optimizes these sub-problems and investigates methods that identify concept relationships. Experiments on concept maps that are manually extracted in six educational areas (computer networks, macroeconomics, precalculus, databases, physics, and geometry) show that our model outperforms supervised learning baselines that solve the two sub-problems separately. Moreover, we observe that incorporating textbook information helps with concept map extraction.",2016.0,"Shuting Wang, Alexander Ororbia, Zhaohui Wu, Kyle Williams, Chen Liang, B. Pursel, C. Lee Giles"
1ba84863e2685c45c5c41953444d9383dc7aa13b,https://www.semanticscholar.org/paper/1ba84863e2685c45c5c41953444d9383dc7aa13b,Efficient Support Vector Classifiers for Named Entity Recognition,"Named Entity (NE) recognition is a task in which proper nouns and numerical information are extracted from documents and are classified into categories such as person, organization, and date. It is a key technology of Information Extraction and Open-Domain Question Answering. First, we show that an NE recognizer based on Support Vector Machines (SVMs) gives better scores than conventional systems. However, off-the-shelf SVM classifiers are too inefficient for this task. Therefore, we present a method that makes the system substantially faster. This approach can also be applied to other similar tasks such as chunking and part-of-speech tagging. We also present an SVM-based feature selection method and an efficient training method.",2002.0,"Hideki Isozaki, H. Kazawa"
1b9251a023186d4af2c1af92483bb74d61340f10,https://www.semanticscholar.org/paper/1b9251a023186d4af2c1af92483bb74d61340f10,Information from SAR images,"Previously the production of focused, undistorted, synthetic-aperture radar (SAR) images in a routine way has been described. The means by which information about the scene can be extracted from the resultant images is discussed. The importance of prior knowledge about the form of the scene for interpreting the image data is shown. Different types of model are introduced and their implications for information extraction are examined.",1991.0,C. Oliver
1b04cc85965ce3e365e9d1522810b4b69b07f479,https://www.semanticscholar.org/paper/1b04cc85965ce3e365e9d1522810b4b69b07f479,Noun Phrase Analysis in Large Unrestricted Text for Information Retrieval,"Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text. This paper reports on the application of a few simple, yet robust and efficient noun-phrase analysis techniques to create better indexing phrases for information retrieval. In particular, we describe an hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics. Results of experiments show that indexing based on such extracted subcompound improves both recall and precision in an information retrieval system. The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction.",1996.0,"David A. Evans, ChengXiang Zhai"
fd268d35d3e3e4d368e94050163fba9ede42ffe2,https://www.semanticscholar.org/paper/fd268d35d3e3e4d368e94050163fba9ede42ffe2,The GENIA corpus: an annotated research abstract corpus in molecular biology domain,"With the information overload in genome-related field, there is an increasing need for natural language processing technology to extract information from literature and various attempts of information extraction using NLP has been being made. We are developing the necessary resources including domain ontology and annotated corpus from research abstracts in MEDLINE database (GENIA corpus). We are building the ontology and the corpus simultaneously, using each other. In this paper we report on our new corpus, its ontological basis, annotation scheme, and statistics of annotated objects. We also describe the tools used for corpus annotation and management.",2002.0,"Tomoko Ohta, Yuka Tateisi, Jin-Dong Kim"
8eff27c8fb1fd7cb1d42be1a8deee99814eba620,https://www.semanticscholar.org/paper/8eff27c8fb1fd7cb1d42be1a8deee99814eba620,Hydrologic and Hydraulic Modeling Support with Geographic Information Systems,"This edited collection deals with the international issue of conserving and allocating water as the world's population continues to grow dramatically. Hydrologic and Hydraulic Modeling Support with Geographic Information Systems discusses applications such as watershed delineation, topographic characteristic extraction, and floodplain extent determination and provides an informed basis for water resource professionals to make sound decisions.",2000.0,"D. Maidment, D. Djokic"
dadef6bb271beda9de0f12cce0a4018cfff3ee42,https://www.semanticscholar.org/paper/dadef6bb271beda9de0f12cce0a4018cfff3ee42,Interfaces for End-User Information Seeking,"Essential features of interfaces to support end-user information seeking are discussed and illustrated. Examples of interfaces to support the following basic information-seeking functions are presented: problem definition, source selection, problem articulation, examination of results, and information extraction. It is argued that present interfaces focus on problem articulation and examination of results functions, and research and development are needed to support the problem definition and information extraction functions. General recommendations for research on interfaces to support end-user information seeking include: attention to multimedia information sources, development of interfaces that integrate information-seeking functions, support for collaborative information seeking, use of multiple input/output devices in parallel, integration of advanced information retrieval techniques in systems for end users, and development of adaptable interfaces to meet individual difference and multicultural needs.",1992.0,G. Marchionini
62c9b4c353afad20f9a1f1d59060066d5aceb708,https://www.semanticscholar.org/paper/62c9b4c353afad20f9a1f1d59060066d5aceb708,Jedi: extracting and synthesizing information from the Web,"Jedi (Java based Extraction and Dissemination of Information) is a lightweight tool for the creation of wrappers and mediators to extract, combine, and reconcile information from several independent information sources. For wrappers it uses attributed grammars, which are evaluated with a fault-tolerant parsing strategy to cope with ambiguous grammars and irregular sources. For mediation it uses a simple generic object-model that can be extended with Java-libraries for specific models such as HTML, XML or the relational model. This paper describes the architecture of Jedi, and then focuses on Jedi's wrapper generator.",1998.0,"Gerald Huck, Péter Fankhauser, K. Aberer, E. Neuhold"
5623a21b4a954989702dc4d6718fd4b3d0c286df,https://www.semanticscholar.org/paper/5623a21b4a954989702dc4d6718fd4b3d0c286df,A Comprehensive Benchmark of Kernel Methods to Extract Protein–Protein Interactions from Literature,"The most important way of conveying new findings in biomedical research is scientific publication. Extraction of protein–protein interactions (PPIs) reported in scientific publications is one of the core topics of text mining in the life sciences. Recently, a new class of such methods has been proposed - convolution kernels that identify PPIs using deep parses of sentences. However, comparing published results of different PPI extraction methods is impossible due to the use of different evaluation corpora, different evaluation metrics, different tuning procedures, etc. In this paper, we study whether the reported performance metrics are robust across different corpora and learning settings and whether the use of deep parsing actually leads to an increase in extraction quality. Our ultimate goal is to identify the one method that performs best in real-life scenarios, where information extraction is performed on unseen text and not on specifically prepared evaluation data. We performed a comprehensive benchmarking of nine different methods for PPI extraction that use convolution kernels on rich linguistic information. Methods were evaluated on five different public corpora using cross-validation, cross-learning, and cross-corpus evaluation. Our study confirms that kernels using dependency trees generally outperform kernels based on syntax trees. However, our study also shows that only the best kernel methods can compete with a simple rule-based approach when the evaluation prevents information leakage between training and test corpora. Our results further reveal that the F-score of many approaches drops significantly if no corpus-specific parameter optimization is applied and that methods reaching a good AUC score often perform much worse in terms of F-score. We conclude that for most kernels no sensible estimation of PPI extraction performance on new text is possible, given the current heterogeneity in evaluation data. Nevertheless, our study shows that three kernels are clearly superior to the other methods.",2010.0,"D. Tikk, Philippe E. Thomas, Peter Palaga, J. Hakenberg, U. Leser"
43bf37c6d9ceef3a15b80bddc19eff96034bbaeb,https://www.semanticscholar.org/paper/43bf37c6d9ceef3a15b80bddc19eff96034bbaeb,Database tomography for information retrieval,"Database tomography is an information extraction and analysis system which operates on textual databases. Its primary use to date has been to identify pervasive technical thrusts and themes, and the interrelationships among these themes and sub-themes, which are intrinsic to large textual databases. Its two main algorithmic components are multiword phrase frequency analysis and phrase proximity analysis. This paper shows how database tomography can be used to enhance information retrieval from large textual databases through the newly developed process of simulated nucleation. The principles of simulated nucleation are presented, and the advantages for information retrieval are delineated. An application is described of developing, from Science Citation Index and Engineering Compendex, a database of journal articles focused on near-Earth space science and technology.",1997.0,"R. Kostoff, H. Eberhart, D. R. Toothman"
8245f6099f547008522ebbe6fb813d8132085746,https://www.semanticscholar.org/paper/8245f6099f547008522ebbe6fb813d8132085746,CRYSTAL: Inducing a Conceptual Dictionary,"One of the central knowledge sources of an information extraction (IE) system IS a dictionary of linguistic patterns that can be used to identify references to relevant information in a text Automatic creation of conceptual dictionaries is important for portability and scalability of an IE system This paper describes CRYSTAL, a system which automatically induces a dictionary of ""concept-node definitions"" sufficient to identify relevant information from a training corpus Each of these concept-node definitions is generalized as far as possible without producing errors, so that a minimum number of dictionary entries cover the positive training instances Because it tests the accuracy of each proposed definition, CRYSTAL can often surpass human intuitions in creating reliable extraction rules.",1995.0,"S. Soderland, David Fisher, J. Aseltine, W. Lehnert"
fbe501b2ce06473e6f42123526cd44e688e361d6,https://www.semanticscholar.org/paper/fbe501b2ce06473e6f42123526cd44e688e361d6,Identifying Expressions of Opinion in Context,"While traditional information extraction systems have been built to answer questions about facts, subjective information extraction systems will answer questions about feelings and opinions. A crucial step towards this goal is identifying the words and phrases that express opinions in text. Indeed, although much previous work has relied on the identification of opinion expressions for a variety of sentiment-based NLP tasks, none has focused directly on this important supporting task. Moreover, none of the proposed methods for identification of opinion expressions has been evaluated at the task that they were designed to perform. We present an approach for identifying opinion expressions that uses conditional random fields and we evaluate the approach at the expression-level using a standard sentiment corpus. Our approach achieves expression-level performance within 5% of the human interannotator agreement.",2007.0,"Eric Breck, Yejin Choi, Claire Cardie"
5534b1771b27979fb235e4a6bdce8109a20d0648,https://www.semanticscholar.org/paper/5534b1771b27979fb235e4a6bdce8109a20d0648,Extracting information masked by the chaotic signal of a time-delay system.,"We further develop the method proposed by Bezruchko et al. [Phys. Rev. E 64, 056216 (2001)] for the estimation of the parameters of time-delay systems from time series. Using this method we demonstrate a possibility of message extraction for a communication system with nonlinear mixing of information signal and chaotic signal of the time-delay system. The message extraction procedure is illustrated using both numerical and experimental data and different kinds of information signals.",2002.0,"V. Ponomarenko, M. Prokhorov"
4cb1021ea513debb7ce4fc3890477b83ac981d62,https://www.semanticscholar.org/paper/4cb1021ea513debb7ce4fc3890477b83ac981d62,Interfaces for end‐user information seeking,"Essential features of interfaces to support end-user information seeking are discussed and illustrated. Examples of interfaces to support the following basic information-seeking functions are presented: problem definition, source selection, problem articulation, examination of results, and information extraction. It is argued that present interfaces focus on problem articulation and examination of results functions, and research and development are needed to support the problem definition and information extraction functions. General recommendations for research on interfaces to support end-user information seeking include: attention to multimedia information sources, development of interfaces that integrate information-seeking functions, support for collaborative information seeking, use of multiple input/output devices in parallel, integration of advanced information retrieval techniques in systems for end users, and development of adaptable interfaces to meet individual difference and multicultural needs.",1992.0,G. Marchionini
5147f98e5090f4daccea7d871b92223ace096ef7,https://www.semanticscholar.org/paper/5147f98e5090f4daccea7d871b92223ace096ef7,How do World-Class Cricket Batsmen Anticipate a Bowler's Intention?,"Four experiments are reported that examine the ability of cricket batsmen of different skill levels to pick up advance information to anticipate the type and length of balls bowled by swing and spin bowlers. The information available upon which to make the predictive judgements was manipulated through a combination of temporal occlusion of the display and selective occlusion or presentation of putative anticipatory cues. In addition to a capability to pick up advance information from the same cues used by intermediate and low-skilled players, highly skilled players demonstrated the additional, unique capability to pick up advance information from some specific early cues (especially bowling hand and arm cues) to which the less skilled players were not attuned. The acquisition of expert perceptual-motor skill appears to involve not only refinement of information extraction but also progression to the use of earlier, kinematically relevant sources of information.",2006.0,"Sean Müller, B. Abernethy, D. Farrow"
8c0918f9801c7053ab7b750eefcf311488b47114,https://www.semanticscholar.org/paper/8c0918f9801c7053ab7b750eefcf311488b47114,Fast and automatic video object segmentation and tracking for content-based applications,"The new video-coding standard MPEG-4 enables content-based functionality, as well as high coding efficiency, by taking into account shape information of moving objects. A novel algorithm for segmentation of moving objects in video sequences and extraction of video object planes (VOPs) is proposed . For the case of multiple video objects in a scene, the extraction of a specific single video object (VO) based on connected components analysis and smoothness of VO displacement in successive frames is also discussed. Our algorithm begins with a robust double-edge map derived from the difference between two successive frames. After removing edge points which belong to the previous frame, the remaining edge map, moving edge (ME), is used to extract the VOP. The proposed algorithm is evaluated on an indoor sequence captured by a low-end camera as well as MPEG-4 test sequences and produces promising results.",2002.0,"Changick Kim, Jenq-Neng Hwang"
c1e2f9126efeadf96f50c4d86a8c1054e867fd63,https://www.semanticscholar.org/paper/c1e2f9126efeadf96f50c4d86a8c1054e867fd63,Small-scale palm oil processing in Africa.,"There is extensive information in the literature about medium- and large-scale palm oil processing, as well as about traditional technologies in Africa, but information on small-scale mills is scarce. This bulletin reviews the processing of palm oil fruits for the extraction of both palm oil and palm kernel oil at the small-scale level, with information on some African manufacturers of appropriate processing equipment. The publication provides basic information to palm oil growers and to private entrepreneurs in Africa interested in investing in small- or medium-scale palm oil factories.",2002.0,Kwasi Poku
c5c08e6dec3bf8a036607593e11e389697e03f45,https://www.semanticscholar.org/paper/c5c08e6dec3bf8a036607593e11e389697e03f45,Entity Linking at Web Scale,"This paper investigates entity linking over millions of high-precision extractions from a corpus of 500 million Web documents, toward the goal of creating a useful knowledge base of general facts. This paper is the first to report on entity linking over this many extractions, and describes new opportunities (such as corpus-level features) and challenges we found when entity linking at Web scale. We present several techniques that we developed and also lessons that we learned. We envision a future where information extraction and entity linking are paired to automatically generate knowledge bases with billions of assertions over millions of linked entities.",2012.0,"Thomas Lin, Mausam, Oren Etzioni"
016baa7871755b810376686e2858a52199ec5eed,https://www.semanticscholar.org/paper/016baa7871755b810376686e2858a52199ec5eed,Extracting Temporal Information from Open Domain Text: A Comparative Exploration,"The utility of data-driven techniques in the end-to-end problem of temporal information extraction is unclear. Recognition of temporal expressions yields readily to machine learning, but normalization seems to call for a rule-based approach. We explore two aspects of the (potential) utility of data-driven methods in the temporal information extraction task. First, we look at whether improving recognition beyond the rule base used by a normalizer has an eect on normalization performance, comparing normalizer performance when fed by several recognition systems. We also perform an error analysis of our normalizer’s performance to uncover aspects of the normalization task that might be amenable to data-driven techniques.",2005.0,"David Ahn, S. F. Adafre, M. de Rijke"
c281747484c2e777cd61569a172d41a0128afd91,https://www.semanticscholar.org/paper/c281747484c2e777cd61569a172d41a0128afd91,Biometric personal identification based on iris patterns,"A new system for personal identification based on iris patterns is presented in this paper. It is composed of iris image acquisition, image preprocessing, feature extraction and classifier design. The algorithm for iris feature extraction is based on texture analysis using multichannel Gabor filtering and wavelet transform. Compared with existing methods, our method employs the rich 2D information of the iris and is translation, rotation, and scale invariant.",2000.0,"Yong Zhu, T. Tan, Yunhong Wang"
03c03dec975554cb02aca1e076106178dbe0a8a0,https://www.semanticscholar.org/paper/03c03dec975554cb02aca1e076106178dbe0a8a0,Named Entity Recognition with a Maximum Entropy Approach,"The named entity recognition (NER) task involves identifying noun phrases that are names, and assigning a class to each name. This task has its origin from the Message Understanding Conferences (MUC) in the 1990s, a series of conferences aimed at evaluating systems that extract information from natural language texts. It became evident that in order to achieve good performance in information extraction, a system needs to be able to recognize names. A separate subtask on NER was created in MUC-6 and MUC-7 (Chinchor, 1998).",2003.0,"Hai Leong Chieu, H. Ng"
db073b550a4d3354a72468903eba2d3c88620b8f,https://www.semanticscholar.org/paper/db073b550a4d3354a72468903eba2d3c88620b8f,A Composite Kernel to Extract Relations between Entities with Both Flat and Structured Features,"This paper proposes a novel composite kernel for relation extraction. The composite kernel consists of two individual kernels: an entity kernel that allows for entity-related features and a convolution parse tree kernel that models syntactic information of relation examples. The motivation of our method is to fully utilize the nice properties of kernel methods to explore diverse knowledge for relation extraction. Our study illustrates that the composite kernel can effectively capture both flat and structured features without the need for extensive feature engineering, and can also easily scale to include more features. Evaluation on the ACE corpus shows that our method outperforms the previous best-reported methods and significantly out-performs previous two dependency tree kernels for relation extraction.",2006.0,"Min Zhang, Jie Zhang, Jian Su, Guodong Zhou"
9cc7525e6d8d0420d8d825d7a00ce76ae68452a4,https://www.semanticscholar.org/paper/9cc7525e6d8d0420d8d825d7a00ce76ae68452a4,Soil-specific limitations for access and analysis of soil microbial communities by metagenomics.,"Metagenomics approaches represent an important way to acquire information on the microbial communities present in complex environments like soil. However, to what extent do these approaches provide us with a true picture of soil microbial diversity? Soil is a challenging environment to work with. Its physicochemical properties affect microbial distributions inside the soil matrix, metagenome extraction and its subsequent analyses. To better understand the bias inherent to soil metagenome 'processing', we focus on soil physicochemical properties and their effects on the perceived bacterial distribution. In the light of this information, each step of soil metagenome processing is then discussed, with an emphasis on strategies for optimal soil sampling. Then, the interaction of cells and DNA with the soil matrix and the consequences for microbial DNA extraction are examined. Soil DNA extraction methods are compared and the veracity of the microbial profiles obtained is discussed. Finally, soil metagenomic sequence analysis and exploitation methods are reviewed.",2011.0,"N. Lombard, Emmanuel Prestat, J. V. van Elsas, P. Simonet"
ce2347d8fbeb3689a989ee13e7e0b4da50a6994a,https://www.semanticscholar.org/paper/ce2347d8fbeb3689a989ee13e7e0b4da50a6994a,Boosted Wrapper Induction,"Recent work in machine learning for information extraction has focused on two distinct sub-problems: the conventional problem of filling template slots from natural language text, and the problem of wrapper induction, learning simple extraction procedures (“wrappers”) for highly structured text such as Web pages produced by CGI scripts. For suitably regular domains, existing wrapper induction algorithms can efficiently learn wrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. Boosting is a technique for improving the performance of a simple machine learning algorithm by repeatedly applying it to the training set with different example weightings. We describe an algorithm that learns simple, low-coverage wrapper-like extraction patterns, which we then apply to conventional information extraction problems using boosting. The result is BWI, a trainable information extraction system with a strong precision bias and F1 performance better than state-of-the-art techniques in many domains.",2000.0,"Dayne Freitag, N. Kushmerick"
af2f78f9e9b562403524758bdeb015d7ea8b56ef,https://www.semanticscholar.org/paper/af2f78f9e9b562403524758bdeb015d7ea8b56ef,A Fingerprint Verification System Based on Triangular Matching and Dynamic Time Warping,"An effective fingerprint verification system is presented. It assumes that an existing reference fingerprint image must validate the identity of a person by means of a test fingerprint image acquired online and in real-time using minutiae matching. The matching system consists of two main blocks: The first allows for the extraction of essential information from the reference image off-line, the second performs the matching itself online. The information is obtained from the reference image by filtering and careful minutiae extraction procedures. The fingerprint identification is based on triangular matching to cope with the strong deformation of fingerprint images due to static friction or finger rolling. The matching is finally validated by dynamic time warping. Results reported on the NIST Special Database 4 reference set, featuring 85 percent correct verification (15 percent false negative) and 0.05 percent false positive, demonstrate the effectiveness of the verification technique.",2000.0,Z. Kovács-Vajna
903579c8503c773531305373b58a1579c41df6da,https://www.semanticscholar.org/paper/903579c8503c773531305373b58a1579c41df6da,"MedScan, a natural language processing engine for MEDLINE abstracts","MOTIVATION
The importance of extracting biomedical information from scientific publications is well recognized. A number of information extraction systems for the biomedical domain have been reported, but none of them have become widely used in practical applications. Most proposals to date make rather simplistic assumptions about the syntactic aspect of natural language. There is an urgent need for a system that has broad coverage and performs well in real-text applications.


RESULTS
We present a general biomedical domain-oriented NLP engine called MedScan that efficiently processes sentences from MEDLINE abstracts and produces a set of regularized logical structures representing the meaning of each sentence. The engine utilizes a specially developed context-free grammar and lexicon. Preliminary evaluation of the system's performance, accuracy, and coverage exhibited encouraging results. Further approaches for increasing the coverage and reducing parsing ambiguity of the engine, as well as its application for information extraction are discussed.",2003.0,"Svetlana Novichkova, S. Egorov, N. Daraselia"
ce762b56380aaaa0cba54f365289a47fa7857de8,https://www.semanticscholar.org/paper/ce762b56380aaaa0cba54f365289a47fa7857de8,Wavelet descriptor of planar curves: theory and applications,"By using the wavelet transform, the authors develop a hierarchical planar curve descriptor that decomposes a curve into components of different scales so that the coarsest scale components carry the global approximation information while the finer scale components contain the local detailed information. They show that the wavelet descriptor has many desirable properties such as multiresolution representation, invariance, uniqueness, stability, and spatial localization. A deformable wavelet descriptor is also proposed by interpreting the wavelet coefficients as random variables. The applications of the wavelet descriptor to character recognition and model-based contour extraction from low SNR images are examined. Numerical experiments are performed to illustrate the performance of the wavelet descriptor.",1996.0,"G. C. Chuang, C.-C. Jay Kuo"
c94de29a2aee5d34a4aa570474776768d00c46f6,https://www.semanticscholar.org/paper/c94de29a2aee5d34a4aa570474776768d00c46f6,Research Synthesis,"In meta-analyses the extraction and coding of information from primary research reports has to be completed in a competent way because these tasks implicate most of the decisions that determine the usefulness of thefinal product. The authors offer guidelines that make it more likely that high-quality information is reliably extracted and codedfrom primary research reports. These guidelines address issues ranging from the selection of items and construction of coding materials to sustaining reliability and vigilance across extended periods of coding. Thereafter; the authors note how the methodology of metaanalysis results in pressure to change the type of information that appears in primary research reports, and close by offering a few conjectures about the future of meta-analysis.",1996.0,"W. Stock, Juan A Gómez Benito, Nekane Balluerka Lasa"
1ebb9b20a7477a0498449968f04fffee7d8165f7,https://www.semanticscholar.org/paper/1ebb9b20a7477a0498449968f04fffee7d8165f7,Thesaurus based automatic keyphrase indexing,We propose a new method that enhances automatic keyphrase extraction by using semantic information on terms and phrases gleaned from a domain-specific thesaurus. We evaluate the results against keyphrase sets assigned by a state-of-the-art keyphrase extraction system and those assigned by six professional indexers,2006.0,"Olena Medelyan, I. Witten"
3a3457d49c92398df2dfe43a4a3bd4bbbabce425,https://www.semanticscholar.org/paper/3a3457d49c92398df2dfe43a4a3bd4bbbabce425,Comparing and evaluating interest points,"Many computer vision tasks rely on feature extraction. Interest points are such features. This paper shows that interest points are geometrically stable under different transformations and have high information content (distinctiveness). These two properties make interest points very successful in the contest of image matching. To measure these two properties quantitatively, we introduce two evaluation criteria: repeatability rate and information content. The quality of the interest points depends on the detector used. In this paper several detectors are compared according to the criteria specified above. We determine which detector gives the best results and show that it satisfies the criteria well.",1998.0,"C. Schmid, R. Mohr, C. Bauckhage"
31aa4b4ddfa454bc0626cac4bf092c8d2fe2c852,https://www.semanticscholar.org/paper/31aa4b4ddfa454bc0626cac4bf092c8d2fe2c852,Record-boundary discovery in Web documents,"Extraction of information from unstructured or semistructured Web documents often requires a recognition and delimitation of records. (By “record” we mean a group of information relevant to some entity.) Without first chunking documents that contain multiple records according to record boundaries, extraction of record information will not likely succeed. In this paper we describe a heuristic approach to discovering record boundaries in Web documents. In our approach, we capture the structure of a document as a tree of nested HTML tags, locate the subtree containing the records of interest, identify candidate separator tags within the subtree using five independent heuristics, and select a consensus separator tag based on a combined heuristic. Our approach is fast (runs linearly for practical cases within the context of the larger data-extraction problem) and accurate (100% in the experiments we conducted).",1999.0,"Yuan Jiang, David W. Embley, D. Olsen, Brigham Young, N. Mangelson, Dennis Ng, David W. Embley"
fad265c5e322a6740009a12edc173860f5f516b1,https://www.semanticscholar.org/paper/fad265c5e322a6740009a12edc173860f5f516b1,Name-It: Naming and Detecting Faces in News Videos,"We developed Name-It, a system that associates faces and names in news videos. It processes information from the videos and can infer possible name candidates for a given face or locate a face in news videos by name. To accomplish this task, the system takes a multimodal video analysis approach: face sequence extraction and similarity evaluation from videos, name extraction from transcripts, and video-caption recognition.",1999.0,"S. Satoh, Yuichi Nakamura, T. Kanade"
e9ff4ec8c381b203955e604076dd471b2ba0f848,https://www.semanticscholar.org/paper/e9ff4ec8c381b203955e604076dd471b2ba0f848,On the provenance of non-answers to queries over extracted data,"In information extraction, uncertainty is ubiquitous. For this reason, it is useful to provide users querying extracted data with explanations for the answers they receive. Providing the provenance for tuples in a query result partially addresses this problem, in that provenance can explain why a tuple is in the result of a query. However, in some cases explaining why a tuple is not in the result may be just as helpful. In this work we focus on providing provenance-style explanations for non-answers and develop a mechanism for providing this new type of provenance. Our experience with an information extraction prototype suggests that our approach can provide effective provenance information that can help a user resolve their doubts over non-answers to a query.",2008.0,"Jiansheng Huang, Ting Chen, A. Doan, J. Naughton"
1a3b6d9064cfab53c1135ff68d5eb02a56e27c55,https://www.semanticscholar.org/paper/1a3b6d9064cfab53c1135ff68d5eb02a56e27c55,Predicting crystal structures with data mining of quantum calculations.,"Predicting and characterizing the crystal structure of materials is a key problem in materials research and development. It is typically addressed with highly accurate quantum mechanical computations on a small set of candidate structures, or with empirical rules that have been extracted from a large amount of experimental information, but have limited predictive power. In this Letter, we transfer the concept of heuristic rule extraction to a large library of ab initio calculated information, and we demonstrate that this can be developed into a tool for crystal structure prediction.",2003.0,"S. Curtarolo, D. Morgan, K. Persson, J. Rodgers, G. Ceder"
39efcdf5ad5e0fe85e64094ca9f90b4a1eb0d9e0,https://www.semanticscholar.org/paper/39efcdf5ad5e0fe85e64094ca9f90b4a1eb0d9e0,基於《知網》的辭彙語義相似度計算 (Word Similarity Computing Based on How-net) [In Chinese],"Word similarity is broadly used in many applications, such as information retrieval, information extraction",2002.0,"Qun Liu, Sujian Li"
6fd0cbf86dcab3f421072b43fb0c672078c86da5,https://www.semanticscholar.org/paper/6fd0cbf86dcab3f421072b43fb0c672078c86da5,Exploiting Channel Diversity in Secret Key Generation From Multipath Fading Randomness,"We design and analyze a method to extract secret keys from the randomness inherent to wireless channels. We study a channel model for a multipath wireless channel and exploit the channel diversity in generating secret key bits. We compare the key extraction methods based both on entire channel state information (CSI) and on single channel parameter such as the received signal strength indicators (RSSI). Due to the reduction in the degree-of-freedom when going from CSI to RSSI, the rate of key extraction based on CSI is far higher than that based on RSSI. This suggests that exploiting channel diversity and making CSI information available to higher layers would greatly benefit the secret key generation. We propose a key generation system based on low-density parity-check (LDPC) codes and describe the design and performance of two systems: one based on binary LDPC codes and the other (useful at higher signal-to-noise ratios) based on four-ary LDPC codes.",2011.0,"Yanpei Liu, S. Draper, A. Sayeed"
a535640a6bd3f3bb2ceee627e89c1ab2482a0d7b,https://www.semanticscholar.org/paper/a535640a6bd3f3bb2ceee627e89c1ab2482a0d7b,Lexical and sublexical semantic preview benefits in Chinese reading.,"Semantic processing from parafoveal words is an elusive phenomenon in alphabetic languages, but it has been demonstrated only for a restricted set of noncompound Chinese characters. Using the gaze-contingent boundary paradigm, this experiment examined whether parafoveal lexical and sublexical semantic information was extracted from compound preview characters. Results generalized parafoveal semantic processing to this representative set of Chinese characters and extended the parafoveal processing to radical (sublexical) level semantic information extraction. Implications for notions of parafoveal information extraction during Chinese reading are discussed.",2012.0,"Ming Yan, W. Zhou, H. Shu, Reinhold Kliegl"
397c5c759debd738f7920731212873e47caf3bd8,https://www.semanticscholar.org/paper/397c5c759debd738f7920731212873e47caf3bd8,Reasons for Permanent Tooth Extractions in Japan,"BACKGROUND There has been no nationwide study in Japan on reasons for extraction of permanent teeth. This survey was aimed to determine the reasons for extraction of permanent teeth in Japan. METHODS Five thousand, one hudred and thirty-one dentists were selected by systematic selection from the 2004 membership directory of the Japan Dental Association. The dentists selected were asked to record the reason for each extraction of permanent teeth during a period of one week from February 1 through 7, 2005. Reasons for tooth extraction were assigned to five groups: caries, fracture of teeth weakened by caries or endodontics, periodontal diseases, orthodontics, and other reasons. RESULTS A total of 2,001 dentists (response rate of 39.1%) returned the questionnaires, and information on 9,115 extracted teeth from 7,499 patients was obtained. The results showed that caries and its sequela (totally 43.3%, 32.7% and 10.6%, respectively) and periodontal disease (41.8%) were the main reasons for teeth extraction. Extraction due to caries or fracture was commonly observed in all age groups over 15 years of age, whereas periodontal disease was predominant in the groups over 45 years of age. CONCLUSIONS Most of the permanent teeth were extracted due to caries and its sequela and periodontal disease. Prevention and care for dental caries for all age groups and periodontal disease for over middle age groups are required.",2006.0,"J. Aida, Y. Ando, R. Akhter, H. Aoyama, M. Masui, M. Morita"
c522e752ab33f980ee531ae307d94e5bb9545fb0,https://www.semanticscholar.org/paper/c522e752ab33f980ee531ae307d94e5bb9545fb0,Ontology research and development. Part 1 - a review of ontology generation,"Ontology is an important emerging discipline that has the huge potential to improve information organization, management and understanding. It has a crucial role to play in enabling content-based access, interoperability, communications, and providing qualitatively new levels of services on the next wave of web transformation in the form of the Semantic Web. The issues pertaining to ontology generation, mapping and maintenance are critical key areas that need to be understood and addressed. This survey is presented in two parts. The first part reviews the state-of-the-art techniques and work done on semi-automatic and automatic ontology generation, as well as the problems facing such research. The second complementary survey is dedicated to ontology mapping and ontology ‘evolving’. Through this survey, we have identified that shallow information extraction and natural language processing techniques are deployed to extract concepts or classes from free-text or semi-structured data. However, relation extraction is a very complex and difficult issue to resolve and it has turned out to be the main impediment to ontology learning and applicability. Further research is encouraged to find appropriate and efficient ways to detect or identify relations through semi-automatic and automatic means.",2002.0,"Ying Ding, S. Foo"
8829b5ca4348c61a1841a3996a1b55495415378c,https://www.semanticscholar.org/paper/8829b5ca4348c61a1841a3996a1b55495415378c,Evolving GATE to meet new challenges in language engineering,"In this paper we present recent work on GATE, a widely-used framework and graphical development environment for creating and deploying Language Engineering components and resources in a robust fashion. The GATE architecture has facilitated the development of a number of successful applications for various language processing tasks (such as Information Extraction, dialogue and summarisation), the building and annotation of corpora and the quantitative evaluations of LE applications. The focus of this paper is on recent developments in response to new challenges in Language Engineering: Semantic Web, integration with Information Retrieval and data mining, and the need for machine learning support.",2004.0,"Kalina Bontcheva, V. Tablan, D. Maynard, H. Cunningham"
f30b903047284e8a253b2da38530b99b6db13317,https://www.semanticscholar.org/paper/f30b903047284e8a253b2da38530b99b6db13317,A Semantic Approach to IE Pattern Induction,This paper presents a novel algorithm for the acquisition of Information Extraction patterns. The approach makes the assumption that useful patterns will have similar meanings to those already identified as relevant. Patterns are compared using a variation of the standard vector space model in which information from an ontology is used to capture semantic similarity. Evaluation shows this algorithm performs well when compared with a previously reported document-centric approach.,2005.0,"Mark Stevenson, M. Greenwood"
ba4bb5bd1057ce49a6143ec93620551004b51216,https://www.semanticscholar.org/paper/ba4bb5bd1057ce49a6143ec93620551004b51216,Entropy: a new definition and its applications,"Shannon's definition of entropy is critically examined and a new definition of classical entropy based on the exponential behavior of information gain is proposed along with its justification. The concept is extended to defining the global, local, and conditional entropy of a gray-level image. Based on these definitions four algorithms for object extraction are developed. One of these algorithms uses a Poisson distribution-based model of an ideal image. A concept of positional entropy giving any information regarding the location of an object in a scene is introduced. >",1991.0,"N. Pal, S. Pal"
9b80a4c2fbd6454e2372d4e72bfc96aabf8d7053,https://www.semanticscholar.org/paper/9b80a4c2fbd6454e2372d4e72bfc96aabf8d7053,"Potential Research Space in MIS: A Framework for Envisioning and Evaluating Research Replication, Extension, and Generation","Replications are an important component of scientific method in that they convert tentative belief to accepted knowledge. Given the espoused importance of replications to the extraction of knowledge from research, there is surprisingly little evidence of its practice or discussion of its importance in the management information systems literature. In this article we develop a framework within which to systematize the conceptualization of replications; we review and illustrate how some key information systems research fits into the framework and examine the factors that influence the selection of a research strategy. Our framework includes a conceptualization of the relationship among replication, extension, and generation in IS research. The concept of ""research space"" is defined and a framework is developed that delineates eight possible research strategies. Finally, the benefits of our framework to salient stakeholders in the research process are outlined.",2002.0,"P. Berthon, L. Pitt, M. Ewing, Christopher L. Carr"
3dd871be1c0db96fba85a1d36e6b4eda53ba4574,https://www.semanticscholar.org/paper/3dd871be1c0db96fba85a1d36e6b4eda53ba4574,Robust Relational Parsing Over Biomedical Literature: Extracting Inhibit Relations,"We describe the design of a robust parser for identifying and extracting biomolecular relations from the biomedical literature. Separate automata over distinct syntactic domains were developed for extraction of nominal-based relational information versus verbal-based relations. This allowed us to optimize the grammars separately for each module, regardless of any specific relation resulting in significantly better performance. A unique feature of this system is the use of text-based anaphora resolution to enhance the results of argument binding in relational extraction. We demonstrate the performance of our system on inhibition-relations, and present our initial results measured against an annotated text used as a gold standard for evaluation purposes. The results represent a significant improvement over previously published results on extracting such relations from Medline: Precision was 90%, Recall 57%, and Partial Recall 22%. These results demonstrate the effectiveness of a corpus-based linguistic approach to information extraction over Medline.",2001.0,"J. Pustejovsky, J. Castaño, Jason Zhang, M. Kotecki, Brent Cochran"
15182c6e680c7543f445bbe8de449afa912eacd4,https://www.semanticscholar.org/paper/15182c6e680c7543f445bbe8de449afa912eacd4,Overview of BioNLP Shared Task 2011,"The BioNLP Shared Task 2011, an information extraction task held over 6 months up to March 2011, met with community-wide participation, receiving 46 final submissions from 24 teams. Five main tasks and three supporting tasks were arranged, and their results show advances in the state of the art in fine-grained biomedical domain information extraction and demonstrate that extraction methods successfully generalize in various aspects.",2011.0,"Jin-Dong Kim, S. Pyysalo, Tomoko Ohta, Robert Bossy, Ngan L. T. Nguyen, Jun'ichi Tsujii"
5b7929b7e1e74865683e3d1dc5bfd062ef1cab6b,https://www.semanticscholar.org/paper/5b7929b7e1e74865683e3d1dc5bfd062ef1cab6b,University of Sheffield: Description of the LaSIE System as Used for MUC-6,"The LaSIE (Large Scale Information Extraction) system has been developed at the University of Sheffield as part of an ongoing research effort into information extraction and, more generally, natural language engineering.",1995.0,"R. Gaizauskas, K. Humphreys, H. Cunningham, Y. Wilks"
eb52373f7266e19efa28043812c9dae96ecd26d1,https://www.semanticscholar.org/paper/eb52373f7266e19efa28043812c9dae96ecd26d1,Using the structure of Web sites for automatic segmentation of tables,"Many Web sites, especially those that dynamically generate HTML pages to display the results of a user's query, present information in the form of list or tables. Current tools that allow applications to programmatically extract this information rely heavily on user input, often in the form of labeled extracted records. The sheer size and rate of growth of the Web make any solution that relies primarily on user input is infeasible in the long term. Fortunately, many Web sites contain much explicit and implicit structure, both in layout and content, that we can exploit for the purpose of information extraction. This paper describes an approach to automatic extraction and segmentation of records from Web tables. Automatic methods do not require any user input, but rely solely on the layout and content of the Web source. Our approach relies on the common structure of many Web sites, which present information as a list or a table, with a link in each entry leading to a detail page containing additional information about that item. We describe two algorithms that use redundancies in the content of table and detail pages to aid in information extraction. The first algorithm encodes additional information provided by detail pages as constraints and finds the segmentation by solving a constraint satisfaction problem. The second algorithm uses probabilistic inference to find the record segmentation. We show how each approach can exploit the web site structure in a general, domain-independent manner, and we demonstrate the effectiveness of each algorithm on a set of twelve Web sites.",2004.0,"Kristina Lerman, L. Getoor, Steven N. Minton, Craig A. Knoblock"
2d9d3c9ba00745d9f36539b6b534fede2c1f76c9,https://www.semanticscholar.org/paper/2d9d3c9ba00745d9f36539b6b534fede2c1f76c9,A Video Watermarking Technique Based on Pseudo-3-D DCT and Quantization Index Modulation,"The increasing popularity of the internet means that digital multimedia are transmitted more rapidly and easily. And people are very aware for media ownership. However, digital watermarking is an efficient and promising means to protect intellectual properties. Based on the intellectual property attention in the information era, how to protect the personal ownership is extremely important and a necessary scheme. In this paper, we propose an effective video watermarking method based on a pseudo-3-D discrete cosine transform (DCT) and quantization index modulation (QIM) against several attacks. The watermark is mainly inserted into the uncompressed domain by adjusting the correlation between DCT coefficients of the selected blocks, and the watermark extraction is blind. This approach consists of a pseudo-3-D DCT, watermark embedding, and extraction. A pseudo-3-D DCT, which is taken DCT transformation twice, will be first utilized to calculate the embedding factor and to obtain the useful messages. Using the QIM, we embed the watermark into the quantization regions from the successive raw frames in the uncompressed domain and record the relative information to create a secret embedding key. This secret embedding key will further apply to extraction. Experimental results demonstrate that the proposed method can survive filtering, compressions, luminance change, and noise attacks with a good invisibility and robustness.",2010.0,"Hui-Yu Huang, Cheng-Han Yang, W. Hsu"
9d963cd3280c732dac9835f42e2769351cc815c1,https://www.semanticscholar.org/paper/9d963cd3280c732dac9835f42e2769351cc815c1,Weakly-supervised discovery of named entities using web search queries,"A seed-based framework for textual information extraction allows for weakly supervised extraction of named entities from anonymized Web search queries. The extraction is guided by a small set of seed named entities, without any need for handcrafted extraction patterns or domain-specific knowledge, allowing for the acquisition of named entities pertaining to various classes of interest to Web search users. Inherently noisy search queries are shown to be a highly valuable, albeit little explored, resource for Web-based named entity discovery.",2007.0,Marius Pasca
ccfd72a2cca655a94cf93e73b7a2c0ec1ccbff20,https://www.semanticscholar.org/paper/ccfd72a2cca655a94cf93e73b7a2c0ec1ccbff20,Organizing and searching the world wide web of facts -- step two: harnessing the wisdom of the crowds,"As part of a large effort to acquire large repositories of facts from unstructured text on the Web, a seed-based framework for textual information extraction allows for weakly supervised extraction of class attributes (e.g., side effects and generic equivalent for drugs) from anonymized query logs. The extraction is guided by a small set of seed attributes, without any need for handcrafted extraction patterns or further domain-specific knowledge. The attributes of classes pertaining to various domains of interest to Web search users have accuracy levels significantly exceeding current state of the art. Inherently noisy search queries are shown to be a highly valuable, albeit unexplored, resource for Web-based information extraction, in particular for the task of class attribute extraction.",2007.0,Marius Pasca
adcf27cb8d6ac38d708c9ee610206aa3093db32f,https://www.semanticscholar.org/paper/adcf27cb8d6ac38d708c9ee610206aa3093db32f,TREC GENOMICS Track Overview,"The first year of TREC Genomics Track featured two tasks: ad hoc retrieval and information extraction. Both tasks centered around the Gene Reference into Function (GeneRIF) resource of the National Library of Medicine, which was used as both pseudorelevance judgments for ad hoc document retrieval as well as target text for information extraction. The track attracted 29 groups who participated in one or both tasks.",2003.0,"W. Hersh, Ravi Teja Bhupatiraju"
2a2d908d2306dd25f5ac6fff20bc83059c824bb1,https://www.semanticscholar.org/paper/2a2d908d2306dd25f5ac6fff20bc83059c824bb1,University of Sheffield: Description of the LaSIE-II System as Used for MUC-7,"The University of She eld NLP group took part in MUC-7 using the LaSIE-II system, an evolution of the LaSIE (Large Scale Information Extraction) system rst created for participation in MUC-6 [9] and part of a larger research e ort into information extraction underway in our group. LaSIE-II was used to carry out all ve of the MUC-7 tasks and was, in fact, the only system to take part in all of the MUC-7 tasks.",1998.0,"K. Humphreys, R. Gaizauskas, Saliha Azzam, C. Huyck, B. Mitchell, H. Cunningham, Y. Wilks"
e864203640eff62a04b21ef82309c0b5fa5ea2ff,https://www.semanticscholar.org/paper/e864203640eff62a04b21ef82309c0b5fa5ea2ff,Algorithms and methods of airborne laser-scanning for forest measurements,"Extracting forest variables from airborne laser scanner has less than 10 years of history. During that time, however, a new area in the field of forest studies has emerged. This paper describes existing algorithms and methods of airborne laser scanning that are used for forest measurements. The methods are divided into the following categories: 1) extraction of DTM (digital terrain model), 2) extraction of canopy height, 3) extraction of statistical variables from laser data, 4) extraction of individual tree information using image processing techniques, 5) integrating aerial image data with laser scanner, 6) use of intensity and waveform information, and 7) use of change detection methods.",2004.0,"J. Hyyppä, H. Hyyppä, P. Litkey, Xiaowei Yu, H. Haggrén, P. Rönnholm, U. Pyysalo, J. Pitkänen, M. Maltamo"
84f47e553be07d2697653c3cecfd5860d9ae967f,https://www.semanticscholar.org/paper/84f47e553be07d2697653c3cecfd5860d9ae967f,A Review of Techniques for Extracting Linear Features from Imagery,"The automated extraction of linear features from remotely sensed imagery has been the subject of extensive research over several decades. Recent studies show promise for extraction of feature information for applications such as updating geographic information systems (GIS). Research has been stimulated by the increase in available imagery in recent years following the launch of several airborne and satellite sensors. However, while the expansion in the range and availability of image data provides new possibilities for deriving image related products, it also places new demands on image processing. Efficiently dealing with the vast amount of available data necessitates an increase in automation, while still taking advantage of the skills of a human operator. This paper provides an overview of the types of imagery being used for linear feature extraction. The paper also describes methods used for feature extraction and considers quantitative and qualitative accuracy assessment of these procedures.",2004.0,Lindi J. Quackenbush
4330d5fa0f2d9fd5c7c395ce4977717054b0b4ee,https://www.semanticscholar.org/paper/4330d5fa0f2d9fd5c7c395ce4977717054b0b4ee,Using Musical Structure to Enhance Automatic Chord Transcription,"Chord extraction from audio is a well-established music computing task, and many valid approaches have been presented in recent years that use different chord templates, smoothing techniques and musical context models. The present work shows that additional exploitation of the repetitive structure of songs can enhance chord extraction, by combining chroma information from multiple occurrences of the same segment type. To justify this claim we modify an existing chord labelling method, providing it with manual or automatic segment labels, and compare chord extraction results on a collection of 125 songs to baseline methods without segmentation information. Our method results in consistent and more readily readable chord labels and provides a statistically significant boost in label accuracy.",2009.0,"Matthias Mauch, K. Noland, S. Dixon"
9bdef4898c3cbdc196a4d076e9bd5df38b97f28d,https://www.semanticscholar.org/paper/9bdef4898c3cbdc196a4d076e9bd5df38b97f28d,An Analysis of Structured Data on the Web,"In this paper, we analyze the nature and distribution of structured data on the Web. Web-scale information extraction, or the problem of creating structured tables using extraction from the entire web, is gathering lots of research interest. We perform a study to understand and quantify the value of Web-scale extraction, and how structured information is distributed amongst top aggregator websites and tail sites for various interesting domains. We believe this is the first study of its kind, and gives us new insights for information extraction over the Web.",2012.0,"Nilesh N. Dalvi, Ashwin Machanavajjhala, B. Pang"
9a0b421df26e5a8e02f277f17ea3ef8bce22e7d4,https://www.semanticscholar.org/paper/9a0b421df26e5a8e02f277f17ea3ef8bce22e7d4,Steganography in Compressed Video Stream,"In this paper, a steganographic algorithm in MPEG compressed video stream was proposed. In each GOP, the control information for to facilitate data extraction was embedded in I frame, in P frames and B frames, the actually transmitted data were repeatedly embedded in motion vectors of macro-blocks that have larger moving speed, for to resist video processing. Data extraction was also performed in compressed video stream without requiring original video. On a GOP by GOP basis, control information in I frame should be extracted firstly, then the embedded data in P and B frames can be extracted based on the control information. Experimental results show that the proposed algorithm has the characteristics of little degrading the visual effect, larger embedding capacity and resisting video processing such as frame adding or frame dropping",2006.0,"Changyong Xu, X. Ping, Tao Zhang"
1d7c366ff41bc3917a0144cd3aac3647066792ab,https://www.semanticscholar.org/paper/1d7c366ff41bc3917a0144cd3aac3647066792ab,A Performance Evaluation of Text-Analysis Technologies,"A performance evaluation of 15 text-analysis systems was recently conducted to realistically assess the state of the art for detailed information extraction from unconstrained continuous text. Reports associated with terrorism were chosen as the target domain, and all systems were tested on a collection of previously unseen texts released by a government agency. Based on multiple strategies for computing each metric, the competing systems were evaluated for recall, precision, and overgeneration. The results support the claim that systems incorporating natural language‐processing techniques are more effective than systems based on stochastic techniques alone. A wide range of language-processing strategies was employed by the top-scoring systems, indicating that many natural language‐processing techniques provide a viable foundation for sophisticated text analysis. Further evaluation is needed to produce a more detailed assessment of the relative merits of specific technologies and establish true performance limits for automated information extraction.",1991.0,"W. Lehnert, B. Sundheim"
fc1d99feacb6ce19faf06e9790ee0fe14231b02d,https://www.semanticscholar.org/paper/fc1d99feacb6ce19faf06e9790ee0fe14231b02d,Challenges inbuilding a DBMS Resource Advisor,"The AVATAR Information Extraction System (IES) at the IBM Almaden Research Center enables highprecision, rule-based, information extraction from text-documents. Drawing from our experience we propose the use of probabilistic database techniques as the formal underpinnings of information extraction systems so as to maintain high precision while increasing recall. This involves building a framework where rule-based annotators can be mapped to queries in a database system. We use examples from AVATAR IES to describe the challenges in achieving this goal. Finally, we show that deriving precision estimates in such a database system presents a significant challenge for probabilistic database systems.",2006.0,"D. Narayanan, Eno Thereska, A. Ailamaki"
2285da81f32179e421688cd13392c00dc3a6404b,https://www.semanticscholar.org/paper/2285da81f32179e421688cd13392c00dc3a6404b,Approaches to text mining for clinical medical records,"Clinical medical records contain a wealth of information, largely in free-text form. Means to extract structured information from free-text records is an important research endeavor. In this paper, we describe a MEDical Information Extraction (MedIE) system that extracts and mines a variety of patient information with breast complaints from free-text clinical records. MedIE is a part of medical text mining project being conducted in Drexel University. Three approaches are proposed to solve different IE tasks and very good performance (precision and recall) was achieved. A graph-based approach which uses the parsing result of link-grammar parser was invented for relation extraction; high accuracy was achieved. A simple but efficient ontology-based approach was adopted to extract medical terms of interest. Finally, an NLP-based feature extraction method coupled with an ID3-based decision tree was used to perform text classification.",2006.0,"Xiaohua Zhou, Hyoil Han, Isaac Chankai, A. Prestrud, A. Brooks"
fbda64fc89aa363ce6d4a184303e007b5251f1e5,https://www.semanticscholar.org/paper/fbda64fc89aa363ce6d4a184303e007b5251f1e5,Description of the UMass System as Used for MUC-6,"Information extraction research at the University of Massachusetts is based on portable, trainable language processing components. Some components are more effective than others, some have been under development longer than others, but in all cases, we are working to eliminate manual knowledge engineering. Although UMass has participated in previous MUC evaluations, all of our information extraction software has been redesigned and rewritten since MUC-5, so we are evaluating a completely new system this year.",1995.0,"David Fisher, S. Soderland, F. Feng, W. Lehnert"
6499360163aeff9dc4ea87370daa78963f2348d9,https://www.semanticscholar.org/paper/6499360163aeff9dc4ea87370daa78963f2348d9,Hierarchical morphological segmentation for image sequence coding,"This paper deals with a hierarchical morphological segmentation algorithm for image sequence coding. Mathematical morphology is very attractive for this purpose because it efficiently deals with geometrical features such as size, shape, contrast, or connectivity that can be considered as segmentation-oriented features. The algorithm follows a top-down procedure. It first takes into account the global information and produces a coarse segmentation, that is, with a small number of regions. Then, the segmentation quality is improved by introducing regions corresponding to more local information. The algorithm, considering sequences as being functions on a 3-D space, directly segments 3-D regions. A 3-D approach is used to get a segmentation that is stable in time and to directly solve the region correspondence problem. Each segmentation stage relies on four basic steps: simplification, marker extraction, decision, and quality estimation. The simplification removes information from the sequence to make it easier to segment. Morphological filters based on partial reconstruction are proven to be very efficient for this purpose, especially in the case of sequences. The marker extraction identifies the presence of homogeneous 3-D regions. It is based on constrained flat region labeling and morphological contrast extraction. The goal of the decision is to precisely locate the contours of regions detected by the marker extraction. This decision is performed by a modified watershed algorithm. Finally, the quality estimation concentrates on the coding residue, all the information about the 3-D regions that have not been properly segmented and therefore coded. The procedure allows the introduction of the texture and contour coding schemes within the segmentation algorithm. The coding residue is transmitted to the next segmentation stage to improve the segmentation and coding quality. Finally, segmentation and coding examples are presented to show the validity and interest of the coding approach.",1994.0,"P. Salembier, M. Pardàs"
06586082a5fca79e2b545d01a26282716bc2144b,https://www.semanticscholar.org/paper/06586082a5fca79e2b545d01a26282716bc2144b,Secure data transmission using video Steganography,"It is very essential to transmit important data like banking and military information in a secure manner. Video Steganography is the process of hiding some secret information inside a video. The addition of this information to the video is not recognizable by the human eye as the change of a pixel color is negligible. This paper aims to provide an efficient and a secure method for video Steganography. The proposed method creates an index for the secret information and the index is placed in a frame of the video itself. With the help of this index, the frames containing the secret information are located. Hence, during the extraction process, instead of analyzing the entire video, the frames containing the secret data are analyzed with the help of index at the receiving end. When steganographed by this method, the probability of finding the hidden information by an attacker is lesser when compared to the normal method of hiding information frame-by-frame in a sequential manner. It also reduces the computational time taken for the extraction process.",2011.0,"R. Balaji, G. Naveen"
4229b702b33cca32ce0f13976373a4950daf5beb,https://www.semanticscholar.org/paper/4229b702b33cca32ce0f13976373a4950daf5beb,The Common Pattern Specification Language,This paper describes the Common Pattern Specification Language (CPSL) that was developed during the TIPSTER program by a committee of researchers from the TIPSTER research sites. Many information extraction systems work by matching regular expressions over the lexical features of input symbols. CPSL was designed as a language for specifying such finite-state grammars for the purpose of specifying information extraction rules in a relatively system-independent way. The adoption of such a common language would enable the creation of shareable resources for the development of rule-based information extraction systems.,1998.0,"D. Appelt, Boyan A. Onyshkevych"
36c4c51917b1f53ee85c459f2597e115df53eb05,https://www.semanticscholar.org/paper/36c4c51917b1f53ee85c459f2597e115df53eb05,Use of syntactic context to produce term association lists for text retrieval,"One aspect of world knowledge essential to information retrieval is knowing when two words are related. Knowing word relatedness allows a system given a user's query terms to retrieve relevant documents not containing those exact terms. Two words can be said to be related if they appear in the same contexts Document co-occurrence gives a measure of word relatedness that has proved to be too rough to be useful. The relatively recent apparition of on-line dictionaries and robust and rapid parsers permits the extraction of finer word contexts from large corpora. In this paper, we will describe such an extraction technique that uses only coarse syntactic analysis and no domain knowledge. This technique produces lists of words related to any work appearing in a corpus. When the closest related terms were used in query expansion of a standard information retrieval testbed, the results were much better than that given by document co-occurence techniques, and slightly better than using unexpanded queries, supporting the contention that semantically similar words were indeed extracted by this technique.",1992.0,G. Grefenstette
f045a79dd6304a9728f48f2c776876801baecc79,https://www.semanticscholar.org/paper/f045a79dd6304a9728f48f2c776876801baecc79,TREC 2003 QA at BBN: Answering Definitional Questions,"For definitional QA, we adopted a hybrid approach that combines several complementary technology components. Information retrieval (IR) was used to retrieve from the corpus the relevant documents for each question. Various linguistic and extraction tools were used to analyze the retrieved texts and to extract various types of kernel facts from which the answer to the question is generated. These tools include name finding, parsing, co-reference resolution, proposition extraction, relation extraction and extraction of structured patterns. All text analysis functions except structured pattern extraction were carried out by Serif, a state of the art information extraction engine (Ramshaw, et al, 2001) from BBN.",2003.0,"Jinxi Xu, A. Licuanan, R. Weischedel"
b2cb303cdc2faa227aacf3a92dd988037ce6dade,https://www.semanticscholar.org/paper/b2cb303cdc2faa227aacf3a92dd988037ce6dade,A Biological Named Entity Recognizer,"In this paper we describe a new named entity extraction system. Our system is based on a manually developed set of rules that rely heavily upon some crucial lexical information, linguistic constraints of English, and contextual information. This system achieves state of art results in the protein name detection task, which is what many of the current name extraction systems do. We discuss the need for detection of chemical names and show that we not only obtain a high degree of success in recognizing chemicals but that this task can help improve the precision of protein name detection as well. We use context and surrounding words for categorization of named entities and find the results obtained are encouraging.",2002.0,"M. Narayanaswamy, K. Ravikumar, K. Vijay-Shanker"
2f95a2a215ae2c4a3b1656bae60400b526556d56,https://www.semanticscholar.org/paper/2f95a2a215ae2c4a3b1656bae60400b526556d56,Learning to Extract Relations from MEDLINE,"Information in text form remains a greatly underutilized resource in biomedical applications. We have begun a research effort aimed at learning routines for automatically mapping information from biomedical text sources, such as MEDLINE, into structured representations, such as knowledge bases. We describe our application, two learning methods that we have applied to this task, and our initial experiments in learning such information-extraction routines. We also present an approach to decreasing the cost of learning information-extraction routines by learning from ""weakly"" labeled training data.",1999.0,Mark W. Craven
3ebef906954a7ea057396593e710a16b6371d5d5,https://www.semanticscholar.org/paper/3ebef906954a7ea057396593e710a16b6371d5d5,Extracting Buildings from Digital Surface models,"This paper describes an approach for building extraction using Digital Surface Models (DSM) as input data. The first task is the detection of areas within the DSM which describe buildings. The second task is the reconstruction of buildings for which we apply parametric and prismatic building models. The main focus is on the detection, namely on the use of height and differential geometric information to discriminate building and vegetation areas. Furthermore, recent results for the extraction of roof structures as first step towards the extraction of polyhedral building descriptions are presented.",1997.0,"A. Brunn, U. Weidner"
13c975366930d792e834066881c9e79ea5f324dc,https://www.semanticscholar.org/paper/13c975366930d792e834066881c9e79ea5f324dc,"CLiDE Pro: The Latest Generation of CLiDE, a Tool for Optical Chemical Structure Recognition","We present CLiDE Pro, the latest version of the output of the long-term CLiDE project for the development of tools for automatic extraction of chemical information from the literature. CLiDE Pro is concerned with the extraction of chemical structure and generic structure information from electronic images of chemical molecules available online as well as pages of scanned chemical documents. The information is extracted in three phases, first the image is segmented into text and graphical regions, then graphical regions are analyzed and where possible the connection tables are reconstructed, and finally any generic structures are interpreted by matching R-groups found in structure diagrams with the ones located in the text. The program has been tested on a large set of images of chemical structures originating from various sources. The results demonstrate good performance in the reconstruction of connection tables with few errors in the interpretation of the individual drawing features found in the structure diagrams. This full test set is presented for use in the validation of other similar systems.",2009.0,"Aniko T. Valko, A. Peter Johnson"
7f06c436c28adbc63bab14f473b1d8d014583481,https://www.semanticscholar.org/paper/7f06c436c28adbc63bab14f473b1d8d014583481,Optimal Depletion of an Uncertain Stock,"to take inventory of the resource wealth of the United States.' This paper characterizes optimal extraction from a resource stock of uncertain size and examines the value of information about the size of the stock assuming costs and social preferences are known. Section 2 describes a method for determining the optimal extraction programme when there are no opportunities for exploration or storage of the resource. The model is an extension of the "" cake-eating "" problem analysed by Koopmans (1973) under conditions of certainty. Given fairly typical assumptions, the optimal rate of extraction when the resource stock is uncertain is less than the optimal rate for the expected value of the stock. That is, uncertainty implies a more conservative extraction policy. Exploration provides information about the cost and location of resource deposits and improves estimates of the total size of the resource endowment. The former aspect of exploration is discussed in Gilbert (1976). By specifying the efficient extraction programme conditional on a particular information structure, the analysis in Section 2 sets the stage for determining the value of exploration information about the size of the stock. This is the subject of Section 3. Conditions are derived that are necessary for efficient investment in exploration information. In particular, we show that if storage costs are negligible or if the marginal value of the resource is unbounded, it is always optimal to invest in exploration in order to maintain a stock of known reserves.",1979.0,R. Gilbert
1f9003cc4cf0822af2b80ffff5bf766f573b7a59,https://www.semanticscholar.org/paper/1f9003cc4cf0822af2b80ffff5bf766f573b7a59,A search result clustering method using informatively named entities,"Clustering the results of a search helps the user to overview the information returned. In this paper, we regard the clustering task as indexing the search results. Here, an index means a structured label list that can makes it easier for the user to comprehend the labels and search results. To realize this goal, we make three proposals. First is to use Named Entity Extraction for term extraction. Second is a new label selecting criterion based on importance in the search result and the relation between terms and search queries. The third is label categorization using category information of labels, which is generated by NE extraction. We implement a prototype system based on these proposals and find that it offers much higher performance than existing methods; we focus on news articles in this paper.",2005.0,"H. Toda, Ryoji Kataoka"
daa9b8c1234cadc2fe6fb3d1109a1e502afe88e8,https://www.semanticscholar.org/paper/daa9b8c1234cadc2fe6fb3d1109a1e502afe88e8,"Berberine: Botanical Occurrence, Traditional Uses, Extraction Methods, and Relevance in Cardiovascular, Metabolic, Hepatic, and Renal Disorders","Berberine-containing plants have been traditionally used in different parts of the world for the treatment of inflammatory disorders, skin diseases, wound healing, reducing fevers, affections of eyes, treatment of tumors, digestive and respiratory diseases, and microbial pathologies. The physico-chemical properties of berberine contribute to the high diversity of extraction and detection methods. Considering its particularities this review describes various methods mentioned in the literature so far with reference to the most important factors influencing berberine extraction. Further, the common separation and detection methods like thin layer chromatography, high performance liquid chromatography, and mass spectrometry are discussed in order to give a complex overview of the existing methods. Additionally, many clinical and experimental studies suggest that berberine has several pharmacological properties, such as immunomodulatory, antioxidative, cardioprotective, hepatoprotective, and renoprotective effects. This review summarizes the main information about botanical occurrence, traditional uses, extraction methods, and pharmacological effects of berberine and berberine-containing plants.",2018.0,"M. Neag, A. Mocan, J. Echeverría, R. Pop, C. Bocsan, G. Crișan, A. Buzoianu"
c1e1fe0081af13d233d3918cf93004d461ae9f90,https://www.semanticscholar.org/paper/c1e1fe0081af13d233d3918cf93004d461ae9f90,"Astaxanthin: Sources, Extraction, Stability, Biological Activities and Its Commercial Applications—A Review","There is currently much interest in biological active compounds derived from natural resources, especially compounds that can efficiently act on molecular targets, which are involved in various diseases. Astaxanthin (3,3′-dihydroxy-β, β′-carotene-4,4′-dione) is a xanthophyll carotenoid, contained in Haematococcus pluvialis, Chlorella zofingiensis, Chlorococcum, and Phaffia rhodozyma. It accumulates up to 3.8% on the dry weight basis in H. pluvialis. Our recent published data on astaxanthin extraction, analysis, stability studies, and its biological activities results were added to this review paper. Based on our results and current literature, astaxanthin showed potential biological activity in in vitro and in vivo models. These studies emphasize the influence of astaxanthin and its beneficial effects on the metabolism in animals and humans. Bioavailability of astaxanthin in animals was enhanced after feeding Haematococcus biomass as a source of astaxanthin. Astaxanthin, used as a nutritional supplement, antioxidant and anticancer agent, prevents diabetes, cardiovascular diseases, and neurodegenerative disorders, and also stimulates immunization. Astaxanthin products are used for commercial applications in the dosage forms as tablets, capsules, syrups, oils, soft gels, creams, biomass and granulated powders. Astaxanthin patent applications are available in food, feed and nutraceutical applications. The current review provides up-to-date information on astaxanthin sources, extraction, analysis, stability, biological activities, health benefits and special attention paid to its commercial applications.",2014.0,"R. Ambati, Phang Siew Moi, Sarada Ravi, Gokare A. Ravishankar"
44a97f4eaaefaf5338f8aed2913d5debb2459f7e,https://www.semanticscholar.org/paper/44a97f4eaaefaf5338f8aed2913d5debb2459f7e,Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning,"Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data. Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level differential privacy applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).",2017.0,"B. Hitaj, G. Ateniese, F. Pérez-Cruz"
8bbc92ddd99339cc482015348328e62fb2c4dd5f,https://www.semanticscholar.org/paper/8bbc92ddd99339cc482015348328e62fb2c4dd5f,Multimodality image registration by maximization of mutual information,"A new approach to the problem of multimodality medical image registration is proposed, using a basic concept from information theory, mutual information (MI), or relative entropy, as a new matching criterion. The method presented in this paper applies MI to measure the statistical dependence or information redundancy between the image intensities of corresponding voxels in both images, which is assumed to be maximal if the images are geometrically aligned. Maximization of MI is a very general and powerful criterion, because no assumptions are made regarding the nature of this dependence and no limiting constraints are imposed on the image content of the modalities involved. The accuracy of the MI criterion is validated for rigid body registration of computed tomography (CT), magnetic resonance (MR), and photon emission tomography (PET) images by comparison with the stereotactic registration solution, while robustness is evaluated with respect to implementation issues, such as interpolation and optimization, and image content, including partial overlap and image degradation. Our results demonstrate that subvoxel accuracy with respect to the stereotactic reference solution can be achieved completely automatically and without any prior segmentation, feature extraction, or other preprocessing steps which makes this method very well suited for clinical applications.",1997.0,"F. Maes, A. Collignon, D. Vandermeulen, G. Marchal, P. Suetens"
25026b61bd339eb5e099fb6825438e68d05ddca6,https://www.semanticscholar.org/paper/25026b61bd339eb5e099fb6825438e68d05ddca6,The extraction of neural strategies from the surface EMG.,"This brief review examines some of the methods used to infer central control strategies from surface electromyogram (EMG) recordings. Among the many uses of the surface EMG in studying the neural control of movement, the review critically evaluates only some of the applications. The focus is on the relations between global features of the surface EMG and the underlying physiological processes. Because direct measurements of motor unit activation are not available and many factors can influence the signal, these relations are frequently misinterpreted. These errors are compounded by the counterintuitive effects that some system parameters can have on the EMG signal. The phenomenon of crosstalk is used as an example of these problems. The review describes the limitations of techniques used to infer the level of muscle activation, the type of motor unit recruited, the upper limit of motor unit recruitment, the average discharge rate, and the degree of synchronization between motor units. Although the global surface EMG is a useful measure of muscle activation and assessment, there are limits to the information that can be extracted from this signal.",2004.0,"D. Farina, R. Merletti, R. Enoka"
327307a28c37db158c1586c2a09d1624b1457c38,https://www.semanticscholar.org/paper/327307a28c37db158c1586c2a09d1624b1457c38,A comparative study on phenolic profiles and antioxidant activities of legumes as affected by extraction solvents.,"The objective of this study was to investigate how 6 commonly used solvent systems affected the yields of phenolic substances and the antioxidant capacity of extracts from 8 major classes of food legumes. Several antioxidant-related phytochemical compositions, namely, total phenolic content (TPC), total flavonoids content (TFC), and condensed tannins content (CTC), were investigated. In addition, antioxidant activities were tested using 2,2-diphenyl-1-picryhydrazyl (DPPH) free radical scavenging, ferric-reducing antioxidant power (FRAP), and the oxygen radical absorbance capacity (ORAC). The results showed that the 50% acetone extracts exhibited the highest TPC for yellow pea, green pea, chickpea, and yellow soybean. Acidic 70% acetone (+0.5% acetic acid) extracts exhibited the highest TPC, TFC, and FRAP values for black bean, lentil, black soybean, and red kidney bean. The 80% acetone extracts exhibited the highest TFC, CTC, and DPPH-free radical scavenging activity for yellow pea, green pea, chickpea, and yellow soybean. The 70% ethanol extracts exhibited the greatest ORAC value for all selected legumes. These results indicated that solvents with different polarity had significant effects on total phenolic contents, extracted components, and antioxidant activities. High correlations between phenolic compositions and antioxidant activities of legume extracts were observed. The information is of interest to the nutraceutical food/ingredient industries since legumes are a rich source of antioxidants.",2007.0,"Baojun Xu, Sam K C Chang"
a3d6967e833d731c9e231ac5a1c33159697fd271,https://www.semanticscholar.org/paper/a3d6967e833d731c9e231ac5a1c33159697fd271,Journal of Theoretical and Applied Information Technology,"Voice recognition is a system to convert spoken wor ds in well-known languages into written languages o r translated as commands for machines, depending on t he purpose. The input for that system is ""voice"", where the system identifies spoken word(s) and the result of the process is written text on the screen or a movement from machine's mechanical parts. This research focused on analysis of matching proce ss to give a command for multipurpose machine such as a robot with Linear Predictive Coding (LPC) and Hi den Markov Model (HMM), where LPC is a method to analyze voice signals by giving character istics into LPC coefficients. In the other hand, HM M is a form of signal modeling where voice signals are a nalyzed to find maximum probability and recognize words given by a new input based from the defined c odebook. This process could recognize five basic movement of a robot: ""forward"", ""reverse"", ""left"", ""right"" and ""stop"" in the desired language. The analysis will b e done by designing the recognition system based fr om LPC extraction, codebook model and HMM training pro cess. The aim of the system is to find accuracy value of the recognition system built to recognize commands even the speaker voice isn't currently sto red in the database.",2012.0,"Wahyu Kusuma, Prince Brave Guhyapati"
3863c181a54465b2fbd7af7fdb6fb165309f5296,https://www.semanticscholar.org/paper/3863c181a54465b2fbd7af7fdb6fb165309f5296,Spoken Language Understanding: Systems for Extracting Semantic Information from Speech,"List of Contributors. Forward. Preface. 1 Introduction (Gokhan Tur and Renato De Mori). 1.1 A Brief History of Spoken Language Understanding. 1.2 Organization of the Book. PART 1 SPOKEN LANGUAGE UNDERSTANDING FOR HUMAN/MACHINE INTERACTIONS. 2 History of Knowledge and Processes for Spoken Language Understanding (Renato De Mori). 2.1 Introduction. 2.2 Meaning Representation and Sentence Interpretation. 2.3 Knowledge Fragments and Semantic Composition. 2.4 Probabilistic Interpretation in SLU Systems. 2.5 Interpretation with Partial Syntactic Analysis. 2.6 Classification Models for Interpretation. 2.7 Advanced Methods and Resources for Semantic Modeling and Interpretation. 2.8 Recent Systems. 2.9 Conclusions. References. 3 Semantic Frame-based Spoken Language Understanding (Ye-Yi Wang, Li Deng and Alex Acero). 3.1 Background. 3.2 Knowledge-based Solutions. 3.3 Data-driven Approaches. 3.4 Summary. References. 4 Intent Determination and Spoken Utterance Classification (Gokhan Tur and Li Deng). 4.1 Background. 4.2 Task Description. 4.3 Technical Challenges. 4.4 Benchmark Data Sets. 4.5 Evaluation Metrics. 4.6 Technical Approaches. 4.7 Discussion and Conclusions. References. 5 Voice Search (Ye-Yi Wang, Dong Yu, Yun-Cheng Ju and Alex Acero). 5.1 Background. 5.2 Technology Review. 5.3 Summary. References. 6 Spoken Question Answering (Sophie Rosset, Olivier Galibert and Lori Lamel). 6.1 Introduction. 6.2 Specific Aspects of Handling Speech in QA Systems. 6.3 QA Evaluation Campaigns. 6.4 Question-answering Systems. 6.5 Projects Integrating Spoken Requests and Question Answering. 6.6 Conclusions. References. 7 SLU in Commercial and Research Spoken Dialogue Systems (David Suendermann and Roberto Pieraccini). 7.1 Why Spoken Dialogue Systems (Do Not) Have to Understand. 7.2 Approaches to SLU for Dialogue Systems. 7.3 From Call Flow to POMDP: How Dialogue Management Integrates with SLU. 7.4 Benchmark Projects and Data Sets. 7.5 Time is Money: The Relationship between SLU and Overall Dialogue System Performance. 7.6 Conclusion. References. 8 Active Learning (Dilek Hakkani-Tur and Giuseppe Riccardi). 8.1 Introduction. 8.2 Motivation. 8.3 Learning Architectures. 8.4 Active Learning Methods. 8.5 Combining Active Learning with Semi-supervised Learning. 8.6 Applications. 8.7 Evaluation of Active Learning Methods. 8.8 Discussion and Conclusions. References. PART 2 SPOKEN LANGUAGE UNDERSTANDING FOR HUMAN/HUMAN CONVERSATIONS. 9 Human/Human Conversation Understanding (Gokhan Tur and Dilek Hakkani-Tur). 9.1 Background. 9.2 Human/Human Conversation Understanding Tasks. 9.3 Dialogue Act Segmentation and Tagging. 9.4 Action Item and Decision Detection. 9.5 Addressee Detection and Co-reference Resolution. 9.6 Hot Spot Detection. 9.7 Subjectivity, Sentiment, and Opinion Detection. 9.8 Speaker Role Detection. 9.9 Modeling Dominance. 9.10 Argument Diagramming. 9.11 Discussion and Conclusions. References. 10 Named Entity Recognition (Frederic Bechet). 10.1 Task Description. 10.2 Challenges Using Speech Input. 10.3 Benchmark Data Sets, Applications. 10.4 Evaluation Metrics. 10.5 Main Approaches for Extracting NEs from Text. 10.6 Comparative Methods for NER from Speech. 10.7 New Trends in NER from Speech. 10.8 Conclusions. References. 11 Topic Segmentation (Matthew Purver). 11.1 Task Description. 11.2 Basic Approaches, and the Challenge of Speech. 11.3 Applications and Benchmark Datasets. 11.4 Evaluation Metrics. 11.5 Technical Approaches. 11.6 New Trends and Future Directions. References. 12 Topic Identification (Timothy J. Hazen). 12.1 Task Description. 12.2 Challenges Using Speech Input. 12.3 Applications and Benchmark Tasks. 12.4 Evaluation Metrics. 12.5 Technical Approaches. 12.6 New Trends and Future Directions. References. 13 Speech Summarization (Yang Liu and Dilek Hakkani-Tur). 13.1 Task Description. 13.2 Challenges when Using Speech Input. 13.3 Data Sets. 13.4 Evaluation Metrics. 13.5 General Approaches. 13.6 More Discussions on Speech versus Text Summarization. 13.7 Conclusions. References. 14 Speech Analytics (I. Dan Melamed and Mazin Gilbert) 14.1 Introduction. 14.2 System Architecture. 14.3 Speech Transcription. 14.4 Text Feature Extraction. 14.5 Acoustic Feature Extraction. 14.6 Relational Feature Extraction. 14.7 DBMS. 14.8 Media Server and Player. 14.9 Trend Analysis. 14.10 Alerting System. 14.11 Conclusion. References. 15 Speech Retrieval (Ciprian Chelba, Timothy J. Hazen, Bhuvana Ramabhadran and Murat Saraclar). 15.1 Task Description. 15.2 Applications. 15.3 Challenges Using Speech Input. 15.4 Evaluation Metrics. 15.5 Benchmark Data Sets. 15.6 Approaches. 15.7 New Trends. 15.8 Discussion and Conclusions. References. Index.",2011.0,"Gokhan Tur, R. Mori"
92dd10ea08afe99c023b9afe9b4b7657e79abef3,https://www.semanticscholar.org/paper/92dd10ea08afe99c023b9afe9b4b7657e79abef3,A framework for non-asymptotic quantum information theory,"This thesis consolidates, improves and extends the smooth entropy framework for non-asymptotic information theory and cryptography. 
We investigate the conditional min- and max-entropy for quantum states, generalizations of classical R\'enyi entropies. We introduce the purified distance, a novel metric for unnormalized quantum states and use it to define smooth entropies as optimizations of the min- and max-entropies over a ball of close states. We explore various properties of these entropies, including data-processing inequalities, chain rules and their classical limits. The most important property is an entropic formulation of the asymptotic equipartition property, which implies that the smooth entropies converge to the von Neumann entropy in the limit of many independent copies. The smooth entropies also satisfy duality and entropic uncertainty relations that provide limits on the power of two different observers to predict the outcome of a measurement on a quantum system. 
Finally, we discuss three example applications of the smooth entropy framework. We show a strong converse statement for source coding with quantum side information, characterize randomness extraction against quantum side information and prove information theoretic security of quantum key distribution using an intuitive argument based on the entropic uncertainty relation.",2012.0,M. Tomamichel
10b31d3058621d7c789cad876ed2dc2653e06b1b,https://www.semanticscholar.org/paper/10b31d3058621d7c789cad876ed2dc2653e06b1b,"Brain Computer Interfaces, a Review","A brain-computer interface (BCI) is a hardware and software communications system that permits cerebral activity alone to control computers or external devices. The immediate goal of BCI research is to provide communications capabilities to severely disabled people who are totally paralyzed or ‘locked in’ by neurological neuromuscular disorders, such as amyotrophic lateral sclerosis, brain stem stroke, or spinal cord injury. Here, we review the state-of-the-art of BCIs, looking at the different steps that form a standard BCI: signal acquisition, preprocessing or signal enhancement, feature extraction, classification and the control interface. We discuss their advantages, drawbacks, and latest advances, and we survey the numerous technologies reported in the scientific literature to design each step of a BCI. First, the review examines the neuroimaging modalities used in the signal acquisition step, each of which monitors a different functional brain activity such as electrical, magnetic or metabolic activity. Second, the review discusses different electrophysiological control signals that determine user intentions, which can be detected in brain activity. Third, the review includes some techniques used in the signal enhancement step to deal with the artifacts in the control signals and improve the performance. Fourth, the review studies some mathematic algorithms used in the feature extraction and classification steps which translate the information in the control signals into commands that operate a computer or other device. Finally, the review provides an overview of various BCI applications that control a range of devices.",2012.0,"Luis F. Nicolás-Alonso, J. G. Gil"
aa4b60b5847999c2f778e3e67ca1f2201e396abb,https://www.semanticscholar.org/paper/aa4b60b5847999c2f778e3e67ca1f2201e396abb,Early processing of visual information.,"An introduction is given to a theory of early visual information processing. The theory has been implemented, and examples are given of images at various stages of analysis. It is argued that the first step of consequence is to compute a primitive but rich description of the grey-level changes present in an image. The description is expressed in a vocabulary of kinds of intensity change (EDGE, SHADING-EDGE, EXTENDED-EDGE, LINE, BLOB etc.). Modifying parameters are bound to the elements in the description, specifying their POSITION, ORIENTATION, TERMINATION points, CONTRAST, SIZE and FUZZINESS. This description is obtained from the intensity array by fixed techniques, and it is called the primal sketch. For most images, the primal sketch is large and unwieldy. The second important step in visual information processing is to group its contents in a way that is appropriate for later recognition. From our ability to interpret drawings with little semantic content, one may infer the presence in our perceptual equipment of symbolic processes that can define ""place-tokens"" in an image in various ways, and can group them according to certain rules. Homomorphic techniques fail to account for many of these grouping phenomena, whose explanations require mechanisms of construction rather than mechanisms of detection. The necessary grouping of elements in the primal sketch may be achieved by a mechanism that has available the processes inferred from above, together with the ability to select items by first order discriminations acting on the elements' parameters. Only occasionally do these mechanisms use downward-flowing information about the contents of the particular image being processed. It is argued that ""non-attentive"" vision is in practice implemented by these grouping operations and first order discriminations acting on the primal sketch. The class of computations so obtained differs slightly from the class of second order operations on the intensity array. The extraction of a form from the primal sketch using these techniques amounts to the separation of figure from ground. It is concluded that most of the separation can be carried out by using techniques that do not depend upon the particular image in question. Therefore, figure-ground separation can normally precede the description of the shape of the extracted form. Up to this point, higher-level knowledge and purpose are brought to bear on only a few of the decisions taken during the processing. This relegates the widespread use of downward-flowing information to a later stage than is found in current machine-vision programs, and implies that such knowledge should influence the control of, rather than interfering with, the actual data-processing that is taking place lower down.",1976.0,D. Marr
