{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nakulreddy0107/NakulReddy_INFO5731_Spring2025/blob/main/Sarasani_Nakul_Assignment_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Monday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (25 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "\n",
        "(3) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(4) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(5)**Collect a total of 10000 reviews** of the top 100 most popular software from G2 and Capterra.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dcff13c-46c4-43f2-c07a-05ca2ad92193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processing query: machine learning\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'machine learning':   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=0&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'machine learning':   6%|▌         | 62/1000 [00:06<01:38,  9.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=100&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=100&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=100&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'machine learning':  13%|█▎        | 133/1000 [00:25<02:56,  4.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=200&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'machine learning':  21%|██        | 207/1000 [00:34<02:10,  6.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=300&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'machine learning':  34%|███▍      | 339/1000 [00:45<01:15,  8.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=500&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'machine learning':  49%|████▉     | 489/1000 [00:57<00:46, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'machine learning':  49%|████▉     | 489/1000 [01:09<00:46, 10.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=machine+learning&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'machine learning':  72%|███████▏  | 723/1000 [01:46<00:40,  6.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 723 papers to papers_machine_learning_20250219.csv\n",
            "\n",
            "Processing query: data science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'data science':   0%|          | 0/1000 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=0&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'data science':  28%|██▊       | 283/1000 [00:16<00:37, 19.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=400&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=400&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'data science':  44%|████▍     | 440/1000 [00:34<00:45, 12.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'data science':  44%|████▍     | 440/1000 [00:53<00:45, 12.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'data science':  60%|█████▉    | 595/1000 [01:28<01:19,  5.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=800&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=800&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=800&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'data science':  68%|██████▊   | 680/1000 [01:47<01:05,  4.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=900&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=900&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=data+science&limit=100&offset=900&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'data science':  76%|███████▋  | 764/1000 [02:08<00:39,  5.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 764 papers to papers_data_science_20250219.csv\n",
            "\n",
            "Processing query: artificial intelligence\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'artificial intelligence':  10%|▉         | 95/1000 [00:05<00:52, 17.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=200&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=200&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=200&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=200&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'artificial intelligence':  16%|█▌        | 159/1000 [00:29<03:13,  4.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=300&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  22%|██▎       | 225/1000 [00:37<02:22,  5.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=400&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=400&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  29%|██▉       | 291/1000 [00:52<02:20,  5.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=500&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  35%|███▌      | 354/1000 [01:00<01:54,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  35%|███▌      | 354/1000 [01:14<01:54,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  43%|████▎     | 431/1000 [01:42<02:57,  3.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  47%|████▋     | 471/1000 [01:56<02:49,  3.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=800&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=800&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'artificial intelligence':  54%|█████▍    | 542/1000 [02:09<02:05,  3.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=900&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=artificial+intelligence&limit=100&offset=900&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'artificial intelligence':  61%|██████▏   | 613/1000 [02:25<01:32,  4.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 613 papers to papers_artificial_intelligence_20250219.csv\n",
            "\n",
            "Processing query: information extraction\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'information extraction':   8%|▊         | 77/1000 [00:01<00:12, 75.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=100&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'information extraction':  24%|██▍       | 245/1000 [00:12<00:40, 18.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=300&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=300&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=300&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'information extraction':  40%|████      | 401/1000 [00:34<00:57, 10.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=500&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=500&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=500&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'information extraction':  40%|████      | 401/1000 [00:48<00:57, 10.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=500&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'information extraction':  49%|████▉     | 490/1000 [00:58<01:22,  6.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=600&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'information extraction':  58%|█████▊    | 576/1000 [01:12<01:07,  6.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n",
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=700&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'information extraction':  66%|██████▌   | 661/1000 [01:26<00:54,  6.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=800&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rFetching papers for 'information extraction':  74%|███████▍  | 741/1000 [01:34<00:37,  6.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching data: 429 Client Error:  for url: https://api.semanticscholar.org/graph/v1/paper/search?query=information+extraction&limit=100&offset=900&fields=title%2Cabstract%2Cyear%2Cauthors%2Curl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fetching papers for 'information extraction':  82%|████████▏ | 821/1000 [01:45<00:23,  7.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 821 papers to papers_information_extraction_20250219.csv\n",
            "\n",
            "Saving combined results...\n",
            "Saved 2921 papers to all_papers_20250219.csv\n",
            "\n",
            "Done! Summary:\n",
            "Total papers collected: 2921\n",
            "Individual files saved for each query\n",
            "Combined results saved to all_papers_20250219.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "def fetch_papers(query, limit=1000):\n",
        "    base_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    headers = {\n",
        "        # Replace with your API key if you have one\n",
        "        # \"x-api-key\": \"YOUR_API_KEY\"\n",
        "    }\n",
        "\n",
        "    papers = []\n",
        "    offset = 0\n",
        "    batch_size = 100\n",
        "    pbar = tqdm(total=min(limit, 1000), desc=f\"Fetching papers for '{query}'\")\n",
        "\n",
        "    while offset < limit:\n",
        "        params = {\n",
        "            \"query\": query,\n",
        "            \"limit\": min(batch_size, limit - offset),\n",
        "            \"offset\": offset,\n",
        "            \"fields\": \"title,abstract,year,authors,url\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(base_url, headers=headers, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "            if not data.get('data'):\n",
        "                break\n",
        "            valid_papers = [p for p in data['data'] if p.get('abstract')]\n",
        "            papers.extend(valid_papers)\n",
        "            pbar.update(len(valid_papers))\n",
        "            offset += batch_size\n",
        "            time.sleep(2)\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching data: {e}\")\n",
        "            time.sleep(5)\n",
        "            continue\n",
        "\n",
        "    pbar.close()\n",
        "    return papers\n",
        "\n",
        "def save_to_csv(papers, filename):\n",
        "    df = pd.DataFrame(papers)\n",
        "    df['authors'] = df['authors'].apply(lambda x: ', '.join([author['name'] for author in x]) if x else '')\n",
        "    df.to_csv(filename, index=False, encoding='utf-8')\n",
        "    print(f\"Saved {len(df)} papers to {filename}\")\n",
        "\n",
        "def main():\n",
        "    queries = [\n",
        "        \"machine learning\",\n",
        "        \"data science\",\n",
        "        \"artificial intelligence\",\n",
        "        \"information extraction\"\n",
        "    ]\n",
        "    all_papers = []\n",
        "\n",
        "    for query in queries:\n",
        "        print(f\"\\nProcessing query: {query}\")\n",
        "        papers = fetch_papers(query, limit=1000)\n",
        "        all_papers.extend(papers)\n",
        "        filename = f\"papers_{query.replace(' ', '_')}_{time.strftime('%Y%m%d')}.csv\"\n",
        "        save_to_csv(papers, filename)\n",
        "    print(\"\\nSaving combined results...\")\n",
        "    filename = f\"all_papers_{time.strftime('%Y%m%d')}.csv\"\n",
        "    save_to_csv(all_papers, filename)\n",
        "\n",
        "    print(\"\\nDone! Summary:\")\n",
        "    print(f\"Total papers collected: {len(all_papers)}\")\n",
        "    print(\"Individual files saved for each query\")\n",
        "    print(f\"Combined results saved to {filename}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "#The CSV file generated\n",
        "\"all_papers_20250219.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (15 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "84061627-ed4e-490f-f568-80fcb2db596a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading CSV file...\n",
            "Cleaning text data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2921/2921 [00:18<00:00, 160.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving cleaned data to cleaned_papers_20240218.csv\n",
            "\n",
            "Example of cleaning steps for the first abstract:\n",
            "\n",
            "Original text:\n",
            "We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the t...\n",
            "\n",
            "After removing noise:\n",
            "We present FashionMNIST a new dataset comprising of x grayscale images of fashion products from categories with images per category The training set has images and the test set has images FashionMNIST...\n",
            "\n",
            "After removing numbers:\n",
            "We present FashionMNIST a new dataset comprising of x grayscale images of fashion products from categories with images per category The training set has images and the test set has images FashionMNIST...\n",
            "\n",
            "After removing stopwords:\n",
            "present FashionMNIST new dataset comprising x grayscale images fashion products categories images per category training set images test set images FashionMNIST intended serve direct dropin replacement...\n",
            "\n",
            "After lowercasing:\n",
            "present fashionmnist new dataset comprising x grayscale images fashion products categories images per category training set images test set images fashionmnist intended serve direct dropin replacement...\n",
            "\n",
            "After stemming:\n",
            "present fashionmnist new dataset compris x grayscal imag fashion product categori imag per categori train set imag test set imag fashionmnist intend serv direct dropin replac origin mnist dataset benc...\n",
            "\n",
            "After lemmatization:\n",
            "present fashionmnist new dataset comprising x grayscale image fashion product category image per category training set image test set image fashionmnist intended serve direct dropin replacement origin...\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def remove_noise(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return ' '.join(filtered_text)\n",
        "\n",
        "def apply_stemming(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
        "    return ' '.join(stemmed_text)\n",
        "\n",
        "def apply_lemmatization(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    word_tokens = word_tokenize(text)\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word) for word in word_tokens]\n",
        "    return ' '.join(lemmatized_text)\n",
        "\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return pd.Series({\n",
        "            'cleaned_no_noise': '',\n",
        "            'cleaned_no_numbers': '',\n",
        "            'cleaned_no_stopwords': '',\n",
        "            'cleaned_lowercase': '',\n",
        "            'cleaned_stemmed': '',\n",
        "            'cleaned_lemmatized': ''\n",
        "        })\n",
        "    cleaned_no_noise = remove_noise(text)\n",
        "    cleaned_no_numbers = remove_numbers(cleaned_no_noise)\n",
        "    cleaned_no_stopwords = remove_stopwords(cleaned_no_numbers)\n",
        "    cleaned_lowercase = cleaned_no_stopwords.lower()\n",
        "    cleaned_stemmed = apply_stemming(cleaned_lowercase)\n",
        "    cleaned_lemmatized = apply_lemmatization(cleaned_lowercase)\n",
        "\n",
        "    return pd.Series({\n",
        "        'cleaned_no_noise': cleaned_no_noise,\n",
        "        'cleaned_no_numbers': cleaned_no_numbers,\n",
        "        'cleaned_no_stopwords': cleaned_no_stopwords,\n",
        "        'cleaned_lowercase': cleaned_lowercase,\n",
        "        'cleaned_stemmed': cleaned_stemmed,\n",
        "        'cleaned_lemmatized': cleaned_lemmatized\n",
        "    })\n",
        "\n",
        "def main():\n",
        "    print(\"Reading CSV file...\")\n",
        "    df = pd.read_csv('/content/all_papers_20250219.csv')\n",
        "    print(\"Cleaning text data...\")\n",
        "    tqdm.pandas()\n",
        "    cleaned_data = df['abstract'].progress_apply(clean_text)\n",
        "    df = pd.concat([df, cleaned_data], axis=1)\n",
        "    output_file = 'cleaned_papers_20240218.csv'\n",
        "    print(f\"\\nSaving cleaned data to {output_file}\")\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(\"\\nExample of cleaning steps for the first abstract:\")\n",
        "    example = df.iloc[0]\n",
        "    print(\"\\nOriginal text:\")\n",
        "    print(example['abstract'][:200] + \"...\")\n",
        "    print(\"\\nAfter removing noise:\")\n",
        "    print(example['cleaned_no_noise'][:200] + \"...\")\n",
        "    print(\"\\nAfter removing numbers:\")\n",
        "    print(example['cleaned_no_numbers'][:200] + \"...\")\n",
        "    print(\"\\nAfter removing stopwords:\")\n",
        "    print(example['cleaned_no_stopwords'][:200] + \"...\")\n",
        "    print(\"\\nAfter lowercasing:\")\n",
        "    print(example['cleaned_lowercase'][:200] + \"...\")\n",
        "    print(\"\\nAfter stemming:\")\n",
        "    print(example['cleaned_stemmed'][:200] + \"...\")\n",
        "    print(\"\\nAfter lemmatization:\")\n",
        "    print(example['cleaned_lemmatized'][:200] + \"...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "\n",
        "#The CSV file generated\n",
        "\"cleaned_papers_20240218.csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (15 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adef0d02-c721-450c-f70d-c1c2e2498970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading cleaned data...\n",
            "\n",
            "=== DETAILED ANALYSIS OF SAMPLE TEXT ===\n",
            "\n",
            "Sample text:\n",
            "present FashionMNIST new dataset comprising x grayscale images fashion products categories images per category training set images test set images FashionMNIST intended serve direct dropin replacement...\n",
            "\n",
            "1. PARTS OF SPEECH ANALYSIS\n",
            "\n",
            "POS Counts:\n",
            "Nouns: 26\n",
            "Verbs: 9\n",
            "Adjectives: 7\n",
            "Adverbs: 1\n",
            "\n",
            "POS Examples:\n",
            "ADJ: present, new, direct\n",
            "NUM: FashionMNIST, FashionMNIST\n",
            "NOUN: dataset, grayscale, fashion\n",
            "VERB: comprising, images, set\n",
            "PUNCT: x\n",
            "ADP: per\n",
            "PROPN: MNIST, URL\n",
            "ADV: freely\n",
            "\n",
            "2. DEPENDENCY PARSING\n",
            "\n",
            "Dependency Tree:\n",
            "  dataset --acl--> comprising\n",
            "  dataset --punct--> x\n",
            "  dataset --appos--> grayscale\n",
            "  images --dobj--> images\n",
            "  images --prep--> per\n",
            "  per --pobj--> images\n",
            "  images --dobj--> images\n",
            "  images --acl--> FashionMNIST\n",
            "  intended --xcomp--> serve\n",
            "  serve --dobj--> learning\n",
            "  images --conj--> splits\n",
            "ROOT --> dataset\n",
            "  dataset --dobj--> URL\n",
            "\n",
            "3. NAMED ENTITY RECOGNITION\n",
            "\n",
            "Entity Counts:\n",
            "ORG: 1\n",
            "Examples: FashionMNIST\n",
            "\n",
            "=== PROCESSING FULL DATASET ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2921/2921 [01:13<00:00, 39.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Overall Statistics:\n",
            "\n",
            "Total POS Counts:\n",
            "Nouns: 168044\n",
            "Verbs: 56083\n",
            "Adjectives: 46331\n",
            "Adverbs: 10692\n",
            "\n",
            "Total Entity Counts:\n",
            "ORG: 7287\n",
            "PERSON: 1542\n",
            "CARDINAL: 1520\n",
            "ORDINAL: 584\n",
            "DATE: 576\n",
            "NORP: 514\n",
            "GPE: 416\n",
            "PRODUCT: 307\n",
            "LOC: 129\n",
            "WORK_OF_ART: 101\n",
            "LAW: 66\n",
            "FAC: 40\n",
            "TIME: 29\n",
            "LANGUAGE: 28\n",
            "EVENT: 19\n",
            "MONEY: 6\n",
            "PERCENT: 3\n",
            "QUANTITY: 3\n",
            "\n",
            "Full analysis results saved to 'text_analysis_results.json'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "\n",
        "class TextAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "    def analyze_pos(self, doc):\n",
        "        \"\"\"Analyze POS tags in the document\"\"\"\n",
        "        pos_counts = {\n",
        "            'Nouns': len([token for token in doc if token.pos_ == 'NOUN' or token.pos_ == 'PROPN']),\n",
        "            'Verbs': len([token for token in doc if token.pos_ == 'VERB']),\n",
        "            'Adjectives': len([token for token in doc if token.pos_ == 'ADJ']),\n",
        "            'Adverbs': len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        }\n",
        "        pos_examples = {}\n",
        "        for token in doc:\n",
        "            if token.pos_ not in pos_examples:\n",
        "                pos_examples[token.pos_] = []\n",
        "            if len(pos_examples[token.pos_]) < 3:\n",
        "                pos_examples[token.pos_].append(token.text)\n",
        "\n",
        "        return pos_counts, pos_examples\n",
        "\n",
        "    def create_dependency_tree_string(self, doc):\n",
        "        \"\"\"Create a string representation of the dependency tree\"\"\"\n",
        "        tree_strings = []\n",
        "        for sent in doc.sents:\n",
        "            words = [(token.text, token.dep_) for token in sent]\n",
        "            for i, (word, dep) in enumerate(words):\n",
        "                token = sent[i]\n",
        "                head_idx = token.head.i - sent.start\n",
        "\n",
        "                if token.dep_ == \"ROOT\":\n",
        "                    tree_str = f\"ROOT --> {word}\"\n",
        "                else:\n",
        "                    if head_idx < i:\n",
        "                        head_word = words[head_idx][0]\n",
        "                        tree_str = f\"  {head_word} --{dep}--> {word}\"\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                tree_strings.append(tree_str)\n",
        "\n",
        "        return \"\\n\".join(tree_strings)\n",
        "\n",
        "    def analyze_parsing(self, doc):\n",
        "\n",
        "        dep_triples = []\n",
        "        for token in doc:\n",
        "            dep_triples.append((token.text, token.dep_, token.head.text))\n",
        "        dep_tree = self.create_dependency_tree_string(doc)\n",
        "\n",
        "        return dep_triples, dep_tree\n",
        "\n",
        "    def analyze_entities(self, doc):\n",
        "        entity_counts = Counter()\n",
        "        entities_list = {}\n",
        "\n",
        "        for ent in doc.ents:\n",
        "            entity_counts[ent.label_] += 1\n",
        "            if ent.label_ not in entities_list:\n",
        "                entities_list[ent.label_] = []\n",
        "            if len(entities_list[ent.label_]) < 5:\n",
        "                entities_list[ent.label_].append(ent.text)\n",
        "\n",
        "        return dict(entity_counts), entities_list\n",
        "\n",
        "def analyze_text(text, analyzer):\n",
        "    if pd.isna(text) or not str(text).strip():\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        text = str(text).strip()\n",
        "        doc = analyzer.nlp(text)\n",
        "        pos_counts, pos_examples = analyzer.analyze_pos(doc)\n",
        "        dep_triples, dep_tree = analyzer.analyze_parsing(doc)\n",
        "        entity_counts, entities_list = analyzer.analyze_entities(doc)\n",
        "\n",
        "        return {\n",
        "            'pos_counts': pos_counts,\n",
        "            'pos_examples': pos_examples,\n",
        "            'dep_triples': dep_triples,\n",
        "            'dep_tree': dep_tree,\n",
        "            'entity_counts': entity_counts,\n",
        "            'entities_list': entities_list\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error analyzing text: {e}\")\n",
        "        return None\n",
        "\n",
        "def main():\n",
        "    analyzer = TextAnalyzer()\n",
        "\n",
        "    print(\"Reading cleaned data...\")\n",
        "    df = pd.read_csv('cleaned_papers_20240218.csv')\n",
        "    sample_text = None\n",
        "    for text in df['cleaned_no_stopwords']:\n",
        "        if pd.notna(text) and str(text).strip():\n",
        "            sample_text = text\n",
        "            break\n",
        "\n",
        "    if sample_text is None:\n",
        "        print(\"No valid sample text found in the dataset!\")\n",
        "        return\n",
        "\n",
        "    print(\"\\n=== DETAILED ANALYSIS OF SAMPLE TEXT ===\")\n",
        "    print(\"\\nSample text:\")\n",
        "    print(sample_text[:200] + \"...\")\n",
        "\n",
        "    sample_analysis = analyze_text(sample_text, analyzer)\n",
        "\n",
        "    if sample_analysis:\n",
        "        print(\"\\n1. PARTS OF SPEECH ANALYSIS\")\n",
        "        print(\"\\nPOS Counts:\")\n",
        "        for pos, count in sample_analysis['pos_counts'].items():\n",
        "            print(f\"{pos}: {count}\")\n",
        "\n",
        "        print(\"\\nPOS Examples:\")\n",
        "        for pos, examples in sample_analysis['pos_examples'].items():\n",
        "            print(f\"{pos}: {', '.join(examples)}\")\n",
        "\n",
        "        print(\"\\n2. DEPENDENCY PARSING\")\n",
        "        print(\"\\nDependency Tree:\")\n",
        "        print(sample_analysis['dep_tree'])\n",
        "\n",
        "        print(\"\\n3. NAMED ENTITY RECOGNITION\")\n",
        "        print(\"\\nEntity Counts:\")\n",
        "        for ent_type, count in sample_analysis['entity_counts'].items():\n",
        "            print(f\"{ent_type}: {count}\")\n",
        "            print(f\"Examples: {', '.join(sample_analysis['entities_list'][ent_type])}\")\n",
        "    print(\"\\n=== PROCESSING FULL DATASET ===\")\n",
        "\n",
        "    total_pos_counts = Counter()\n",
        "    total_entity_counts = Counter()\n",
        "    dataset_analyses = []\n",
        "    for text in tqdm(df['cleaned_no_stopwords']):\n",
        "        analysis = analyze_text(text, analyzer)\n",
        "        if analysis:\n",
        "            dataset_analyses.append(analysis)\n",
        "            total_pos_counts.update(analysis['pos_counts'])\n",
        "            total_entity_counts.update(analysis['entity_counts'])\n",
        "    print(\"\\nOverall Statistics:\")\n",
        "    print(\"\\nTotal POS Counts:\")\n",
        "    for pos, count in total_pos_counts.items():\n",
        "        print(f\"{pos}: {count}\")\n",
        "\n",
        "    print(\"\\nTotal Entity Counts:\")\n",
        "    for entity_type, count in sorted(total_entity_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "        print(f\"{entity_type}: {count}\")\n",
        "    with open('text_analysis_results.json', 'w') as f:\n",
        "        json.dump(dataset_analyses, f, indent=2)\n",
        "\n",
        "    print(\"\\nFull analysis results saved to 'text_analysis_results.json'\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "#The Json file generated\n",
        "\"text_analysis_results.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Following Questions must answer using AI assitance**"
      ],
      "metadata": {
        "id": "EcVqy1yj3wja"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 4 (20 points)."
      ],
      "metadata": {
        "id": "kEdcyHX8VaDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. (PART-1)\n",
        "Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        " The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        " The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "(PART -2)\n",
        "\n",
        "1.   **Preprocess Data**: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "2. Perform **Data Quality** operations.\n",
        "\n",
        "\n",
        "Preprocessing:\n",
        "Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "Data Quality:\n",
        "Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n"
      ],
      "metadata": {
        "id": "1Ung5_YW3C6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Github MarketPlace page:\n",
        "https://github.com/marketplace?type=actions"
      ],
      "metadata": {
        "id": "CTOfUpatronW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "BASE_URL = \"https://github.com/marketplace?type=actions&page=\"\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',\n",
        "    'Accept': 'text/html,application/json',\n",
        "    'Accept-Language': 'en-US,en;q=0.9',\n",
        "}\n",
        "OUTPUT_FILE = \"github_marketplace_actions.csv\"\n",
        "\n",
        "def scrape_github_marketplace(max_pages=10, delay=2):\n",
        "    data = []\n",
        "    for page in range(1, max_pages + 1):\n",
        "        url = BASE_URL + str(page)\n",
        "        print(url)\n",
        "        response = requests.get(url, headers=headers)\n",
        "\n",
        "        if response.status_code != 200:\n",
        "            print(f\"Failed to retrieve page {page}: {response.status_code}\")\n",
        "            continue\n",
        "\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        action_cards = soup.find_all(\"div\", class_=\"flex-1\")\n",
        "\n",
        "        if not action_cards:\n",
        "            print(\"No more actions found, stopping scrape.\")\n",
        "            break\n",
        "\n",
        "        for card in action_cards:\n",
        "            name_tag = card.find(\"h3\")\n",
        "            desc_tag = card.find(\"p\")\n",
        "            link_tag = card.find(\"a\")\n",
        "\n",
        "            name = name_tag.text.strip() if name_tag else \"No Name\"\n",
        "            description = desc_tag.text.strip() if desc_tag else \"No Description\"\n",
        "            url = \"https://github.com\" + link_tag[\"href\"] if link_tag else \"No URL\"\n",
        "\n",
        "            data.append([name, description, url, page])\n",
        "\n",
        "        print(f\"Scraped page {page}\")\n",
        "        time.sleep(delay)\n",
        "\n",
        "    save_to_csv(data)\n",
        "\n",
        "def save_to_csv(data):\n",
        "    with open(OUTPUT_FILE, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"Product Name\", \"Description\", \"URL\", \"Page Number\"])\n",
        "        writer.writerows(data)\n",
        "    print(f\"Data saved to {OUTPUT_FILE}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    scrape_github_marketplace(max_pages=10, delay=2)\n",
        "\n",
        "#The CSV file generated\n",
        "\"github_marketplace_actions.csv\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOb6TyZLkJ2T",
        "outputId": "20b065be-b79c-4e01-a6ad-3ed96d8bdc10"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://github.com/marketplace?type=actions&page=1\n",
            "Scraped page 1\n",
            "https://github.com/marketplace?type=actions&page=2\n",
            "Scraped page 2\n",
            "https://github.com/marketplace?type=actions&page=3\n",
            "Scraped page 3\n",
            "https://github.com/marketplace?type=actions&page=4\n",
            "Scraped page 4\n",
            "https://github.com/marketplace?type=actions&page=5\n",
            "Scraped page 5\n",
            "https://github.com/marketplace?type=actions&page=6\n",
            "Scraped page 6\n",
            "https://github.com/marketplace?type=actions&page=7\n",
            "Scraped page 7\n",
            "https://github.com/marketplace?type=actions&page=8\n",
            "Scraped page 8\n",
            "https://github.com/marketplace?type=actions&page=9\n",
            "Scraped page 9\n",
            "https://github.com/marketplace?type=actions&page=10\n",
            "Scraped page 10\n",
            "Data saved to github_marketplace_actions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "0E9iqBPK87z4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "file_path = \"github_marketplace_actions.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "custom_stopwords = set([\n",
        "    \"a\", \"an\", \"and\", \"are\", \"as\", \"at\", \"be\", \"but\", \"by\", \"for\", \"if\", \"in\", \"into\", \"is\", \"it\", \"no\", \"not\",\n",
        "    \"of\", \"on\", \"or\", \"such\", \"that\", \"the\", \"their\", \"then\", \"there\", \"these\", \"they\", \"this\", \"to\", \"was\",\n",
        "    \"will\", \"with\"\n",
        "])\n",
        "lemmatization_dict = {\n",
        "    \"scans\": \"scan\",\n",
        "    \"generators\": \"generator\",\n",
        "    \"plugins\": \"plugin\",\n",
        "    \"actions\": \"action\",\n",
        "    \"tools\": \"tool\"\n",
        "}\n",
        "\n",
        "def simple_clean_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [lemmatization_dict.get(word, word) for word in tokens if word not in custom_stopwords]\n",
        "    return ' '.join(tokens)\n",
        "df['Product Name'] = df['Product Name'].apply(simple_clean_text)\n",
        "df['Description'] = df['Description'].apply(simple_clean_text)\n",
        "df = df[(df['Product Name'] != 'name') & (df['Description'] != 'description')]\n",
        "df = df[df['URL'].str.startswith(\"https://github.com/marketplace/actions/\")]\n",
        "df.to_csv(\"cleaned_github_marketplace_actions.csv\", index=False)\n",
        "print(\"Data cleaning complete. Saved to 'cleaned_github_marketplace_actions.csv'\")\n",
        "\n",
        "#The CSV file generated\n",
        "\"cleaned_github_marketplace_actions.csv\""
      ],
      "metadata": {
        "id": "4dtco9K--ks6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46f6960-2146-4244-c7d2-671dad333885"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning complete. Saved to 'cleaned_github_marketplace_actions.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Question 5 (20 points)\n",
        "\n",
        "PART 1:\n",
        "Web Scrape  tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.)\n",
        "The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "Part 2:\n",
        "Perform data cleaning procedures\n",
        "\n",
        "A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "**Note**\n",
        "\n",
        "1.   Follow tutorials provided in canvas to obtain api keys. Use ChatGPT to get the code. Make sure the file is downloaded and saved.\n",
        "2.   Make sure you divide GPT code as shown in tutorials, dont make multiple requestes.\n"
      ],
      "metadata": {
        "id": "3WeD70ty3Gui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAALx4zQEAAAAA3XWhS9AlPYvu05rUtU%2B5R23OsqA%3DUFKNB6nfwGvDr1AmbVXTNjzkEeO466mxdElHntuY5zYrPSXqp6\"\n",
        "client = tweepy.Client(bearer_token)\n",
        "query = \"#MachineLearning OR #AI OR #ArtificialIntelligence -is:retweet lang:en\"\n",
        "tweets = client.search_recent_tweets(query=query, max_results=99)\n",
        "tweets_data = []\n",
        "for tweet in tweets.data:\n",
        "    tweet_data = {\n",
        "        'tweet_id': tweet.id,\n",
        "        'username': tweet.author_id,\n",
        "        'tweet_text': tweet.text\n",
        "    }\n",
        "    tweets_data.append(tweet_data)\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(tweets_data)\n",
        "df.to_csv('cleaned_tweets.csv', index=False)\n",
        "print(\"Data saved to cleaned_tweets.csv\")\n",
        "\n",
        "\n",
        "\n",
        "#The CSV file generated\n",
        "\"cleaned_tweets.csv\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDBQfLRargki",
        "outputId": "d9b8d14e-758c-4bde-9229-d27c8b9f5ec1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data saved to cleaned_tweets.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AI prompt for question 4 & 5 used ChatGPT\n",
        "\n",
        "#4\n",
        "# # (PART-1) Web scraping data from the GitHub Marketplace to gather details about popular actions. Using Python, the process begins by sending HTTP requests to multiple pages of the marketplace (1000 products), handling pagination through dynamic page numbers. The key details extracted include the product name, a short description, and the URL.\n",
        "\n",
        "# The extracted data is stored in a structured CSV format with columns for product name, description, URL, and page number. A time delay is introduced between requests to avoid server overload. ChatGPT can assist by helping with the parsing of HTML, error handling, and generating reports based on the data collected.\n",
        "\n",
        "# The goal is to complete the scraping within a specified time limit, ensuring that the process is efficient and adheres to GitHub’s usage guidelines.\n",
        "\n",
        "# (PART -2)\n",
        "\n",
        "# Preprocess Data: Clean the text by tokenizing, removing stopwords, and converting to lowercase.\n",
        "\n",
        "# Perform Data Quality operations.\n",
        "\n",
        "# Preprocessing: Preprocessing involves cleaning the text by removing noise such as special characters, HTML tags, and unnecessary whitespace. It also includes tasks like tokenization, stopword removal, and lemmatization to standardize the text for analysis.\n",
        "\n",
        "# Data Quality: Data quality checks ensure completeness, consistency, and accuracy by verifying that all required columns are filled and formatted correctly. Additionally, it involves identifying and removing duplicates, handling missing values, and ensuring the data reflects the true content accurately.\n",
        "\n",
        "#5\n",
        "# PART 1: Web Scrape tweets from Twitter using the Tweepy API, specifically targeting hashtags related to subtopics (machine learning or artificial intelligence.) The extracted data includes the tweet ID, username, and text.\n",
        "\n",
        "# Part 2: Perform data cleaning procedures\n",
        "\n",
        "# A final data quality check ensures the completeness and consistency of the dataset. The cleaned data is then saved into a CSV file for further analysis.\n",
        "\n",
        "\n",
        "#here is the link for the colab:\n",
        "\"https://colab.research.google.com/drive/1EVc0NllqygYk0ay4MLe_SiGeUf3ujkst?usp=sharing\"\n",
        "\n",
        "#I've uploaded all the csv files generated in the github for all thw questions.\n",
        "\n"
      ],
      "metadata": {
        "id": "Mm_0-yYaBp_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Write your response below\n",
        "Fill out survey and provide your valuable feedback.\n",
        "\n",
        "https://docs.google.com/forms/d/e/1FAIpQLSd_ObuA3iNoL7Az_C-2NOfHodfKCfDzHZtGRfIker6WyZqTtA/viewform?usp=dialog"
      ],
      "metadata": {
        "id": "JbTa-jDS-KFI"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}